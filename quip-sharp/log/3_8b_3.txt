[Stage: Convert to HF format] K=3
W0413 08:45:48.866823 845518 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0413 08:45:48.894532 845518 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0413 08:45:49.198172 845518 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0413 08:45:49.198296 845518 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0413 08:45:49.198340 845518 utils.py:162] NumExpr defaulting to 16 threads.
I0413 08:45:49.320198 845518 config.py:58] PyTorch version 2.4.0 available.
W0413 08:45:49.694968 845518 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0413 08:46:10.596995 845518 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

W0413 08:46:10.598340 845518 warnings.py:110] /opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:00<00:00,  8.48it/s]Loading checkpoint shards:  50%|█████     | 2/4 [00:00<00:00,  8.71it/s]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:00<00:00,  8.96it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  9.60it/s]
Some weights of the model checkpoint at meta-llama/Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.Qidxs', 'model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.Wscale', 'model.layers.0.mlp.down_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.Qidxs', 'model.layers.0.mlp.upgate_proj.SU', 'model.layers.0.mlp.upgate_proj.SV', 'model.layers.0.mlp.upgate_proj.Wscale', 'model.layers.0.mlp.upgate_proj.codebook_id', 'model.layers.0.mlp.upgate_proj.fuse_scales', 'model.layers.0.self_attn.o_proj.Qidxs', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.Wscale', 'model.layers.0.self_attn.o_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.Qidxs', 'model.layers.0.self_attn.qkv_proj.SU', 'model.layers.0.self_attn.qkv_proj.SV', 'model.layers.0.self_attn.qkv_proj.Wscale', 'model.layers.0.self_attn.qkv_proj.codebook_id', 'model.layers.0.self_attn.qkv_proj.fuse_scales', 'model.layers.1.mlp.down_proj.Qidxs', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.Wscale', 'model.layers.1.mlp.down_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.Qidxs', 'model.layers.1.mlp.upgate_proj.SU', 'model.layers.1.mlp.upgate_proj.SV', 'model.layers.1.mlp.upgate_proj.Wscale', 'model.layers.1.mlp.upgate_proj.codebook_id', 'model.layers.1.mlp.upgate_proj.fuse_scales', 'model.layers.1.self_attn.o_proj.Qidxs', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.Wscale', 'model.layers.1.self_attn.o_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.Qidxs', 'model.layers.1.self_attn.qkv_proj.SU', 'model.layers.1.self_attn.qkv_proj.SV', 'model.layers.1.self_attn.qkv_proj.Wscale', 'model.layers.1.self_attn.qkv_proj.codebook_id', 'model.layers.1.self_attn.qkv_proj.fuse_scales', 'model.layers.10.mlp.down_proj.Qidxs', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.Wscale', 'model.layers.10.mlp.down_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.Qidxs', 'model.layers.10.mlp.upgate_proj.SU', 'model.layers.10.mlp.upgate_proj.SV', 'model.layers.10.mlp.upgate_proj.Wscale', 'model.layers.10.mlp.upgate_proj.codebook_id', 'model.layers.10.mlp.upgate_proj.fuse_scales', 'model.layers.10.self_attn.o_proj.Qidxs', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.Wscale', 'model.layers.10.self_attn.o_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.Qidxs', 'model.layers.10.self_attn.qkv_proj.SU', 'model.layers.10.self_attn.qkv_proj.SV', 'model.layers.10.self_attn.qkv_proj.Wscale', 'model.layers.10.self_attn.qkv_proj.codebook_id', 'model.layers.10.self_attn.qkv_proj.fuse_scales', 'model.layers.11.mlp.down_proj.Qidxs', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.Wscale', 'model.layers.11.mlp.down_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.Qidxs', 'model.layers.11.mlp.upgate_proj.SU', 'model.layers.11.mlp.upgate_proj.SV', 'model.layers.11.mlp.upgate_proj.Wscale', 'model.layers.11.mlp.upgate_proj.codebook_id', 'model.layers.11.mlp.upgate_proj.fuse_scales', 'model.layers.11.self_attn.o_proj.Qidxs', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.Wscale', 'model.layers.11.self_attn.o_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.Qidxs', 'model.layers.11.self_attn.qkv_proj.SU', 'model.layers.11.self_attn.qkv_proj.SV', 'model.layers.11.self_attn.qkv_proj.Wscale', 'model.layers.11.self_attn.qkv_proj.codebook_id', 'model.layers.11.self_attn.qkv_proj.fuse_scales', 'model.layers.12.mlp.down_proj.Qidxs', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.Wscale', 'model.layers.12.mlp.down_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.Qidxs', 'model.layers.12.mlp.upgate_proj.SU', 'model.layers.12.mlp.upgate_proj.SV', 'model.layers.12.mlp.upgate_proj.Wscale', 'model.layers.12.mlp.upgate_proj.codebook_id', 'model.layers.12.mlp.upgate_proj.fuse_scales', 'model.layers.12.self_attn.o_proj.Qidxs', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.Wscale', 'model.layers.12.self_attn.o_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.Qidxs', 'model.layers.12.self_attn.qkv_proj.SU', 'model.layers.12.self_attn.qkv_proj.SV', 'model.layers.12.self_attn.qkv_proj.Wscale', 'model.layers.12.self_attn.qkv_proj.codebook_id', 'model.layers.12.self_attn.qkv_proj.fuse_scales', 'model.layers.13.mlp.down_proj.Qidxs', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.Wscale', 'model.layers.13.mlp.down_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.Qidxs', 'model.layers.13.mlp.upgate_proj.SU', 'model.layers.13.mlp.upgate_proj.SV', 'model.layers.13.mlp.upgate_proj.Wscale', 'model.layers.13.mlp.upgate_proj.codebook_id', 'model.layers.13.mlp.upgate_proj.fuse_scales', 'model.layers.13.self_attn.o_proj.Qidxs', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.Wscale', 'model.layers.13.self_attn.o_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.Qidxs', 'model.layers.13.self_attn.qkv_proj.SU', 'model.layers.13.self_attn.qkv_proj.SV', 'model.layers.13.self_attn.qkv_proj.Wscale', 'model.layers.13.self_attn.qkv_proj.codebook_id', 'model.layers.13.self_attn.qkv_proj.fuse_scales', 'model.layers.14.mlp.down_proj.Qidxs', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.Wscale', 'model.layers.14.mlp.down_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.Qidxs', 'model.layers.14.mlp.upgate_proj.SU', 'model.layers.14.mlp.upgate_proj.SV', 'model.layers.14.mlp.upgate_proj.Wscale', 'model.layers.14.mlp.upgate_proj.codebook_id', 'model.layers.14.mlp.upgate_proj.fuse_scales', 'model.layers.14.self_attn.o_proj.Qidxs', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.Wscale', 'model.layers.14.self_attn.o_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.Qidxs', 'model.layers.14.self_attn.qkv_proj.SU', 'model.layers.14.self_attn.qkv_proj.SV', 'model.layers.14.self_attn.qkv_proj.Wscale', 'model.layers.14.self_attn.qkv_proj.codebook_id', 'model.layers.14.self_attn.qkv_proj.fuse_scales', 'model.layers.15.mlp.down_proj.Qidxs', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.Wscale', 'model.layers.15.mlp.down_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.Qidxs', 'model.layers.15.mlp.upgate_proj.SU', 'model.layers.15.mlp.upgate_proj.SV', 'model.layers.15.mlp.upgate_proj.Wscale', 'model.layers.15.mlp.upgate_proj.codebook_id', 'model.layers.15.mlp.upgate_proj.fuse_scales', 'model.layers.15.self_attn.o_proj.Qidxs', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.Wscale', 'model.layers.15.self_attn.o_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.Qidxs', 'model.layers.15.self_attn.qkv_proj.SU', 'model.layers.15.self_attn.qkv_proj.SV', 'model.layers.15.self_attn.qkv_proj.Wscale', 'model.layers.15.self_attn.qkv_proj.codebook_id', 'model.layers.15.self_attn.qkv_proj.fuse_scales', 'model.layers.16.mlp.down_proj.Qidxs', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.Wscale', 'model.layers.16.mlp.down_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.Qidxs', 'model.layers.16.mlp.upgate_proj.SU', 'model.layers.16.mlp.upgate_proj.SV', 'model.layers.16.mlp.upgate_proj.Wscale', 'model.layers.16.mlp.upgate_proj.codebook_id', 'model.layers.16.mlp.upgate_proj.fuse_scales', 'model.layers.16.self_attn.o_proj.Qidxs', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.Wscale', 'model.layers.16.self_attn.o_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.Qidxs', 'model.layers.16.self_attn.qkv_proj.SU', 'model.layers.16.self_attn.qkv_proj.SV', 'model.layers.16.self_attn.qkv_proj.Wscale', 'model.layers.16.self_attn.qkv_proj.codebook_id', 'model.layers.16.self_attn.qkv_proj.fuse_scales', 'model.layers.17.mlp.down_proj.Qidxs', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.Wscale', 'model.layers.17.mlp.down_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.Qidxs', 'model.layers.17.mlp.upgate_proj.SU', 'model.layers.17.mlp.upgate_proj.SV', 'model.layers.17.mlp.upgate_proj.Wscale', 'model.layers.17.mlp.upgate_proj.codebook_id', 'model.layers.17.mlp.upgate_proj.fuse_scales', 'model.layers.17.self_attn.o_proj.Qidxs', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.Wscale', 'model.layers.17.self_attn.o_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.Qidxs', 'model.layers.17.self_attn.qkv_proj.SU', 'model.layers.17.self_attn.qkv_proj.SV', 'model.layers.17.self_attn.qkv_proj.Wscale', 'model.layers.17.self_attn.qkv_proj.codebook_id', 'model.layers.17.self_attn.qkv_proj.fuse_scales', 'model.layers.18.mlp.down_proj.Qidxs', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.Wscale', 'model.layers.18.mlp.down_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.Qidxs', 'model.layers.18.mlp.upgate_proj.SU', 'model.layers.18.mlp.upgate_proj.SV', 'model.layers.18.mlp.upgate_proj.Wscale', 'model.layers.18.mlp.upgate_proj.codebook_id', 'model.layers.18.mlp.upgate_proj.fuse_scales', 'model.layers.18.self_attn.o_proj.Qidxs', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.Wscale', 'model.layers.18.self_attn.o_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.Qidxs', 'model.layers.18.self_attn.qkv_proj.SU', 'model.layers.18.self_attn.qkv_proj.SV', 'model.layers.18.self_attn.qkv_proj.Wscale', 'model.layers.18.self_attn.qkv_proj.codebook_id', 'model.layers.18.self_attn.qkv_proj.fuse_scales', 'model.layers.19.mlp.down_proj.Qidxs', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.Wscale', 'model.layers.19.mlp.down_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.Qidxs', 'model.layers.19.mlp.upgate_proj.SU', 'model.layers.19.mlp.upgate_proj.SV', 'model.layers.19.mlp.upgate_proj.Wscale', 'model.layers.19.mlp.upgate_proj.codebook_id', 'model.layers.19.mlp.upgate_proj.fuse_scales', 'model.layers.19.self_attn.o_proj.Qidxs', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.Wscale', 'model.layers.19.self_attn.o_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.Qidxs', 'model.layers.19.self_attn.qkv_proj.SU', 'model.layers.19.self_attn.qkv_proj.SV', 'model.layers.19.self_attn.qkv_proj.Wscale', 'model.layers.19.self_attn.qkv_proj.codebook_id', 'model.layers.19.self_attn.qkv_proj.fuse_scales', 'model.layers.2.mlp.down_proj.Qidxs', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.Wscale', 'model.layers.2.mlp.down_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.Qidxs', 'model.layers.2.mlp.upgate_proj.SU', 'model.layers.2.mlp.upgate_proj.SV', 'model.layers.2.mlp.upgate_proj.Wscale', 'model.layers.2.mlp.upgate_proj.codebook_id', 'model.layers.2.mlp.upgate_proj.fuse_scales', 'model.layers.2.self_attn.o_proj.Qidxs', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.Wscale', 'model.layers.2.self_attn.o_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.Qidxs', 'model.layers.2.self_attn.qkv_proj.SU', 'model.layers.2.self_attn.qkv_proj.SV', 'model.layers.2.self_attn.qkv_proj.Wscale', 'model.layers.2.self_attn.qkv_proj.codebook_id', 'model.layers.2.self_attn.qkv_proj.fuse_scales', 'model.layers.20.mlp.down_proj.Qidxs', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.Wscale', 'model.layers.20.mlp.down_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.Qidxs', 'model.layers.20.mlp.upgate_proj.SU', 'model.layers.20.mlp.upgate_proj.SV', 'model.layers.20.mlp.upgate_proj.Wscale', 'model.layers.20.mlp.upgate_proj.codebook_id', 'model.layers.20.mlp.upgate_proj.fuse_scales', 'model.layers.20.self_attn.o_proj.Qidxs', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.Wscale', 'model.layers.20.self_attn.o_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.Qidxs', 'model.layers.20.self_attn.qkv_proj.SU', 'model.layers.20.self_attn.qkv_proj.SV', 'model.layers.20.self_attn.qkv_proj.Wscale', 'model.layers.20.self_attn.qkv_proj.codebook_id', 'model.layers.20.self_attn.qkv_proj.fuse_scales', 'model.layers.21.mlp.down_proj.Qidxs', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.Wscale', 'model.layers.21.mlp.down_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.Qidxs', 'model.layers.21.mlp.upgate_proj.SU', 'model.layers.21.mlp.upgate_proj.SV', 'model.layers.21.mlp.upgate_proj.Wscale', 'model.layers.21.mlp.upgate_proj.codebook_id', 'model.layers.21.mlp.upgate_proj.fuse_scales', 'model.layers.21.self_attn.o_proj.Qidxs', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.Wscale', 'model.layers.21.self_attn.o_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.Qidxs', 'model.layers.21.self_attn.qkv_proj.SU', 'model.layers.21.self_attn.qkv_proj.SV', 'model.layers.21.self_attn.qkv_proj.Wscale', 'model.layers.21.self_attn.qkv_proj.codebook_id', 'model.layers.21.self_attn.qkv_proj.fuse_scales', 'model.layers.22.mlp.down_proj.Qidxs', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.Wscale', 'model.layers.22.mlp.down_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.Qidxs', 'model.layers.22.mlp.upgate_proj.SU', 'model.layers.22.mlp.upgate_proj.SV', 'model.layers.22.mlp.upgate_proj.Wscale', 'model.layers.22.mlp.upgate_proj.codebook_id', 'model.layers.22.mlp.upgate_proj.fuse_scales', 'model.layers.22.self_attn.o_proj.Qidxs', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.Wscale', 'model.layers.22.self_attn.o_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.Qidxs', 'model.layers.22.self_attn.qkv_proj.SU', 'model.layers.22.self_attn.qkv_proj.SV', 'model.layers.22.self_attn.qkv_proj.Wscale', 'model.layers.22.self_attn.qkv_proj.codebook_id', 'model.layers.22.self_attn.qkv_proj.fuse_scales', 'model.layers.23.mlp.down_proj.Qidxs', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.Wscale', 'model.layers.23.mlp.down_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.Qidxs', 'model.layers.23.mlp.upgate_proj.SU', 'model.layers.23.mlp.upgate_proj.SV', 'model.layers.23.mlp.upgate_proj.Wscale', 'model.layers.23.mlp.upgate_proj.codebook_id', 'model.layers.23.mlp.upgate_proj.fuse_scales', 'model.layers.23.self_attn.o_proj.Qidxs', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.Wscale', 'model.layers.23.self_attn.o_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.Qidxs', 'model.layers.23.self_attn.qkv_proj.SU', 'model.layers.23.self_attn.qkv_proj.SV', 'model.layers.23.self_attn.qkv_proj.Wscale', 'model.layers.23.self_attn.qkv_proj.codebook_id', 'model.layers.23.self_attn.qkv_proj.fuse_scales', 'model.layers.24.mlp.down_proj.Qidxs', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.Wscale', 'model.layers.24.mlp.down_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.Qidxs', 'model.layers.24.mlp.upgate_proj.SU', 'model.layers.24.mlp.upgate_proj.SV', 'model.layers.24.mlp.upgate_proj.Wscale', 'model.layers.24.mlp.upgate_proj.codebook_id', 'model.layers.24.mlp.upgate_proj.fuse_scales', 'model.layers.24.self_attn.o_proj.Qidxs', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.Wscale', 'model.layers.24.self_attn.o_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.Qidxs', 'model.layers.24.self_attn.qkv_proj.SU', 'model.layers.24.self_attn.qkv_proj.SV', 'model.layers.24.self_attn.qkv_proj.Wscale', 'model.layers.24.self_attn.qkv_proj.codebook_id', 'model.layers.24.self_attn.qkv_proj.fuse_scales', 'model.layers.25.mlp.down_proj.Qidxs', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.Wscale', 'model.layers.25.mlp.down_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.Qidxs', 'model.layers.25.mlp.upgate_proj.SU', 'model.layers.25.mlp.upgate_proj.SV', 'model.layers.25.mlp.upgate_proj.Wscale', 'model.layers.25.mlp.upgate_proj.codebook_id', 'model.layers.25.mlp.upgate_proj.fuse_scales', 'model.layers.25.self_attn.o_proj.Qidxs', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.Wscale', 'model.layers.25.self_attn.o_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.Qidxs', 'model.layers.25.self_attn.qkv_proj.SU', 'model.layers.25.self_attn.qkv_proj.SV', 'model.layers.25.self_attn.qkv_proj.Wscale', 'model.layers.25.self_attn.qkv_proj.codebook_id', 'model.layers.25.self_attn.qkv_proj.fuse_scales', 'model.layers.26.mlp.down_proj.Qidxs', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.Wscale', 'model.layers.26.mlp.down_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.Qidxs', 'model.layers.26.mlp.upgate_proj.SU', 'model.layers.26.mlp.upgate_proj.SV', 'model.layers.26.mlp.upgate_proj.Wscale', 'model.layers.26.mlp.upgate_proj.codebook_id', 'model.layers.26.mlp.upgate_proj.fuse_scales', 'model.layers.26.self_attn.o_proj.Qidxs', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.Wscale', 'model.layers.26.self_attn.o_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.Qidxs', 'model.layers.26.self_attn.qkv_proj.SU', 'model.layers.26.self_attn.qkv_proj.SV', 'model.layers.26.self_attn.qkv_proj.Wscale', 'model.layers.26.self_attn.qkv_proj.codebook_id', 'model.layers.26.self_attn.qkv_proj.fuse_scales', 'model.layers.27.mlp.down_proj.Qidxs', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.Wscale', 'model.layers.27.mlp.down_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.Qidxs', 'model.layers.27.mlp.upgate_proj.SU', 'model.layers.27.mlp.upgate_proj.SV', 'model.layers.27.mlp.upgate_proj.Wscale', 'model.layers.27.mlp.upgate_proj.codebook_id', 'model.layers.27.mlp.upgate_proj.fuse_scales', 'model.layers.27.self_attn.o_proj.Qidxs', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.Wscale', 'model.layers.27.self_attn.o_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.Qidxs', 'model.layers.27.self_attn.qkv_proj.SU', 'model.layers.27.self_attn.qkv_proj.SV', 'model.layers.27.self_attn.qkv_proj.Wscale', 'model.layers.27.self_attn.qkv_proj.codebook_id', 'model.layers.27.self_attn.qkv_proj.fuse_scales', 'model.layers.28.mlp.down_proj.Qidxs', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.Wscale', 'model.layers.28.mlp.down_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.Qidxs', 'model.layers.28.mlp.upgate_proj.SU', 'model.layers.28.mlp.upgate_proj.SV', 'model.layers.28.mlp.upgate_proj.Wscale', 'model.layers.28.mlp.upgate_proj.codebook_id', 'model.layers.28.mlp.upgate_proj.fuse_scales', 'model.layers.28.self_attn.o_proj.Qidxs', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.Wscale', 'model.layers.28.self_attn.o_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.Qidxs', 'model.layers.28.self_attn.qkv_proj.SU', 'model.layers.28.self_attn.qkv_proj.SV', 'model.layers.28.self_attn.qkv_proj.Wscale', 'model.layers.28.self_attn.qkv_proj.codebook_id', 'model.layers.28.self_attn.qkv_proj.fuse_scales', 'model.layers.29.mlp.down_proj.Qidxs', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.Wscale', 'model.layers.29.mlp.down_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.Qidxs', 'model.layers.29.mlp.upgate_proj.SU', 'model.layers.29.mlp.upgate_proj.SV', 'model.layers.29.mlp.upgate_proj.Wscale', 'model.layers.29.mlp.upgate_proj.codebook_id', 'model.layers.29.mlp.upgate_proj.fuse_scales', 'model.layers.29.self_attn.o_proj.Qidxs', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.Wscale', 'model.layers.29.self_attn.o_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.Qidxs', 'model.layers.29.self_attn.qkv_proj.SU', 'model.layers.29.self_attn.qkv_proj.SV', 'model.layers.29.self_attn.qkv_proj.Wscale', 'model.layers.29.self_attn.qkv_proj.codebook_id', 'model.layers.29.self_attn.qkv_proj.fuse_scales', 'model.layers.3.mlp.down_proj.Qidxs', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.Wscale', 'model.layers.3.mlp.down_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.Qidxs', 'model.layers.3.mlp.upgate_proj.SU', 'model.layers.3.mlp.upgate_proj.SV', 'model.layers.3.mlp.upgate_proj.Wscale', 'model.layers.3.mlp.upgate_proj.codebook_id', 'model.layers.3.mlp.upgate_proj.fuse_scales', 'model.layers.3.self_attn.o_proj.Qidxs', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.Wscale', 'model.layers.3.self_attn.o_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.Qidxs', 'model.layers.3.self_attn.qkv_proj.SU', 'model.layers.3.self_attn.qkv_proj.SV', 'model.layers.3.self_attn.qkv_proj.Wscale', 'model.layers.3.self_attn.qkv_proj.codebook_id', 'model.layers.3.self_attn.qkv_proj.fuse_scales', 'model.layers.30.mlp.down_proj.Qidxs', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.Wscale', 'model.layers.30.mlp.down_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.Qidxs', 'model.layers.30.mlp.upgate_proj.SU', 'model.layers.30.mlp.upgate_proj.SV', 'model.layers.30.mlp.upgate_proj.Wscale', 'model.layers.30.mlp.upgate_proj.codebook_id', 'model.layers.30.mlp.upgate_proj.fuse_scales', 'model.layers.30.self_attn.o_proj.Qidxs', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.Wscale', 'model.layers.30.self_attn.o_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.Qidxs', 'model.layers.30.self_attn.qkv_proj.SU', 'model.layers.30.self_attn.qkv_proj.SV', 'model.layers.30.self_attn.qkv_proj.Wscale', 'model.layers.30.self_attn.qkv_proj.codebook_id', 'model.layers.30.self_attn.qkv_proj.fuse_scales', 'model.layers.31.mlp.down_proj.Qidxs', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.Wscale', 'model.layers.31.mlp.down_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.Qidxs', 'model.layers.31.mlp.upgate_proj.SU', 'model.layers.31.mlp.upgate_proj.SV', 'model.layers.31.mlp.upgate_proj.Wscale', 'model.layers.31.mlp.upgate_proj.codebook_id', 'model.layers.31.mlp.upgate_proj.fuse_scales', 'model.layers.31.self_attn.o_proj.Qidxs', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.Wscale', 'model.layers.31.self_attn.o_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.Qidxs', 'model.layers.31.self_attn.qkv_proj.SU', 'model.layers.31.self_attn.qkv_proj.SV', 'model.layers.31.self_attn.qkv_proj.Wscale', 'model.layers.31.self_attn.qkv_proj.codebook_id', 'model.layers.31.self_attn.qkv_proj.fuse_scales', 'model.layers.4.mlp.down_proj.Qidxs', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.Wscale', 'model.layers.4.mlp.down_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.Qidxs', 'model.layers.4.mlp.upgate_proj.SU', 'model.layers.4.mlp.upgate_proj.SV', 'model.layers.4.mlp.upgate_proj.Wscale', 'model.layers.4.mlp.upgate_proj.codebook_id', 'model.layers.4.mlp.upgate_proj.fuse_scales', 'model.layers.4.self_attn.o_proj.Qidxs', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.Wscale', 'model.layers.4.self_attn.o_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.Qidxs', 'model.layers.4.self_attn.qkv_proj.SU', 'model.layers.4.self_attn.qkv_proj.SV', 'model.layers.4.self_attn.qkv_proj.Wscale', 'model.layers.4.self_attn.qkv_proj.codebook_id', 'model.layers.4.self_attn.qkv_proj.fuse_scales', 'model.layers.5.mlp.down_proj.Qidxs', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.Wscale', 'model.layers.5.mlp.down_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.Qidxs', 'model.layers.5.mlp.upgate_proj.SU', 'model.layers.5.mlp.upgate_proj.SV', 'model.layers.5.mlp.upgate_proj.Wscale', 'model.layers.5.mlp.upgate_proj.codebook_id', 'model.layers.5.mlp.upgate_proj.fuse_scales', 'model.layers.5.self_attn.o_proj.Qidxs', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.Wscale', 'model.layers.5.self_attn.o_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.Qidxs', 'model.layers.5.self_attn.qkv_proj.SU', 'model.layers.5.self_attn.qkv_proj.SV', 'model.layers.5.self_attn.qkv_proj.Wscale', 'model.layers.5.self_attn.qkv_proj.codebook_id', 'model.layers.5.self_attn.qkv_proj.fuse_scales', 'model.layers.6.mlp.down_proj.Qidxs', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.Wscale', 'model.layers.6.mlp.down_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.Qidxs', 'model.layers.6.mlp.upgate_proj.SU', 'model.layers.6.mlp.upgate_proj.SV', 'model.layers.6.mlp.upgate_proj.Wscale', 'model.layers.6.mlp.upgate_proj.codebook_id', 'model.layers.6.mlp.upgate_proj.fuse_scales', 'model.layers.6.self_attn.o_proj.Qidxs', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.Wscale', 'model.layers.6.self_attn.o_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.Qidxs', 'model.layers.6.self_attn.qkv_proj.SU', 'model.layers.6.self_attn.qkv_proj.SV', 'model.layers.6.self_attn.qkv_proj.Wscale', 'model.layers.6.self_attn.qkv_proj.codebook_id', 'model.layers.6.self_attn.qkv_proj.fuse_scales', 'model.layers.7.mlp.down_proj.Qidxs', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.Wscale', 'model.layers.7.mlp.down_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.Qidxs', 'model.layers.7.mlp.upgate_proj.SU', 'model.layers.7.mlp.upgate_proj.SV', 'model.layers.7.mlp.upgate_proj.Wscale', 'model.layers.7.mlp.upgate_proj.codebook_id', 'model.layers.7.mlp.upgate_proj.fuse_scales', 'model.layers.7.self_attn.o_proj.Qidxs', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.Wscale', 'model.layers.7.self_attn.o_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.Qidxs', 'model.layers.7.self_attn.qkv_proj.SU', 'model.layers.7.self_attn.qkv_proj.SV', 'model.layers.7.self_attn.qkv_proj.Wscale', 'model.layers.7.self_attn.qkv_proj.codebook_id', 'model.layers.7.self_attn.qkv_proj.fuse_scales', 'model.layers.8.mlp.down_proj.Qidxs', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.Wscale', 'model.layers.8.mlp.down_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.Qidxs', 'model.layers.8.mlp.upgate_proj.SU', 'model.layers.8.mlp.upgate_proj.SV', 'model.layers.8.mlp.upgate_proj.Wscale', 'model.layers.8.mlp.upgate_proj.codebook_id', 'model.layers.8.mlp.upgate_proj.fuse_scales', 'model.layers.8.self_attn.o_proj.Qidxs', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.Wscale', 'model.layers.8.self_attn.o_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.Qidxs', 'model.layers.8.self_attn.qkv_proj.SU', 'model.layers.8.self_attn.qkv_proj.SV', 'model.layers.8.self_attn.qkv_proj.Wscale', 'model.layers.8.self_attn.qkv_proj.codebook_id', 'model.layers.8.self_attn.qkv_proj.fuse_scales', 'model.layers.9.mlp.down_proj.Qidxs', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.Wscale', 'model.layers.9.mlp.down_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.Qidxs', 'model.layers.9.mlp.upgate_proj.SU', 'model.layers.9.mlp.upgate_proj.SV', 'model.layers.9.mlp.upgate_proj.Wscale', 'model.layers.9.mlp.upgate_proj.codebook_id', 'model.layers.9.mlp.upgate_proj.fuse_scales', 'model.layers.9.self_attn.o_proj.Qidxs', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.Wscale', 'model.layers.9.self_attn.o_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.Qidxs', 'model.layers.9.self_attn.qkv_proj.SU', 'model.layers.9.self_attn.qkv_proj.SV', 'model.layers.9.self_attn.qkv_proj.Wscale', 'model.layers.9.self_attn.qkv_proj.codebook_id', 'model.layers.9.self_attn.qkv_proj.fuse_scales']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
W0413 08:46:13.584251 845518 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:47: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0413 08:46:13.585738 845518 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_qkv.pt',

W0413 08:46:13.597012 845518 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:61: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0413 08:46:13.604645 845518 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:66: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0413 08:46:13.650387 845518 warnings.py:110] /workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0413 08:46:13.668082 845518 hfize_llama.py:78] loaded layer 0 down
I0413 08:46:13.731742 845518 hfize_llama.py:78] loaded layer 1 down
I0413 08:46:13.801637 845518 hfize_llama.py:78] loaded layer 2 down
I0413 08:46:13.867756 845518 hfize_llama.py:78] loaded layer 3 down
I0413 08:46:13.933518 845518 hfize_llama.py:78] loaded layer 4 down
I0413 08:46:13.996337 845518 hfize_llama.py:78] loaded layer 5 down
I0413 08:46:14.062575 845518 hfize_llama.py:78] loaded layer 6 down
I0413 08:46:14.122480 845518 hfize_llama.py:78] loaded layer 7 down
I0413 08:46:14.187963 845518 hfize_llama.py:78] loaded layer 8 down
I0413 08:46:14.251370 845518 hfize_llama.py:78] loaded layer 9 down
I0413 08:46:14.330828 845518 hfize_llama.py:78] loaded layer 10 down
I0413 08:46:14.397011 845518 hfize_llama.py:78] loaded layer 11 down
I0413 08:46:14.479950 845518 hfize_llama.py:78] loaded layer 12 down
I0413 08:46:14.562459 845518 hfize_llama.py:78] loaded layer 13 down
I0413 08:46:14.645843 845518 hfize_llama.py:78] loaded layer 14 down
I0413 08:46:14.727801 845518 hfize_llama.py:78] loaded layer 15 down
I0413 08:46:14.823986 845518 hfize_llama.py:78] loaded layer 16 down
I0413 08:46:14.886810 845518 hfize_llama.py:78] loaded layer 17 down
I0413 08:46:14.953147 845518 hfize_llama.py:78] loaded layer 18 down
I0413 08:46:15.023937 845518 hfize_llama.py:78] loaded layer 19 down
I0413 08:46:15.110531 845518 hfize_llama.py:78] loaded layer 20 down
I0413 08:46:15.173864 845518 hfize_llama.py:78] loaded layer 21 down
I0413 08:46:15.247380 845518 hfize_llama.py:78] loaded layer 22 down
I0413 08:46:15.307296 845518 hfize_llama.py:78] loaded layer 23 down
I0413 08:46:15.373560 845518 hfize_llama.py:78] loaded layer 24 down
I0413 08:46:15.438871 845518 hfize_llama.py:78] loaded layer 25 down
I0413 08:46:15.505256 845518 hfize_llama.py:78] loaded layer 26 down
I0413 08:46:15.575662 845518 hfize_llama.py:78] loaded layer 27 down
I0413 08:46:15.641219 845518 hfize_llama.py:78] loaded layer 28 down
I0413 08:46:15.706850 845518 hfize_llama.py:78] loaded layer 29 down
I0413 08:46:15.772075 845518 hfize_llama.py:78] loaded layer 30 down
I0413 08:46:15.840585 845518 hfize_llama.py:78] loaded layer 31 down
I0413 08:46:15.840681 845518 hfize_llama.py:80] saving model...
I0413 08:46:26.887646 845518 hfize_llama.py:87] successfully loaded hfized model
I0413 08:46:26.887795 845518 hfize_llama.py:89] generating some text...
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
I0413 08:46:43.406221 845518 hfize_llama.py:100] <|begin_of_text|>It is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife. The fact that the good fortune in question is the result of a lucky lottery win is a minor detail. As a single man with a good fortune, the hero of The Unlikely Pilgrimage of Harold Fry is in want of a
I0413 08:46:43.406408 845518 hfize_llama.py:101] elapsed: 16.5185124874115
[Stage: End-to-End Finetuning] K=3
W0413 08:46:46.246209 846019 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0413 08:46:46.274016 846019 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0413 08:46:46.577720 846019 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0413 08:46:46.577840 846019 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0413 08:46:46.577883 846019 utils.py:162] NumExpr defaulting to 16 threads.
I0413 08:46:46.699574 846019 config.py:58] PyTorch version 2.4.0 available.
W0413 08:46:47.081730 846019 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

W0413 08:47:08.061625 846019 warnings.py:110] /opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:02<00:06,  2.17s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:04<00:04,  2.17s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:06<00:02,  2.18s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.49s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.74s/it]
/opt/conda/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
[Stage: Eval PPL] K=3
[Stage: Eval Zero-shot] K=3
