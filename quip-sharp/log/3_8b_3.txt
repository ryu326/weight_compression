[Stage: Quantize with Finetuning] K=3
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 15, in <module>
    from lib import codebook, utils
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
[Stage: Convert to HF format] K=3
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
[Stage: End-to-End Finetuning] K=3
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 22, in <module>
    from lib import codebook, utils
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
[Stage: Re-convert to HF (post-e2e)] K=3
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
[Stage: Eval PPL] K=3
I0412 06:28:22.357349 1904 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 06:28:22.357450 1904 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 06:28:22.357494 1904 utils.py:162] NumExpr defaulting to 16 threads.
I0412 06:28:22.839384 1904 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/eval/eval_ppl.py", line 22, in <module>
    from lib.utils import gptq_data_utils
  File "/workspace/Weight_compression/quip-sharp/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/workspace/Weight_compression/quip-sharp/lib/utils/data_utils.py", line 8, in <module>
    from lib import codebook
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py", line 2, in <module>
    import quiptools_cuda
ModuleNotFoundError: No module named 'quiptools_cuda'
[Stage: Eval Zero-shot] K=3
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/eval/eval_zeroshot.py", line 9, in <module>
    from lm_eval import evaluator, tasks
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/evaluator.py", line 12, in <module>
    import lm_eval.api.metrics
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/registry.py", line 4, in <module>
    import evaluate as hf_evaluate
  File "/opt/conda/lib/python3.11/site-packages/evaluate/__init__.py", line 29, in <module>
    from .evaluation_suite import EvaluationSuite
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluation_suite/__init__.py", line 10, in <module>
    from ..evaluator import evaluator
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluator/__init__.py", line 17, in <module>
    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS
  File "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py", line 26, in <module>
    from ..image_processing_utils import BaseImageProcessor
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_processing_utils.py", line 21, in <module>
    from .image_transforms import center_crop, normalize, rescale
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_transforms.py", line 22, in <module>
    from .image_utils import (
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_utils.py", line 58, in <module>
    from torchvision.transforms import InterpolationMode
  File "/opt/conda/lib/python3.11/site-packages/torchvision/__init__.py", line 10, in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torchvision/_meta_registrations.py", line 163, in <module>
    @torch.library.register_fake("torchvision::nms")
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 654, in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 154, in _register_fake
    handle = entry.abstract_impl.register(func_to_register, source)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_library/abstract_impl.py", line 31, in register
    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: operator torchvision::nms does not exist
[Stage: Quantize with Finetuning] K=3
W0412 06:32:48.433545 6299 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 06:32:48.467156 6299 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 06:32:48.718280 6299 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 06:32:48.718437 6299 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 06:32:48.718488 6299 utils.py:162] NumExpr defaulting to 16 threads.
I0412 06:32:48.849512 6299 config.py:58] PyTorch version 2.4.0 available.
W0412 06:32:49.197248 6299 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/quantize_finetune_llama.py", line 15, in <module>
    from lib import codebook, utils
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/latticee8_padded12.py", line 19, in <module>
    from lib.utils.matmul_had import matmul_hadU_cuda, matmul_hadUt_cuda
  File "/workspace/Weight_compression/quip-sharp/lib/utils/__init__.py", line 7, in <module>
    from .matmul_kron import *
  File "/workspace/Weight_compression/quip-sharp/lib/utils/matmul_kron.py", line 3, in <module>
    import primefac
ModuleNotFoundError: No module named 'primefac'
[Stage: Convert to HF format] K=3
W0412 06:32:51.546331 6651 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 06:32:51.579721 6651 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 06:32:51.875337 6651 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 06:32:51.875519 6651 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 06:32:51.875567 6651 utils.py:162] NumExpr defaulting to 16 threads.
I0412 06:32:52.004884 6651 config.py:58] PyTorch version 2.4.0 available.
W0412 06:32:52.368434 6651 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/latticee8_padded12.py", line 19, in <module>
    from lib.utils.matmul_had import matmul_hadU_cuda, matmul_hadUt_cuda
  File "/workspace/Weight_compression/quip-sharp/lib/utils/__init__.py", line 7, in <module>
    from .matmul_kron import *
  File "/workspace/Weight_compression/quip-sharp/lib/utils/matmul_kron.py", line 3, in <module>
    import primefac
ModuleNotFoundError: No module named 'primefac'
[Stage: End-to-End Finetuning] K=3
W0412 06:32:54.784241 6859 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 06:32:54.816107 6859 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 06:32:55.084123 6859 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 06:32:55.084289 6859 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 06:32:55.084361 6859 utils.py:162] NumExpr defaulting to 16 threads.
I0412 06:32:55.214220 6859 config.py:58] PyTorch version 2.4.0 available.
W0412 06:32:55.590736 6859 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/finetune_e2e_llama.py", line 22, in <module>
    from lib import codebook, utils
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/latticee8_padded12.py", line 19, in <module>
    from lib.utils.matmul_had import matmul_hadU_cuda, matmul_hadUt_cuda
  File "/workspace/Weight_compression/quip-sharp/lib/utils/__init__.py", line 7, in <module>
    from .matmul_kron import *
  File "/workspace/Weight_compression/quip-sharp/lib/utils/matmul_kron.py", line 3, in <module>
    import primefac
ModuleNotFoundError: No module named 'primefac'
[Stage: Re-convert to HF (post-e2e)] K=3
W0412 06:32:57.935619 7067 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 06:32:57.968356 7067 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

I0412 06:32:58.222912 7067 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 06:32:58.223023 7067 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 06:32:58.223069 7067 utils.py:162] NumExpr defaulting to 16 threads.
I0412 06:32:58.345924 7067 config.py:58] PyTorch version 2.4.0 available.
W0412 06:32:58.679946 7067 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/latticee8_padded12.py", line 19, in <module>
    from lib.utils.matmul_had import matmul_hadU_cuda, matmul_hadUt_cuda
  File "/workspace/Weight_compression/quip-sharp/lib/utils/__init__.py", line 7, in <module>
    from .matmul_kron import *
  File "/workspace/Weight_compression/quip-sharp/lib/utils/matmul_kron.py", line 3, in <module>
    import primefac
ModuleNotFoundError: No module named 'primefac'
[Stage: Eval PPL] K=3
I0412 06:33:00.718945 7275 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0412 06:33:00.719303 7275 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0412 06:33:00.719351 7275 utils.py:162] NumExpr defaulting to 16 threads.
I0412 06:33:01.107143 7275 config.py:58] PyTorch version 2.4.0 available.
W0412 06:33:01.271205 7275 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:6: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decode_matvec_e8p")

W0412 06:33:01.310749 7275 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py:25: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::decompress_packed_e8p")

W0412 06:33:01.490013 7275 warnings.py:110] /workspace/Weight_compression/quip-sharp/lib/utils/matmul_had.py:92: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
  @torch.library.impl_abstract("quip_lib::hadamard")

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/quip-sharp/eval/eval_ppl.py", line 22, in <module>
    from lib.utils import gptq_data_utils
  File "/workspace/Weight_compression/quip-sharp/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/workspace/Weight_compression/quip-sharp/lib/utils/data_utils.py", line 8, in <module>
    from lib import codebook
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/__init__.py", line 40, in <module>
    from . import (latticee8_padded12, latticee8_padded12_rvq3bit,
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/latticee8_padded12_rvq4bit.py", line 108, in <module>
    _E8P_GRID, _E8P_GRID_IDX, _PARITY_IDX = get_full_grid(_E8P_PACKED_ABS_CACHED)
                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/quip-sharp/lib/codebook/latticee8_padded12_rvq4bit.py", line 91, in get_full_grid
    parity = parity ^ ((signs >> i) & 1)
    ^^^^^^
KeyboardInterrupt
