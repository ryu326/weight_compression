I0402 08:33:35.371154 1509661 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:33:35.371284 1509661 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:33:35.371324 1509661 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:33:35.586045 1509661 config.py:58] PyTorch version 2.4.0 available.
W0402 08:33:37.804700 1509661 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.66it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.26it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  4.28it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  4.58it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  4.46it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.41it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.66it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.47it/s]
I0402 08:33:40.201895 1509661 quantize_finetune_llama.py:135] loaded model
I0402 08:34:02.219467 1509661 quantize_finetune_llama.py:139] loaded dataset and devset
I0402 08:34:08.641763 1509661 quantize_finetune_llama.py:159] layer 0 gpu 0
I0402 08:34:12.267935 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 0 in 3.48132061958313s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0402 08:34:30.964186 1509661 quantize_finetune_llama.py:159] layer 1 gpu 1
I0402 08:34:32.959666 1510928 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:34:32.959842 1510928 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:34:32.959902 1510928 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:34:33.159935 1510928 config.py:58] PyTorch version 2.4.0 available.
I0402 08:34:34.302735 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 1 in 3.1785645484924316s
I0402 08:34:34.796288 1509661 quantize_finetune_llama.py:159] layer 2 gpu 2
I0402 08:34:35.670023 1510928 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:34:36.085899 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 08:34:36.724438 1511186 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:34:36.724561 1511186 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:34:36.724617 1511186 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:34:36.914543 1511186 config.py:58] PyTorch version 2.4.0 available.
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:34:37.952820 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 2 in 3.013610601425171s
I0402 08:34:38.448156 1509661 quantize_finetune_llama.py:159] layer 3 gpu 3
  3%|▎         | 1/32 [00:01<00:49,  1.60s/it]  6%|▋         | 2/32 [00:01<00:25,  1.18it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s]I0402 08:34:39.537699 1511186 data_utils.py:336] using 256 training seqs, 128 validation seqs
 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s]W0402 08:34:40.081685 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s]I0402 08:34:40.476147 1511719 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:34:40.476314 1511719 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:34:40.476390 1511719 utils.py:162] NumExpr defaulting to 16 threads.
 19%|█▉        | 6/32 [00:03<00:10,  2.50it/s]I0402 08:34:40.677726 1511719 config.py:58] PyTorch version 2.4.0 available.
 22%|██▏       | 7/32 [00:03<00:09,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s]  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:34:41.409515 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 3 in 2.831002712249756s
 28%|██▊       | 9/32 [00:04<00:08,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.91it/s]I0402 08:34:41.881623 1509661 quantize_finetune_llama.py:159] layer 4 gpu 0
 34%|███▍      | 11/32 [00:04<00:07,  2.95it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.01it/s] 41%|████      | 13/32 [00:05<00:06,  3.00it/s]  3%|▎         | 1/32 [00:01<00:53,  1.74s/it] 44%|████▍     | 14/32 [00:05<00:06,  3.00it/s]I0402 08:34:43.165941 1511719 data_utils.py:336] using 256 training seqs, 128 validation seqs
  6%|▋         | 2/32 [00:02<00:27,  1.09it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.03it/s]W0402 08:34:43.545667 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 50%|█████     | 16/32 [00:06<00:05,  3.05it/s]I0402 08:34:43.861383 1512243 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:34:43.861539 1512243 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:34:43.861602 1512243 utils.py:162] NumExpr defaulting to 16 threads.
 12%|█▎        | 4/32 [00:02<00:14,  1.91it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.06it/s]I0402 08:34:44.168973 1512243 config.py:58] PyTorch version 2.4.0 available.
 16%|█▌        | 5/32 [00:03<00:12,  2.20it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.09it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s]  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.10it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.07it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.09it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.09it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.84it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.11it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.87it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.07it/s]  3%|▎         | 1/32 [00:01<00:51,  1.65s/it]I0402 08:34:46.537687 1512243 data_utils.py:336] using 256 training seqs, 128 validation seqs
 38%|███▊      | 12/32 [00:05<00:06,  2.88it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.06it/s]  6%|▋         | 2/32 [00:01<00:26,  1.14it/s]W0402 08:34:46.849048 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:05<00:06,  2.93it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.08it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.93it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.07it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.95it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.95it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.07it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s]  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:06<00:05,  2.98it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.10it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.09it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.96it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.12it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s]100%|██████████| 32/32 [00:11<00:00,  3.15it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
 59%|█████▉    | 19/32 [00:07<00:04,  3.02it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.01it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s]  3%|▎         | 1/32 [00:01<00:51,  1.65s/it] 66%|██████▌   | 21/32 [00:08<00:03,  2.99it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.84it/s]  6%|▋         | 2/32 [00:01<00:26,  1.13it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.98it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.97it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.91it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.19it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.04it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.95it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.42it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.06it/s] 50%|█████     | 16/32 [00:06<00:05,  2.96it/s]W0402 08:34:51.447000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.448000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.448000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.448000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.448000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.448000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.448000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.474000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.474000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.475000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.475000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.475000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.05it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.95it/s]W0402 08:34:51.760000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.760000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.760000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.761000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:51.761000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:04<00:08,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.05it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.96it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.03it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.97it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.04it/s]W0402 08:34:52.612000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.612000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.612000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.612000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.612000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.612000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.612000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.630000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.630000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.630000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.630000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.630000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:08<00:04,  2.97it/s]W0402 08:34:52.830000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.830000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.830000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.830000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:52.830000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:05<00:07,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.03it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.96it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  3.06it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.98it/s] 41%|████      | 13/32 [00:05<00:06,  2.96it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.95it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.94it/s]W0402 08:34:53.923000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.923000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.923000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.923000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.923000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.923000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.923000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.941000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.941000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.941000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.941000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:53.941000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.96it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.94it/s] 50%|█████     | 16/32 [00:06<00:05,  2.93it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.92it/s]W0402 08:34:54.826000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:54.826000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:54.826000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:54.826000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:54.826000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.95it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.97it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 88%|████████▊ | 28/32 [00:10<00:01,  2.96it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.99it/s]W0402 08:34:55.752000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.752000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.752000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.752000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.752000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.753000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.753000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.778000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.778000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.778000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.778000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:55.778000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.98it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.01it/s]W0402 08:34:56.059000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.060000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.060000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.060000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.060000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.01it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s]100%|██████████| 32/32 [00:12<00:00,  2.92it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
 72%|███████▏  | 23/32 [00:09<00:03,  2.92it/s]W0402 08:34:56.936000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.936000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.936000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.937000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.937000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.937000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.937000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.954000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.954000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.954000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.954000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:56.954000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:57.147000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:57.147000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:57.147000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:57.147000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:57.147000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.90it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s]W0402 08:34:58.292000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.292000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.292000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.292000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.292000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.293000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.293000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.312000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.313000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.313000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.313000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:58.313000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s]W0402 08:34:59.225000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.225000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.225000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.225000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.225000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.315000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.316000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.316000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.316000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.316000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.316000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.316000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s]W0402 08:34:59.342000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.342000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.342000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.342000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.342000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.623000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.623000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.623000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.623000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:34:59.623000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
W0402 08:35:00.524000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.524000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.524000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.524000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.524000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.524000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.524000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.543000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.544000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.544000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.544000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.544000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
I0402 08:35:00.682139 1510928 finetune.py:45] layer 0_v initial loss 0.00013459190085995942
W0402 08:35:00.682475 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:35:00.754000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.754000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.754000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.754000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:00.754000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.830205 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:35:01.916000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.916000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.916000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.916000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.916000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.916000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.916000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.935000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.935000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.935000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.935000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:01.935000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.532000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.532000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.532000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.532000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.532000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.532000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.532000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.560000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.560000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.560000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.560000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.560000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.840000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.841000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.841000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.841000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.841000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.860000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.861000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.861000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.861000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:02.861000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
0_v proxy err 0.019532809033989906 tr(WHW.T) 1.3175290822982788
  0%|          | 0/32 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 08:35:03.739000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.739000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.739000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.739000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.739000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.739000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.739000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.758000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.758000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.758000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.758000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.758000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.965000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.965000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.965000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.965000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:03.966000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s]W0402 08:35:05.116000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.116000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.116000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.116000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.117000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.117000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.117000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.136000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.136000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.136000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.136000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:05.136000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s]W0402 08:35:06.048000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:06.049000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:06.049000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:06.049000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:06.049000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
I0402 08:35:06.107653 1511186 finetune.py:45] layer 1_v initial loss 0.003532135160639882
W0402 08:35:06.109128 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s]W0402 08:35:07.804201 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s]1_v proxy err 0.01828262209892273 tr(WHW.T) 5.699800491333008
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:35:08.921272 1511719 finetune.py:45] layer 2_v initial loss 0.0007412380073219538
W0402 08:35:08.921657 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]W0402 08:35:10.025890 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s]2_v proxy err 0.025783024728298187 tr(WHW.T) 26.839576721191406
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.75it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s]I0402 08:35:11.927957 1512243 finetune.py:45] layer 3_v initial loss 0.001402330002747476
W0402 08:35:11.928387 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s]  3%|▎         | 1/32 [00:00<00:29,  1.05it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s]W0402 08:35:12.884032 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s]3_v proxy err 0.03216426074504852 tr(WHW.T) 40.46714782714844
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.90it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s]  3%|▎         | 1/32 [00:00<00:28,  1.11it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s]  6%|▋         | 2/32 [00:01<00:17,  1.72it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 50%|█████     | 16/32 [00:06<00:06,  2.63it/s]  9%|▉         | 3/32 [00:01<00:13,  2.10it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.82it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.69it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s]W0402 08:35:20.128000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.128000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.128000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.128000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.128000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.128000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.129000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.157000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.157000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.157000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.157000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.157000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s]W0402 08:35:20.331000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.331000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.332000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.332000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.332000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s]W0402 08:35:20.550000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.550000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.551000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.551000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.551000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.551000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.551000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.571000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.571000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.571000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.571000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.571000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s]W0402 08:35:20.635000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.635000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.635000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.635000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:20.635000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:06<00:04,  2.82it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s]W0402 08:35:21.348000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s]W0402 08:35:21.653000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.653000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.653000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.654000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.654000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.654000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.654000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.675000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.676000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.676000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.676000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.676000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.937000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.937000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.937000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.937000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:21.937000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.80it/s]W0402 08:35:22.281000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
W0402 08:35:26.447000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.447000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.447000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.447000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.448000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.448000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.448000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.477000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.477000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.477000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.477000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.477000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.640000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.640000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.641000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.641000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.641000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.856000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.856000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.856000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.857000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.857000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.857000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.857000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.878000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.878000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.878000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.878000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.878000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.942000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.942000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.942000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.942000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:26.942000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.658000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.961000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.962000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.962000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.962000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.962000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.962000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.962000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:27.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.240000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.240000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.240000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.240000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.240000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
I0402 08:35:28.298979 1510928 finetune.py:45] layer 0_q initial loss 0.0001342055038549006
W0402 08:35:28.299275 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:35:28.398000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.399000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.399000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.399000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.399000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.399000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.399000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.430000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.430000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.430000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.430000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.430000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.574000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.595000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.595000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.596000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.596000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.596000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.827000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.827000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.827000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.827000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.827000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.827000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.827000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.848000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.849000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.849000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.849000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.849000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.914000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.914000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.914000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.914000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:28.914000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.310856 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:35:29.657000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.962000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.962000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.962000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.962000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.962000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.963000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.963000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.985000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.985000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.985000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.985000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:29.985000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:30.243000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:30.243000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:30.243000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:30.243000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:30.243000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
0_q proxy err 0.0003133666468784213 tr(WHW.T) 6234.0732421875
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:35:30.613000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.011000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.012000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.012000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.012000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.012000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.012000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.012000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.043000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.043000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.044000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.044000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.044000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:27,  1.12it/s]W0402 08:35:31.220000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.220000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.220000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.220000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.220000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.450000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.450000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.450000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.450000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.450000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.450000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.450000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.473000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.474000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.474000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.474000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.474000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:17,  1.72it/s]W0402 08:35:31.541000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.541000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.541000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.541000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:31.541000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.31it/s]W0402 08:35:32.289000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s]W0402 08:35:32.620000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.621000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.621000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.621000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.621000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.621000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.621000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.644000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.644000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.644000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.645000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.645000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.908000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.908000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.908000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.908000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:35:32.908000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s]W0402 08:35:33.269000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s]I0402 08:35:35.245855 1511186 finetune.py:45] layer 1_q initial loss 0.0036281615030020475
W0402 08:35:35.246191 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s]W0402 08:35:36.769918 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s]I0402 08:35:37.457871 1511719 finetune.py:45] layer 2_q initial loss 0.000740682939067483
W0402 08:35:37.458362 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s]1_q proxy err 0.0003237796772737056 tr(WHW.T) 7568.30322265625
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.78it/s]  3%|▎         | 1/32 [00:00<00:26,  1.16it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s]W0402 08:35:39.018703 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:16,  1.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s]I0402 08:35:39.499142 1512243 finetune.py:45] layer 3_q initial loss 0.0014017380308359861
W0402 08:35:39.499347 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:01<00:11,  2.34it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s]W0402 08:35:40.455596 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s]2_q proxy err 0.00283824373036623 tr(WHW.T) 7137.59423828125
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s]3_q proxy err 0.004042757675051689 tr(WHW.T) 6654.2763671875
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.91it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s]  3%|▎         | 1/32 [00:00<00:28,  1.07it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
  3%|▎         | 1/32 [00:00<00:26,  1.17it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.70it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.75it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s]I0402 08:35:48.377530 1510928 finetune.py:45] layer 0_k initial loss 0.0001339128939434886
W0402 08:35:48.377824 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:10<00:01,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s]W0402 08:35:49.347298 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.80it/s]0_k proxy err 0.00029483725666068494 tr(WHW.T) 2167.80859375
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s]  3%|▎         | 1/32 [00:00<00:22,  1.36it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s]  6%|▋         | 2/32 [00:01<00:15,  1.99it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s]  9%|▉         | 3/32 [00:01<00:12,  2.34it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.56it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.68it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
 22%|██▏       | 7/32 [00:02<00:08,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.80it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s] 41%|████      | 13/32 [00:04<00:06,  2.84it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.81it/s] 50%|█████     | 16/32 [00:05<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s]I0402 08:35:57.337589 1511186 finetune.py:45] layer 1_k initial loss 0.0037301701959222555
W0402 08:35:57.337828 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.78it/s]W0402 08:35:58.390112 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.78it/s]1_k proxy err 0.0003244929830543697 tr(WHW.T) 3947.499267578125
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s]I0402 08:35:59.857802 1512243 finetune.py:45] layer 3_k initial loss 0.0014019941445440054
W0402 08:35:59.858117 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s]  3%|▎         | 1/32 [00:00<00:24,  1.29it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.77it/s]I0402 08:36:00.387008 1511719 finetune.py:45] layer 2_k initial loss 0.0007412500563077629
W0402 08:36:00.387469 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:01<00:16,  1.83it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s]W0402 08:36:00.872112 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.85it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.34it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.46it/s]W0402 08:36:01.773988 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_k proxy err 0.004334603436291218 tr(WHW.T) 3660.859619140625
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.61it/s]  3%|▎         | 1/32 [00:00<00:24,  1.27it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s]2_k proxy err 0.003110866993665695 tr(WHW.T) 3891.80859375
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:16,  1.87it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s]  9%|▉         | 3/32 [00:01<00:13,  2.16it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s]  6%|▋         | 2/32 [00:01<00:16,  1.83it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s]  9%|▉         | 3/32 [00:01<00:13,  2.17it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.74it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.58it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s]I0402 08:36:08.591891 1510928 finetune.py:45] layer 0_o initial loss 0.00013268410111777484
W0402 08:36:08.592181 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s]W0402 08:36:09.574822 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s]0_o proxy err 0.004276420455425978 tr(WHW.T) 0.23014843463897705
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 72%|███████▏  | 23/32 [00:08<00:03,  2.70it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s]  3%|▎         | 1/32 [00:01<00:56,  1.81s/it] 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.61it/s]  6%|▋         | 2/32 [00:03<00:46,  1.54s/it]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.64it/s]  9%|▉         | 3/32 [00:04<00:42,  1.47s/it]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 12%|█▎        | 4/32 [00:05<00:40,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.44s/it]I0402 08:36:19.014306 1511186 finetune.py:45] layer 1_o initial loss 0.0027100436855107546
W0402 08:36:19.014647 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it]W0402 08:36:20.112366 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 08:36:20.847027 1512243 finetune.py:45] layer 3_o initial loss 0.001408813870511949
W0402 08:36:20.847354 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it]1_o proxy err 0.015589438378810883 tr(WHW.T) 0.31429046392440796
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:36:21.898286 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it]I0402 08:36:23.075456 1511719 finetune.py:45] layer 2_o initial loss 0.0007226089364849031
W0402 08:36:23.076076 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

3_o proxy err 0.02350390888750553 tr(WHW.T) 0.9357947111129761
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it]W0402 08:36:24.347997 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it]2_o proxy err 0.014836015179753304 tr(WHW.T) 0.5573468208312988
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:05<00:46,  1.62s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.40s/it]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.40s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 41%|████      | 13/32 [00:18<00:26,  1.39s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.39s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 50%|█████     | 16/32 [00:22<00:22,  1.39s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.39s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 59%|█████▉    | 19/32 [00:26<00:18,  1.39s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.39s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.50s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.40s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 41%|████      | 13/32 [00:19<00:28,  1.52s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.40s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:32,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.52s/it] 75%|███████▌  | 24/32 [00:33<00:11,  1.40s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.41s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.40s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.40s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.41s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.41s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.41s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.40s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.41s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.47s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.43s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.45s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.48s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.51s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it]I0402 08:37:03.451641 1510928 finetune.py:45] layer 0_up initial loss 0.00012715072080027312
W0402 08:37:03.451855 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:40<00:05,  1.48s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it]W0402 08:37:04.279235 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.47s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.53s/it]0_up proxy err 0.03325975686311722 tr(WHW.T) 101.63944244384766
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.52s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.46s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.54s/it]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.46s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]  6%|▋         | 2/32 [00:03<00:47,  1.57s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 91%|█████████ | 29/32 [00:43<00:04,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
  9%|▉         | 3/32 [00:04<00:43,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.51s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it]I0402 08:37:16.424444 1512243 finetune.py:45] layer 3_up initial loss 0.001370768528431654
W0402 08:37:16.424736 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it]W0402 08:37:17.324749 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 08:37:17.787330 1511186 finetune.py:45] layer 1_up initial loss 0.0025181344244629145
W0402 08:37:17.787555 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it]3_up proxy err 0.043774038553237915 tr(WHW.T) 316.0293273925781
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:37:18.785537 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it]1_up proxy err 0.039296772330999374 tr(WHW.T) 159.80691528320312
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it]I0402 08:37:22.719566 1511719 finetune.py:45] layer 2_up initial loss 0.0007085048709996045
W0402 08:37:22.720054 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it]  9%|▉         | 3/32 [00:04<00:44,  1.55s/it]W0402 08:37:23.866426 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:18<00:27,  1.43s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it]2_up proxy err 0.04553699120879173 tr(WHW.T) 225.97427368164062
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 50%|█████     | 16/32 [00:23<00:22,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.41s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.46s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.43s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.47s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.44s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.42s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.44s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.49s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.46s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.47s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it]I0402 08:37:59.197212 1510928 finetune.py:45] layer 0_gate initial loss 0.00012130838149460033
W0402 08:37:59.197573 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.48s/it]W0402 08:37:59.904881 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.46s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.46s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it]0_gate proxy err 0.02303973026573658 tr(WHW.T) 179.7012481689453
  0%|          | 0/112 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.45s/it]  1%|          | 1/112 [00:00<01:28,  1.26it/s] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it]  2%|▏         | 2/112 [00:01<00:58,  1.88it/s]  3%|▎         | 3/112 [00:01<00:49,  2.20it/s]  4%|▎         | 4/112 [00:01<00:44,  2.41it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
  4%|▍         | 5/112 [00:02<00:42,  2.55it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it]  5%|▌         | 6/112 [00:02<00:40,  2.62it/s]  6%|▋         | 7/112 [00:02<00:39,  2.66it/s]  7%|▋         | 8/112 [00:03<00:38,  2.70it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it]  8%|▊         | 9/112 [00:03<00:37,  2.76it/s] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it]  9%|▉         | 10/112 [00:04<00:36,  2.76it/s] 10%|▉         | 11/112 [00:04<00:36,  2.77it/s] 11%|█         | 12/112 [00:04<00:35,  2.78it/s]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 12%|█▏        | 13/112 [00:05<00:35,  2.78it/s] 91%|█████████ | 29/32 [00:42<00:04,  1.46s/it] 12%|█▎        | 14/112 [00:05<00:35,  2.77it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.77it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.76it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.76it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 16%|█▌        | 18/112 [00:06<00:34,  2.76it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.76it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.76it/s] 19%|█▉        | 21/112 [00:07<00:33,  2.76it/s] 97%|█████████▋| 31/32 [00:45<00:01,  1.48s/it] 20%|█▉        | 22/112 [00:08<00:32,  2.75it/s] 21%|██        | 23/112 [00:08<00:32,  2.75it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.75it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.75it/s]I0402 08:38:12.295773 1512243 finetune.py:45] layer 3_gate initial loss 0.001314474269747734
W0402 08:38:12.295937 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:47<00:00,  1.49s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 23%|██▎       | 26/112 [00:09<00:30,  2.79it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.79it/s]W0402 08:38:13.066530 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 28/112 [00:10<00:30,  2.77it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.77it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.79it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.78it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.77it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.81it/s] 30%|███       | 34/112 [00:12<00:28,  2.78it/s]I0402 08:38:15.725125 1511186 finetune.py:45] layer 1_gate initial loss 0.002101548947393894
W0402 08:38:15.725541 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 35/112 [00:13<00:27,  2.77it/s]3_gate proxy err 0.02400818094611168 tr(WHW.T) 875.7509765625
  0%|          | 0/112 [00:00<?, ?it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.77it/s]W0402 08:38:16.465245 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 33%|███▎      | 37/112 [00:13<00:27,  2.76it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.76it/s]  1%|          | 1/112 [00:00<01:35,  1.17it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.76it/s]  2%|▏         | 2/112 [00:01<01:02,  1.75it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.75it/s]  3%|▎         | 3/112 [00:01<00:52,  2.09it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.76it/s]  4%|▎         | 4/112 [00:01<00:47,  2.29it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.77it/s]  4%|▍         | 5/112 [00:02<00:44,  2.42it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.78it/s]  5%|▌         | 6/112 [00:02<00:41,  2.53it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.79it/s]  6%|▋         | 7/112 [00:03<00:40,  2.61it/s] 40%|████      | 45/112 [00:16<00:24,  2.78it/s]  7%|▋         | 8/112 [00:03<00:39,  2.66it/s]1_gate proxy err 0.028147483244538307 tr(WHW.T) 270.8945617675781
  0%|          | 0/112 [00:00<?, ?it/s] 41%|████      | 46/112 [00:16<00:23,  2.78it/s]  8%|▊         | 9/112 [00:03<00:38,  2.71it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s]  9%|▉         | 10/112 [00:04<00:37,  2.74it/s]I0402 08:38:20.455339 1511719 finetune.py:45] layer 2_gate initial loss 0.0006730083259753883
W0402 08:38:20.455802 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  1%|          | 1/112 [00:00<01:34,  1.17it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.79it/s] 10%|▉         | 11/112 [00:04<00:36,  2.75it/s]  2%|▏         | 2/112 [00:01<01:03,  1.74it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.78it/s] 11%|█         | 12/112 [00:04<00:36,  2.77it/s]  3%|▎         | 3/112 [00:01<00:52,  2.07it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.79it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.80it/s]W0402 08:38:21.412662 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  4%|▎         | 4/112 [00:01<00:47,  2.27it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.80it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.82it/s]  4%|▍         | 5/112 [00:02<00:44,  2.39it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.78it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.83it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.79it/s]  5%|▌         | 6/112 [00:02<00:42,  2.48it/s] 14%|█▍        | 16/112 [00:06<00:33,  2.84it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.78it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.81it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.80it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.82it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s] 50%|█████     | 56/112 [00:20<00:20,  2.79it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.81it/s]  8%|▊         | 9/112 [00:03<00:40,  2.56it/s] 51%|█████     | 57/112 [00:20<00:19,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.83it/s]  9%|▉         | 10/112 [00:04<00:39,  2.61it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.79it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.83it/s] 10%|▉         | 11/112 [00:04<00:38,  2.63it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.83it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.79it/s] 11%|█         | 12/112 [00:05<00:37,  2.66it/s]2_gate proxy err 0.029659638181328773 tr(WHW.T) 449.34307861328125
  0%|          | 0/112 [00:00<?, ?it/s] 21%|██        | 23/112 [00:08<00:31,  2.85it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.79it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.68it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.80it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.77it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.67it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.82it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.80it/s]  1%|          | 1/112 [00:00<01:31,  1.21it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.69it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.84it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.78it/s]  2%|▏         | 2/112 [00:01<01:01,  1.79it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.69it/s] 24%|██▍       | 27/112 [00:10<00:29,  2.85it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.78it/s]  3%|▎         | 3/112 [00:01<00:51,  2.11it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.70it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.87it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.79it/s]  4%|▎         | 4/112 [00:01<00:46,  2.32it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.72it/s] 26%|██▌       | 29/112 [00:10<00:28,  2.88it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.78it/s]  4%|▍         | 5/112 [00:02<00:43,  2.45it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.71it/s] 27%|██▋       | 30/112 [00:11<00:28,  2.86it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.79it/s]  5%|▌         | 6/112 [00:02<00:41,  2.53it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.72it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.85it/s] 61%|██████    | 68/112 [00:24<00:15,  2.79it/s]  6%|▋         | 7/112 [00:03<00:40,  2.59it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.72it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.85it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.78it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.72it/s] 29%|██▉       | 33/112 [00:12<00:27,  2.85it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.78it/s]  8%|▊         | 9/112 [00:03<00:38,  2.66it/s] 21%|██        | 23/112 [00:09<00:32,  2.72it/s] 30%|███       | 34/112 [00:12<00:27,  2.84it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.79it/s]  9%|▉         | 10/112 [00:04<00:38,  2.67it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.72it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.85it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.78it/s] 10%|▉         | 11/112 [00:04<00:37,  2.68it/s] 32%|███▏      | 36/112 [00:13<00:26,  2.86it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.72it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.78it/s] 11%|█         | 12/112 [00:04<00:37,  2.69it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.85it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.73it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.78it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.70it/s] 34%|███▍      | 38/112 [00:13<00:25,  2.86it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.73it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.78it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.70it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.87it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.72it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.78it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.70it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.88it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.73it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.78it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.70it/s] 37%|███▋      | 41/112 [00:15<00:24,  2.89it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.73it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.78it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.87it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.69it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.72it/s] 71%|███████   | 79/112 [00:28<00:11,  2.77it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.82it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.70it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.73it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.79it/s] 39%|███▉      | 44/112 [00:16<00:23,  2.84it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.70it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.72it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.78it/s] 40%|████      | 45/112 [00:16<00:23,  2.85it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.71it/s] 30%|███       | 34/112 [00:13<00:28,  2.73it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.78it/s] 41%|████      | 46/112 [00:16<00:23,  2.86it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.71it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.73it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.79it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.88it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.71it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.73it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.78it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.88it/s] 21%|██        | 23/112 [00:08<00:32,  2.71it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.73it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.78it/s] 44%|████▍     | 49/112 [00:17<00:21,  2.88it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.70it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.73it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.78it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.87it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.67it/s] 35%|███▍      | 39/112 [00:14<00:27,  2.68it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.78it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.87it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.62it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.78it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.65it/s] 46%|████▋     | 52/112 [00:18<00:20,  2.88it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.62it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.78it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.65it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.86it/s] 25%|██▌       | 28/112 [00:10<00:32,  2.59it/s] 80%|████████  | 90/112 [00:32<00:07,  2.78it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.86it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.62it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.78it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.61it/s] 49%|████▉     | 55/112 [00:19<00:19,  2.87it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.64it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.78it/s] 50%|█████     | 56/112 [00:20<00:19,  2.87it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.62it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.66it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.78it/s] 51%|█████     | 57/112 [00:20<00:19,  2.87it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.63it/s] 40%|████      | 45/112 [00:17<00:24,  2.69it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.87it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.78it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.65it/s] 41%|████      | 46/112 [00:17<00:24,  2.70it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.88it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.78it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.65it/s] 42%|████▏     | 47/112 [00:17<00:24,  2.70it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.86it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.79it/s] 30%|███       | 34/112 [00:13<00:29,  2.65it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.69it/s] 54%|█████▍    | 61/112 [00:22<00:17,  2.84it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.78it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.66it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.83it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.79it/s] 32%|███▏      | 36/112 [00:13<00:29,  2.60it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.64it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.80it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.78it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.61it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.65it/s] 57%|█████▋    | 64/112 [00:23<00:16,  2.83it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.78it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.62it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.66it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.85it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.78it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.62it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.66it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.85it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.78it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.61it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.64it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.86it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.78it/s] 61%|██████    | 68/112 [00:24<00:15,  2.87it/s] 37%|███▋      | 41/112 [00:15<00:27,  2.59it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.62it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.77it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.83it/s] 50%|█████     | 56/112 [00:21<00:21,  2.64it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.60it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.77it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.83it/s] 51%|█████     | 57/112 [00:21<00:20,  2.62it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.59it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.78it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.84it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.60it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.58it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.77it/s] 64%|██████▍   | 72/112 [00:25<00:14,  2.84it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.77it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.58it/s] 40%|████      | 45/112 [00:17<00:26,  2.56it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.85it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.78it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.61it/s] 41%|████      | 46/112 [00:17<00:25,  2.59it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.84it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.77it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.62it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.59it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.85it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.77it/s] 55%|█████▌    | 62/112 [00:23<00:19,  2.62it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.59it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.85it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]100%|██████████| 112/112 [00:40<00:00,  2.75it/s]
 56%|█████▋    | 63/112 [00:24<00:18,  2.63it/s] 44%|████▍     | 49/112 [00:18<00:24,  2.60it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.85it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.62it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.58it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.82it/s] 58%|█████▊    | 65/112 [00:24<00:18,  2.61it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.58it/s] 71%|███████   | 79/112 [00:28<00:11,  2.78it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.61it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.59it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.76it/s] 60%|█████▉    | 67/112 [00:25<00:17,  2.62it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.78it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.60it/s] 61%|██████    | 68/112 [00:25<00:16,  2.65it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.76it/s] 48%|████▊     | 54/112 [00:20<00:22,  2.62it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.78it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.63it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.60it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.79it/s] 62%|██████▎   | 70/112 [00:26<00:16,  2.62it/s] 50%|█████     | 56/112 [00:21<00:21,  2.60it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.82it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.63it/s] 51%|█████     | 57/112 [00:22<00:21,  2.61it/s] 77%|███████▋  | 86/112 [00:30<00:09,  2.83it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.63it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.61it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.85it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.65it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.64it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.85it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.67it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.65it/s] 79%|███████▉  | 89/112 [00:31<00:08,  2.84it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.66it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.63it/s] 80%|████████  | 90/112 [00:32<00:07,  2.85it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.66it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.63it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.86it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.65it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.63it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.86it/s]W0402 08:38:49.293000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.293000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.293000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.293000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.293000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.293000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.293000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.333000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.333000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.333000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.333000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.333000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:29<00:12,  2.66it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.64it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.87it/s]W0402 08:38:49.498000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.498000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.498000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.499000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.499000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:30<00:12,  2.66it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.87it/s]W0402 08:38:49.802000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.802000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.802000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.803000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.803000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.803000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.803000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 58%|█████▊    | 65/112 [00:25<00:17,  2.61it/s]W0402 08:38:49.832000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.833000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.833000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.833000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.833000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.904000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.904000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.904000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.904000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:38:49.904000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 71%|███████▏  | 80/112 [00:30<00:12,  2.65it/s] 85%|████████▍ | 95/112 [00:34<00:05,  2.85it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.62it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.66it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.84it/s] 60%|█████▉    | 67/112 [00:25<00:17,  2.64it/s]W0402 08:38:50.820000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:50.832000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:50.840000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:38:50.840000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:34<00:05,  2.86it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.65it/s] 61%|██████    | 68/112 [00:26<00:16,  2.63it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.87it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.66it/s]W0402 08:38:51.268000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.268000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.268000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.268000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.268000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.268000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.268000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.297000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.297000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.297000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.297000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.297000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 62%|██████▏   | 69/112 [00:26<00:16,  2.63it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.84it/s]W0402 08:38:51.587000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.587000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.587000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.587000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.587000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.587000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.587000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.587000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:31<00:10,  2.65it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.64it/s]W0402 08:38:51.866000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.866000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.866000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.866000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:38:51.866000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:35<00:04,  2.84it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.66it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.64it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.85it/s]W0402 08:38:52.270000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:38:52.274000 140620992259904 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:32<00:09,  2.63it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.62it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.82it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.65it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.62it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.82it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.67it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.65it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.84it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.66it/s] 67%|██████▋   | 75/112 [00:28<00:14,  2.64it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.83it/s] 80%|████████  | 90/112 [00:34<00:08,  2.67it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.66it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.81it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.70it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.67it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.77it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.71it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.68it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.73it/s] 83%|████████▎ | 93/112 [00:35<00:06,  2.71it/s] 71%|███████   | 79/112 [00:30<00:12,  2.70it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.72it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.71it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.68it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.71it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.70it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.68it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.70it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.72it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.70it/s]100%|██████████| 112/112 [00:40<00:00,  2.69it/s]100%|██████████| 112/112 [00:40<00:00,  2.79it/s]
 87%|████████▋ | 97/112 [00:36<00:05,  2.72it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.69it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.70it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.68it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.68it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.69it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.71it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.70it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.71it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.65it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.71it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.66it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.71it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.65it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.70it/s]I0402 08:38:59.031347 1510928 finetune.py:45] layer 0_down initial loss 0.00011914921924471855
W0402 08:38:59.031665 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 80%|████████  | 90/112 [00:34<00:08,  2.65it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.70it/s]W0402 08:38:59.523427 1510928 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 81%|████████▏ | 91/112 [00:34<00:07,  2.66it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.69it/s]0_down proxy err 0.025055058300495148 tr(WHW.T) 0.4814107120037079
 82%|████████▏ | 92/112 [00:35<00:07,  2.65it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.70it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.65it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.70it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.64it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.68it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.65it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.70it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.67it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.70it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.65it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.64it/s]
W0402 08:39:02.149000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.149000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.149000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.149000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.149000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.150000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.150000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.189000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.189000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.189000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.190000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.190000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:37<00:05,  2.63it/s]W0402 08:39:02.354000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.354000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.354000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.354000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.354000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:37<00:04,  2.62it/s]W0402 08:39:02.659000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.659000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.659000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.659000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.659000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.659000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.659000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.689000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.689000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.689000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.689000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.689000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.753000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.754000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.754000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.754000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:02.754000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:38<00:04,  2.61it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.62it/s]W0402 08:39:03.654000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:03.666000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:03.673000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:03.673000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 102/112 [00:38<00:03,  2.63it/s]W0402 08:39:04.104000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.105000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.105000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.105000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.105000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.105000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.105000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 92%|█████████▏| 103/112 [00:39<00:03,  2.63it/s]W0402 08:39:04.133000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.133000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.133000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.133000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.133000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.419000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.419000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.419000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.419000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.419000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.419000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.419000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.419000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 93%|█████████▎| 104/112 [00:39<00:03,  2.65it/s]W0402 08:39:04.713000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.713000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.713000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.713000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:04.713000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 105/112 [00:40<00:02,  2.67it/s]W0402 08:39:05.113000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:05.118000 140575274571584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:40<00:02,  2.68it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.68it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.68it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.71it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.70it/s]100%|██████████| 112/112 [00:42<00:00,  2.72it/s]100%|██████████| 112/112 [00:42<00:00,  2.62it/s]
W0402 08:39:07.938000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.938000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.938000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.938000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.938000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.938000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.939000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.985000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.985000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.985000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:07.985000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.163000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.164000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.164000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.164000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.164000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.488000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.488000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.488000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.489000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.489000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.489000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.489000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.521000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.522000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.522000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.522000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.522000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.591000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.591000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.591000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.591000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:08.591000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.522000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.527000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.533000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.534000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.954000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.954000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.954000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.954000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.954000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.955000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.955000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:09.984000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.271000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.271000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.271000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.271000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.271000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.271000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.271000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.271000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.551000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.552000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.552000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.552000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.552000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.955000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:10.960000 139837607733056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 08:39:11.584360 1512243 finetune.py:45] layer 3_down initial loss 0.0012803553836420178
W0402 08:39:11.584704 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:39:12.053286 1512243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

3_down proxy err 0.046473756432533264 tr(WHW.T) 2.1166553497314453
W0402 08:39:13.353000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.354000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.354000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.354000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.354000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.354000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.354000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.400000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.400000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.400000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.400000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.400000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.576000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.576000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.576000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.576000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.576000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.896000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.896000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.896000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.896000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.897000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.897000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.897000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.929000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.929000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.929000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.929000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.929000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:13.999000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:14.000000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:14.000000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:14.000000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:14.000000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:14.987000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:14.992000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:14.999000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:14.999000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.452000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.453000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.453000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.454000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.454000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.454000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.454000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.488000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.488000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.488000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.488000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.488000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.791000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.792000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.792000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.792000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.792000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.792000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.792000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:15.792000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:16.078000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:16.078000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:16.078000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:16.078000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:16.078000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:16.481000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:16.486000 140648985102144 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 08:39:17.974580 1511186 finetune.py:45] layer 1_down initial loss 0.0020944427233189344
W0402 08:39:17.974824 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:39:18.455233 1511186 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

1_down proxy err 0.000728182727470994 tr(WHW.T) 67.10469055175781
I0402 08:39:22.244930 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 4 in 1.2328071594238281s
I0402 08:39:22.634561 1509661 quantize_finetune_llama.py:159] layer 5 gpu 1
I0402 08:39:24.454539 1511719 finetune.py:45] layer 2_down initial loss 0.0006571190897375345
W0402 08:39:24.456273 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 08:39:24.724452 1516496 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:39:24.724651 1516496 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:39:24.724717 1516496 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:39:24.992629 1516496 config.py:58] PyTorch version 2.4.0 available.
W0402 08:39:25.506840 1511719 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

2_down proxy err 0.041928041726350784 tr(WHW.T) 1.2005820274353027
I0402 08:39:27.208901 1516496 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:39:27.586471 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:46,  1.51s/it]I0402 08:39:30.283602 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 5 in 1.170736312866211s
  6%|▋         | 2/32 [00:01<00:24,  1.22it/s]I0402 08:39:30.793036 1509661 quantize_finetune_llama.py:159] layer 6 gpu 2
  9%|▉         | 3/32 [00:02<00:17,  1.67it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.03it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.54it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.72it/s]I0402 08:39:32.382450 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 6 in 1.1709461212158203s
 25%|██▌       | 8/32 [00:03<00:08,  2.85it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.89it/s]I0402 08:39:32.792698 1517084 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:39:32.792894 1517084 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:39:32.792963 1517084 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:39:32.889354 1509661 quantize_finetune_llama.py:159] layer 7 gpu 3
I0402 08:39:33.007902 1517084 config.py:58] PyTorch version 2.4.0 available.
 31%|███▏      | 10/32 [00:04<00:07,  2.91it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.97it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.98it/s] 41%|████      | 13/32 [00:05<00:06,  3.02it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.05it/s]I0402 08:39:34.508408 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 7 in 1.180912971496582s
 47%|████▋     | 15/32 [00:06<00:05,  3.01it/s]I0402 08:39:34.904941 1517256 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:39:34.905142 1517256 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:39:34.905223 1517256 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:39:34.983363 1509661 quantize_finetune_llama.py:159] layer 8 gpu 0
 50%|█████     | 16/32 [00:06<00:05,  2.92it/s]I0402 08:39:35.225450 1517256 config.py:58] PyTorch version 2.4.0 available.
 53%|█████▎    | 17/32 [00:06<00:05,  2.95it/s]I0402 08:39:35.569201 1517084 data_utils.py:336] using 256 training seqs, 128 validation seqs
 56%|█████▋    | 18/32 [00:07<00:04,  2.97it/s]W0402 08:39:35.944404 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 19/32 [00:07<00:04,  2.99it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.04it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.08it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.11it/s]I0402 08:39:37.040426 1517488 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:39:37.040550 1517488 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:39:37.040606 1517488 utils.py:162] NumExpr defaulting to 16 threads.
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:39:37.224800 1517488 config.py:58] PyTorch version 2.4.0 available.
 72%|███████▏  | 23/32 [00:08<00:02,  3.11it/s]I0402 08:39:37.635348 1517256 data_utils.py:336] using 256 training seqs, 128 validation seqs
 75%|███████▌  | 24/32 [00:09<00:02,  3.10it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s]W0402 08:39:38.204997 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:09<00:01,  3.08it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.09it/s]  3%|▎         | 1/32 [00:01<00:49,  1.59s/it] 88%|████████▊ | 28/32 [00:10<00:01,  3.11it/s]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.11it/s]  9%|▉         | 3/32 [00:02<00:18,  1.58it/s]I0402 08:39:39.500966 1517488 data_utils.py:336] using 256 training seqs, 128 validation seqs
 94%|█████████▍| 30/32 [00:10<00:00,  3.09it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.10it/s]W0402 08:39:39.959345 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s]  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:11<00:00,  3.09it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s]  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s]  3%|▎         | 1/32 [00:01<00:49,  1.58s/it] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]  6%|▋         | 2/32 [00:01<00:25,  1.18it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.77it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s]W0402 08:39:42.534000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.535000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.535000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.535000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.535000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.535000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.535000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s]W0402 08:39:42.560000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.560000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.560000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.561000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.561000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:49,  1.60s/it] 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s]W0402 08:39:42.852000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.852000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.853000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.853000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:42.853000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.82it/s]  6%|▋         | 2/32 [00:01<00:25,  1.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.83it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.48it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.83it/s]W0402 08:39:43.680000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.680000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.680000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.680000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.680000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.680000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.680000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.699000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.699000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.699000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.699000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.699000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s]W0402 08:39:43.901000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.901000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.901000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.901000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:43.901000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.48it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.86it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.92it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.96it/s]W0402 08:39:44.986000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:44.986000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:44.986000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:44.986000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:44.986000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:44.987000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:44.987000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:45.004000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:45.004000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:45.004000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:45.004000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:45.005000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.88it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.99it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.94it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.00it/s]W0402 08:39:45.844000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:45.845000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:45.845000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:45.845000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:45.845000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.99it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.03it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.00it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.99it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 41%|████      | 13/32 [00:05<00:06,  3.03it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.05it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.02it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.06it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.99it/s] 50%|█████     | 16/32 [00:06<00:05,  3.02it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.96it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.98it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.89it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.95it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.91it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.91it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.92it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.88it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.90it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.90it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.90it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.90it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s]I0402 08:39:51.716979 1516496 finetune.py:45] layer 4_v initial loss 0.001396491308696568
W0402 08:39:51.717407 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.91it/s]W0402 08:39:52.011000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.011000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.011000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.011000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.011000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.012000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.012000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.037000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.037000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.037000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.037000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.037000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s]W0402 08:39:52.332000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.333000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.333000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.333000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:52.333000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
W0402 08:39:52.683100 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
W0402 08:39:53.190000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.190000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.190000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.190000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.191000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.191000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.191000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.209000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.209000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.209000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.209000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.209000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.415000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.415000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.415000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.415000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:53.415000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
4_v proxy err 0.03257840499281883 tr(WHW.T) 38.379119873046875
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:39:54.542000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.543000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.543000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.543000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.543000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.543000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.543000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.560000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.560000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.560000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.561000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:54.561000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:17,  1.72it/s]W0402 08:39:55.034000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.035000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.035000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.035000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.035000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.035000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.035000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.062000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.062000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.062000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.062000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.062000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.349000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.349000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.349000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.349000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.349000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:13,  2.12it/s]W0402 08:39:55.398000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.398000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.398000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.398000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.398000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.659000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.660000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.660000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.660000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.660000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.660000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.660000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.686000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.686000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.686000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.686000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.686000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s]W0402 08:39:55.973000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.973000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.973000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.973000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:55.973000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s]W0402 08:39:56.174000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.174000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.174000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.174000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.174000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.174000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.174000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.192000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.192000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.192000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.193000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.193000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.392000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.392000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.393000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.393000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.393000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.73it/s]W0402 08:39:56.815000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.816000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.816000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.816000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.816000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.816000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.816000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.835000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.835000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.835000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.835000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:56.835000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.047000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.048000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.048000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.048000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.048000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s]W0402 08:39:57.463000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.463000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.463000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.463000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.463000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.463000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.463000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.481000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.481000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.481000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.481000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:57.481000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s]W0402 08:39:58.236000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.236000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.237000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.237000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.237000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.237000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.237000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.258000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.258000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.258000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.258000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.258000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.320000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.320000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.321000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.321000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:58.321000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 08:39:59.097000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:39:59.097000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:39:59.097000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:39:59.097000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:39:59.098000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.71it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s]I0402 08:40:01.304118 1517084 finetune.py:45] layer 5_v initial loss 0.001098369131796062
W0402 08:40:01.304454 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s]W0402 08:40:02.482041 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s]5_v proxy err 0.030795998871326447 tr(WHW.T) 37.700050354003906
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s]  9%|▉         | 3/32 [00:01<00:14,  1.94it/s]I0402 08:40:05.387103 1517256 finetune.py:45] layer 6_v initial loss 0.0019264717120677233
W0402 08:40:05.387365 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
I0402 08:40:06.068315 1517488 finetune.py:45] layer 7_v initial loss 0.002091852482408285
W0402 08:40:06.068696 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s]W0402 08:40:06.787511 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s]W0402 08:40:07.612277 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:04<00:08,  2.59it/s]6_v proxy err 0.032362304627895355 tr(WHW.T) 42.837894439697266
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s]7_v proxy err 0.02701776660978794 tr(WHW.T) 53.14668655395508
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.69it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.75it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.27it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s]W0402 08:40:11.928000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:11.929000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:11.929000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:11.929000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:11.929000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:11.930000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:11.930000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s]W0402 08:40:11.959000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:11.959000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:11.959000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:11.959000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:11.959000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s]W0402 08:40:12.130000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.130000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.130000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.130000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.130000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s]W0402 08:40:12.347000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.347000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.347000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.348000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.348000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.348000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.348000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.369000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.369000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.369000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.369000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.369000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.434000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.434000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.434000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.434000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:12.435000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s]W0402 08:40:13.148000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s]W0402 08:40:13.461000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.461000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.461000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.461000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.461000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.461000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.461000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.483000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.483000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.483000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.483000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.483000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s]W0402 08:40:13.728000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.728000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.729000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.729000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:13.729000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s]W0402 08:40:14.071000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s]W0402 08:40:20.983000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:20.983000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:20.983000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:20.983000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:20.983000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:20.983000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:20.983000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
W0402 08:40:21.013000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.013000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.013000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.014000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.014000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.181000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.182000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.182000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.182000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.182000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
I0402 08:40:21.341148 1516496 finetune.py:45] layer 4_q initial loss 0.0013956482289358974
W0402 08:40:21.341391 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:40:21.409000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.409000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.409000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.409000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.409000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.409000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.409000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.433000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.433000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.433000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.433000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.433000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.500000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.500000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.500000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.500000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:21.500000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.237000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.558000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.558000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.558000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.558000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.558000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.558000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.558000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.578000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.578000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.578000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.579000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.579000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.636845 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:40:22.828000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.828000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.828000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.828000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:22.828000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:23.169000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
4_q proxy err 0.003235634882003069 tr(WHW.T) 6741.380859375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.18it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s]W0402 08:40:25.297000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.297000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.297000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.297000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.297000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.297000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.297000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.327000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.327000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.327000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.328000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.328000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.498000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.498000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.498000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.498000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.498000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:11,  2.34it/s]W0402 08:40:25.717000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.717000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.717000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.718000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.718000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.718000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.718000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.739000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.739000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.739000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.739000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.739000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.803000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.803000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.803000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.803000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:25.803000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:10,  2.50it/s]W0402 08:40:26.051000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.052000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.052000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.052000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.052000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.052000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.052000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.082000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.082000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.082000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.082000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.082000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.250000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.250000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.250000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.251000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.251000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s]W0402 08:40:26.466000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.466000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.466000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.467000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.467000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.467000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.467000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.489000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.489000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.489000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.489000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.490000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.514000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.553000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.554000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.554000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.554000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.554000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s]W0402 08:40:26.818000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.818000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.819000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.819000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.819000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.819000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.819000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.839000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.839000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.839000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.840000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:26.840000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s]W0402 08:40:27.100000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.100000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.101000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.101000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.101000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.286000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s]W0402 08:40:27.457000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.602000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.602000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.602000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.602000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.602000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.602000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.602000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.623000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.623000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.623000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.623000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.623000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s]W0402 08:40:27.871000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.872000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.872000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.872000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:40:27.872000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s]W0402 08:40:28.228000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s]I0402 08:40:29.048745 1517084 finetune.py:45] layer 5_q initial loss 0.0010992777533829212
W0402 08:40:29.049087 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s]W0402 08:40:30.192460 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s]5_q proxy err 0.0045095644891262054 tr(WHW.T) 6497.876953125
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s]  6%|▋         | 2/32 [00:01<00:17,  1.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s]I0402 08:40:34.526879 1517256 finetune.py:45] layer 6_q initial loss 0.0019228934543207288
W0402 08:40:34.527209 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:10<00:01,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s]I0402 08:40:35.184939 1517488 finetune.py:45] layer 7_q initial loss 0.0020873877219855785
W0402 08:40:35.185157 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s]W0402 08:40:35.570347 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s]W0402 08:40:36.543021 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_q proxy err 0.005076950415968895 tr(WHW.T) 6019.40673828125
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s]  3%|▎         | 1/32 [00:00<00:28,  1.11it/s]7_q proxy err 0.004902765620499849 tr(WHW.T) 6036.439453125
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s]  6%|▋         | 2/32 [00:01<00:17,  1.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s]  3%|▎         | 1/32 [00:00<00:27,  1.11it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.80it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s]  6%|▋         | 2/32 [00:01<00:17,  1.70it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.54it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.75it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.68it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 47%|████▋     | 15/32 [00:06<00:06,  2.72it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s]I0402 08:40:43.981203 1516496 finetune.py:45] layer 4_k initial loss 0.001393842976540327
W0402 08:40:43.982078 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s]W0402 08:40:45.305953 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.73it/s]4_k proxy err 0.003246287116780877 tr(WHW.T) 3941.66064453125
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.72it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s]  6%|▋         | 2/32 [00:01<00:16,  1.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.71it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.39it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.71it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
 19%|█▉        | 6/32 [00:02<00:09,  2.62it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s]I0402 08:40:49.926328 1517084 finetune.py:45] layer 5_k initial loss 0.0010981691302731633
W0402 08:40:49.926519 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:03<00:08,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.85it/s]W0402 08:40:50.977691 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s]5_k proxy err 0.004220715723931789 tr(WHW.T) 4151.982421875
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s]  3%|▎         | 1/32 [00:00<00:24,  1.26it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s]  6%|▋         | 2/32 [00:01<00:16,  1.83it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s]  9%|▉         | 3/32 [00:01<00:13,  2.14it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.83it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s]I0402 08:40:56.682785 1517256 finetune.py:45] layer 6_k initial loss 0.0019218969391658902
W0402 08:40:56.683169 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 08:40:56.722392 1517488 finetune.py:45] layer 7_k initial loss 0.00208860170096159
W0402 08:40:56.722851 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.70it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s]W0402 08:40:57.737052 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s]W0402 08:40:57.960454 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
 50%|█████     | 16/32 [00:06<00:05,  2.77it/s]6_k proxy err 0.00395787600427866 tr(WHW.T) 4418.953125
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s]7_k proxy err 0.004082172643393278 tr(WHW.T) 4611.79638671875
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s]  6%|▋         | 2/32 [00:01<00:16,  1.86it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.14it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.69it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.50it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.58it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.62it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.65it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.69it/s]I0402 08:41:04.860498 1516496 finetune.py:45] layer 4_o initial loss 0.001361953211016953
W0402 08:41:04.860698 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:05<00:06,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s]W0402 08:41:05.796679 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s]4_o proxy err 0.027277551591396332 tr(WHW.T) 1.3690094947814941
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.71it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.74it/s]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
I0402 08:41:11.161127 1517084 finetune.py:45] layer 5_o initial loss 0.0010899635963141918
W0402 08:41:11.161490 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
  9%|▉         | 3/32 [00:04<00:45,  1.55s/it]W0402 08:41:12.148304 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it]5_o proxy err 0.026357555761933327 tr(WHW.T) 1.8119544982910156
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it]  3%|▎         | 1/32 [00:01<01:00,  1.97s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it]I0402 08:41:17.855672 1517488 finetune.py:45] layer 7_o initial loss 0.0020115633960813284
W0402 08:41:17.856100 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:45,  1.57s/it]I0402 08:41:18.877006 1517256 finetune.py:45] layer 6_o initial loss 0.0018515209667384624
W0402 08:41:18.877353 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it]W0402 08:41:18.984888 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it]W0402 08:41:20.114587 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_o proxy err 0.032178279012441635 tr(WHW.T) 3.787720203399658
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it]6_o proxy err 0.03421332314610481 tr(WHW.T) 2.5981621742248535
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it]  3%|▎         | 1/32 [00:02<01:02,  2.01s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it]  3%|▎         | 1/32 [00:02<01:02,  2.01s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it]  6%|▋         | 2/32 [00:03<00:52,  1.76s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 41%|████      | 13/32 [00:19<00:26,  1.42s/it]  9%|▉         | 3/32 [00:05<00:47,  1.65s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 50%|█████     | 16/32 [00:23<00:22,  1.40s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:28,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.40s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 41%|████      | 13/32 [00:19<00:27,  1.43s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.40s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.42s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.40s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.42s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.40s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.40s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.40s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.46s/it] 41%|████      | 13/32 [00:20<00:28,  1.50s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.42s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.40s/it] 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.42s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.41s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.50s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.42s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.41s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.50s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.42s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.41s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.50s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.41s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.42s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.49s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.45s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.43s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.45s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.44s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.46s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.43s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
I0402 08:42:00.148854 1516496 finetune.py:45] layer 4_up initial loss 0.001324453391134739
W0402 08:42:00.149222 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:40<00:07,  1.43s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it]W0402 08:42:01.024565 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:41<00:05,  1.44s/it]4_up proxy err 0.04252329468727112 tr(WHW.T) 400.241455078125
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it]  3%|▎         | 1/32 [00:01<00:56,  1.83s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.44s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it]  6%|▋         | 2/32 [00:03<00:47,  1.59s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it]I0402 08:42:07.316743 1517084 finetune.py:45] layer 5_up initial loss 0.0010597507935017347
W0402 08:42:07.317129 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:47<00:00,  1.45s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
W0402 08:42:08.246950 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]5_up proxy err 0.041442591696977615 tr(WHW.T) 497.4553527832031
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it]  6%|▋         | 2/32 [00:03<00:47,  1.57s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it]  9%|▉         | 3/32 [00:04<00:43,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it]I0402 08:42:15.472323 1517488 finetune.py:45] layer 7_up initial loss 0.0019207327859476209
W0402 08:42:15.472604 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:42:16.388783 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:07<00:38,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it]7_up proxy err 0.036517851054668427 tr(WHW.T) 651.590087890625
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:42:17.688438 1517256 finetune.py:45] layer 6_up initial loss 0.0017537426901981235
W0402 08:42:17.688783 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it]W0402 08:42:18.725317 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.44s/it]6_up proxy err 0.03916439786553383 tr(WHW.T) 571.9508666992188
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:49,  1.67s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.42s/it] 41%|████      | 13/32 [00:19<00:27,  1.44s/it]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.41s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.50s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.41s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 50%|█████     | 16/32 [00:22<00:22,  1.41s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.44s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.41s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.40s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.45s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.41s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.45s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.45s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.45s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.41s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.45s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.48s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.41s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.48s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.43s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.44s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.46s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it]100%|██████████| 32/32 [00:45<00:00,  1.47s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it]I0402 08:42:56.306421 1516496 finetune.py:45] layer 4_gate initial loss 0.001253810478374362
W0402 08:42:56.306831 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it]W0402 08:42:57.061920 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it]4_gate proxy err 0.01921461522579193 tr(WHW.T) 1579.121337890625
  0%|          | 0/112 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it]  1%|          | 1/112 [00:00<01:32,  1.19it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it]  2%|▏         | 2/112 [00:01<01:01,  1.80it/s]  3%|▎         | 3/112 [00:01<00:50,  2.14it/s] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it]  4%|▎         | 4/112 [00:01<00:45,  2.35it/s]  4%|▍         | 5/112 [00:02<00:42,  2.49it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.46s/it]I0402 08:43:02.634234 1517084 finetune.py:45] layer 5_gate initial loss 0.001001545344479382
W0402 08:43:02.634564 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  5%|▌         | 6/112 [00:02<00:40,  2.60it/s]  6%|▋         | 7/112 [00:02<00:39,  2.66it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it]  7%|▋         | 8/112 [00:03<00:38,  2.70it/s]W0402 08:43:03.466612 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  8%|▊         | 9/112 [00:03<00:37,  2.74it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.46s/it]  9%|▉         | 10/112 [00:04<00:36,  2.77it/s] 10%|▉         | 11/112 [00:04<00:36,  2.76it/s] 11%|█         | 12/112 [00:04<00:36,  2.77it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s]100%|██████████| 32/32 [00:47<00:00,  1.46s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 12%|█▎        | 14/112 [00:05<00:35,  2.79it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.79it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.80it/s]5_gate proxy err 0.018376944586634636 tr(WHW.T) 1974.3070068359375
  0%|          | 0/112 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 15%|█▌        | 17/112 [00:06<00:33,  2.80it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.79it/s]  1%|          | 1/112 [00:00<01:27,  1.26it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.80it/s]  2%|▏         | 2/112 [00:01<00:58,  1.86it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.81it/s]  3%|▎         | 3/112 [00:01<00:49,  2.20it/s]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 19%|█▉        | 21/112 [00:07<00:32,  2.80it/s]  4%|▎         | 4/112 [00:01<00:44,  2.42it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s]  4%|▍         | 5/112 [00:02<00:41,  2.56it/s] 21%|██        | 23/112 [00:08<00:31,  2.80it/s]  5%|▌         | 6/112 [00:02<00:40,  2.61it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.80it/s]  6%|▋         | 7/112 [00:02<00:39,  2.69it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.80it/s]  7%|▋         | 8/112 [00:03<00:37,  2.74it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.79it/s]  8%|▊         | 9/112 [00:03<00:37,  2.78it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.79it/s]  9%|▉         | 10/112 [00:03<00:36,  2.81it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.80it/s] 10%|▉         | 11/112 [00:04<00:35,  2.83it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.79it/s] 11%|█         | 12/112 [00:04<00:35,  2.83it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.80it/s] 12%|█▏        | 13/112 [00:05<00:34,  2.83it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.79it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.84it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.79it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.85it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.79it/s] 14%|█▍        | 16/112 [00:06<00:33,  2.85it/s] 30%|███       | 34/112 [00:12<00:27,  2.79it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.85it/s]I0402 08:43:13.014503 1517488 finetune.py:45] layer 7_gate initial loss 0.0018373499624431133
W0402 08:43:13.014818 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 35/112 [00:13<00:27,  2.79it/s] 16%|█▌        | 18/112 [00:06<00:32,  2.86it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.86it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.85it/s]W0402 08:43:13.821822 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 38/112 [00:14<00:26,  2.79it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.84it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.79it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.82it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.79it/s] 21%|██        | 23/112 [00:08<00:31,  2.81it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.79it/s] 21%|██▏       | 24/112 [00:08<00:31,  2.83it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.79it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.84it/s]I0402 08:43:15.611887 1517256 finetune.py:45] layer 6_gate initial loss 0.0016983490204438567
W0402 08:43:15.612179 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 23%|██▎       | 26/112 [00:09<00:30,  2.84it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.79it/s] 24%|██▍       | 27/112 [00:09<00:29,  2.85it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.79it/s]W0402 08:43:16.407280 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 28/112 [00:10<00:29,  2.83it/s] 40%|████      | 45/112 [00:16<00:24,  2.79it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.84it/s] 41%|████      | 46/112 [00:16<00:23,  2.78it/s]7_gate proxy err 0.015042005106806755 tr(WHW.T) 2636.09716796875
  0%|          | 0/112 [00:00<?, ?it/s] 27%|██▋       | 30/112 [00:11<00:28,  2.83it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.78it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.81it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.79it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.82it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.78it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.78it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.77it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s] 30%|███       | 34/112 [00:12<00:28,  2.77it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.78it/s]  4%|▎         | 4/112 [00:02<00:48,  2.25it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.79it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.78it/s]  4%|▍         | 5/112 [00:02<00:44,  2.38it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s]6_gate proxy err 0.015454749576747417 tr(WHW.T) 2580.454833984375
  0%|          | 0/112 [00:00<?, ?it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.78it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.79it/s]  5%|▌         | 6/112 [00:02<00:42,  2.47it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.79it/s] 34%|███▍      | 38/112 [00:13<00:26,  2.81it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.78it/s]  1%|          | 1/112 [00:00<01:35,  1.17it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.83it/s] 50%|█████     | 56/112 [00:20<00:20,  2.78it/s]  7%|▋         | 8/112 [00:03<00:40,  2.56it/s]  2%|▏         | 2/112 [00:01<01:02,  1.75it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.81it/s] 51%|█████     | 57/112 [00:20<00:19,  2.78it/s]  8%|▊         | 9/112 [00:03<00:39,  2.59it/s]  3%|▎         | 3/112 [00:01<00:53,  2.05it/s] 37%|███▋      | 41/112 [00:14<00:25,  2.79it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.78it/s]  9%|▉         | 10/112 [00:04<00:39,  2.60it/s]  4%|▎         | 4/112 [00:01<00:47,  2.26it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.81it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.78it/s] 10%|▉         | 11/112 [00:04<00:38,  2.64it/s]  4%|▍         | 5/112 [00:02<00:44,  2.40it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.79it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.78it/s] 11%|█         | 12/112 [00:05<00:37,  2.66it/s]  5%|▌         | 6/112 [00:02<00:42,  2.48it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.79it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.78it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.66it/s]  6%|▋         | 7/112 [00:03<00:41,  2.52it/s] 40%|████      | 45/112 [00:16<00:23,  2.80it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.78it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.64it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s] 41%|████      | 46/112 [00:16<00:23,  2.80it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.78it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.64it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.82it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.78it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.83it/s]  9%|▉         | 10/112 [00:04<00:40,  2.54it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.78it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.64it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.84it/s] 10%|▉         | 11/112 [00:04<00:39,  2.57it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.78it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.61it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.83it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.77it/s] 11%|█         | 12/112 [00:05<00:39,  2.56it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.60it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.83it/s] 61%|██████    | 68/112 [00:24<00:15,  2.78it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.56it/s] 46%|████▋     | 52/112 [00:18<00:21,  2.84it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.78it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.85it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.59it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.77it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.55it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.83it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.59it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.78it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 49%|████▉     | 55/112 [00:19<00:20,  2.84it/s] 21%|██        | 23/112 [00:09<00:33,  2.62it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.78it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.57it/s] 50%|█████     | 56/112 [00:20<00:19,  2.84it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.63it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.77it/s] 51%|█████     | 57/112 [00:20<00:19,  2.85it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.57it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.77it/s] 22%|██▏       | 25/112 [00:09<00:33,  2.61it/s] 52%|█████▏    | 58/112 [00:20<00:19,  2.83it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.57it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.78it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.63it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.81it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.60it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.77it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.63it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.81it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.60it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.78it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.63it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.82it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.77it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.83it/s] 21%|██        | 23/112 [00:09<00:34,  2.56it/s] 71%|███████   | 79/112 [00:28<00:11,  2.77it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.64it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.84it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.58it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.78it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.62it/s] 57%|█████▋    | 64/112 [00:23<00:16,  2.83it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.77it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.58it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.60it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.83it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.77it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.59it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.83it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.61it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.78it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.58it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.84it/s] 30%|███       | 34/112 [00:13<00:30,  2.60it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.77it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.61it/s] 61%|██████    | 68/112 [00:24<00:15,  2.85it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.63it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.77it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.64it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.85it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.66it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.78it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.85it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.65it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.66it/s] 78%|███████▊  | 87/112 [00:31<00:09,  2.77it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.86it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.66it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.68it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.77it/s] 64%|██████▍   | 72/112 [00:25<00:14,  2.85it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.66it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.69it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.76it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.83it/s] 29%|██▉       | 33/112 [00:13<00:29,  2.67it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.68it/s] 80%|████████  | 90/112 [00:32<00:07,  2.78it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.84it/s] 30%|███       | 34/112 [00:13<00:29,  2.63it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.68it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.76it/s] 67%|██████▋   | 75/112 [00:26<00:13,  2.82it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.65it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.69it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.76it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.82it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.66it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.70it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.77it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.83it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.67it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.68it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.76it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.80it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.67it/s] 40%|████      | 45/112 [00:17<00:24,  2.69it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.76it/s] 71%|███████   | 79/112 [00:28<00:11,  2.80it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.67it/s] 41%|████      | 46/112 [00:17<00:24,  2.68it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.77it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.78it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.69it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.68it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.77it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.80it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.70it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.69it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.77it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.83it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.69it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.69it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.77it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.82it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.70it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.68it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.77it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.83it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.70it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.77it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.82it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.69it/s] 40%|████      | 45/112 [00:17<00:24,  2.70it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.77it/s] 77%|███████▋  | 86/112 [00:30<00:09,  2.79it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.69it/s] 41%|████      | 46/112 [00:17<00:24,  2.70it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.76it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.80it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.70it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.71it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.80it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.76it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.69it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.69it/s] 79%|███████▉  | 89/112 [00:31<00:08,  2.82it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.77it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.70it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.69it/s] 80%|████████  | 90/112 [00:32<00:07,  2.83it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.76it/s] 50%|█████     | 56/112 [00:21<00:20,  2.70it/s] 45%|████▍     | 50/112 [00:19<00:22,  2.70it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.83it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.76it/s] 51%|█████     | 57/112 [00:21<00:20,  2.69it/s] 82%|████████▏ | 92/112 [00:32<00:07,  2.84it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.70it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.77it/s] 52%|█████▏    | 58/112 [00:22<00:19,  2.70it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.85it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.71it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.77it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.72it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.85it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.67it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.76it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.71it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.83it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.76it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.67it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.69it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.82it/s]100%|██████████| 112/112 [00:40<00:00,  2.76it/s]100%|██████████| 112/112 [00:40<00:00,  2.75it/s]
 49%|████▉     | 55/112 [00:21<00:21,  2.67it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.69it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.80it/s] 50%|█████     | 56/112 [00:21<00:20,  2.67it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.69it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.76it/s] 51%|█████     | 57/112 [00:22<00:20,  2.68it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.69it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.73it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.69it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.70it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.77it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.69it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.70it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.77it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.69it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.71it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.77it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.70it/s] 61%|██████    | 68/112 [00:26<00:16,  2.71it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.78it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.70it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.71it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.80it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.70it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.72it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.81it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.70it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.71it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.82it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.69it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.71it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.83it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.70it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.71it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.84it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.69it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.70it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.85it/s] 61%|██████    | 68/112 [00:26<00:16,  2.69it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.71it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.84it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.70it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.71it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.85it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.70it/s]100%|██████████| 112/112 [00:40<00:00,  2.85it/s]100%|██████████| 112/112 [00:40<00:00,  2.79it/s]
W0402 08:43:46.424000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.425000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.425000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.425000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.425000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.425000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.425000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 77/112 [00:29<00:12,  2.70it/s]W0402 08:43:46.466000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.466000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.466000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.466000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.466000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.629000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.629000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.629000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.629000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.630000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 63%|██████▎   | 71/112 [00:27<00:15,  2.69it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s]W0402 08:43:46.941000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.941000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.941000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.941000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.941000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.942000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.942000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.975000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.975000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.975000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.975000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:46.975000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:47.046000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:47.046000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:47.046000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:47.046000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:47.046000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 64%|██████▍   | 72/112 [00:27<00:14,  2.69it/s] 71%|███████   | 79/112 [00:30<00:12,  2.70it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.69it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.70it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.70it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.70it/s]W0402 08:43:48.004000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.011000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.017000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.017000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 75/112 [00:28<00:13,  2.70it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.70it/s]W0402 08:43:48.468000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.468000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.468000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.469000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.469000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.469000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.469000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.496000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.496000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.497000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.497000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.497000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 68%|██████▊   | 76/112 [00:29<00:13,  2.69it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.70it/s]W0402 08:43:48.778000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.778000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.778000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.778000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.778000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.778000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.778000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:48.778000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 77/112 [00:29<00:12,  2.70it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.71it/s]W0402 08:43:49.057000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:49.057000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:49.057000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:49.057000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:49.057000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s] 76%|███████▌  | 85/112 [00:32<00:09,  2.72it/s]W0402 08:43:49.458000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:49.464000 140570017802048 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:30<00:12,  2.71it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.71it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.71it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.72it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.71it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.71it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.67it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.69it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.68it/s] 80%|████████  | 90/112 [00:34<00:08,  2.70it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.70it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.71it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.69it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.71it/s]W0402 08:43:52.091000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.091000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.091000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.091000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.091000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.091000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.092000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.135000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.135000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.135000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.135000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.135000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:32<00:09,  2.71it/s]W0402 08:43:52.327000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.327000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.327000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.327000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.327000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:06,  2.73it/s]W0402 08:43:52.649000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.649000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.649000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.649000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.650000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.650000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.650000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:33<00:09,  2.72it/s]W0402 08:43:52.682000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.682000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.683000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.683000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.683000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:35<00:06,  2.73it/s]W0402 08:43:52.753000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.753000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.753000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.754000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:52.754000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:33<00:08,  2.67it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.72it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.70it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.74it/s]W0402 08:43:53.727000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:53.732000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:53.739000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:53.739000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 90/112 [00:34<00:08,  2.70it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.73it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.70it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.68it/s]W0402 08:43:54.196000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.196000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.196000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.196000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.197000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.197000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.197000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.227000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.227000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.227000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.227000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.228000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.528000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.528000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.528000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.528000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.528000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.528000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.528000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.528000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:35<00:07,  2.69it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.68it/s]W0402 08:43:54.824000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.824000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.825000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.825000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:54.825000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:07,  2.68it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.68it/s]W0402 08:43:55.259000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:43:55.265000 140653971162944 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:35<00:06,  2.70it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.70it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.71it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.72it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.71it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.71it/s]I0402 08:43:56.273097 1516496 finetune.py:45] layer 4_down initial loss 0.0012194359442219138
W0402 08:43:56.273358 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 87%|████████▋ | 97/112 [00:36<00:05,  2.71it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.72it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.72it/s]W0402 08:43:56.748724 1516496 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 94%|█████████▍| 105/112 [00:39<00:02,  2.72it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.70it/s]4_down proxy err 0.04808895289897919 tr(WHW.T) 3.4171693325042725
 95%|█████████▍| 106/112 [00:40<00:02,  2.70it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.70it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.71it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.68it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.69it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.68it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.70it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.71it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.72it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.73it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.70it/s]100%|██████████| 112/112 [00:42<00:00,  2.72it/s]100%|██████████| 112/112 [00:42<00:00,  2.65it/s]
 95%|█████████▍| 106/112 [00:40<00:02,  2.71it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.66it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.63it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.62it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.60it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.60it/s]I0402 08:44:01.916452 1517084 finetune.py:45] layer 5_down initial loss 0.0009751723264344037
W0402 08:44:01.916637 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 112/112 [00:42<00:00,  2.59it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
W0402 08:44:02.389653 1517084 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

5_down proxy err 0.047181855887174606 tr(WHW.T) 4.908621311187744
W0402 08:44:05.713000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.713000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.713000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.713000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.713000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.713000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.714000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.757000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.757000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.757000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.757000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.758000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.922000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.922000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.922000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.922000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:05.922000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.225000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.225000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.225000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.225000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.225000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.225000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.225000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.257000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.257000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.258000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.258000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.258000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
I0402 08:44:06.271581 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 8 in 1.4291048049926758s
W0402 08:44:06.336000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.336000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.336000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.336000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:06.336000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
I0402 08:44:06.779212 1509661 quantize_finetune_llama.py:159] layer 9 gpu 1
W0402 08:44:07.300000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.313000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.321000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.321000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.784000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.784000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.784000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.784000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.784000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.784000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.784000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.812000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.812000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.812000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.812000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:07.812000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.095000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.095000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.095000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.095000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.095000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.096000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.096000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.096000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.283000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.283000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.284000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.284000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.284000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.284000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.284000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.323000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.323000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.323000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.323000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.323000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.384000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.384000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.385000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.385000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.385000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.493000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.493000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.493000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.493000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.493000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
I0402 08:44:08.728056 1521875 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:44:08.728185 1521875 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:44:08.728248 1521875 utils.py:162] NumExpr defaulting to 16 threads.
W0402 08:44:08.800000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.805000 140488552572736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.806000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.806000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.806000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.806000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.806000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.806000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.806000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.839000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.839000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.839000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.839000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.839000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.904000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.904000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.904000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.904000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:08.904000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
I0402 08:44:08.904900 1521875 config.py:58] PyTorch version 2.4.0 available.
W0402 08:44:09.862000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:09.875000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:09.882000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:09.882000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.303000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.304000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.304000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.304000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.304000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.304000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.304000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.333000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.333000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.333000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.333000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.333000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.633000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.633000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.633000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.633000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.634000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.634000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.634000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.634000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.934000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.934000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.934000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.934000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:10.935000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
I0402 08:44:10.979921 1521875 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:44:11.324295 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:44:11.376000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:11.381000 140412957398848 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]  9%|▉         | 3/32 [00:01<00:15,  1.83it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.50it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.71it/s] 22%|██▏       | 7/32 [00:03<00:08,  2.87it/s]I0402 08:44:15.617702 1517488 finetune.py:45] layer 7_down initial loss 0.001767211826518178
W0402 08:44:15.618044 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:08,  2.99it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.07it/s]W0402 08:44:16.193990 1517488 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 31%|███▏      | 10/32 [00:04<00:07,  3.13it/s]7_down proxy err 0.04656505584716797 tr(WHW.T) 6.803934097290039
 34%|███▍      | 11/32 [00:04<00:06,  3.17it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.17it/s] 41%|████      | 13/32 [00:05<00:05,  3.19it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.19it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.22it/s] 50%|█████     | 16/32 [00:05<00:04,  3.21it/s]I0402 08:44:18.391288 1517256 finetune.py:45] layer 6_down initial loss 0.0016270302003249526
W0402 08:44:18.391488 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:06<00:04,  3.21it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.24it/s]W0402 08:44:19.081612 1517256 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 59%|█████▉    | 19/32 [00:06<00:04,  3.24it/s]6_down proxy err 0.045230865478515625 tr(WHW.T) 6.227620601654053
 62%|██████▎   | 20/32 [00:07<00:03,  3.24it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.24it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.22it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.19it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.19it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.19it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.19it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.17it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.18it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.19it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.19it/s]I0402 08:44:22.816604 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 9 in 1.1335458755493164s
 97%|█████████▋| 31/32 [00:10<00:00,  3.18it/s]100%|██████████| 32/32 [00:11<00:00,  3.07it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]
I0402 08:44:23.323179 1509661 quantize_finetune_llama.py:159] layer 10 gpu 2
I0402 08:44:24.981400 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 10 in 1.2427880764007568s
I0402 08:44:25.357039 1522591 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:44:25.357228 1522591 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:44:25.357290 1522591 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:44:25.488327 1509661 quantize_finetune_llama.py:159] layer 11 gpu 3
I0402 08:44:25.570928 1522591 config.py:58] PyTorch version 2.4.0 available.
W0402 08:44:25.732000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.732000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.732000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.732000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.732000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.732000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.732000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.758000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.758000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.758000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.758000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:25.758000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.035000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.035000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.035000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.035000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.035000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.896000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.897000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.897000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.897000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.897000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.897000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.897000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.914000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.914000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.914000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.914000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:26.914000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:27.104000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:27.104000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:27.104000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:27.104000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:27.105000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
I0402 08:44:27.145716 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 11 in 1.2434449195861816s
I0402 08:44:27.477648 1522753 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:44:27.477857 1522753 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:44:27.477928 1522753 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:44:27.596049 1509661 quantize_finetune_llama.py:159] layer 12 gpu 0
I0402 08:44:27.787201 1522753 config.py:58] PyTorch version 2.4.0 available.
I0402 08:44:28.104680 1522591 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:44:28.229000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.229000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.229000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.229000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.229000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.229000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.229000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.247000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.248000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.248000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.248000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.248000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:28.482772 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:44:29.109000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:29.110000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:29.110000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:29.110000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:29.110000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
I0402 08:44:29.544939 1523114 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:44:29.545061 1523114 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:44:29.545116 1523114 utils.py:162] NumExpr defaulting to 16 threads.
  0%|          | 0/32 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0402 08:44:29.760997 1523114 config.py:58] PyTorch version 2.4.0 available.
I0402 08:44:30.092851 1522753 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:44:30.447998 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:49,  1.61s/it]  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s]  9%|▉         | 3/32 [00:02<00:18,  1.56it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s]I0402 08:44:32.345255 1523114 data_utils.py:336] using 256 training seqs, 128 validation seqs
 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s]W0402 08:44:32.727884 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s]  3%|▎         | 1/32 [00:01<00:48,  1.56s/it] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s]  6%|▋         | 2/32 [00:01<00:25,  1.17it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.59it/s]  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:18,  1.60it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.18it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.74it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s]I0402 08:44:35.151719 1521875 finetune.py:45] layer 8_v initial loss 0.0024042087607085705
W0402 08:44:35.152117 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:05<00:07,  2.76it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s]  3%|▎         | 1/32 [00:01<00:50,  1.63s/it] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.61it/s]  6%|▋         | 2/32 [00:01<00:26,  1.14it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.85it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.69it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.89it/s]W0402 08:44:36.173474 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:04<00:08,  2.75it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.79it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.20it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.92it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.43it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.96it/s]8_v proxy err 0.03488663211464882 tr(WHW.T) 53.42280197143555
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.84it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.94it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s]  3%|▎         | 1/32 [00:00<00:27,  1.12it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.96it/s] 50%|█████     | 16/32 [00:06<00:05,  2.91it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.98it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s]  9%|▉         | 3/32 [00:01<00:13,  2.13it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.80it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.96it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.93it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.39it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.58it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.97it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.97it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.91it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.93it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.92it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.96it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.99it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.83it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.97it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.88it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.00it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.83it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.01it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.01it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.98it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 62%|██████▎   | 20/32 [00:08<00:04,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.01it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.00it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.86it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.94it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.90it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.94it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.95it/s]100%|██████████| 32/32 [00:12<00:00,  2.96it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 56%|█████▋    | 18/32 [00:06<00:04,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.95it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.96it/s]W0402 08:44:44.443000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.444000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.444000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.444000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.444000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.444000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.444000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.471000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.471000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.471000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.471000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.472000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s]W0402 08:44:44.769000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.769000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.769000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.769000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:44.769000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:07<00:03,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.96it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.00it/s]W0402 08:44:45.590000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.590000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.590000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.590000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.590000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.590000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.590000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.608000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.608000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.608000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.608000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.608000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  3.03it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s]W0402 08:44:45.806000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.806000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.806000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.807000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:45.807000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  3.02it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
W0402 08:44:46.035000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.035000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.035000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.035000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.035000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.036000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.036000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.063000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.063000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.063000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.063000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.063000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:08<00:02,  2.85it/s]W0402 08:44:46.350000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.350000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.350000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.350000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.350000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.84it/s]W0402 08:44:46.929000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.929000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.929000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.929000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.929000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.929000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.929000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.946000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.947000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.947000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.947000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:46.947000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:09<00:01,  2.82it/s]W0402 08:44:47.212000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.212000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.212000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.212000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.212000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.212000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.212000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.231000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.231000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.231000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.231000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.231000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.434000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.434000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.434000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.435000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.435000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s]W0402 08:44:47.802000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.802000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.803000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.803000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:47.803000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.86it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 08:44:48.500000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.501000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.501000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.501000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.501000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.501000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.501000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.518000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.518000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.518000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.518000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.519000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.85it/s]W0402 08:44:48.593000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.593000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.594000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.594000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.594000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.594000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.594000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.620000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.620000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.620000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.620000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.620000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
W0402 08:44:48.907000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.907000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.907000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.907000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:48.907000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.383000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.383000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.383000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.383000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.383000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.783000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.783000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.783000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.783000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.783000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.783000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.783000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.802000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.802000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.802000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.802000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:49.802000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:50.011000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:50.012000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:50.012000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:50.012000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:50.012000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 08:44:51.169000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.170000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.171000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.171000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.171000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.171000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.171000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.188000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.188000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.188000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.188000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:51.188000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:52.046000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:52.047000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:52.047000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:52.047000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:52.047000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0402 08:44:54.092988 1522591 finetune.py:45] layer 9_v initial loss 0.0026431588921695948
W0402 08:44:54.094250 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:44:54.234000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.234000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.234000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.234000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.234000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.234000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.234000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.265000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.265000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.265000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.266000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.266000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.440000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.440000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.441000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.441000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.441000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.677000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.677000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.677000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.678000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.678000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.678000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.678000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.701000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.701000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.701000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.701000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.701000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.769000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.769000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.769000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.769000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:54.769000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.521000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.676731 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:44:55.848000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.848000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.848000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.848000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.848000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.849000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.849000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.872000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.872000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.872000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.872000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:55.872000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:44:56.134000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:44:56.134000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:44:56.134000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:44:56.134000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:44:56.134000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
I0402 08:44:56.330779 1522753 finetune.py:45] layer 10_v initial loss 0.002172774402424693
W0402 08:44:56.331505 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:44:56.489000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
9_v proxy err 0.035444267094135284 tr(WHW.T) 72.69827270507812
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:44:57.593857 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:29,  1.07it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s]10_v proxy err 0.033175814896821976 tr(WHW.T) 60.08286666870117
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:44:59.142136 1523114 finetune.py:45] layer 11_v initial loss 0.002689509652554989
W0402 08:44:59.142549 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s]W0402 08:45:00.626579 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s]11_v proxy err 0.02733604609966278 tr(WHW.T) 74.2698974609375
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s]  3%|▎         | 1/32 [00:00<00:29,  1.07it/s]I0402 08:45:02.801299 1521875 finetune.py:45] layer 8_q initial loss 0.002402673941105604
W0402 08:45:02.801568 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s]W0402 08:45:03.774996 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s]8_q proxy err 0.006373678799718618 tr(WHW.T) 5514.4345703125
  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s]  3%|▎         | 1/32 [00:00<00:26,  1.19it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s]  6%|▋         | 2/32 [00:01<00:16,  1.83it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.45it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.73it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.78it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.88it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.94it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
 41%|████      | 13/32 [00:04<00:06,  2.96it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.97it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 50%|█████     | 16/32 [00:05<00:05,  2.95it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.95it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
 56%|█████▋    | 18/32 [00:06<00:04,  2.93it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.90it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.88it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.91it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.93it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
 84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s]W0402 08:45:14.745000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.746000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.746000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.746000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.746000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.746000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.746000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.775000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.775000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.775000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.775000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.776000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.90it/s]W0402 08:45:14.954000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.954000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.954000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.954000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:14.954000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s]W0402 08:45:15.180000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.180000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.180000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.180000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.180000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.180000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.180000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.202000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.202000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.202000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.202000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.202000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.266000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.266000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.266000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.266000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:15.267000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s]W0402 08:45:15.969000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.153000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.153000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.153000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.153000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.153000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.153000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.153000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.182000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.182000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.182000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.182000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.182000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]
W0402 08:45:16.273000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.273000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.274000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.274000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.274000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.274000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.274000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.294000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.294000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.294000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.294000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.294000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.344000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.344000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.344000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.344000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.344000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.541000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.541000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.541000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.541000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.541000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.571000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.571000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.571000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.572000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.572000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.572000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.572000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.594000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.594000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.594000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.594000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.594000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.662000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.662000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.662000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.663000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.663000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:16.876000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.392000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.701000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.701000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.701000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.701000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.701000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.701000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.701000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.723000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.723000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.723000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.723000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.723000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.971000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.971000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.971000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.971000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:17.971000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:18.344000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.460000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.460000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.460000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.460000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.460000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.460000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.460000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.491000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.491000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.491000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.491000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.491000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.662000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.663000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.663000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.663000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.663000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.881000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.881000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.881000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.881000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.882000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.882000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.882000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.902000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.903000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.903000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.903000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.903000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.966000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.966000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.966000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.966000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:19.966000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:20.686000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:20.992000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:20.992000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:45:20.992000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:20.992000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:45:20.992000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:20.992000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:20.992000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.014000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.014000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.014000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.015000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.015000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.264000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.264000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.264000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.264000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.264000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:45:21.602000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0402 08:45:22.864017 1521875 finetune.py:45] layer 8_k initial loss 0.002398329321295023
W0402 08:45:22.864315 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 08:45:23.958075 1522591 finetune.py:45] layer 9_q initial loss 0.0026546968147158623
W0402 08:45:23.958261 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:45:24.036015 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:45:25.004392 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_k proxy err 0.00467942887917161 tr(WHW.T) 4670.25390625
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:45:25.279927 1522753 finetune.py:45] layer 10_q initial loss 0.0021739760413765907
W0402 08:45:25.280310 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:23,  1.32it/s]9_q proxy err 0.006731034256517887 tr(WHW.T) 5306.75244140625
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:15,  1.95it/s]W0402 08:45:26.356478 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:12,  2.27it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.50it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]10_q proxy err 0.007176572922617197 tr(WHW.T) 5567.9375
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.65it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.73it/s]  9%|▉         | 3/32 [00:01<00:14,  1.98it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.79it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s]  3%|▎         | 1/32 [00:00<00:26,  1.17it/s]I0402 08:45:28.271617 1523114 finetune.py:45] layer 11_q initial loss 0.0026909089647233486
W0402 08:45:28.271967 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:08,  2.85it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.86it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s]  9%|▉         | 3/32 [00:01<00:13,  2.16it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.88it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s]W0402 08:45:29.599043 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:10,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.90it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.58it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 41%|████      | 13/32 [00:04<00:06,  2.90it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s]11_q proxy err 0.007207292597740889 tr(WHW.T) 5157.30615234375
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.91it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 50%|█████     | 16/32 [00:05<00:05,  2.91it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s]  3%|▎         | 1/32 [00:00<00:27,  1.14it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.81it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.90it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.90it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.92it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.79it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.91it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.91it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.91it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.84it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.91it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.77it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
I0402 08:45:43.202969 1521875 finetune.py:45] layer 8_o initial loss 0.0023022249806672335
W0402 08:45:43.203350 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:45:44.345160 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_o proxy err 0.04200670123100281 tr(WHW.T) 3.732954978942871
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:45:45.851084 1522591 finetune.py:45] layer 9_k initial loss 0.00264900759793818
W0402 08:45:45.851306 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 08:45:46.533549 1522753 finetune.py:45] layer 10_k initial loss 0.002175386529415846
W0402 08:45:46.533826 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:45:46.887971 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:58,  1.90s/it]W0402 08:45:47.606822 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_k proxy err 0.005110096652060747 tr(WHW.T) 4332.2958984375
  0%|          | 0/32 [00:00<?, ?it/s]10_k proxy err 0.005165755748748779 tr(WHW.T) 4710.4462890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:03<00:48,  1.60s/it]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s]  6%|▋         | 2/32 [00:01<00:15,  1.91it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.26it/s]  9%|▉         | 3/32 [00:01<00:12,  2.25it/s]I0402 08:45:50.212072 1523114 finetune.py:45] layer 11_k initial loss 0.00269185658544302
W0402 08:45:50.212471 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:43,  1.50s/it] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.58it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s]W0402 08:45:51.456492 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s] 12%|█▎        | 4/32 [00:06<00:40,  1.46s/it] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s]11_k proxy err 0.005850715562701225 tr(WHW.T) 4189.68798828125
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 16%|█▌        | 5/32 [00:07<00:38,  1.44s/it] 38%|███▊      | 12/32 [00:05<00:07,  2.52it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.78it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s] 41%|████      | 13/32 [00:05<00:07,  2.51it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.79it/s] 19%|█▉        | 6/32 [00:08<00:36,  1.42s/it] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s] 22%|██▏       | 7/32 [00:10<00:35,  1.41s/it] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.50it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.57it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.49it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it] 34%|███▍      | 11/32 [00:04<00:08,  2.50it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.79it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.52it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.55it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.54it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.75it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.57it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 34%|███▍      | 11/32 [00:15<00:29,  1.41s/it] 69%|██████▉   | 22/32 [00:09<00:03,  2.54it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.48it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.50it/s] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 81%|████████▏ | 26/32 [00:10<00:02,  2.46it/s] 84%|████████▍ | 27/32 [00:11<00:02,  2.41it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.47it/s] 41%|████      | 13/32 [00:18<00:27,  1.42s/it] 91%|█████████ | 29/32 [00:11<00:01,  2.50it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.53it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it]I0402 08:46:07.184709 1522753 finetune.py:45] layer 10_o initial loss 0.0021189628168940544
W0402 08:46:07.184945 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:21<00:24,  1.43s/it]I0402 08:46:07.856025 1522591 finetune.py:45] layer 9_o initial loss 0.0022464205976575613
W0402 08:46:07.856256 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:46:08.198986 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:23<00:22,  1.43s/it]W0402 08:46:08.860876 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_o proxy err 0.042101576924324036 tr(WHW.T) 4.188048362731934
  0%|          | 0/32 [00:00<?, ?it/s]9_o proxy err 0.04271920770406723 tr(WHW.T) 4.439431190490723
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.42s/it]  3%|▎         | 1/32 [00:02<01:04,  2.08s/it]  6%|▋         | 2/32 [00:03<00:48,  1.60s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it]I0402 08:46:13.140359 1523114 finetune.py:45] layer 11_o initial loss 0.002450088504701853
W0402 08:46:13.140594 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:53,  1.78s/it]  9%|▉         | 3/32 [00:04<00:43,  1.52s/it]W0402 08:46:14.231339 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:28<00:17,  1.42s/it]  9%|▉         | 3/32 [00:05<00:48,  1.69s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it]11_o proxy err 0.04368627816438675 tr(WHW.T) 4.454067230224609
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.42s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 25%|██▌       | 8/32 [00:13<00:38,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.43s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.45s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.58s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.56s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.44s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.54s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 38%|███▊      | 12/32 [00:19<00:30,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.52s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.44s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.43s/it]100%|██████████| 32/32 [00:45<00:00,  1.44s/it]100%|██████████| 32/32 [00:45<00:00,  1.44s/it]
 44%|████▍     | 14/32 [00:22<00:27,  1.51s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.50s/it] 50%|█████     | 16/32 [00:23<00:22,  1.44s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 50%|█████     | 16/32 [00:25<00:23,  1.49s/it] 41%|████      | 13/32 [00:20<00:28,  1.49s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 59%|█████▉    | 19/32 [00:27<00:19,  1.46s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.47s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.48s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it]I0402 08:46:39.063425 1521875 finetune.py:45] layer 8_up initial loss 0.0021723066456615925
W0402 08:46:39.063740 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:29<00:19,  1.47s/it]W0402 08:46:39.921848 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.46s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.47s/it]8_up proxy err 0.037198394536972046 tr(WHW.T) 668.69921875
  0%|          | 0/32 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.48s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.48s/it]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it]  6%|▋         | 2/32 [00:03<00:47,  1.58s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.48s/it]  9%|▉         | 3/32 [00:04<00:43,  1.50s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.42s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.48s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.41s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.41s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.41s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.49s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.41s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.49s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it]100%|██████████| 32/32 [00:46<00:00,  1.41s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.49s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 41%|████      | 13/32 [00:18<00:27,  1.43s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.43s/it]I0402 08:47:03.242115 1522753 finetune.py:45] layer 10_up initial loss 0.002025988418608904
W0402 08:47:03.242874 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
W0402 08:47:04.140322 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:23<00:22,  1.43s/it]10_up proxy err 0.03626645728945732 tr(WHW.T) 749.0686645507812
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it]I0402 08:47:06.949459 1522591 finetune.py:45] layer 9_up initial loss 0.002185325138270855
W0402 08:47:06.949737 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:26<00:20,  1.43s/it]  3%|▎         | 1/32 [00:01<00:56,  1.81s/it]W0402 08:47:08.037328 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:03<00:47,  1.58s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it]9_up proxy err 0.035777945071458817 tr(WHW.T) 723.7196044921875
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.43s/it]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it]I0402 08:47:12.064701 1523114 finetune.py:45] layer 11_up initial loss 0.002376534277573228
W0402 08:47:12.064958 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:51,  1.72s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it]W0402 08:47:13.020229 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it]11_up proxy err 0.03604681044816971 tr(WHW.T) 789.370361328125
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.44s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.45s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.45s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.44s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.45s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 59%|█████▉    | 19/32 [00:27<00:19,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.48s/it]I0402 08:47:34.925346 1521875 finetune.py:45] layer 8_gate initial loss 0.0020374625455588102
W0402 08:47:34.925668 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:26<00:22,  1.49s/it]W0402 08:47:35.629469 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it] 66%|██████▌   | 21/32 [00:30<00:16,  1.47s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it]8_gate proxy err 0.014408105984330177 tr(WHW.T) 2931.24951171875
  0%|          | 0/112 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:29<00:19,  1.50s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.46s/it]  1%|          | 1/112 [00:00<01:29,  1.24it/s]  2%|▏         | 2/112 [00:01<00:58,  1.87it/s]  3%|▎         | 3/112 [00:01<00:49,  2.19it/s] 62%|██████▎   | 20/32 [00:30<00:18,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it]  4%|▎         | 4/112 [00:01<00:45,  2.39it/s] 75%|███████▌  | 24/32 [00:35<00:11,  1.45s/it]  4%|▍         | 5/112 [00:02<00:42,  2.54it/s]  5%|▌         | 6/112 [00:02<00:40,  2.62it/s]  6%|▋         | 7/112 [00:02<00:39,  2.67it/s] 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.47s/it]  7%|▋         | 8/112 [00:03<00:38,  2.72it/s] 78%|███████▊  | 25/32 [00:36<00:10,  1.43s/it]  8%|▊         | 9/112 [00:03<00:37,  2.75it/s]  9%|▉         | 10/112 [00:04<00:37,  2.76it/s] 10%|▉         | 11/112 [00:04<00:36,  2.78it/s] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it] 11%|█         | 12/112 [00:04<00:35,  2.78it/s] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.79it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.81it/s] 62%|██████▎   | 20/32 [00:30<00:17,  1.47s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 14%|█▍        | 16/112 [00:06<00:34,  2.80it/s] 84%|████████▍ | 27/32 [00:39<00:07,  1.43s/it] 15%|█▌        | 17/112 [00:06<00:33,  2.80it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.80it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.80it/s] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it] 18%|█▊        | 20/112 [00:07<00:32,  2.80it/s] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 19%|█▉        | 21/112 [00:07<00:32,  2.80it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s] 21%|██        | 23/112 [00:08<00:31,  2.80it/s] 69%|██████▉   | 22/32 [00:33<00:14,  1.47s/it] 21%|██▏       | 24/112 [00:09<00:31,  2.80it/s] 91%|█████████ | 29/32 [00:42<00:04,  1.42s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 22%|██▏       | 25/112 [00:09<00:31,  2.80it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.79it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.80it/s] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 25%|██▌       | 28/112 [00:10<00:30,  2.79it/s] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it] 26%|██▌       | 29/112 [00:10<00:29,  2.79it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.80it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.80it/s] 97%|█████████▋| 31/32 [00:45<00:01,  1.42s/it] 29%|██▊       | 32/112 [00:11<00:28,  2.79it/s] 75%|███████▌  | 24/32 [00:36<00:11,  1.47s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it] 29%|██▉       | 33/112 [00:12<00:28,  2.80it/s] 30%|███       | 34/112 [00:12<00:27,  2.80it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.79it/s]100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.50s/it] 33%|███▎      | 37/112 [00:13<00:26,  2.79it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.78it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.78it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.80it/s] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it] 37%|███▋      | 41/112 [00:15<00:25,  2.79it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.78it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.80it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.79it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 40%|████      | 45/112 [00:16<00:24,  2.79it/s] 94%|█████████▍| 30/32 [00:45<00:02,  1.50s/it] 41%|████      | 46/112 [00:16<00:23,  2.80it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.80it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.77it/s] 88%|████████▊ | 28/32 [00:41<00:05,  1.46s/it] 44%|████▍     | 49/112 [00:17<00:22,  2.79it/s] 97%|█████████▋| 31/32 [00:47<00:01,  1.49s/it] 45%|████▍     | 50/112 [00:18<00:22,  2.80it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.78it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.78it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.46s/it] 47%|████▋     | 53/112 [00:19<00:21,  2.79it/s]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 48%|████▊     | 54/112 [00:19<00:20,  2.79it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.77it/s] 50%|█████     | 56/112 [00:20<00:20,  2.78it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 51%|█████     | 57/112 [00:20<00:19,  2.79it/s]I0402 08:47:59.457126 1522753 finetune.py:45] layer 10_gate initial loss 0.0019483600044623017
W0402 08:47:59.457460 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 52%|█████▏    | 58/112 [00:21<00:19,  2.77it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.79it/s]W0402 08:48:00.183781 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 54%|█████▎    | 60/112 [00:21<00:18,  2.79it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 54%|█████▍    | 61/112 [00:22<00:18,  2.78it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.78it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.79it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.78it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.78it/s]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 59%|█████▉    | 66/112 [00:24<00:16,  2.79it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.78it/s] 61%|██████    | 68/112 [00:24<00:15,  2.78it/s]10_gate proxy err 0.014053546823561192 tr(WHW.T) 3045.35498046875
  0%|          | 0/112 [00:00<?, ?it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.79it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.78it/s]  1%|          | 1/112 [00:00<01:30,  1.23it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.78it/s]  2%|▏         | 2/112 [00:01<00:59,  1.84it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.78it/s]  3%|▎         | 3/112 [00:01<00:49,  2.19it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.78it/s]  4%|▎         | 4/112 [00:01<00:44,  2.41it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.78it/s]  4%|▍         | 5/112 [00:02<00:42,  2.54it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.78it/s]  5%|▌         | 6/112 [00:02<00:40,  2.60it/s]I0402 08:48:05.940229 1522591 finetune.py:45] layer 9_gate initial loss 0.00209055352024734
W0402 08:48:05.940575 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 68%|██████▊   | 76/112 [00:27<00:12,  2.78it/s]  6%|▋         | 7/112 [00:02<00:39,  2.68it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.78it/s]  7%|▋         | 8/112 [00:03<00:38,  2.72it/s]W0402 08:48:06.854122 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 70%|██████▉   | 78/112 [00:28<00:12,  2.78it/s]  8%|▊         | 9/112 [00:03<00:37,  2.76it/s] 71%|███████   | 79/112 [00:28<00:11,  2.78it/s]  9%|▉         | 10/112 [00:04<00:36,  2.79it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.78it/s] 10%|▉         | 11/112 [00:04<00:36,  2.79it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.78it/s] 11%|█         | 12/112 [00:04<00:35,  2.81it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.78it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.82it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.78it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.79it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.78it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.80it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.77it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.79it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.78it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.79it/s]9_gate proxy err 0.013912508264183998 tr(WHW.T) 3179.91943359375
  0%|          | 0/112 [00:00<?, ?it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.78it/s]I0402 08:48:10.112004 1523114 finetune.py:45] layer 11_gate initial loss 0.0022483717184513807
W0402 08:48:10.112232 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 18/112 [00:06<00:33,  2.78it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.77it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.79it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.79it/s]W0402 08:48:10.890124 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  1%|          | 1/112 [00:00<01:34,  1.17it/s] 80%|████████  | 90/112 [00:32<00:07,  2.78it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.80it/s]  2%|▏         | 2/112 [00:01<01:03,  1.73it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.77it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.79it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.77it/s] 21%|██        | 23/112 [00:08<00:31,  2.80it/s]  4%|▎         | 4/112 [00:02<00:48,  2.21it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.78it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.81it/s]  4%|▍         | 5/112 [00:02<00:46,  2.31it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.77it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.80it/s]  5%|▌         | 6/112 [00:02<00:44,  2.38it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.78it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.78it/s]  6%|▋         | 7/112 [00:03<00:42,  2.48it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.78it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.80it/s]  7%|▋         | 8/112 [00:03<00:41,  2.53it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.78it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.80it/s]  8%|▊         | 9/112 [00:03<00:40,  2.57it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.78it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.81it/s]11_gate proxy err 0.013621768914163113 tr(WHW.T) 3167.24609375
  0%|          | 0/112 [00:00<?, ?it/s]  9%|▉         | 10/112 [00:04<00:39,  2.60it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.78it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.80it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.78it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.80it/s] 10%|▉         | 11/112 [00:04<00:38,  2.60it/s]  1%|          | 1/112 [00:00<01:30,  1.23it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.77it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.81it/s] 11%|█         | 12/112 [00:05<00:38,  2.62it/s]  2%|▏         | 2/112 [00:01<01:00,  1.81it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.82it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.78it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.62it/s]  3%|▎         | 3/112 [00:01<00:51,  2.13it/s] 30%|███       | 34/112 [00:12<00:27,  2.82it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.76it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.62it/s]  4%|▎         | 4/112 [00:01<00:46,  2.33it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.82it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.78it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s]  4%|▍         | 5/112 [00:02<00:44,  2.41it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.80it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.78it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s]  5%|▌         | 6/112 [00:02<00:43,  2.46it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.78it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.76it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.57it/s]  6%|▋         | 7/112 [00:03<00:41,  2.53it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.77it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.77it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.59it/s]  7%|▋         | 8/112 [00:03<00:40,  2.58it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.77it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.78it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.61it/s]  8%|▊         | 9/112 [00:03<00:39,  2.63it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.79it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.76it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.62it/s]  9%|▉         | 10/112 [00:04<00:38,  2.65it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.79it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.77it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.62it/s] 10%|▉         | 11/112 [00:04<00:37,  2.67it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.80it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.77it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.63it/s] 11%|█         | 12/112 [00:04<00:37,  2.68it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.80it/s]100%|██████████| 112/112 [00:40<00:00,  2.77it/s]100%|██████████| 112/112 [00:40<00:00,  2.76it/s]
 21%|██        | 23/112 [00:09<00:33,  2.64it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.70it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.80it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.71it/s] 40%|████      | 45/112 [00:16<00:24,  2.75it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.63it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.69it/s] 41%|████      | 46/112 [00:16<00:24,  2.72it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.63it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.69it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.75it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.62it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.68it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.73it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.76it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.63it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.69it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.77it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.69it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 46%|████▌     | 51/112 [00:18<00:22,  2.74it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.65it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.60it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.71it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.58it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.54it/s] 47%|████▋     | 53/112 [00:19<00:22,  2.58it/s] 29%|██▊       | 32/112 [00:12<00:32,  2.48it/s] 20%|█▉        | 22/112 [00:08<00:36,  2.48it/s] 48%|████▊     | 54/112 [00:19<00:22,  2.58it/s] 21%|██        | 23/112 [00:09<00:35,  2.49it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.49it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.60it/s] 21%|██▏       | 24/112 [00:09<00:35,  2.51it/s] 30%|███       | 34/112 [00:13<00:31,  2.48it/s] 50%|█████     | 56/112 [00:20<00:21,  2.63it/s] 22%|██▏       | 25/112 [00:09<00:33,  2.56it/s] 31%|███▏      | 35/112 [00:13<00:30,  2.53it/s] 51%|█████     | 57/112 [00:21<00:20,  2.69it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.56it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.64it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.58it/s]W0402 08:48:24.896000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.896000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.896000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.896000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.897000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.897000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.897000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.936000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.936000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.936000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.936000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:24.936000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 59/112 [00:21<00:19,  2.75it/s]W0402 08:48:25.100000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.100000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.100000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.100000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.100000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 28/112 [00:11<00:31,  2.66it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.60it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.78it/s]W0402 08:48:25.399000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.399000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.399000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.399000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.399000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.400000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.400000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.430000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.430000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.430000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.431000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.431000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.497000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.497000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.497000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.497000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:25.497000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 26%|██▌       | 29/112 [00:11<00:31,  2.67it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.61it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.78it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.70it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.77it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.59it/s] 37%|███▋      | 41/112 [00:16<00:29,  2.45it/s]W0402 08:48:26.452000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.465000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.474000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.474000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 63/112 [00:23<00:18,  2.71it/s] 29%|██▊       | 32/112 [00:12<00:31,  2.52it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.71it/s] 38%|███▊      | 42/112 [00:16<00:28,  2.44it/s]W0402 08:48:26.925000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.926000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.926000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.926000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.926000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.926000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.926000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.954000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.954000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.954000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.954000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:26.954000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 29%|██▉       | 33/112 [00:12<00:30,  2.55it/s] 58%|█████▊    | 65/112 [00:23<00:17,  2.73it/s]W0402 08:48:27.239000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.239000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.239000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.239000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.240000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.240000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.240000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.240000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 43/112 [00:17<00:27,  2.48it/s] 30%|███       | 34/112 [00:13<00:30,  2.59it/s]W0402 08:48:27.523000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.524000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.524000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.524000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:27.524000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 39%|███▉      | 44/112 [00:17<00:27,  2.52it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.59it/s]W0402 08:48:27.929000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 60%|█████▉    | 67/112 [00:24<00:16,  2.76it/s]W0402 08:48:27.934000 140609460860736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 40%|████      | 45/112 [00:17<00:26,  2.52it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 61%|██████    | 68/112 [00:25<00:15,  2.76it/s] 41%|████      | 46/112 [00:18<00:25,  2.54it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.77it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.56it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.78it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.63it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.58it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.79it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.66it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.60it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.77it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.68it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.62it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.70it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.73it/s] 46%|████▌     | 51/112 [00:20<00:22,  2.65it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.70it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.70it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.66it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.72it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.68it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.68it/s] 39%|███▉      | 44/112 [00:17<00:24,  2.74it/s] 68%|██████▊   | 76/112 [00:27<00:13,  2.67it/s] 48%|████▊     | 54/112 [00:21<00:21,  2.68it/s] 40%|████      | 45/112 [00:17<00:24,  2.74it/s] 69%|██████▉   | 77/112 [00:28<00:13,  2.66it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.68it/s] 41%|████      | 46/112 [00:17<00:24,  2.75it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.66it/s] 50%|█████     | 56/112 [00:22<00:20,  2.68it/s] 42%|████▏     | 47/112 [00:18<00:23,  2.74it/s] 71%|███████   | 79/112 [00:29<00:12,  2.65it/s] 51%|█████     | 57/112 [00:22<00:20,  2.69it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.74it/s] 71%|███████▏  | 80/112 [00:29<00:12,  2.65it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.69it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.76it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.65it/s] 53%|█████▎    | 59/112 [00:23<00:19,  2.70it/s] 45%|████▍     | 50/112 [00:19<00:22,  2.76it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.64it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.69it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.74it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.64it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.69it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.75it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.64it/s] 55%|█████▌    | 62/112 [00:24<00:18,  2.69it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.76it/s] 76%|███████▌  | 85/112 [00:31<00:10,  2.64it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.69it/s]I0402 08:48:34.799459 1521875 finetune.py:45] layer 8_down initial loss 0.0019590866286307573
W0402 08:48:34.799710 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 48%|████▊     | 54/112 [00:20<00:21,  2.74it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.68it/s] 57%|█████▋    | 64/112 [00:25<00:17,  2.68it/s] 49%|████▉     | 55/112 [00:21<00:20,  2.73it/s]W0402 08:48:35.277154 1521875 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 78%|███████▊  | 87/112 [00:32<00:09,  2.73it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.67it/s] 50%|█████     | 56/112 [00:21<00:20,  2.72it/s]8_down proxy err 0.046931806951761246 tr(WHW.T) 7.259359359741211
 79%|███████▊  | 88/112 [00:32<00:08,  2.75it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.66it/s] 51%|█████     | 57/112 [00:21<00:20,  2.72it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.78it/s] 60%|█████▉    | 67/112 [00:26<00:16,  2.66it/s] 52%|█████▏    | 58/112 [00:22<00:19,  2.72it/s] 80%|████████  | 90/112 [00:33<00:07,  2.77it/s] 61%|██████    | 68/112 [00:26<00:16,  2.66it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.72it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.78it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.65it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.71it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.76it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.65it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.72it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.73it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.65it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.74it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.66it/s] 56%|█████▋    | 63/112 [00:24<00:17,  2.73it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.75it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.73it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.66it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.76it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.71it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.65it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.77it/s] 59%|█████▉    | 66/112 [00:25<00:16,  2.71it/s] 67%|██████▋   | 75/112 [00:29<00:13,  2.65it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.79it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.72it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.65it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.80it/s] 61%|██████    | 68/112 [00:25<00:16,  2.73it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.66it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.80it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.73it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.78it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.66it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.73it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.80it/s] 71%|███████   | 79/112 [00:30<00:12,  2.65it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.74it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.81it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.66it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.74it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.82it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.66it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.73it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.80it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.65it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.81it/s] 66%|██████▌   | 74/112 [00:28<00:13,  2.73it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.62it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.82it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.73it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.64it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.81it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.73it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.64it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.78it/s] 69%|██████▉   | 77/112 [00:29<00:12,  2.72it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.64it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.80it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.73it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.63it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.80it/s] 71%|███████   | 79/112 [00:29<00:12,  2.72it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.64it/s]100%|██████████| 112/112 [00:41<00:00,  2.82it/s]100%|██████████| 112/112 [00:41<00:00,  2.73it/s]
 71%|███████▏  | 80/112 [00:30<00:11,  2.72it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.66it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 80%|████████  | 90/112 [00:34<00:08,  2.65it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.72it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.65it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.71it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.66it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.71it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.65it/s] 76%|███████▌  | 85/112 [00:32<00:09,  2.72it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.66it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.72it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.66it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.72it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.65it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.72it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.66it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.72it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.66it/s] 80%|████████  | 90/112 [00:33<00:08,  2.72it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.66it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.72it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.66it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.69it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.66it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.69it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.65it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.70it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.65it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.71it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.66it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.72it/s]W0402 08:48:50.380000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.380000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.381000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.381000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.381000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.381000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.381000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.425000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.425000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.426000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.426000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.426000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 105/112 [00:40<00:02,  2.65it/s]W0402 08:48:50.589000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.589000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.589000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.589000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.589000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:36<00:05,  2.71it/s]W0402 08:48:50.890000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.890000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.890000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.890000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.890000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.890000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.890000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.923000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.923000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.923000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.923000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.923000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:40<00:02,  2.66it/s]W0402 08:48:50.988000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.988000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.988000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.988000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:50.989000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:36<00:05,  2.72it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.65it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.71it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.66it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.72it/s]W0402 08:48:51.918000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:51.932000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:51.939000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:51.939000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:41<00:01,  2.66it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.73it/s]W0402 08:48:52.370000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.370000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.370000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.370000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.370000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.370000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.370000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.397000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.397000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.397000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.397000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.397000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:42<00:00,  2.66it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.72it/s]W0402 08:48:52.685000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.686000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.686000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.686000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.686000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.686000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.686000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.686000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:42<00:00,  2.66it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.72it/s]W0402 08:48:52.962000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.962000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.963000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.963000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:52.963000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 93%|█████████▎| 104/112 [00:39<00:02,  2.71it/s]100%|██████████| 112/112 [00:43<00:00,  2.64it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]
W0402 08:48:53.361000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:53.366000 140139396437824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 105/112 [00:39<00:02,  2.71it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.67it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.64it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.65it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.67it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.66it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.69it/s]100%|██████████| 112/112 [00:42<00:00,  2.72it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]
W0402 08:48:59.194000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.194000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.194000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.195000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.195000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.195000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.195000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.236000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.236000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.236000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.236000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.237000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.402000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.402000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.402000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.402000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.402000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.703000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.703000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.703000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.704000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.704000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.704000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.704000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.734000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.734000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.734000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.735000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.735000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.800000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.800000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.800000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.800000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:48:59.800000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
I0402 08:49:00.221059 1522753 finetune.py:45] layer 10_down initial loss 0.0018708858406171203
W0402 08:49:00.221418 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:49:00.685813 1522753 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 08:49:00.708000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:00.723000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:00.731000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:00.731000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
10_down proxy err 0.046746838837862015 tr(WHW.T) 8.421324729919434
W0402 08:49:01.172000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.172000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.172000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.172000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.172000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.172000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.172000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.200000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.200000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.200000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.200000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.200000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.481000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.481000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.481000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.482000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.482000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.482000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.482000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.482000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.759000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.759000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.759000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.760000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:01.760000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.040000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.040000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.040000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.040000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.041000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.041000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.041000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.081000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.081000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.081000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.081000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.081000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.165000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.170000 139816813610816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.250000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.250000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.251000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.251000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.251000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.554000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.554000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.554000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.554000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.555000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.555000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.555000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.585000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.585000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.585000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.585000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.585000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.653000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.653000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.653000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.653000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:02.653000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:03.585000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:03.598000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:03.605000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:03.606000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.058000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.058000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.058000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.058000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.058000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.058000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.058000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.089000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.089000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.089000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.089000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.089000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.410000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.410000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.411000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.411000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.411000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.411000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.411000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.411000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.712000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.712000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.712000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.712000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:04.712000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:05.150000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:05.155000 140104995858240 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 08:49:09.090837 1522591 finetune.py:45] layer 9_down initial loss 0.0020018909126520157
W0402 08:49:09.091011 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:49:09.589121 1522591 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

9_down proxy err 0.046795982867479324 tr(WHW.T) 8.00308609008789
I0402 08:49:12.085635 1523114 finetune.py:45] layer 11_down initial loss 0.0021638423204421997
W0402 08:49:12.085901 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:49:12.536464 1523114 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

11_down proxy err 0.04532122239470482 tr(WHW.T) 9.07177734375
I0402 08:49:13.460147 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 12 in 1.3713123798370361s
I0402 08:49:13.989521 1509661 quantize_finetune_llama.py:159] layer 13 gpu 1
I0402 08:49:15.596590 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 13 in 1.157951831817627s
I0402 08:49:16.058539 1527566 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:49:16.058801 1527566 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:49:16.058874 1527566 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:49:16.098128 1509661 quantize_finetune_llama.py:159] layer 14 gpu 2
I0402 08:49:16.269677 1527566 config.py:58] PyTorch version 2.4.0 available.
I0402 08:49:17.919868 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 14 in 1.4016096591949463s
I0402 08:49:18.036923 1527725 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:49:18.037101 1527725 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:49:18.037161 1527725 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:49:18.351719 1527725 config.py:58] PyTorch version 2.4.0 available.
I0402 08:49:18.376101 1509661 quantize_finetune_llama.py:159] layer 15 gpu 3
I0402 08:49:18.729544 1527566 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:49:19.142672 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 08:49:20.046327 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 15 in 1.2427222728729248s
I0402 08:49:20.355642 1528094 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:49:20.355784 1528094 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:49:20.355902 1528094 utils.py:162] NumExpr defaulting to 16 threads.
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:49:20.500805 1509661 quantize_finetune_llama.py:159] layer 16 gpu 0
I0402 08:49:20.757535 1528094 config.py:58] PyTorch version 2.4.0 available.
I0402 08:49:20.899067 1527725 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:49:21.505301 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:48,  1.56s/it]  6%|▋         | 2/32 [00:01<00:24,  1.21it/s]  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:49:22.579445 1528595 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:49:22.579607 1528595 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:49:22.579673 1528595 utils.py:162] NumExpr defaulting to 16 threads.
  9%|▉         | 3/32 [00:02<00:17,  1.68it/s]I0402 08:49:22.795009 1528595 config.py:58] PyTorch version 2.4.0 available.
 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s]I0402 08:49:23.071242 1528094 data_utils.py:336] using 256 training seqs, 128 validation seqs
 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s]W0402 08:49:23.495124 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:03<00:10,  2.58it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.73it/s]  3%|▎         | 1/32 [00:01<00:49,  1.59s/it] 25%|██▌       | 8/32 [00:03<00:08,  2.89it/s]  6%|▋         | 2/32 [00:01<00:25,  1.18it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.98it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s]  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.92it/s]I0402 08:49:24.999207 1528595 data_utils.py:336] using 256 training seqs, 128 validation seqs
 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.90it/s]W0402 08:49:25.320095 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.92it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.48it/s] 41%|████      | 13/32 [00:05<00:06,  2.93it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s]  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s]  3%|▎         | 1/32 [00:01<00:51,  1.66s/it] 47%|████▋     | 15/32 [00:06<00:05,  2.95it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.89it/s]  6%|▋         | 2/32 [00:01<00:26,  1.14it/s] 50%|█████     | 16/32 [00:06<00:05,  2.92it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.96it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.01it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.96it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.93it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.01it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.93it/s]  3%|▎         | 1/32 [00:01<00:50,  1.64s/it] 41%|████      | 13/32 [00:05<00:06,  3.01it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.05it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.95it/s]  9%|▉         | 3/32 [00:02<00:17,  1.62it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.07it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.93it/s] 50%|█████     | 16/32 [00:06<00:05,  3.08it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.82it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.04it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.84it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.48it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.94it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.05it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.95it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.07it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.95it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.98it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.09it/s] 41%|████      | 13/32 [00:05<00:06,  2.98it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.00it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.06it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.98it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.94it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.01it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.08it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.00it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.99it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.01it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.10it/s] 50%|█████     | 16/32 [00:06<00:05,  3.02it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.03it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.12it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.00it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.03it/s] 41%|████      | 13/32 [00:05<00:06,  3.07it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.13it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.04it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.10it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.12it/s]100%|██████████| 32/32 [00:11<00:00,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
 59%|█████▉    | 19/32 [00:07<00:04,  3.05it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.11it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.12it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.02it/s] 50%|█████     | 16/32 [00:06<00:05,  3.09it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.06it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.97it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.04it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.01it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.00it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.94it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.89it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.92it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 66%|██████▌   | 21/32 [00:08<00:03,  2.88it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s]W0402 08:49:34.936000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:34.936000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:34.936000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:34.936000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:34.936000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:34.937000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:34.937000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s]W0402 08:49:34.964000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:34.964000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:34.964000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:34.964000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:34.964000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:35.258000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:35.258000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:35.258000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:35.258000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:35.258000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:08<00:03,  2.91it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.92it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s]W0402 08:49:36.095000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.095000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.095000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.095000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.095000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.095000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.095000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.114000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.114000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.114000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.114000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.114000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.89it/s]W0402 08:49:36.315000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.315000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.315000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.315000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.315000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.91it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.92it/s]W0402 08:49:36.838000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.838000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.838000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.838000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.838000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.839000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.839000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.864000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.864000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.864000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.864000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:36.864000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.94it/s]100%|██████████| 32/32 [00:12<00:00,  2.97it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
W0402 08:49:37.145000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.145000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.145000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.145000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.145000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s]W0402 08:49:37.394000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.394000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.394000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.395000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.395000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.395000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.395000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.412000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.412000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.412000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.412000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:37.412000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.92it/s]W0402 08:49:38.001000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.001000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.001000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.001000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.001000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.001000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.002000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s]W0402 08:49:38.020000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.020000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.020000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.020000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.020000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.227000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.227000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.227000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.227000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.227000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.284000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.284000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.284000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.285000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:38.285000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 08:49:39.366000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.366000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.366000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.366000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.366000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.366000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.366000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.384000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.384000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.384000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.384000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.385000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.468000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.468000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.468000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.468000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.468000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.468000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.468000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.495000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.496000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.496000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.496000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.496000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.795000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.795000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.795000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.795000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:39.795000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.272000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.272000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.272000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.272000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.272000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.659000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.659000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.659000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.659000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.659000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.659000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.660000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.678000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.678000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.678000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.678000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.678000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.881000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.881000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.881000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.881000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:40.881000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 08:49:41.126000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.126000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.126000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.126000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.126000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.127000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.127000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.156000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.156000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.156000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.156000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.156000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.455000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.455000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.455000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.455000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:41.455000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.010000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.010000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.010000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.010000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.010000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.010000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.010000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.029000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.029000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.029000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.029000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.029000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.330000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.330000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.330000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.330000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.331000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.331000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.331000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.349000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.349000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.349000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.349000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.349000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.559000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.560000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.560000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.560000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.560000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.926000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.926000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.927000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.927000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:42.927000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 08:49:43.718000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.719000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.719000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.719000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.719000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.719000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.719000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.738000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.738000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.738000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.738000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:43.738000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:49:44.619000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:49:44.619000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:49:44.620000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:49:44.620000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:49:44.620000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
I0402 08:49:45.240032 1527566 finetune.py:45] layer 12_v initial loss 0.002689361572265625
W0402 08:49:45.240406 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 08:49:46.235581 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 08:49:46.701391 1527725 finetune.py:45] layer 13_v initial loss 0.0038573783822357655
W0402 08:49:46.701761 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

12_v proxy err 0.03501548990607262 tr(WHW.T) 68.45219421386719
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:49:47.971294 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s]13_v proxy err 0.036613211035728455 tr(WHW.T) 71.04966735839844
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s]I0402 08:49:49.306360 1528094 finetune.py:45] layer 14_v initial loss 0.003027624450623989
W0402 08:49:49.306706 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.68it/s]W0402 08:49:50.432606 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:18,  1.61it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s]14_v proxy err 0.03354126587510109 tr(WHW.T) 74.802978515625
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s]I0402 08:49:51.808891 1528595 finetune.py:45] layer 15_v initial loss 0.0038989633321762085
W0402 08:49:51.810223 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.82it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s]W0402 08:49:53.474636 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.27it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.82it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s]15_v proxy err 0.04364098981022835 tr(WHW.T) 67.86465454101562
  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]  6%|▋         | 2/32 [00:01<00:17,  1.67it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.78it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.82it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.82it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s]W0402 08:50:04.420000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.421000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.421000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.421000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.421000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.421000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.421000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.449000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.449000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.449000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.449000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.449000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.612000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.612000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.612000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.612000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.612000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:10<00:02,  2.73it/s]W0402 08:50:04.828000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.828000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.828000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.828000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.828000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.829000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.829000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.851000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.851000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.851000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.851000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.851000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.914000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.914000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.914000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.914000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:04.915000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s]W0402 08:50:05.617000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s]W0402 08:50:05.920000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.920000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.920000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.920000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.920000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.920000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.921000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.942000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.942000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.942000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.942000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:05.942000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s]W0402 08:50:06.172000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.172000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.172000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.172000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.173000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.173000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.173000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.188000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.188000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.188000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.188000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.188000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.202000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.202000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.202000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.202000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.202000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.368000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.368000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.368000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.368000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.368000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s]W0402 08:50:06.528000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.586000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.586000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.586000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.586000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.586000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.586000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.586000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.609000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.609000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.609000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.609000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.609000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.677000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.677000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.677000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.677000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:06.677000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0402 08:50:07.400000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.712000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.712000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.712000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.712000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.712000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.712000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.712000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.734000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.734000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.734000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.734000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.734000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.977000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.977000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.977000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.977000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:07.977000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.240000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.240000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.240000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.240000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.240000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.240000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.240000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.272000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.273000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.273000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.273000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.273000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.343000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.452000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.452000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.452000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.452000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.452000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.684000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.685000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.685000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.685000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.685000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.685000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.685000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.708000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.708000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.708000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.708000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.709000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.775000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.775000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.775000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.775000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:08.775000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.516000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.837000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.837000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.837000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.837000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.837000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.837000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.838000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.859000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.859000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.860000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.860000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:09.860000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:10.120000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:10.120000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:10.120000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:10.121000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:10.121000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:10.481000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.840000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.840000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.840000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.840000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.840000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.840000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.840000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.869000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.869000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.870000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.870000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:11.870000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.038000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.038000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.038000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.039000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.039000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.257000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.257000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.257000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.257000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.257000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.257000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.258000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.278000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.278000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.278000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.278000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.279000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.342000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.342000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.342000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.342000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:12.342000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.053000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.355000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.356000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.356000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.356000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.356000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.356000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.356000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.377000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.377000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.377000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.377000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.377000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.626000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.626000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.626000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.626000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:50:13.626000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
I0402 08:50:13.765028 1527566 finetune.py:45] layer 12_q initial loss 0.0026875955518335104
W0402 08:50:13.765249 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:50:13.962000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0402 08:50:14.659592 1527725 finetune.py:45] layer 13_q initial loss 0.0038560989778488874
W0402 08:50:14.659782 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:50:14.728023 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:50:15.722562 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_q proxy err 0.005038645584136248 tr(WHW.T) 6419.8271484375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.17it/s]13_q proxy err 0.007955142296850681 tr(WHW.T) 5299.77734375
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:50:16.842196 1528094 finetune.py:45] layer 14_q initial loss 0.003028196282684803
W0402 08:50:16.842397 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:01<00:16,  1.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.17it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.39it/s]  3%|▎         | 1/32 [00:00<00:27,  1.13it/s]W0402 08:50:17.937737 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:10,  2.55it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s]14_q proxy err 0.007160615641623735 tr(WHW.T) 5552.1181640625
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.83it/s]  3%|▎         | 1/32 [00:00<00:27,  1.13it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.85it/s]  6%|▋         | 2/32 [00:01<00:17,  1.72it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s]I0402 08:50:20.762924 1528595 finetune.py:45] layer 15_q initial loss 0.0039104013703763485
W0402 08:50:20.763159 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:05<00:06,  2.85it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.31it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.84it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.86it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s]W0402 08:50:22.052229 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.82it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.69it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s]15_q proxy err 0.007206588983535767 tr(WHW.T) 6709.19140625
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.83it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.78it/s]  3%|▎         | 1/32 [00:00<00:26,  1.15it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s]  6%|▋         | 2/32 [00:01<00:17,  1.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.82it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.88it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.88it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.84it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.88it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.88it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.82it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.90it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.89it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.88it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 91%|█████████ | 29/32 [00:10<00:01,  2.88it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.85it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.72it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
I0402 08:50:35.181199 1527725 finetune.py:45] layer 13_k initial loss 0.003849669126793742
W0402 08:50:35.181528 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 08:50:35.372122 1527566 finetune.py:45] layer 12_k initial loss 0.002690272405743599
W0402 08:50:35.372357 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:50:36.341259 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:50:36.347859 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_k proxy err 0.0045533934608101845 tr(WHW.T) 4340.4931640625
  0%|          | 0/32 [00:00<?, ?it/s]13_k proxy err 0.006035333964973688 tr(WHW.T) 4513.9775390625
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:50:37.735247 1528094 finetune.py:45] layer 14_k initial loss 0.0030249860137701035
W0402 08:50:37.735485 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:23,  1.30it/s]  3%|▎         | 1/32 [00:00<00:24,  1.28it/s]  6%|▋         | 2/32 [00:01<00:15,  1.92it/s]  6%|▋         | 2/32 [00:01<00:16,  1.87it/s]W0402 08:50:38.838830 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:12,  2.26it/s]  9%|▉         | 3/32 [00:01<00:13,  2.17it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.45it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.59it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s]14_k proxy err 0.005313213914632797 tr(WHW.T) 4950.548828125
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.58it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.72it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.62it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s]  3%|▎         | 1/32 [00:00<00:24,  1.29it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s]  6%|▋         | 2/32 [00:01<00:16,  1.87it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.17it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.33it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s]I0402 08:50:42.307764 1528595 finetune.py:45] layer 15_k initial loss 0.00391272222623229
W0402 08:50:42.308159 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.60it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.67it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s]W0402 08:50:43.659813 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.77it/s]15_k proxy err 0.005628866143524647 tr(WHW.T) 4509.1435546875
  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.85it/s]  3%|▎         | 1/32 [00:00<00:23,  1.30it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.80it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.89it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.62it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.81it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.62it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.75it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.74it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.65it/s]I0402 08:50:55.806573 1527725 finetune.py:45] layer 13_o initial loss 0.0036099287681281567
W0402 08:50:55.806946 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s]I0402 08:50:56.299695 1527566 finetune.py:45] layer 12_o initial loss 0.0024207844398915768
W0402 08:50:56.299902 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]W0402 08:50:56.970960 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
W0402 08:50:57.342074 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_o proxy err 0.04109556972980499 tr(WHW.T) 6.745936870574951
  0%|          | 0/32 [00:00<?, ?it/s]12_o proxy err 0.041277460753917694 tr(WHW.T) 5.624481201171875
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:50:58.885267 1528094 finetune.py:45] layer 14_o initial loss 0.0030030596535652876
W0402 08:50:58.885507 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:50:59.849945 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<01:01,  1.97s/it]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it]14_o proxy err 0.04294627532362938 tr(WHW.T) 6.85344123840332
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it]  6%|▋         | 2/32 [00:03<00:48,  1.60s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it]  9%|▉         | 3/32 [00:04<00:45,  1.55s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it]I0402 08:51:04.587186 1528595 finetune.py:45] layer 15_o initial loss 0.0037499188911169767
W0402 08:51:04.587477 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it]W0402 08:51:05.827858 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it]15_o proxy err 0.04570693522691727 tr(WHW.T) 6.766373634338379
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.43s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.39s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.39s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.39s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 41%|████      | 13/32 [00:18<00:26,  1.39s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:24,  1.39s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.46s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.38s/it] 41%|████      | 13/32 [00:18<00:26,  1.42s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 50%|█████     | 16/32 [00:22<00:22,  1.39s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.41s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.39s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.45s/it] 59%|█████▉    | 19/32 [00:27<00:17,  1.38s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.38s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.41s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.39s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.47s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.39s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.42s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it] 50%|█████     | 16/32 [00:23<00:23,  1.49s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.38s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 75%|███████▌  | 24/32 [00:33<00:11,  1.38s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.44s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.38s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.48s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 78%|███████▊  | 25/32 [00:35<00:10,  1.43s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.39s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.39s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.48s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.42s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.43s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.38s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.39s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.48s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.44s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.39s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.43s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.39s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.43s/it]100%|██████████| 32/32 [00:45<00:00,  1.39s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it]100%|██████████| 32/32 [00:45<00:00,  1.44s/it]100%|██████████| 32/32 [00:45<00:00,  1.44s/it]
 97%|█████████▋| 31/32 [00:44<00:01,  1.43s/it] 81%|████████▏ | 26/32 [00:38<00:09,  1.50s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it] 88%|████████▊ | 28/32 [00:41<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.51s/it]I0402 08:51:50.671442 1527725 finetune.py:45] layer 13_up initial loss 0.00345896789804101
W0402 08:51:50.671717 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:51:51.549332 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:44<00:03,  1.51s/it]I0402 08:51:52.404629 1527566 finetune.py:45] layer 12_up initial loss 0.002300097607076168
W0402 08:51:52.404874 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

13_up proxy err 0.03321314975619316 tr(WHW.T) 916.3327026367188
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:51:53.234446 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]12_up proxy err 0.033677343279123306 tr(WHW.T) 855.3870849609375
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:51:54.573160 1528094 finetune.py:45] layer 14_up initial loss 0.0028295456431806087
W0402 08:51:54.573400 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:58,  1.89s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
W0402 08:51:55.410741 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:03<00:48,  1.62s/it]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it]14_up proxy err 0.03571302443742752 tr(WHW.T) 932.0479125976562
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.50s/it]  9%|▉         | 3/32 [00:04<00:45,  1.55s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it]I0402 08:52:03.707966 1528595 finetune.py:45] layer 15_up initial loss 0.003442447865381837
W0402 08:52:03.708377 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it]W0402 08:52:04.712348 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it]15_up proxy err 0.03587434068322182 tr(WHW.T) 988.9022827148438
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.48s/it]  6%|▋         | 2/32 [00:03<00:52,  1.74s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.48s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it] 41%|████      | 13/32 [00:18<00:27,  1.42s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.54s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.44s/it] 50%|█████     | 16/32 [00:23<00:22,  1.43s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.50s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.40s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.43s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 50%|█████     | 16/32 [00:23<00:22,  1.43s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.43s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.43s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.39s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.43s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.43s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.39s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.38s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 62%|██████▎   | 20/32 [00:29<00:16,  1.42s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.43s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.38s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.39s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.47s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.38s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.43s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.38s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.38s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.41s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.47s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.43s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.39s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.38s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.41s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.43s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.38s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.41s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.43s/it]100%|██████████| 32/32 [00:45<00:00,  1.38s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 91%|█████████ | 29/32 [00:41<00:04,  1.41s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.43s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
 97%|█████████▋| 31/32 [00:44<00:01,  1.44s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.49s/it]I0402 08:52:45.386420 1527725 finetune.py:45] layer 13_gate initial loss 0.0032617419492453337
W0402 08:52:45.386592 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:52:46.132926 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it]I0402 08:52:48.698240 1527566 finetune.py:45] layer 12_gate initial loss 0.0022303839214146137
W0402 08:52:48.698498 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

13_gate proxy err 0.012164930813014507 tr(WHW.T) 3562.817138671875
  0%|          | 0/112 [00:00<?, ?it/s]W0402 08:52:49.424381 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:43<00:04,  1.50s/it]  1%|          | 1/112 [00:00<01:35,  1.17it/s]I0402 08:52:50.402119 1528094 finetune.py:45] layer 14_gate initial loss 0.002642239211127162
W0402 08:52:50.402437 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  2%|▏         | 2/112 [00:01<01:02,  1.76it/s]  3%|▎         | 3/112 [00:01<00:51,  2.12it/s] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it]W0402 08:52:51.103212 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  4%|▎         | 4/112 [00:01<00:45,  2.37it/s]  4%|▍         | 5/112 [00:02<00:42,  2.50it/s]  5%|▌         | 6/112 [00:02<00:40,  2.61it/s]  6%|▋         | 7/112 [00:03<00:39,  2.63it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]12_gate proxy err 0.012888206169009209 tr(WHW.T) 3184.862548828125
  0%|          | 0/112 [00:00<?, ?it/s]  7%|▋         | 8/112 [00:03<00:39,  2.65it/s]  8%|▊         | 9/112 [00:03<00:38,  2.70it/s]  9%|▉         | 10/112 [00:04<00:37,  2.73it/s]  1%|          | 1/112 [00:00<01:29,  1.24it/s] 10%|▉         | 11/112 [00:04<00:37,  2.72it/s]  2%|▏         | 2/112 [00:01<00:59,  1.83it/s]14_gate proxy err 0.011816672049462795 tr(WHW.T) 4249.7998046875
  0%|          | 0/112 [00:00<?, ?it/s]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]
 11%|█         | 12/112 [00:04<00:36,  2.76it/s]  3%|▎         | 3/112 [00:01<00:49,  2.19it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s]  4%|▎         | 4/112 [00:01<00:45,  2.39it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.80it/s]  1%|          | 1/112 [00:00<01:29,  1.24it/s]  4%|▍         | 5/112 [00:02<00:43,  2.48it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.79it/s]  2%|▏         | 2/112 [00:01<01:00,  1.82it/s]  5%|▌         | 6/112 [00:02<00:41,  2.54it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.82it/s]  3%|▎         | 3/112 [00:01<00:49,  2.18it/s]  6%|▋         | 7/112 [00:02<00:40,  2.61it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.83it/s]  4%|▎         | 4/112 [00:01<00:44,  2.40it/s]  7%|▋         | 8/112 [00:03<00:38,  2.67it/s] 16%|█▌        | 18/112 [00:06<00:32,  2.85it/s]  4%|▍         | 5/112 [00:02<00:42,  2.53it/s]  8%|▊         | 9/112 [00:03<00:38,  2.70it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.87it/s]  5%|▌         | 6/112 [00:02<00:40,  2.63it/s]  9%|▉         | 10/112 [00:04<00:37,  2.73it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.86it/s]  6%|▋         | 7/112 [00:02<00:39,  2.69it/s] 10%|▉         | 11/112 [00:04<00:36,  2.73it/s] 19%|█▉        | 21/112 [00:07<00:31,  2.87it/s]  7%|▋         | 8/112 [00:03<00:37,  2.74it/s] 11%|█         | 12/112 [00:04<00:36,  2.74it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.88it/s]  8%|▊         | 9/112 [00:03<00:37,  2.76it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.72it/s] 21%|██        | 23/112 [00:08<00:30,  2.88it/s]  9%|▉         | 10/112 [00:04<00:36,  2.78it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.73it/s] 21%|██▏       | 24/112 [00:09<00:30,  2.85it/s] 10%|▉         | 11/112 [00:04<00:36,  2.78it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.73it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.85it/s] 11%|█         | 12/112 [00:04<00:36,  2.77it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.71it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.85it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.72it/s] 24%|██▍       | 27/112 [00:10<00:29,  2.87it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.81it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.72it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.87it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.82it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.72it/s] 26%|██▌       | 29/112 [00:10<00:28,  2.87it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.79it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.71it/s] 27%|██▋       | 30/112 [00:11<00:28,  2.84it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.79it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.85it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.71it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.80it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.84it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.71it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.79it/s] 29%|██▉       | 33/112 [00:12<00:27,  2.86it/s] 21%|██        | 23/112 [00:08<00:32,  2.71it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.81it/s] 30%|███       | 34/112 [00:12<00:27,  2.86it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.71it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.81it/s]I0402 08:53:01.937780 1528595 finetune.py:45] layer 15_gate initial loss 0.0032601228449493647
W0402 08:53:01.937998 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 35/112 [00:12<00:26,  2.87it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.71it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.81it/s] 32%|███▏      | 36/112 [00:13<00:26,  2.87it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.72it/s] 21%|██        | 23/112 [00:08<00:31,  2.82it/s]W0402 08:53:02.732756 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 33%|███▎      | 37/112 [00:13<00:26,  2.87it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.75it/s] 21%|██▏       | 24/112 [00:08<00:31,  2.82it/s] 34%|███▍      | 38/112 [00:13<00:25,  2.87it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.76it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.83it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.89it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.74it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.84it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.88it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.73it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.82it/s] 37%|███▋      | 41/112 [00:14<00:24,  2.87it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.72it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.83it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.88it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.83it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.71it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.87it/s] 27%|██▋       | 30/112 [00:11<00:28,  2.83it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.71it/s] 39%|███▉      | 44/112 [00:15<00:23,  2.88it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.84it/s] 30%|███       | 34/112 [00:12<00:28,  2.73it/s] 40%|████      | 45/112 [00:16<00:23,  2.89it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.83it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.72it/s] 41%|████      | 46/112 [00:16<00:23,  2.87it/s]15_gate proxy err 0.011262346059083939 tr(WHW.T) 5081.4052734375
  0%|          | 0/112 [00:00<?, ?it/s] 29%|██▉       | 33/112 [00:12<00:27,  2.83it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.72it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.87it/s] 30%|███       | 34/112 [00:12<00:27,  2.83it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.72it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.86it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.78it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.70it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.82it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.73it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.82it/s]  2%|▏         | 2/112 [00:01<01:03,  1.72it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.81it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.74it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.84it/s]  3%|▎         | 3/112 [00:01<00:53,  2.05it/s] 34%|███▍      | 38/112 [00:13<00:26,  2.81it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.77it/s] 46%|████▋     | 52/112 [00:18<00:21,  2.86it/s]  4%|▎         | 4/112 [00:01<00:47,  2.26it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.83it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.87it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.78it/s]  4%|▍         | 5/112 [00:02<00:44,  2.40it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.84it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.88it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.78it/s]  5%|▌         | 6/112 [00:02<00:42,  2.49it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.83it/s] 49%|████▉     | 55/112 [00:19<00:19,  2.88it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.79it/s]  6%|▋         | 7/112 [00:03<00:41,  2.54it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.84it/s] 50%|█████     | 56/112 [00:20<00:19,  2.87it/s] 40%|████      | 45/112 [00:16<00:23,  2.80it/s]  7%|▋         | 8/112 [00:03<00:40,  2.57it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.82it/s] 51%|█████     | 57/112 [00:20<00:19,  2.87it/s] 41%|████      | 46/112 [00:17<00:23,  2.79it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.81it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.86it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s]  9%|▉         | 10/112 [00:04<00:39,  2.59it/s] 40%|████      | 45/112 [00:16<00:23,  2.82it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.87it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.75it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 41%|████      | 46/112 [00:16<00:23,  2.81it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.86it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.73it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.82it/s] 11%|█         | 12/112 [00:05<00:39,  2.56it/s] 54%|█████▍    | 61/112 [00:21<00:17,  2.87it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.72it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.83it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.87it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.57it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.73it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.83it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.87it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.70it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.83it/s] 57%|█████▋    | 64/112 [00:22<00:16,  2.88it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.72it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.83it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.88it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.73it/s] 46%|████▋     | 52/112 [00:18<00:21,  2.83it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.87it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.58it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.73it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.78it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.83it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.56it/s] 50%|█████     | 56/112 [00:20<00:20,  2.71it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.78it/s] 61%|██████    | 68/112 [00:24<00:15,  2.79it/s] 51%|█████     | 57/112 [00:21<00:20,  2.70it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.55it/s] 49%|████▉     | 55/112 [00:19<00:20,  2.80it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.82it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.73it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 50%|█████     | 56/112 [00:20<00:19,  2.81it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.84it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.72it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.61it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.86it/s] 51%|█████     | 57/112 [00:20<00:19,  2.83it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.75it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.63it/s] 64%|██████▍   | 72/112 [00:25<00:13,  2.87it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.83it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.77it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.88it/s] 21%|██        | 23/112 [00:09<00:33,  2.64it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.83it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.77it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.89it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.83it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.64it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.77it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.86it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.82it/s] 22%|██▏       | 25/112 [00:10<00:32,  2.64it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.79it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.87it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.83it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.65it/s] 58%|█████▊    | 65/112 [00:24<00:16,  2.79it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.87it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.82it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.65it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.88it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.78it/s] 57%|█████▋    | 64/112 [00:23<00:16,  2.83it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.66it/s] 71%|███████   | 79/112 [00:28<00:11,  2.89it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.79it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.81it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.65it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.89it/s] 61%|██████    | 68/112 [00:25<00:15,  2.80it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.82it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.65it/s] 72%|███████▏  | 81/112 [00:28<00:10,  2.90it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.79it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.83it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.65it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.90it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.77it/s] 61%|██████    | 68/112 [00:24<00:15,  2.84it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.89it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.76it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.81it/s] 29%|██▉       | 33/112 [00:13<00:29,  2.64it/s] 75%|███████▌  | 84/112 [00:29<00:09,  2.88it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.78it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.82it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.89it/s] 30%|███       | 34/112 [00:13<00:29,  2.67it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.78it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.83it/s] 77%|███████▋  | 86/112 [00:30<00:09,  2.85it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.67it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.78it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.79it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.84it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.68it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.79it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.79it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.85it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.69it/s] 68%|██████▊   | 76/112 [00:28<00:12,  2.79it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.81it/s] 79%|███████▉  | 89/112 [00:31<00:08,  2.86it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.70it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.80it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.82it/s] 80%|████████  | 90/112 [00:32<00:07,  2.87it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.70it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.83it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.79it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.88it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.71it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.84it/s] 71%|███████   | 79/112 [00:29<00:11,  2.79it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.87it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.71it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.83it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.79it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.88it/s] 71%|███████   | 79/112 [00:28<00:11,  2.84it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.72it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.80it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.89it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.84it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.72it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.80it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.89it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.85it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.79it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.71it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.89it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.85it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.79it/s] 40%|████      | 45/112 [00:17<00:24,  2.72it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.89it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.83it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.79it/s] 41%|████      | 46/112 [00:17<00:24,  2.72it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.88it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.84it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.79it/s] 42%|████▏     | 47/112 [00:18<00:23,  2.72it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.89it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.85it/s] 78%|███████▊  | 87/112 [00:32<00:08,  2.80it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.73it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.89it/s] 77%|███████▋  | 86/112 [00:30<00:09,  2.83it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.79it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.73it/s] 90%|█████████ | 101/112 [00:35<00:03,  2.86it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.83it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.79it/s] 45%|████▍     | 50/112 [00:19<00:22,  2.72it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.87it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.84it/s] 80%|████████  | 90/112 [00:33<00:07,  2.79it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.72it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.88it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.84it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.79it/s] 93%|█████████▎| 104/112 [00:36<00:02,  2.88it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.72it/s] 80%|████████  | 90/112 [00:32<00:07,  2.83it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.78it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.85it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.72it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.80it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.79it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.84it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.72it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.81it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.79it/s] 96%|█████████▌| 107/112 [00:37<00:01,  2.86it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.71it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.82it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.79it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.86it/s] 50%|█████     | 56/112 [00:21<00:20,  2.71it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.81it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.79it/s] 97%|█████████▋| 109/112 [00:38<00:01,  2.84it/s] 51%|█████     | 57/112 [00:21<00:20,  2.71it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.81it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.80it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.86it/s] 52%|█████▏    | 58/112 [00:22<00:19,  2.71it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.82it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.79it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.87it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.72it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.83it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.78it/s]100%|██████████| 112/112 [00:39<00:00,  2.84it/s]100%|██████████| 112/112 [00:39<00:00,  2.82it/s]
 54%|█████▎    | 60/112 [00:23<00:19,  2.71it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.84it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.79it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.72it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.80it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.78it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.75it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.77it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.72it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.78it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.79it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.72it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.78it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.79it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.72it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.75it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.74it/s] 59%|█████▉    | 66/112 [00:25<00:16,  2.73it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.77it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.76it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.72it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.79it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.77it/s] 61%|██████    | 68/112 [00:25<00:16,  2.72it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.80it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.77it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.72it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.82it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.78it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.83it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.72it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.78it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.80it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.71it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.79it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.82it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.72it/s]100%|██████████| 112/112 [00:41<00:00,  2.78it/s]100%|██████████| 112/112 [00:41<00:00,  2.73it/s]
 99%|█████████▉| 111/112 [00:39<00:00,  2.79it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.72it/s]100%|██████████| 112/112 [00:40<00:00,  2.79it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]
 66%|██████▌   | 74/112 [00:28<00:14,  2.69it/s]W0402 08:53:34.578000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.578000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.578000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.578000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.578000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.578000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.579000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 75/112 [00:28<00:13,  2.66it/s]W0402 08:53:34.624000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.624000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.624000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.625000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.625000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.804000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.805000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.805000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.805000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:34.805000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 68%|██████▊   | 76/112 [00:28<00:13,  2.64it/s]W0402 08:53:35.131000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.131000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.131000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.131000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.131000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.131000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.131000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.165000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.165000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.165000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.165000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.165000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.234000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.234000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.234000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.234000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:35.234000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 77/112 [00:29<00:13,  2.67it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.66it/s] 71%|███████   | 79/112 [00:30<00:12,  2.68it/s]W0402 08:53:36.164000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.177000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.185000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.185000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 71%|███████▏  | 80/112 [00:30<00:11,  2.70it/s]W0402 08:53:36.611000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.611000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.611000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.612000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.612000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.612000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.612000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.638000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.639000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.639000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.639000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.639000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 81/112 [00:30<00:11,  2.71it/s]W0402 08:53:36.925000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.925000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.925000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.925000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.925000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.925000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.925000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:36.925000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:37.201000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:37.201000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:37.201000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:37.202000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:37.202000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:31<00:11,  2.71it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.71it/s]W0402 08:53:37.601000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:37.605000 140233204397888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:31<00:10,  2.70it/s] 76%|███████▌  | 85/112 [00:32<00:09,  2.71it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.72it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.72it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.73it/s]W0402 08:53:39.539000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.539000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.539000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.539000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.540000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.540000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.540000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.581000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.581000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.581000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.582000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.582000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.751000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.751000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.751000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.751000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.751000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:33<00:08,  2.73it/s]W0402 08:53:39.826000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.826000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.826000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.827000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.827000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.827000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.827000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.871000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.871000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.871000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.872000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:39.872000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.049000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.049000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.049000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.049000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.049000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.057000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.057000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.057000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.057000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.057000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.057000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.057000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.088000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.088000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.088000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.088000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.089000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 90/112 [00:34<00:08,  2.73it/s]W0402 08:53:40.154000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.154000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.155000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.155000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.155000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.369000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.369000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.369000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.369000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.370000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.370000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.370000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.402000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.402000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.402000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.403000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.403000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.473000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.473000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.473000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.473000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:40.473000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:34<00:07,  2.75it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.75it/s]W0402 08:53:41.078000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.091000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.099000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.099000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:06,  2.76it/s]W0402 08:53:41.439000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.445000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.452000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.452000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.529000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.529000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.529000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.529000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.530000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.530000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.530000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.557000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.557000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.557000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.557000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.557000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:35<00:06,  2.75it/s]W0402 08:53:41.840000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.840000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.840000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.840000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.840000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.840000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.840000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.840000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.898000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.898000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.898000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.898000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.898000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.898000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.898000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.926000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.927000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.927000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.927000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:41.927000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 95/112 [00:35<00:06,  2.75it/s]W0402 08:53:42.121000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.121000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.121000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.121000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.121000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.224000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.225000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.225000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.225000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.225000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.225000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.225000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.225000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 96/112 [00:36<00:05,  2.75it/s]W0402 08:53:42.521000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.521000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.521000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.521000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.521000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.533000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.538000 139896638801728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:36<00:05,  2.75it/s]W0402 08:53:42.951000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:42.957000 140518695466816 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:36<00:05,  2.76it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.75it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.75it/s]I0402 08:53:44.148770 1527725 finetune.py:45] layer 13_down initial loss 0.003098412649706006
W0402 08:53:44.149070 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 90%|█████████ | 101/112 [00:38<00:04,  2.70it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.68it/s]W0402 08:53:44.646924 1527725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 92%|█████████▏| 103/112 [00:38<00:03,  2.67it/s]13_down proxy err 0.04491398110985756 tr(WHW.T) 11.704399108886719
 93%|█████████▎| 104/112 [00:39<00:03,  2.65it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.65it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.63it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.63it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.63it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.62it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.62it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.62it/s]100%|██████████| 112/112 [00:42<00:00,  2.61it/s]100%|██████████| 112/112 [00:42<00:00,  2.65it/s]
I0402 08:53:49.503050 1527566 finetune.py:45] layer 12_down initial loss 0.0021390903275460005
W0402 08:53:49.503427 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 08:53:49.618019 1528094 finetune.py:45] layer 14_down initial loss 0.002527531934902072
W0402 08:53:49.618335 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:53:50.015507 1527566 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 08:53:50.087156 1528094 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

12_down proxy err 0.043865807354450226 tr(WHW.T) 10.028185844421387
14_down proxy err 0.04669305682182312 tr(WHW.T) 13.352069854736328
I0402 08:53:53.877955 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 16 in 1.0952460765838623s
I0402 08:53:54.399893 1509661 quantize_finetune_llama.py:159] layer 17 gpu 1
W0402 08:53:54.465000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.465000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.465000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.465000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.466000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.466000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.466000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.509000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.509000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.509000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.509000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.509000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.681000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.682000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.682000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.682000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.682000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.989000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.989000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.989000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.989000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.990000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.990000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:54.990000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:55.023000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:55.023000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:55.023000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:55.023000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:55.023000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:55.093000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:55.093000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:55.093000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:55.093000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:55.093000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
I0402 08:53:55.947211 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 17 in 1.1443815231323242s
W0402 08:53:56.039000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.052000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.060000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.060000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
I0402 08:53:56.433294 1532898 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:53:56.433479 1532898 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:53:56.433555 1532898 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:53:56.465442 1509661 quantize_finetune_llama.py:159] layer 18 gpu 2
W0402 08:53:56.512000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.512000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.512000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.512000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.512000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.512000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.513000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.545000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.545000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.546000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.546000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.546000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
I0402 08:53:56.792055 1532898 config.py:58] PyTorch version 2.4.0 available.
W0402 08:53:56.842000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.842000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.842000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.842000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.843000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.843000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.843000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:53:56.843000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:57.145000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:53:57.145000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:53:57.145000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:53:57.145000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:53:57.145000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:57.569000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:53:57.574000 140225062143808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 08:53:58.413541 1533044 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:53:58.413701 1533044 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:53:58.413763 1533044 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:53:58.769959 1533044 config.py:58] PyTorch version 2.4.0 available.
I0402 08:53:59.185223 1532898 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:53:59.522039 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:54:01.026756 1533044 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:54:01.379731 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:44,  1.44s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:16,  1.76it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:03<00:09,  2.61it/s]  3%|▎         | 1/32 [00:01<00:45,  1.48s/it] 22%|██▏       | 7/32 [00:03<00:09,  2.75it/s]  6%|▋         | 2/32 [00:01<00:24,  1.24it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.88it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.97it/s]  9%|▉         | 3/32 [00:02<00:17,  1.70it/s]I0402 08:54:04.479382 1528595 finetune.py:45] layer 15_down initial loss 0.0031810610089451075
W0402 08:54:04.479682 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:04<00:07,  3.04it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.06it/s]W0402 08:54:04.994925 1528595 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 34%|███▍      | 11/32 [00:04<00:06,  3.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s]15_down proxy err 0.0468023382127285 tr(WHW.T) 16.894805908203125
 38%|███▊      | 12/32 [00:04<00:06,  3.10it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.56it/s] 41%|████      | 13/32 [00:05<00:06,  3.13it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.12it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.11it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  3.13it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.94it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.13it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.98it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.09it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.96it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.07it/s] 41%|████      | 13/32 [00:05<00:06,  2.97it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.10it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.99it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.11it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.02it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.14it/s] 50%|█████     | 16/32 [00:06<00:05,  3.05it/s]I0402 08:54:08.708503 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 18 in 1.1458580493927002s
 72%|███████▏  | 23/32 [00:08<00:02,  3.11it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.99it/s]I0402 08:54:09.218660 1509661 quantize_finetune_llama.py:159] layer 19 gpu 3
 75%|███████▌  | 24/32 [00:08<00:02,  3.08it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.96it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.06it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.96it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.06it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.07it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.99it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.11it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.02it/s]I0402 08:54:10.830940 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 19 in 1.2061643600463867s
 91%|█████████ | 29/32 [00:10<00:00,  3.10it/s] 72%|███████▏  | 23/32 [00:08<00:03,  3.00it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.08it/s]I0402 08:54:11.229868 1534058 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:54:11.230070 1534058 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:54:11.230144 1534058 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:54:11.332042 1509661 quantize_finetune_llama.py:159] layer 20 gpu 0
 75%|███████▌  | 24/32 [00:09<00:02,  2.97it/s]I0402 08:54:11.483311 1534058 config.py:58] PyTorch version 2.4.0 available.
 97%|█████████▋| 31/32 [00:11<00:00,  3.07it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  3.13it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
 81%|████████▏ | 26/32 [00:09<00:02,  2.97it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.95it/s]I0402 08:54:13.289342 1534223 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:54:13.289497 1534223 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:54:13.289571 1534223 utils.py:162] NumExpr defaulting to 16 threads.
 94%|█████████▍| 30/32 [00:11<00:00,  2.99it/s]I0402 08:54:13.512041 1534223 config.py:58] PyTorch version 2.4.0 available.
 97%|█████████▋| 31/32 [00:11<00:00,  2.96it/s]I0402 08:54:13.925865 1534058 data_utils.py:336] using 256 training seqs, 128 validation seqs
100%|██████████| 32/32 [00:11<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
W0402 08:54:14.131000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.131000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.131000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.131000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.131000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.131000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.131000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.158000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.158000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.158000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.159000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.159000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.373650 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:54:14.453000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.454000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.454000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.454000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:14.454000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.321000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.322000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.322000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.322000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.322000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.322000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.322000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.340000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.340000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.340000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.341000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.341000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:54:15.551000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.552000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.552000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.552000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:15.552000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
I0402 08:54:15.913568 1534223 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:54:16.331941 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:54:16.482000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.482000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.482000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.482000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.482000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.482000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.482000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.509000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.509000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.510000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.510000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.510000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.675000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.676000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.676000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.676000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.676000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.676000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.676000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.693000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.693000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.693000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.693000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.693000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.798000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.798000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.798000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.798000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:16.798000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:48,  1.55s/it]  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:25,  1.17it/s]W0402 08:54:17.527000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.528000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.528000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.528000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.528000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.620000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.620000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.620000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.620000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.620000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.620000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.620000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.638000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.638000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.638000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.638000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.638000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:02<00:18,  1.60it/s]W0402 08:54:17.838000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.838000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.838000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.838000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:17.838000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s]W0402 08:54:18.961000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.962000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.962000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.962000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.962000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.962000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.962000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.981000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.981000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.981000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.981000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:18.981000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:50,  1.62s/it] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.52it/s]  9%|▉         | 3/32 [00:02<00:18,  1.58it/s]W0402 08:54:19.884000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:19.884000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:19.884000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:19.885000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:19.885000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.18it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 34%|███▍      | 11/32 [00:05<00:07,  2.66it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.69it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.62it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.74it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.72it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s]I0402 08:54:23.466601 1532898 finetune.py:45] layer 16_v initial loss 0.003676139982417226
W0402 08:54:23.467010 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:08<00:04,  2.75it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.78it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s]W0402 08:54:24.637844 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.74it/s]16_v proxy err 0.03554375842213631 tr(WHW.T) 68.60269165039062
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s]I0402 08:54:26.275851 1533044 finetune.py:45] layer 17_v initial loss 0.00499684177339077
W0402 08:54:26.278262 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.89it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.90it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.92it/s]  9%|▉         | 3/32 [00:01<00:13,  2.10it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.78it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.96it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s]W0402 08:54:27.998811 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:10<00:01,  2.98it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
 88%|████████▊ | 28/32 [00:11<00:01,  2.96it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.94it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.68it/s]17_v proxy err 0.046600982546806335 tr(WHW.T) 69.1786880493164
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.96it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
 31%|███▏      | 10/32 [00:04<00:07,  2.82it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s]W0402 08:54:30.724000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:30.724000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:30.724000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:30.724000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:30.724000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:30.724000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:30.724000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:14,  2.03it/s]W0402 08:54:30.751000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:30.751000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:30.752000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:30.752000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:30.752000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.82it/s]W0402 08:54:31.052000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.052000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.052000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.052000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.052000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.88it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s]W0402 08:54:31.870000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.870000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.870000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.870000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.870000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.871000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.871000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.888000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.888000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.888000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.888000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:31.888000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.85it/s]W0402 08:54:32.079000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.079000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.079000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.080000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.080000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s]W0402 08:54:32.235000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.235000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.235000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.236000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.236000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.236000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.236000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.263000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.263000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.263000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.263000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.263000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s]W0402 08:54:32.560000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.560000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.560000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.560000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:32.560000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s]W0402 08:54:33.172000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.173000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.173000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.173000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.173000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.173000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.173000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.190000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.190000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.190000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.190000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.190000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s]W0402 08:54:33.381000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.381000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.381000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.381000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.381000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.381000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.381000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.398000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.398000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.398000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.398000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.399000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s]W0402 08:54:33.593000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.593000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.593000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.593000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:33.593000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.86it/s]W0402 08:54:34.011000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.011000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.011000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.011000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.011000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 08:54:34.652000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.653000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.653000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.653000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.653000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.653000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.653000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.670000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.670000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.670000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.671000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:34.671000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:08<00:02,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s]W0402 08:54:35.522000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:35.522000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:35.522000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:35.522000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:35.522000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 88%|████████▊ | 28/32 [00:10<00:01,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s]I0402 08:54:39.953141 1534058 finetune.py:45] layer 18_v initial loss 0.0044527738355100155
W0402 08:54:39.953516 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s]W0402 08:54:41.046603 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0402 08:54:42.034159 1534223 finetune.py:45] layer 19_v initial loss 0.006517632864415646
W0402 08:54:42.034535 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

18_v proxy err 0.0387101024389267 tr(WHW.T) 73.61924743652344
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:54:43.025000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.026000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.026000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.026000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.026000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.026000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.026000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]W0402 08:54:43.057000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.058000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.058000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.058000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.058000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.151597 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:54:43.225000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.225000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.225000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.225000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.225000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:18,  1.63it/s]W0402 08:54:43.455000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.456000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.456000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.456000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.456000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.456000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.456000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.479000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.479000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.479000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.479000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.479000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.542000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.543000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.543000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.543000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:43.543000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:14,  1.98it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s]19_v proxy err 0.03766006976366043 tr(WHW.T) 87.24554443359375
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:54:44.238000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.539000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.539000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.539000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.539000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.539000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.539000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.539000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s]W0402 08:54:44.559000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.560000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.560000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.560000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.560000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.803000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.803000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.803000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.803000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:44.804000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s]W0402 08:54:45.146000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s]  6%|▋         | 2/32 [00:01<00:17,  1.67it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.29it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s]W0402 08:54:46.653000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.653000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.653000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.653000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.653000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.654000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.654000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.682000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.683000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.683000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.683000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.683000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s]W0402 08:54:46.846000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.846000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.846000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.846000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:46.846000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s]W0402 08:54:47.066000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.066000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.066000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.066000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.066000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.066000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.066000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.087000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.087000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.087000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.087000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.087000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.151000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.151000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.151000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.152000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:47.152000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s]W0402 08:54:47.866000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s]W0402 08:54:48.172000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.173000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.173000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.173000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.173000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.173000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.173000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.196000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.196000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.196000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.196000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.196000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s]W0402 08:54:48.456000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.457000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.457000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.457000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:54:48.457000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s]W0402 08:54:48.803000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.56it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.60it/s]I0402 08:54:51.200124 1532898 finetune.py:45] layer 16_q initial loss 0.003684215946123004
W0402 08:54:51.200369 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.54it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s]W0402 08:54:52.427954 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.57it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s]16_q proxy err 0.006891399621963501 tr(WHW.T) 6124.5419921875
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s]I0402 08:54:55.225518 1533044 finetune.py:45] layer 17_q initial loss 0.005003747995942831
W0402 08:54:55.225790 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.46it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s]W0402 08:54:56.361082 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:09,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s]17_q proxy err 0.00699388375505805 tr(WHW.T) 6716.6806640625
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s]  3%|▎         | 1/32 [00:00<00:27,  1.11it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s]  6%|▋         | 2/32 [00:01<00:17,  1.70it/s] 41%|████      | 13/32 [00:05<00:06,  2.85it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.94it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 50%|█████     | 16/32 [00:06<00:05,  2.95it/s]W0402 08:55:00.019000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.019000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.019000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.019000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.019000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.019000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.020000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.048000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.048000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.048000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.048000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.048000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.96it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s]W0402 08:55:00.218000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.218000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.218000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.219000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.219000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.431000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.431000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.431000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.431000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.431000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.431000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.431000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.451000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.451000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.451000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.451000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.451000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s]W0402 08:55:00.513000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.513000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.513000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.513000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:00.513000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.97it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.98it/s]W0402 08:55:01.209000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s]W0402 08:55:01.516000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.516000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.516000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.516000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.516000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.516000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.516000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.536000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.536000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.537000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.537000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.537000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s]W0402 08:55:01.782000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.782000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.782000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.782000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.782000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.792000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.792000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.793000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.793000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.793000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.793000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.793000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.822000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.822000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.823000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.823000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.823000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.96it/s]W0402 08:55:01.985000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.985000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.985000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.985000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:01.985000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s]W0402 08:55:02.113000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:08<00:03,  2.98it/s]W0402 08:55:02.200000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.200000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.200000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.200000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.200000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.201000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.201000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.223000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.223000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.223000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.223000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.223000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.290000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.290000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.290000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.290000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:02.290000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.96it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.96it/s]W0402 08:55:02.998000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s]W0402 08:55:03.309000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.309000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.309000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.309000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.310000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.310000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.310000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.331000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.331000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.331000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.331000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.331000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:09<00:01,  2.97it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s]W0402 08:55:03.579000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.579000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.579000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.579000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:55:03.579000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.98it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s]W0402 08:55:03.920000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:01,  2.95it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s]I0402 08:55:09.160849 1534058 finetune.py:45] layer 18_q initial loss 0.004450236912816763
W0402 08:55:09.161365 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0402 08:55:10.555740 1534223 finetune.py:45] layer 19_q initial loss 0.006505027879029512
W0402 08:55:10.556277 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:55:11.047492 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 08:55:11.889270 1532898 finetune.py:45] layer 16_k initial loss 0.003688905155286193
W0402 08:55:11.889644 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:55:12.037253 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_q proxy err 0.008549897000193596 tr(WHW.T) 5733.501953125
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:55:13.024784 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_q proxy err 0.00822586938738823 tr(WHW.T) 6149.1044921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]  3%|▎         | 1/32 [00:00<00:27,  1.12it/s]  6%|▋         | 2/32 [00:01<00:17,  1.67it/s]16_k proxy err 0.004942372906953096 tr(WHW.T) 4878.88427734375
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s]  3%|▎         | 1/32 [00:00<00:23,  1.32it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s]  6%|▋         | 2/32 [00:01<00:15,  1.93it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s]  9%|▉         | 3/32 [00:01<00:12,  2.26it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.60it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.70it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s]I0402 08:55:16.896304 1533044 finetune.py:45] layer 17_k initial loss 0.0049994932487607
W0402 08:55:16.896587 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:02<00:09,  2.76it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.84it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.81it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s]W0402 08:55:18.100245 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.74it/s] 41%|████      | 13/32 [00:04<00:06,  2.86it/s]17_k proxy err 0.0062700980342924595 tr(WHW.T) 4246.078125
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.87it/s]  3%|▎         | 1/32 [00:00<00:24,  1.28it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.80it/s] 50%|█████     | 16/32 [00:05<00:05,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s]  6%|▋         | 2/32 [00:01<00:16,  1.82it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.71it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.89it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.89it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.89it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.88it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.89it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.89it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.57it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.58it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.60it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.63it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
 97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0402 08:55:32.443077 1534223 finetune.py:45] layer 19_k initial loss 0.006513556465506554
W0402 08:55:32.443445 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 08:55:32.929964 1534058 finetune.py:45] layer 18_k initial loss 0.00444835564121604
W0402 08:55:32.930558 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 08:55:33.207831 1532898 finetune.py:45] layer 16_o initial loss 0.0035288978833705187
W0402 08:55:33.208270 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:55:33.578770 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:55:34.384379 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_k proxy err 0.007181690540164709 tr(WHW.T) 3978.64404296875
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:55:35.006727 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:00<00:23,  1.33it/s]18_k proxy err 0.006789988838136196 tr(WHW.T) 4452.47412109375
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:01<00:15,  1.94it/s]  9%|▉         | 3/32 [00:01<00:12,  2.26it/s]16_o proxy err 0.038296960294246674 tr(WHW.T) 7.369767189025879
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.44it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.56it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.64it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.26it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.70it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.78it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s]I0402 08:55:38.874884 1533044 finetune.py:45] layer 17_o initial loss 0.005063225980848074
W0402 08:55:38.875276 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.53it/s]  6%|▋         | 2/32 [00:03<00:47,  1.58s/it] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s]W0402 08:55:40.058398 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:08,  2.60it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.85it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s]  9%|▉         | 3/32 [00:04<00:43,  1.49s/it] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s]17_o proxy err 0.039777759462594986 tr(WHW.T) 6.400020122528076
  0%|          | 0/32 [00:00<?, ?it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.86it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 12%|█▎        | 4/32 [00:06<00:40,  1.46s/it] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it] 75%|███████▌  | 24/32 [00:08<00:02,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 16%|█▌        | 5/32 [00:07<00:38,  1.43s/it] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.84it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s] 19%|█▉        | 6/32 [00:08<00:36,  1.42s/it] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
 22%|██▏       | 7/32 [00:10<00:35,  1.41s/it] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.71it/s] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 34%|███▍      | 11/32 [00:15<00:30,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.52s/it]I0402 08:55:52.738894 1534223 finetune.py:45] layer 19_o initial loss 0.00629086559638381
W0402 08:55:52.739139 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it]W0402 08:55:53.757477 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 08:55:54.757658 1534058 finetune.py:45] layer 18_o initial loss 0.004555948544293642
W0402 08:55:54.758011 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

19_o proxy err 0.040564846247434616 tr(WHW.T) 3.939014434814453
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:18<00:26,  1.42s/it] 28%|██▊       | 9/32 [00:13<00:35,  1.52s/it]W0402 08:55:55.708435 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it]  3%|▎         | 1/32 [00:01<00:57,  1.85s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it]18_o proxy err 0.038369495421648026 tr(WHW.T) 4.730301856994629
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:21<00:24,  1.41s/it]  6%|▋         | 2/32 [00:03<00:47,  1.59s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.52s/it]  3%|▎         | 1/32 [00:01<01:00,  1.97s/it] 50%|█████     | 16/32 [00:22<00:22,  1.41s/it]  9%|▉         | 3/32 [00:04<00:43,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:40,  1.45s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.43s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 19%|█▉        | 6/32 [00:08<00:36,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.41s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.40s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 28%|██▊       | 9/32 [00:12<00:32,  1.39s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.41s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.39s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.39s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.42s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.39s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 41%|████      | 13/32 [00:18<00:26,  1.39s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it] 44%|████▍     | 14/32 [00:19<00:25,  1.39s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.42s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 50%|█████     | 16/32 [00:22<00:22,  1.39s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.47s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.42s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.39s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.43s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.38s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.40s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.47s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.44s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.46s/it] 66%|██████▌   | 21/32 [00:31<00:15,  1.45s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.45s/it]100%|██████████| 32/32 [00:47<00:00,  1.46s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it]I0402 08:56:29.861530 1532898 finetune.py:45] layer 16_up initial loss 0.0033309818245470524
W0402 08:56:29.861694 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 78%|███████▊  | 25/32 [00:35<00:10,  1.45s/it]W0402 08:56:30.664221 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it]16_up proxy err 0.03948863223195076 tr(WHW.T) 981.8323974609375
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:36<00:08,  1.44s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.41s/it]  6%|▋         | 2/32 [00:03<00:46,  1.56s/it] 81%|████████▏ | 26/32 [00:38<00:09,  1.55s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.45s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.54s/it]I0402 08:56:37.458047 1533044 finetune.py:45] layer 17_up initial loss 0.004829442128539085
W0402 08:56:37.458419 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:42<00:02,  1.44s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it]W0402 08:56:38.305089 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:41<00:06,  1.52s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it]17_up proxy err 0.03871089965105057 tr(WHW.T) 1061.939208984375
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.51s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it] 94%|█████████▍| 30/32 [00:44<00:03,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.44s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it]I0402 08:56:47.486764 1534223 finetune.py:45] layer 19_up initial loss 0.00612637447193265
W0402 08:56:47.486927 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it]W0402 08:56:48.287295 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it]19_up proxy err 0.04424893483519554 tr(WHW.T) 1067.8548583984375
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:10<00:38,  1.52s/it] 41%|████      | 13/32 [00:18<00:27,  1.43s/it]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it]I0402 08:56:52.528016 1534058 finetune.py:45] layer 18_up initial loss 0.004365900531411171
W0402 08:56:52.528385 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:46,  1.56s/it]W0402 08:56:53.395430 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:13<00:34,  1.52s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.43s/it]  9%|▉         | 3/32 [00:04<00:43,  1.49s/it]18_up proxy err 0.04211154207587242 tr(WHW.T) 1055.73193359375
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 50%|█████     | 16/32 [00:23<00:22,  1.43s/it] 12%|█▎        | 4/32 [00:05<00:40,  1.45s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it]  3%|▎         | 1/32 [00:01<00:57,  1.86s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.43s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.43s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.40s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.43s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.45s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.40s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.47s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.43s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 44%|████▍     | 14/32 [00:19<00:25,  1.39s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.47s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.43s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 50%|█████     | 16/32 [00:22<00:22,  1.40s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.40s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.44s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.40s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.48s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it] 59%|█████▉    | 19/32 [00:26<00:18,  1.40s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.44s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 50%|█████     | 16/32 [00:23<00:23,  1.48s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.44s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.46s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 78%|███████▊  | 25/32 [00:35<00:10,  1.47s/it]I0402 08:57:25.618276 1532898 finetune.py:45] layer 16_gate initial loss 0.003154915291815996
W0402 08:57:25.618553 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it]W0402 08:57:26.329720 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:37<00:08,  1.46s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 84%|████████▍ | 27/32 [00:38<00:07,  1.46s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it]16_gate proxy err 0.013863622210919857 tr(WHW.T) 4852.61962890625
  0%|          | 0/112 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it]  1%|          | 1/112 [00:00<01:26,  1.28it/s] 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it]  2%|▏         | 2/112 [00:01<00:58,  1.90it/s]  3%|▎         | 3/112 [00:01<00:49,  2.22it/s] 91%|█████████ | 29/32 [00:41<00:04,  1.43s/it]  4%|▎         | 4/112 [00:01<00:44,  2.41it/s]  4%|▍         | 5/112 [00:02<00:41,  2.55it/s] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it]  5%|▌         | 6/112 [00:02<00:40,  2.63it/s]  6%|▋         | 7/112 [00:02<00:39,  2.68it/s] 94%|█████████▍| 30/32 [00:42<00:02,  1.42s/it]  7%|▋         | 8/112 [00:03<00:38,  2.72it/s]  8%|▊         | 9/112 [00:03<00:37,  2.73it/s]  9%|▉         | 10/112 [00:03<00:36,  2.77it/s] 81%|████████▏ | 26/32 [00:38<00:08,  1.50s/it] 10%|▉         | 11/112 [00:04<00:36,  2.78it/s] 97%|█████████▋| 31/32 [00:44<00:01,  1.41s/it] 11%|█         | 12/112 [00:04<00:36,  2.78it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.78it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.80it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it] 13%|█▎        | 15/112 [00:05<00:34,  2.79it/s]100%|██████████| 32/32 [00:45<00:00,  1.40s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 14%|█▍        | 16/112 [00:06<00:34,  2.79it/s]I0402 08:57:35.439351 1533044 finetune.py:45] layer 17_gate initial loss 0.004597653169184923
W0402 08:57:35.439570 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 15%|█▌        | 17/112 [00:06<00:34,  2.79it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.78it/s] 88%|████████▊ | 28/32 [00:41<00:05,  1.50s/it]W0402 08:57:36.161795 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 17%|█▋        | 19/112 [00:07<00:33,  2.81it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.80it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.79it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.50s/it] 21%|██        | 23/112 [00:08<00:31,  2.79it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.78it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.78it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.77it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.49s/it] 24%|██▍       | 27/112 [00:10<00:30,  2.78it/s]17_gate proxy err 0.013997437432408333 tr(WHW.T) 5241.20556640625
  0%|          | 0/112 [00:00<?, ?it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.78it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.77it/s]  1%|          | 1/112 [00:00<01:29,  1.24it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.78it/s]  2%|▏         | 2/112 [00:01<01:00,  1.81it/s] 97%|█████████▋| 31/32 [00:45<00:01,  1.48s/it] 28%|██▊       | 31/112 [00:11<00:29,  2.78it/s]  3%|▎         | 3/112 [00:01<00:50,  2.14it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.78it/s]  4%|▎         | 4/112 [00:01<00:46,  2.34it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.78it/s]  4%|▍         | 5/112 [00:02<00:43,  2.46it/s] 30%|███       | 34/112 [00:12<00:28,  2.78it/s]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
  5%|▌         | 6/112 [00:02<00:41,  2.55it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.79it/s]I0402 08:57:42.351398 1534223 finetune.py:45] layer 19_gate initial loss 0.0059480322524905205
W0402 08:57:42.351567 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 7/112 [00:03<00:40,  2.60it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s]  7%|▋         | 8/112 [00:03<00:40,  2.58it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.79it/s]W0402 08:57:43.046380 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 38/112 [00:14<00:26,  2.79it/s]  8%|▊         | 9/112 [00:03<00:40,  2.56it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.77it/s]  9%|▉         | 10/112 [00:04<00:39,  2.58it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.79it/s] 10%|▉         | 11/112 [00:04<00:38,  2.59it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.79it/s] 11%|█         | 12/112 [00:04<00:38,  2.61it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.77it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.60it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.79it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.61it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.79it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s] 40%|████      | 45/112 [00:16<00:24,  2.78it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.61it/s]19_gate proxy err 0.019042886793613434 tr(WHW.T) 4580.61328125
  0%|          | 0/112 [00:00<?, ?it/s] 41%|████      | 46/112 [00:16<00:23,  2.79it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.61it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.78it/s]  1%|          | 1/112 [00:00<01:29,  1.25it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.60it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.78it/s]  2%|▏         | 2/112 [00:01<01:00,  1.83it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.61it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.79it/s]  3%|▎         | 3/112 [00:01<00:50,  2.16it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.60it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.78it/s]  4%|▎         | 4/112 [00:01<00:45,  2.36it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.60it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.78it/s]  4%|▍         | 5/112 [00:02<00:42,  2.50it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.79it/s]  5%|▌         | 6/112 [00:02<00:40,  2.59it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.77it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.59it/s]  6%|▋         | 7/112 [00:02<00:39,  2.64it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.78it/s] 22%|██▏       | 25/112 [00:09<00:33,  2.60it/s]  7%|▋         | 8/112 [00:03<00:38,  2.68it/s] 50%|█████     | 56/112 [00:20<00:20,  2.78it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s]  8%|▊         | 9/112 [00:03<00:37,  2.72it/s]I0402 08:57:49.953462 1534058 finetune.py:45] layer 18_gate initial loss 0.0041929553262889385
W0402 08:57:49.953779 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 51%|█████     | 57/112 [00:20<00:19,  2.78it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s]  9%|▉         | 10/112 [00:04<00:37,  2.70it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.78it/s] 10%|▉         | 11/112 [00:04<00:37,  2.72it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.63it/s]W0402 08:57:50.662730 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 59/112 [00:21<00:19,  2.78it/s] 11%|█         | 12/112 [00:04<00:36,  2.71it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.78it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.73it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.62it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.78it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.75it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.62it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.78it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.76it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.78it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.77it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.78it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.76it/s] 30%|███       | 34/112 [00:13<00:30,  2.59it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.78it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.77it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.61it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.78it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.77it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.58it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.75it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.58it/s] 61%|██████    | 68/112 [00:24<00:15,  2.78it/s]18_gate proxy err 0.017266813665628433 tr(WHW.T) 4660.9404296875
  0%|          | 0/112 [00:00<?, ?it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.76it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.59it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.77it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.75it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.78it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.62it/s] 21%|██        | 23/112 [00:08<00:32,  2.76it/s]  1%|          | 1/112 [00:00<01:32,  1.20it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.77it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.75it/s]  2%|▏         | 2/112 [00:01<01:01,  1.79it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.77it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.64it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.77it/s]  3%|▎         | 3/112 [00:01<00:51,  2.11it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.78it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.65it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.78it/s]  4%|▎         | 4/112 [00:01<00:46,  2.31it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.77it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.67it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.77it/s]  4%|▍         | 5/112 [00:02<00:43,  2.44it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.78it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.66it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.75it/s]  5%|▌         | 6/112 [00:02<00:42,  2.50it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.78it/s] 40%|████      | 45/112 [00:17<00:25,  2.64it/s] 26%|██▌       | 29/112 [00:10<00:30,  2.73it/s]  6%|▋         | 7/112 [00:03<00:41,  2.53it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.75it/s] 41%|████      | 46/112 [00:17<00:25,  2.61it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.74it/s]  7%|▋         | 8/112 [00:03<00:40,  2.54it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.78it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.75it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.60it/s]  8%|▊         | 9/112 [00:03<00:40,  2.56it/s] 71%|███████   | 79/112 [00:28<00:11,  2.78it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.74it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.62it/s]  9%|▉         | 10/112 [00:04<00:39,  2.60it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.76it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.75it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.64it/s] 10%|▉         | 11/112 [00:04<00:38,  2.64it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.77it/s] 30%|███       | 34/112 [00:12<00:28,  2.77it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.65it/s] 11%|█         | 12/112 [00:04<00:37,  2.66it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.78it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.77it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.64it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.65it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.77it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.77it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.62it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.65it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.77it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.76it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.64it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.66it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.78it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.76it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.65it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.68it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.77it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.77it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.66it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.71it/s] 78%|███████▊  | 87/112 [00:31<00:09,  2.77it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.78it/s] 50%|█████     | 56/112 [00:21<00:21,  2.65it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.78it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.69it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.77it/s] 51%|█████     | 57/112 [00:22<00:21,  2.61it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.77it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.66it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.77it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.63it/s] 80%|████████  | 90/112 [00:32<00:07,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.68it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.77it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.77it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.65it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.69it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.78it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.77it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.66it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.70it/s] 40%|████      | 45/112 [00:16<00:24,  2.77it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.78it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.66it/s] 21%|██        | 23/112 [00:09<00:32,  2.71it/s] 41%|████      | 46/112 [00:17<00:24,  2.74it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.77it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.66it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.71it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.74it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.77it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.66it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.70it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.75it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.77it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.67it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.71it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.76it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.77it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.67it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.72it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.77it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.77it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.68it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.73it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.77it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.77it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.70it/s] 60%|█████▉    | 67/112 [00:25<00:17,  2.63it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.76it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.77it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.66it/s] 61%|██████    | 68/112 [00:26<00:16,  2.61it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.77it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.76it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.68it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.64it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.76it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.77it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.69it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.76it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.65it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.77it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.72it/s] 50%|█████     | 56/112 [00:20<00:20,  2.78it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.66it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.77it/s] 30%|███       | 34/112 [00:13<00:28,  2.71it/s] 51%|█████     | 57/112 [00:21<00:19,  2.78it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.66it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.77it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.77it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.71it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.66it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.77it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.78it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.70it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.63it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.77it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.78it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.70it/s] 67%|██████▋   | 75/112 [00:28<00:14,  2.64it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.77it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.78it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.72it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.66it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.77it/s] 55%|█████▌    | 62/112 [00:22<00:18,  2.76it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.73it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.67it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.77it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.75it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.73it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.66it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.77it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.72it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.72it/s] 71%|███████   | 79/112 [00:30<00:12,  2.65it/s]100%|██████████| 112/112 [00:40<00:00,  2.76it/s]100%|██████████| 112/112 [00:40<00:00,  2.75it/s]
 58%|█████▊    | 65/112 [00:24<00:17,  2.70it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.70it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.65it/s] 59%|█████▉    | 66/112 [00:24<00:17,  2.68it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.70it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.65it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.71it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.66it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.67it/s] 40%|████      | 45/112 [00:17<00:24,  2.72it/s] 61%|██████    | 68/112 [00:25<00:16,  2.68it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.67it/s] 41%|████      | 46/112 [00:17<00:24,  2.72it/s] 62%|██████▏   | 69/112 [00:25<00:16,  2.69it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.68it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.71it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.69it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.67it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.70it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.69it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.67it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.72it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.70it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.67it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.74it/s] 45%|████▍     | 50/112 [00:19<00:22,  2.71it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.67it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.74it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.71it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.67it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.76it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.72it/s] 80%|████████  | 90/112 [00:34<00:08,  2.67it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.77it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.73it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.67it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.77it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.71it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.68it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.78it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.72it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.69it/s] 71%|███████   | 79/112 [00:29<00:11,  2.77it/s] 50%|█████     | 56/112 [00:21<00:20,  2.74it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.69it/s]W0402 08:58:15.461000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.461000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.461000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.462000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.462000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.462000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.462000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.503000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.503000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.503000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.503000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.503000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 71%|███████▏  | 80/112 [00:29<00:11,  2.77it/s] 51%|█████     | 57/112 [00:21<00:20,  2.69it/s]W0402 08:58:15.665000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.665000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.665000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.666000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.666000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 95/112 [00:36<00:06,  2.64it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.76it/s] 52%|█████▏    | 58/112 [00:21<00:20,  2.65it/s]W0402 08:58:15.972000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.972000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.972000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.972000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.972000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.972000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:15.972000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:16.003000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:16.003000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:16.004000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:16.004000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:16.004000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:16.069000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:16.069000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:16.069000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:16.069000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:16.069000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 96/112 [00:36<00:06,  2.65it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.76it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.67it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.65it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.73it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.64it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.66it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.75it/s]W0402 08:58:16.995000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.000000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.007000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.007000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 54%|█████▍    | 61/112 [00:23<00:19,  2.67it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.67it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.75it/s]W0402 08:58:17.433000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.433000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.433000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.433000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.433000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.433000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.433000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.463000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.463000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.463000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.463000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.463000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 55%|█████▌    | 62/112 [00:23<00:18,  2.67it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.66it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.76it/s]W0402 08:58:17.739000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.739000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.740000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.740000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.740000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.740000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.740000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:17.740000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 63/112 [00:23<00:18,  2.68it/s]W0402 08:58:18.014000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:18.014000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:18.014000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:18.014000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:18.014000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 90%|█████████ | 101/112 [00:38<00:04,  2.66it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.77it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.68it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.63it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.75it/s]W0402 08:58:18.422000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:18.426000 140065958823744 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 58%|█████▊    | 65/112 [00:24<00:17,  2.65it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.75it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.64it/s] 59%|█████▉    | 66/112 [00:24<00:17,  2.67it/s] 80%|████████  | 90/112 [00:33<00:07,  2.76it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.66it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.69it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.75it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.66it/s] 61%|██████    | 68/112 [00:25<00:16,  2.68it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.77it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.67it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.69it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.73it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.68it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.70it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.68it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.69it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.70it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.64it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.73it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.71it/s] 86%|████████▌ | 96/112 [00:35<00:06,  2.62it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.73it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.71it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.60it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.73it/s]100%|██████████| 112/112 [00:42<00:00,  2.71it/s]100%|██████████| 112/112 [00:42<00:00,  2.62it/s]
 88%|████████▊ | 98/112 [00:36<00:05,  2.60it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.72it/s] 88%|████████▊ | 99/112 [00:36<00:05,  2.60it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.69it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.58it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.66it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.57it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.65it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.57it/s] 71%|███████   | 79/112 [00:29<00:12,  2.66it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.56it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.65it/s] 93%|█████████▎| 104/112 [00:38<00:03,  2.56it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.66it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.68it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.52it/s]I0402 08:58:25.189292 1532898 finetune.py:45] layer 16_down initial loss 0.003038390539586544
W0402 08:58:25.189466 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 74%|███████▍  | 83/112 [00:31<00:10,  2.69it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.56it/s]W0402 08:58:25.621195 1532898 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 75%|███████▌  | 84/112 [00:31<00:10,  2.70it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.63it/s]16_down proxy err 0.04764479398727417 tr(WHW.T) 17.929277420043945
 76%|███████▌  | 85/112 [00:32<00:10,  2.70it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.68it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.70it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.71it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.72it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.70it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.75it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.72it/s]100%|██████████| 112/112 [00:41<00:00,  2.73it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
 79%|███████▉  | 89/112 [00:33<00:08,  2.71it/s] 80%|████████  | 90/112 [00:33<00:08,  2.68it/s]W0402 08:58:28.014000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.014000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.014000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.014000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.014000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.015000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.015000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.056000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.056000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.057000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.057000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.057000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.230000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.230000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.230000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.230000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.230000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:34<00:07,  2.69it/s]W0402 08:58:28.535000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.535000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.535000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.536000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.536000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.536000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.536000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.565000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.565000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.565000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.566000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.566000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:34<00:07,  2.69it/s]W0402 08:58:28.632000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.632000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.632000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.633000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:28.633000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:07,  2.68it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.70it/s]W0402 08:58:29.560000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:29.574000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:29.582000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:29.582000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 95/112 [00:35<00:06,  2.70it/s]W0402 08:58:30.022000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.023000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.023000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.023000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.023000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.023000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.023000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.054000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.054000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.054000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.054000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.054000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 96/112 [00:36<00:05,  2.69it/s]W0402 08:58:30.339000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.339000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.339000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.339000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.340000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.340000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.340000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.340000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:36<00:05,  2.69it/s]W0402 08:58:30.624000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.624000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.624000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.624000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:30.624000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:36<00:05,  2.69it/s]W0402 08:58:31.035000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:31.040000 139742127298368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:37<00:04,  2.69it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.69it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.70it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.69it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.69it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.67it/s]W0402 08:58:33.145000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.145000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.145000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.145000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.145000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.145000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.146000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.186000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.186000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.186000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.186000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.186000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.353000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.353000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.353000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.353000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.353000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 105/112 [00:39<00:02,  2.62it/s]W0402 08:58:33.653000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.653000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.654000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.654000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.654000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.654000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.654000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.682000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.682000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.682000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.682000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.682000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.749000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.749000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.749000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.749000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:33.749000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:39<00:02,  2.61it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.61it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.60it/s]W0402 08:58:34.656000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:34.669000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:34.676000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:34.676000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:41<00:01,  2.60it/s]W0402 08:58:35.097000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.097000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.097000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.097000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.097000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.098000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.098000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.127000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.127000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.127000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.127000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.127000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.406000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.406000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.406000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.406000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.407000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.407000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.407000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.407000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:41<00:00,  2.59it/s]W0402 08:58:35.678000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.678000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.679000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.679000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:35.679000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:41<00:00,  2.59it/s]W0402 08:58:36.080000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:36.085000 139838345955136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:42<00:00,  2.59it/s]100%|██████████| 112/112 [00:42<00:00,  2.65it/s]
I0402 08:58:37.832064 1533044 finetune.py:45] layer 17_down initial loss 0.00444292277097702
W0402 08:58:37.832223 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 08:58:38.302805 1533044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

17_down proxy err 0.048495031893253326 tr(WHW.T) 21.307649612426758
W0402 08:58:41.987000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:41.987000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:41.987000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:41.987000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:41.987000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:41.987000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:41.987000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.028000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.028000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.028000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.028000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.028000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.195000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.195000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.195000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.195000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.195000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.501000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.501000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.501000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.501000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.501000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.501000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.501000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
I0402 08:58:42.524478 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 20 in 1.7447805404663086s
W0402 08:58:42.531000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.531000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.531000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.531000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.531000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.602000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.602000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.602000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.602000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:42.602000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
I0402 08:58:42.700222 1534223 finetune.py:45] layer 19_down initial loss 0.005773527082055807
W0402 08:58:42.700766 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 08:58:43.022657 1509661 quantize_finetune_llama.py:159] layer 21 gpu 1
W0402 08:58:43.240076 1534223 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 08:58:43.569000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:43.575000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:43.581000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:43.581000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
19_down proxy err 0.04968562722206116 tr(WHW.T) 21.985565185546875
W0402 08:58:44.028000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.028000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.029000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.029000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.029000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.029000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.029000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.055000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.056000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.056000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.056000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.056000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.334000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.334000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.334000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.334000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.334000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.334000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.335000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.335000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.615000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.615000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.615000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.615000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:58:44.615000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
I0402 08:58:44.979155 1538312 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:58:44.979312 1538312 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:58:44.979375 1538312 utils.py:162] NumExpr defaulting to 16 threads.
W0402 08:58:45.020000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:58:45.025000 140027053250368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 08:58:45.294378 1538312 config.py:58] PyTorch version 2.4.0 available.
I0402 08:58:47.553251 1538312 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:58:47.909727 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]  9%|▉         | 3/32 [00:01<00:15,  1.83it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s]I0402 08:58:52.003173 1534058 finetune.py:45] layer 18_down initial loss 0.004063266795128584
W0402 08:58:52.003447 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:03<00:08,  2.84it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.94it/s]W0402 08:58:52.493710 1534058 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 28%|██▊       | 9/32 [00:03<00:07,  3.02it/s]18_down proxy err 0.0495634563267231 tr(WHW.T) 21.0947265625
 31%|███▏      | 10/32 [00:04<00:07,  3.07it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.10it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.13it/s] 41%|████      | 13/32 [00:05<00:06,  3.16it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.16it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.12it/s] 50%|█████     | 16/32 [00:06<00:05,  3.12it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.14it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.14it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.16it/s]I0402 08:58:56.077754 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 21 in 1.1620919704437256s
 62%|██████▎   | 20/32 [00:07<00:03,  3.12it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.08it/s]I0402 08:58:56.578908 1509661 quantize_finetune_llama.py:159] layer 22 gpu 2
 69%|██████▉   | 22/32 [00:08<00:03,  3.06it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.08it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.06it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.10it/s]I0402 08:58:58.171042 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 22 in 1.1844091415405273s
 81%|████████▏ | 26/32 [00:09<00:01,  3.13it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.06it/s]I0402 08:58:58.651924 1538976 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:58:58.652062 1538976 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:58:58.652125 1538976 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:58:58.685524 1509661 quantize_finetune_llama.py:159] layer 23 gpu 3
I0402 08:58:59.247961 1538976 config.py:58] PyTorch version 2.4.0 available.
 88%|████████▊ | 28/32 [00:10<00:01,  2.23it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.43it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.57it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.69it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
I0402 08:59:00.735589 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 23 in 1.2526195049285889s
I0402 08:59:01.048826 1539137 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:59:01.049012 1539137 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:59:01.049123 1539137 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:59:01.136890 1509661 quantize_finetune_llama.py:159] layer 24 gpu 0
I0402 08:59:01.247700 1539137 config.py:58] PyTorch version 2.4.0 available.
I0402 08:59:01.588955 1538976 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:59:02.117688 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:59:03.148000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.148000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.149000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.149000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.149000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.149000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.149000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:59:03.176000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.176000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.176000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.176000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.176000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
I0402 08:59:03.225870 1539380 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 08:59:03.226054 1539380 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 08:59:03.226163 1539380 utils.py:162] NumExpr defaulting to 16 threads.
I0402 08:59:03.418621 1539380 config.py:58] PyTorch version 2.4.0 available.
W0402 08:59:03.478000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.478000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.478000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.479000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:03.479000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
I0402 08:59:03.609997 1539137 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:59:04.138485 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 08:59:04.336000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.336000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.336000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.336000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.336000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.336000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.337000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.355000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.355000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.355000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.355000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.355000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.555000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.556000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.556000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.556000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:04.556000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:46,  1.49s/it]  6%|▋         | 2/32 [00:01<00:24,  1.24it/s]  9%|▉         | 3/32 [00:02<00:17,  1.69it/s]I0402 08:59:05.563579 1539380 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 08:59:05.667000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.667000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.667000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.667000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.667000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.667000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.667000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s]W0402 08:59:05.685000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.686000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.686000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.686000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.686000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:05.919925 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s]  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.48it/s]W0402 08:59:06.544000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:06.544000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:06.544000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:06.544000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:06.544000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s]  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 28%|██▊       | 9/32 [00:04<00:08,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.85it/s]  3%|▎         | 1/32 [00:01<00:53,  1.72s/it] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s]  6%|▋         | 2/32 [00:02<00:27,  1.10it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.87it/s]  3%|▎         | 1/32 [00:01<00:51,  1.67s/it]  9%|▉         | 3/32 [00:02<00:18,  1.53it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s]  6%|▋         | 2/32 [00:02<00:26,  1.12it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.14it/s]  9%|▉         | 3/32 [00:02<00:18,  1.54it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.85it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.14it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.62it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.84it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.81it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.87it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.89it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s]I0402 08:59:12.612033 1538312 finetune.py:45] layer 20_v initial loss 0.006101725157350302
W0402 08:59:12.613812 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:06<00:05,  2.91it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.92it/s] 50%|█████     | 16/32 [00:06<00:05,  2.98it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.88it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.95it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.97it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.93it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.96it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.02it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.93it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.99it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.06it/s]W0402 08:59:14.437618 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:07<00:04,  2.98it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.99it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.04it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.97it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.04it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  3.01it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 72%|███████▏  | 23/32 [00:09<00:02,  3.07it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.00it/s]20_v proxy err 0.040878500789403915 tr(WHW.T) 90.61302185058594
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.04it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.96it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.05it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.95it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.01it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.02it/s]  6%|▋         | 2/32 [00:01<00:17,  1.70it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.95it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.05it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.98it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.05it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.99it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.07it/s]W0402 08:59:17.656000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.656000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.656000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.656000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.656000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.656000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.656000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.682000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.682000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.682000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.682000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.682000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  3.00it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.08it/s]W0402 08:59:17.958000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.958000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.958000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.958000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:17.958000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:11<00:00,  3.01it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s]100%|██████████| 32/32 [00:12<00:00,  3.09it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  3.02it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s]W0402 08:59:18.794000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.794000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.794000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.794000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.794000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.794000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.794000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.812000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.812000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.812000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.812000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:18.812000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.97it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s]W0402 08:59:19.005000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:19.005000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:19.005000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:19.005000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:19.005000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:12<00:00,  2.97it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s]W0402 08:59:20.117000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.117000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.117000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.117000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.117000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.117000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.117000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.136000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.136000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.136000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.136000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.136000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s]W0402 08:59:20.650000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.650000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.650000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.650000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.650000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.651000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.651000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.677000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.677000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.677000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.677000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.678000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:06,  2.73it/s]W0402 08:59:20.967000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.967000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.967000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.967000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.967000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.987000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.987000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.987000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.987000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:20.988000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.73it/s]W0402 08:59:21.584000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.584000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.584000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.585000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.585000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.585000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.585000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.611000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.611000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.611000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.611000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.612000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 08:59:21.791000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.791000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.791000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.791000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.791000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.792000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.792000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.809000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.809000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.809000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.809000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.809000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.75it/s]W0402 08:59:21.892000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.892000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.892000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.892000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:21.892000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.000000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.000000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.000000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.000000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.000000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s]W0402 08:59:22.749000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.749000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.749000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.749000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.749000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.749000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.749000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.768000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.768000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.768000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.768000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.768000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s]W0402 08:59:22.981000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.981000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.981000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.981000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:22.981000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.115000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.115000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.115000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.115000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.115000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.116000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.116000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.134000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.134000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.134000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.134000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:23.134000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s]W0402 08:59:24.019000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.020000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.020000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.020000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.020000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s]W0402 08:59:24.172000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.172000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.172000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.172000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.172000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.172000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.172000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.192000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.192000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.193000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.193000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:24.193000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:08<00:03,  2.67it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s]W0402 08:59:25.116000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:25.116000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:25.117000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:25.117000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:25.117000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s]I0402 08:59:26.746253 1538976 finetune.py:45] layer 21_v initial loss 0.00915310438722372
W0402 08:59:26.746476 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
W0402 08:59:28.047805 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_v proxy err 0.04035191610455513 tr(WHW.T) 95.41011047363281
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:59:29.978263 1539137 finetune.py:45] layer 22_v initial loss 0.007181605324149132
W0402 08:59:29.978437 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]W0402 08:59:31.056905 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:14,  1.95it/s]I0402 08:59:31.251917 1539380 finetune.py:45] layer 23_v initial loss 0.010679668746888638
W0402 08:59:31.252128 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s]22_v proxy err 0.04406123235821724 tr(WHW.T) 101.29380798339844
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:59:32.183118 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s]W0402 08:59:32.804000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.804000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.804000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.804000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.804000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.804000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.804000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.833000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.834000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.834000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.834000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:32.834000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.004000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.004000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.004000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.004000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.004000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]23_v proxy err 0.04601619765162468 tr(WHW.T) 112.7859115600586
  0%|          | 0/32 [00:00<?, ?it/s]W0402 08:59:33.226000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.226000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.226000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.226000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.226000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.226000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.226000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.246000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.247000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.247000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.247000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.247000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.310000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.311000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.311000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.311000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:33.311000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s]W0402 08:59:34.021000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:27,  1.14it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s]W0402 08:59:34.331000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.331000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.331000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.331000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.331000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.331000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.331000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.353000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.353000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.353000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.353000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.353000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:17,  1.76it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s]W0402 08:59:34.600000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.600000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.600000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.600000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:34.600000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:13,  2.13it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s]W0402 08:59:34.950000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:01<00:11,  2.34it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.78it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.75it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.79it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
I0402 08:59:41.821966 1538312 finetune.py:45] layer 20_q initial loss 0.006102945189923048
W0402 08:59:41.822415 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s]W0402 08:59:42.857180 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s]20_q proxy err 0.00891244038939476 tr(WHW.T) 5690.95703125
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s]  3%|▎         | 1/32 [00:00<00:26,  1.16it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.66it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.46it/s]W0402 08:59:46.542000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.542000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.542000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.542000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.542000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.542000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.542000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.571000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.571000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.571000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.571000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.571000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s]W0402 08:59:46.735000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.735000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.735000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.735000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.735000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s]W0402 08:59:46.950000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.950000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.950000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.950000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.950000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.950000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.950000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.971000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.971000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.971000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.971000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:46.971000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:47.034000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:47.034000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:47.034000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:47.034000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:47.034000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s]W0402 08:59:47.740000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.051000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.052000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.052000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.052000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.052000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.052000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.052000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s]W0402 08:59:48.073000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.073000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.073000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.073000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.074000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.315000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.315000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.315000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.316000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:48.316000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s]W0402 08:59:48.646000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:04<00:07,  2.69it/s]W0402 08:59:49.076000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.076000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.076000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.076000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.076000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.077000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.077000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.106000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.106000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.106000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.107000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.107000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:07,  2.71it/s]W0402 08:59:49.277000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.278000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.278000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.278000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.278000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.503000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.503000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.503000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.503000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.503000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.503000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.503000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.524000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.525000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.525000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.525000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.525000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s]W0402 08:59:49.586000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.587000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.587000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.587000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:49.587000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s]W0402 08:59:50.297000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.629000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.630000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.630000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.630000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.630000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.630000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.630000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s]W0402 08:59:50.652000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.652000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.653000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.653000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.653000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.916000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.917000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.917000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.917000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:50.917000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s]W0402 08:59:51.281000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s]W0402 08:59:51.521000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.521000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.521000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.521000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.522000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.522000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.522000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.551000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.552000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.552000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.552000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.552000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s]W0402 08:59:51.717000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.717000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.717000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.717000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.717000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.940000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.941000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.941000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.941000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.941000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.941000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.941000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.961000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.961000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.961000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.961000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:51.961000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:52.025000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:52.025000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:52.025000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:52.025000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:52.025000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s]W0402 08:59:52.736000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s]W0402 08:59:53.051000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.051000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.051000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.051000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.051000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.051000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.051000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.072000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.072000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.073000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.073000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.073000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s]W0402 08:59:53.322000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.322000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.322000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.322000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 08:59:53.322000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s]W0402 08:59:53.661000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s]I0402 08:59:54.453891 1538976 finetune.py:45] layer 21_q initial loss 0.00915437564253807
W0402 08:59:54.454361 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s]W0402 08:59:55.528642 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
21_q proxy err 0.007093490567058325 tr(WHW.T) 6793.6064453125
  0%|          | 0/32 [00:00<?, ?it/s]I0402 08:59:57.357189 1539137 finetune.py:45] layer 22_q initial loss 0.007178200408816338
W0402 08:59:57.357499 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:28,  1.10it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s]  9%|▉         | 3/32 [00:01<00:13,  2.10it/s]W0402 08:59:58.382485 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.60it/s]22_q proxy err 0.008012384176254272 tr(WHW.T) 5947.34326171875
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s]  3%|▎         | 1/32 [00:00<00:26,  1.15it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s]I0402 09:00:00.567294 1539380 finetune.py:45] layer 23_q initial loss 0.010664251632988453
W0402 09:00:00.567639 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:01<00:16,  1.77it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.14it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s]W0402 09:00:01.886026 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.70it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.86it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s]23_q proxy err 0.007878230884671211 tr(WHW.T) 6408.4541015625
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s]I0402 09:00:03.081593 1538312 finetune.py:45] layer 20_k initial loss 0.006084911059588194
W0402 09:00:03.081794 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.82it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s]  3%|▎         | 1/32 [00:00<00:28,  1.11it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s]W0402 09:00:04.252672 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:05<00:06,  2.85it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.87it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.87it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s]20_k proxy err 0.006759473122656345 tr(WHW.T) 4224.9970703125
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:06<00:05,  2.88it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.87it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s]  3%|▎         | 1/32 [00:00<00:24,  1.29it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s]  6%|▋         | 2/32 [00:01<00:16,  1.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.87it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.86it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.38it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.60it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.87it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.63it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s]I0402 09:00:14.438900 1538976 finetune.py:45] layer 21_k initial loss 0.009154696017503738
W0402 09:00:14.439099 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
W0402 09:00:15.463315 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.77it/s]21_k proxy err 0.006379314232617617 tr(WHW.T) 4413.919921875
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s]  3%|▎         | 1/32 [00:00<00:24,  1.27it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
I0402 09:00:17.450480 1539137 finetune.py:45] layer 22_k initial loss 0.007174693513661623
W0402 09:00:17.450821 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:01<00:15,  1.88it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.40it/s]W0402 09:00:18.450754 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.60it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s]22_k proxy err 0.006535954773426056 tr(WHW.T) 4309.5947265625
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s]  3%|▎         | 1/32 [00:00<00:23,  1.33it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.79it/s]  6%|▋         | 2/32 [00:01<00:15,  1.95it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s]  9%|▉         | 3/32 [00:01<00:12,  2.28it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.45it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.68it/s]I0402 09:00:22.531664 1539380 finetune.py:45] layer 23_k initial loss 0.010659677907824516
W0402 09:00:22.531905 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.82it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.82it/s]W0402 09:00:23.627180 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.85it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.83it/s] 41%|████      | 13/32 [00:04<00:06,  2.85it/s]I0402 09:00:24.718064 1538312 finetune.py:45] layer 20_o initial loss 0.006199739873409271
W0402 09:00:24.718291 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

23_k proxy err 0.006957349833101034 tr(WHW.T) 4211.31982421875
  0%|          | 0/32 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.86it/s]  3%|▎         | 1/32 [00:00<00:23,  1.30it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s]W0402 09:00:25.694784 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s]  6%|▋         | 2/32 [00:01<00:16,  1.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.88it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.84it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.89it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.35it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.90it/s]20_o proxy err 0.04242526367306709 tr(WHW.T) 4.07815408706665
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.89it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.88it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.85it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.85it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.83it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s]100%|██████████| 32/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.75it/s] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it]I0402 09:00:34.645427 1538976 finetune.py:45] layer 21_o initial loss 0.009651587344706059
W0402 09:00:34.645613 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s]W0402 09:00:35.652873 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s] 19%|█▉        | 6/32 [00:09<00:37,  1.44s/it] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s]21_o proxy err 0.030801648274064064 tr(WHW.T) 7.5875244140625
  0%|          | 0/32 [00:00<?, ?it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it]I0402 09:00:37.762483 1539137 finetune.py:45] layer 22_o initial loss 0.007111218757927418
W0402 09:00:37.762742 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:00:38.711482 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.44s/it]22_o proxy err 0.04552210867404938 tr(WHW.T) 5.007936477661133
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.45s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.50s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it]I0402 09:00:44.492784 1539380 finetune.py:45] layer 23_o initial loss 0.010518188588321209
W0402 09:00:44.493107 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:07<00:39,  1.48s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.45s/it]W0402 09:00:45.463977 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it]23_o proxy err 0.03873201459646225 tr(WHW.T) 6.165833473205566
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.44s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.44s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.45s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.40s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.40s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.45s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.40s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.50s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.40s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.45s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.39s/it] 50%|█████     | 16/32 [00:23<00:22,  1.40s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.39s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.40s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 50%|█████     | 16/32 [00:22<00:22,  1.40s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.39s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 59%|█████▉    | 19/32 [00:26<00:18,  1.39s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.40s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.39s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.40s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.39s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.39s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.40s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.39s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 75%|███████▌  | 24/32 [00:33<00:11,  1.38s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.40s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.39s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.40s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.39s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.41s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.39s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.41s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.39s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.40s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.50s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.39s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.40s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it]I0402 09:01:21.821688 1538312 finetune.py:45] layer 20_up initial loss 0.005983884446322918
W0402 09:01:21.821911 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:42<00:02,  1.39s/it]100%|██████████| 32/32 [00:45<00:00,  1.40s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
 75%|███████▌  | 24/32 [00:35<00:12,  1.50s/it]W0402 09:01:22.648330 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:43<00:01,  1.41s/it]20_up proxy err 0.04522781819105148 tr(WHW.T) 1127.510498046875
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.46s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it]I0402 09:01:29.529996 1538976 finetune.py:45] layer 21_up initial loss 0.009243790991604328
W0402 09:01:29.530161 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:43<00:04,  1.45s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it]W0402 09:01:30.428191 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:44<00:02,  1.45s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it]21_up proxy err 0.04366377741098404 tr(WHW.T) 1237.08935546875
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:01:32.474414 1539137 finetune.py:45] layer 22_up initial loss 0.0068945991806685925
W0402 09:01:32.474682 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 31/32 [00:46<00:01,  1.45s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.46s/it]W0402 09:01:33.300210 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:58,  1.87s/it]100%|██████████| 32/32 [00:47<00:00,  1.45s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it]22_up proxy err 0.045663848519325256 tr(WHW.T) 1249.8564453125
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it] 25%|██▌       | 8/32 [00:11<00:35,  1.47s/it]  3%|▎         | 1/32 [00:01<00:56,  1.84s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it]  6%|▋         | 2/32 [00:03<00:47,  1.59s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it]I0402 09:01:42.764421 1539380 finetune.py:45] layer 23_up initial loss 0.01011789869517088
W0402 09:01:42.764653 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it]W0402 09:01:43.870350 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it]23_up proxy err 0.04669149965047836 tr(WHW.T) 1298.444580078125
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it] 50%|█████     | 16/32 [00:23<00:23,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.41s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.45s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.41s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 50%|█████     | 16/32 [00:22<00:22,  1.40s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.41s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.45s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.40s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.40s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.44s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.40s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.40s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.40s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.40s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.47s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.45s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.40s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.45s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.40s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.41s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]100%|██████████| 32/32 [00:46<00:00,  1.46s/it]
 88%|████████▊ | 28/32 [00:39<00:05,  1.40s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.48s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.40s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.40s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.41s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.49s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.40s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.41s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it]100%|██████████| 32/32 [00:45<00:00,  1.40s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]
 94%|█████████▍| 30/32 [00:42<00:02,  1.41s/it] 69%|██████▉   | 22/32 [00:32<00:15,  1.51s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it]I0402 09:02:18.873430 1538312 finetune.py:45] layer 20_gate initial loss 0.0057314541190862656
W0402 09:02:18.873651 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:02:19.575955 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 75%|███████▌  | 24/32 [00:35<00:11,  1.50s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it]20_gate proxy err 0.02054712362587452 tr(WHW.T) 4549.56396484375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:28,  1.25it/s]  2%|▏         | 2/112 [00:01<00:59,  1.85it/s] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it]  3%|▎         | 3/112 [00:01<00:49,  2.19it/s]I0402 09:02:24.316403 1538976 finetune.py:45] layer 21_gate initial loss 0.00883555132895708
W0402 09:02:24.316667 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  4%|▎         | 4/112 [00:01<00:45,  2.39it/s]  4%|▍         | 5/112 [00:02<00:42,  2.52it/s]W0402 09:02:25.096794 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  5%|▌         | 6/112 [00:02<00:40,  2.60it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it]  6%|▋         | 7/112 [00:02<00:39,  2.65it/s]  7%|▋         | 8/112 [00:03<00:38,  2.70it/s]  8%|▊         | 9/112 [00:03<00:37,  2.73it/s]  9%|▉         | 10/112 [00:04<00:37,  2.74it/s] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 10%|▉         | 11/112 [00:04<00:36,  2.75it/s] 11%|█         | 12/112 [00:04<00:36,  2.74it/s]I0402 09:02:27.615272 1539137 finetune.py:45] layer 22_gate initial loss 0.006568020675331354
W0402 09:02:27.615460 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▏        | 13/112 [00:05<00:36,  2.73it/s]21_gate proxy err 0.01991327293217182 tr(WHW.T) 5024.1337890625
  0%|          | 0/112 [00:00<?, ?it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.73it/s]W0402 09:02:28.326738 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 13%|█▎        | 15/112 [00:05<00:35,  2.73it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.74it/s]  1%|          | 1/112 [00:00<01:30,  1.22it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.75it/s]  2%|▏         | 2/112 [00:01<01:01,  1.80it/s] 16%|█▌        | 18/112 [00:06<00:34,  2.76it/s]  3%|▎         | 3/112 [00:01<00:51,  2.10it/s] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 17%|█▋        | 19/112 [00:07<00:33,  2.75it/s]  4%|▎         | 4/112 [00:01<00:46,  2.31it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.75it/s]  4%|▍         | 5/112 [00:02<00:43,  2.44it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.75it/s]  5%|▌         | 6/112 [00:02<00:41,  2.55it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.75it/s]22_gate proxy err 0.021466774865984917 tr(WHW.T) 4890.9677734375
  0%|          | 0/112 [00:00<?, ?it/s]  6%|▋         | 7/112 [00:03<00:39,  2.64it/s] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 21%|██        | 23/112 [00:08<00:32,  2.75it/s]  7%|▋         | 8/112 [00:03<00:38,  2.68it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.75it/s]  1%|          | 1/112 [00:00<01:28,  1.25it/s]  8%|▊         | 9/112 [00:03<00:37,  2.72it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.73it/s]  2%|▏         | 2/112 [00:01<00:59,  1.85it/s]  9%|▉         | 10/112 [00:04<00:37,  2.74it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.74it/s]  3%|▎         | 3/112 [00:01<00:49,  2.20it/s] 10%|▉         | 11/112 [00:04<00:36,  2.76it/s]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 24%|██▍       | 27/112 [00:10<00:30,  2.74it/s]  4%|▎         | 4/112 [00:01<00:44,  2.42it/s] 11%|█         | 12/112 [00:04<00:35,  2.78it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.72it/s]  4%|▍         | 5/112 [00:02<00:42,  2.54it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s] 26%|██▌       | 29/112 [00:10<00:30,  2.68it/s]  5%|▌         | 6/112 [00:02<00:40,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.81it/s]  6%|▋         | 7/112 [00:02<00:38,  2.70it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.82it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.66it/s]  7%|▋         | 8/112 [00:03<00:38,  2.68it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.77it/s] 28%|██▊       | 31/112 [00:11<00:30,  2.68it/s]  8%|▊         | 9/112 [00:03<00:37,  2.73it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.79it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.71it/s]  9%|▉         | 10/112 [00:04<00:36,  2.77it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.81it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.70it/s] 10%|▉         | 11/112 [00:04<00:36,  2.77it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.79it/s] 30%|███       | 34/112 [00:12<00:28,  2.72it/s] 11%|█         | 12/112 [00:04<00:35,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.81it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.71it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.81it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.70it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.81it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.82it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.69it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.82it/s] 21%|██        | 23/112 [00:08<00:31,  2.82it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.68it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.82it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.82it/s] 35%|███▍      | 39/112 [00:14<00:27,  2.67it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.83it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.82it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.67it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.82it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.81it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.67it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.81it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.81it/s] 38%|███▊      | 42/112 [00:15<00:26,  2.68it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.79it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.79it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.67it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.80it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.80it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.66it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.81it/s] 40%|████      | 45/112 [00:16<00:25,  2.67it/s] 21%|██        | 23/112 [00:08<00:31,  2.81it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.82it/s] 41%|████      | 46/112 [00:17<00:24,  2.67it/s] 21%|██▏       | 24/112 [00:08<00:31,  2.82it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.82it/s] 42%|████▏     | 47/112 [00:17<00:24,  2.68it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.81it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.82it/s]I0402 09:02:40.586320 1539380 finetune.py:45] layer 23_gate initial loss 0.010016627609729767
W0402 09:02:40.586762 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 43%|████▎     | 48/112 [00:18<00:24,  2.66it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.80it/s] 30%|███       | 34/112 [00:12<00:27,  2.81it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.64it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.78it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.78it/s]W0402 09:02:41.385483 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 45%|████▍     | 50/112 [00:18<00:23,  2.66it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.78it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.78it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.78it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.67it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.78it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.79it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.79it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.67it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.81it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.81it/s] 47%|████▋     | 53/112 [00:19<00:22,  2.66it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.82it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.83it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.65it/s] 29%|██▉       | 33/112 [00:12<00:27,  2.82it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.82it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.66it/s] 30%|███       | 34/112 [00:12<00:27,  2.83it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.83it/s] 50%|█████     | 56/112 [00:21<00:21,  2.66it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.84it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.84it/s] 51%|█████     | 57/112 [00:21<00:20,  2.66it/s] 32%|███▏      | 36/112 [00:13<00:26,  2.83it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.83it/s] 52%|█████▏    | 58/112 [00:21<00:20,  2.68it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.83it/s]23_gate proxy err 0.023239679634571075 tr(WHW.T) 4776.345703125
  0%|          | 0/112 [00:00<?, ?it/s] 40%|████      | 45/112 [00:16<00:23,  2.84it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.69it/s] 34%|███▍      | 38/112 [00:13<00:26,  2.81it/s] 41%|████      | 46/112 [00:16<00:23,  2.82it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.70it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.79it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s]  1%|          | 1/112 [00:00<01:33,  1.18it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.70it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.80it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.81it/s]  2%|▏         | 2/112 [00:01<01:02,  1.77it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.83it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.82it/s]  3%|▎         | 3/112 [00:01<00:51,  2.10it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.72it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.82it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.81it/s]  4%|▎         | 4/112 [00:01<00:47,  2.29it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.72it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.83it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.82it/s]  4%|▍         | 5/112 [00:02<00:44,  2.42it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.73it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.84it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.83it/s]  5%|▌         | 6/112 [00:02<00:42,  2.51it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.72it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.81it/s] 40%|████      | 45/112 [00:16<00:23,  2.81it/s]  6%|▋         | 7/112 [00:03<00:41,  2.54it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.71it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.81it/s] 41%|████      | 46/112 [00:16<00:23,  2.80it/s]  7%|▋         | 8/112 [00:03<00:40,  2.59it/s] 61%|██████    | 68/112 [00:25<00:16,  2.72it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.80it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s]  8%|▊         | 9/112 [00:03<00:39,  2.61it/s] 62%|██████▏   | 69/112 [00:25<00:16,  2.67it/s] 50%|█████     | 56/112 [00:20<00:20,  2.79it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.78it/s]  9%|▉         | 10/112 [00:04<00:38,  2.63it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.70it/s] 51%|█████     | 57/112 [00:20<00:19,  2.81it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.80it/s] 10%|▉         | 11/112 [00:04<00:37,  2.66it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.72it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.81it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.80it/s] 11%|█         | 12/112 [00:04<00:37,  2.67it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.73it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.81it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.82it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.68it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.74it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.83it/s] 46%|████▋     | 52/112 [00:18<00:21,  2.83it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.68it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.74it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.83it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.83it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.83it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.84it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.72it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.67it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.84it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.84it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.74it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.69it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.82it/s] 50%|█████     | 56/112 [00:20<00:19,  2.82it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.74it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.69it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.83it/s] 51%|█████     | 57/112 [00:20<00:19,  2.83it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.73it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.67it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.83it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.78it/s] 71%|███████   | 79/112 [00:29<00:12,  2.74it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.67it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.82it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.80it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.66it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.83it/s] 61%|██████    | 68/112 [00:24<00:15,  2.82it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.72it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.65it/s] 54%|█████▍    | 61/112 [00:22<00:17,  2.83it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.82it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.72it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.66it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.83it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.82it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.71it/s] 21%|██        | 23/112 [00:09<00:33,  2.64it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.84it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.83it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.71it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.65it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.79it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.78it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.73it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.66it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.80it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.80it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.73it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.66it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.81it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.81it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.74it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.67it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.81it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.82it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.74it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.67it/s] 61%|██████    | 68/112 [00:24<00:15,  2.81it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.81it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.73it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.66it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.82it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.82it/s] 80%|████████  | 90/112 [00:33<00:08,  2.74it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.67it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.81it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.82it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.75it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.67it/s] 71%|███████   | 79/112 [00:28<00:11,  2.83it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.82it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.74it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.84it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.80it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.67it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.74it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.82it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.80it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.67it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.75it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.83it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.81it/s] 30%|███       | 34/112 [00:13<00:29,  2.66it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.73it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.84it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.83it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.66it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.74it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.84it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.82it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.68it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.75it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.85it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.83it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.67it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.73it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.83it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.82it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.67it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.75it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.84it/s] 71%|███████   | 79/112 [00:28<00:11,  2.82it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.69it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.75it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.84it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.83it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.68it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.75it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.85it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.83it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.68it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.75it/s] 80%|████████  | 90/112 [00:32<00:07,  2.82it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.80it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.68it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.74it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.80it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.79it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.67it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.74it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.81it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.80it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.68it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.75it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.83it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.82it/s] 40%|████      | 45/112 [00:17<00:25,  2.68it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.84it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.74it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.83it/s] 41%|████      | 46/112 [00:17<00:24,  2.68it/s] 85%|████████▍ | 95/112 [00:34<00:05,  2.84it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.75it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.83it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.68it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.83it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.75it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.82it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.67it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.84it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.83it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.74it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.84it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.67it/s] 80%|████████  | 90/112 [00:32<00:07,  2.83it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.74it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.83it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.68it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.82it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.75it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.84it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.83it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.67it/s]100%|██████████| 112/112 [00:41<00:00,  2.74it/s]100%|██████████| 112/112 [00:41<00:00,  2.69it/s]
 90%|█████████ | 101/112 [00:36<00:03,  2.83it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.83it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.68it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.84it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.84it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.65it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.85it/s] 85%|████████▍ | 95/112 [00:34<00:05,  2.84it/s] 48%|████▊     | 54/112 [00:20<00:22,  2.61it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.85it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.84it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.63it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.85it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.84it/s] 50%|█████     | 56/112 [00:21<00:21,  2.64it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.84it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.82it/s] 51%|█████     | 57/112 [00:21<00:20,  2.62it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.83it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.83it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.65it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.82it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.82it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.64it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.80it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.79it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.62it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.81it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.80it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.64it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.83it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.82it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.63it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]
 93%|█████████▎| 104/112 [00:37<00:02,  2.82it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.66it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.82it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.67it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.75it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.65it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.75it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.67it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.77it/s]W0402 09:03:10.283000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.283000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.283000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.284000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.284000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.284000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.284000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.327000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.327000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.327000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.327000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.327000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 60%|█████▉    | 67/112 [00:25<00:16,  2.68it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.76it/s]W0402 09:03:10.501000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.502000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.502000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.502000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.502000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 110/112 [00:39<00:00,  2.79it/s] 61%|██████    | 68/112 [00:25<00:16,  2.67it/s]W0402 09:03:10.805000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.805000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.805000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.805000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.805000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.805000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.805000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.836000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.836000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.837000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.837000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.837000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.903000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.903000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.903000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.903000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:10.903000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:39<00:00,  2.83it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.68it/s]100%|██████████| 112/112 [00:40<00:00,  2.84it/s]100%|██████████| 112/112 [00:40<00:00,  2.78it/s]
 62%|██████▎   | 70/112 [00:26<00:15,  2.68it/s]W0402 09:03:11.827000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:11.833000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:11.838000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:11.839000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 63%|██████▎   | 71/112 [00:27<00:15,  2.68it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.68it/s]W0402 09:03:12.263000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.263000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.263000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.264000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.264000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.264000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.264000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.293000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.293000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.293000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.293000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.293000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.585000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.586000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.586000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.586000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.586000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.586000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.586000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.586000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 65%|██████▌   | 73/112 [00:27<00:14,  2.67it/s]W0402 09:03:12.863000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.863000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.863000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.863000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:12.863000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 74/112 [00:28<00:14,  2.69it/s]W0402 09:03:13.269000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:13.274000 139625575790400 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 75/112 [00:28<00:13,  2.69it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.67it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.65it/s]W0402 09:03:14.200000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.201000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.201000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.201000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.201000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.201000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.201000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.240000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.241000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.241000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.241000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.241000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.405000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.405000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.405000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.405000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.405000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:29<00:13,  2.60it/s]W0402 09:03:14.704000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.704000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.705000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.705000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.705000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.705000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.705000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.733000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.733000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.733000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.734000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.734000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.798000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.798000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.798000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.798000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:14.798000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:30<00:12,  2.59it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.60it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.60it/s]W0402 09:03:15.702000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:15.714000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:15.722000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:15.722000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:31<00:11,  2.61it/s]W0402 09:03:16.146000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.146000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.146000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.146000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.146000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.147000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.147000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.174000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.175000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.175000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.175000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.175000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:31<00:11,  2.61it/s]W0402 09:03:16.463000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.463000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.463000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.463000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.463000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.463000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.464000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.464000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.741000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.741000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.741000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.741000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:16.741000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:32<00:10,  2.61it/s]W0402 09:03:17.042000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.042000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.042000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.042000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.042000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.042000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.042000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.083000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.083000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.083000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.083000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.083000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.150000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.155000 140445485397824 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 76%|███████▌  | 85/112 [00:32<00:10,  2.61it/s]W0402 09:03:17.250000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.250000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.250000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.250000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.250000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:32<00:09,  2.62it/s]W0402 09:03:17.560000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.560000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.560000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.561000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.561000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.561000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.561000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.593000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.593000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.593000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.593000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.593000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.658000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.658000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.658000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.658000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:17.659000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:33<00:09,  2.62it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.63it/s]W0402 09:03:18.561000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:18.574000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:18.582000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:18.582000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:33<00:08,  2.63it/s]W0402 09:03:19.032000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.032000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.032000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.033000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.033000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.033000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.033000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.064000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.064000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.065000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.065000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.065000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 90/112 [00:34<00:08,  2.64it/s]W0402 09:03:19.366000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.366000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.366000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.366000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.366000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.366000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.366000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.366000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:34<00:07,  2.65it/s]W0402 09:03:19.665000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.665000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.665000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.665000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:19.665000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:35<00:07,  2.64it/s]W0402 09:03:20.101000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:20.106000 139816213272384 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:07,  2.64it/s]I0402 09:03:20.344094 1538312 finetune.py:45] layer 20_down initial loss 0.005592158064246178
W0402 09:03:20.344356 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 94/112 [00:35<00:06,  2.66it/s]W0402 09:03:20.823282 1538312 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 85%|████████▍ | 95/112 [00:36<00:06,  2.67it/s]20_down proxy err 0.049219369888305664 tr(WHW.T) 23.935514450073242
 86%|████████▌ | 96/112 [00:36<00:05,  2.69it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.71it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.72it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.73it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.73it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.72it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.71it/s]I0402 09:03:23.685408 1538976 finetune.py:45] layer 21_down initial loss 0.008563960902392864
W0402 09:03:23.685752 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 92%|█████████▏| 103/112 [00:39<00:03,  2.74it/s]W0402 09:03:24.194402 1538976 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 93%|█████████▎| 104/112 [00:39<00:02,  2.74it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.76it/s]21_down proxy err 0.04649203643202782 tr(WHW.T) 29.132450103759766
 95%|█████████▍| 106/112 [00:40<00:02,  2.75it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.77it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.77it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.76it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.75it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.75it/s]I0402 09:03:26.801485 1539137 finetune.py:45] layer 22_down initial loss 0.006375759840011597
W0402 09:03:26.801757 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 112/112 [00:42<00:00,  2.73it/s]100%|██████████| 112/112 [00:42<00:00,  2.64it/s]
W0402 09:03:27.236702 1539137 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

22_down proxy err 0.04683291167020798 tr(WHW.T) 30.777223587036133
I0402 09:03:27.791917 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 24 in 1.190661907196045s
I0402 09:03:28.299600 1509661 quantize_finetune_llama.py:159] layer 25 gpu 1
I0402 09:03:30.315854 1543699 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:03:30.316031 1543699 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:03:30.316101 1543699 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:03:30.627675 1543699 config.py:58] PyTorch version 2.4.0 available.
I0402 09:03:30.786214 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 25 in 1.32183837890625s
I0402 09:03:31.313560 1509661 quantize_finetune_llama.py:159] layer 26 gpu 2
I0402 09:03:33.007096 1543699 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 09:03:33.310018 1543931 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:03:33.310173 1543931 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:03:33.310228 1543931 utils.py:162] NumExpr defaulting to 16 threads.
W0402 09:03:33.366202 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:03:33.643000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.644000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.644000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.644000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.644000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.644000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.644000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
I0402 09:03:33.662374 1543931 config.py:58] PyTorch version 2.4.0 available.
W0402 09:03:33.685000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.685000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.685000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.685000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.685000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.856000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.856000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.856000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.856000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:33.857000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.163000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.163000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.164000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.164000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.164000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.164000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.164000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.193000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.193000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.193000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.193000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.193000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.261000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.261000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.261000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.261000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:34.261000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:03:35.205000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.211000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.217000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.217000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.680000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.680000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.681000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.681000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.681000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.681000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.681000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.712000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.712000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.712000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.712000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:35.712000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:47,  1.52s/it]W0402 09:03:36.019000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.019000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.019000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.020000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.020000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.020000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.020000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.020000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
I0402 09:03:36.040704 1543931 data_utils.py:336] using 256 training seqs, 128 validation seqs
  6%|▋         | 2/32 [00:01<00:24,  1.24it/s]W0402 09:03:36.309000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.309000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.309000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.309000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.309000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.417144 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:02<00:16,  1.72it/s]W0402 09:03:36.721000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:36.725000 140470619199296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:02<00:13,  2.09it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s]  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:03<00:09,  2.63it/s] 22%|██▏       | 7/32 [00:03<00:08,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.95it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.03it/s] 31%|███▏      | 10/32 [00:04<00:07,  3.07it/s]  3%|▎         | 1/32 [00:01<00:48,  1.57s/it] 34%|███▍      | 11/32 [00:04<00:06,  3.13it/s]  6%|▋         | 2/32 [00:01<00:25,  1.20it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.17it/s]  9%|▉         | 3/32 [00:02<00:17,  1.66it/s] 41%|████      | 13/32 [00:05<00:05,  3.19it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.22it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.52it/s] 50%|█████     | 16/32 [00:06<00:05,  3.18it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.18it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.22it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.22it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.23it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.23it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.97it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.24it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.01it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.25it/s] 41%|████      | 13/32 [00:05<00:06,  3.03it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.27it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.07it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.28it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.09it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.28it/s] 50%|█████     | 16/32 [00:06<00:05,  3.10it/s]I0402 09:03:43.794912 1539380 finetune.py:45] layer 23_down initial loss 0.009540953673422337
W0402 09:03:43.795384 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:09<00:01,  3.28it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.10it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.29it/s]W0402 09:03:44.339706 1539380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 56%|█████▋    | 18/32 [00:07<00:04,  3.11it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.27it/s]23_down proxy err 0.04715314134955406 tr(WHW.T) 32.75361251831055
 59%|█████▉    | 19/32 [00:07<00:04,  3.11it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.26it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.12it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.26it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.13it/s]100%|██████████| 32/32 [00:11<00:00,  3.26it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  3.10it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.05it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.03it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.96it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.99it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.02it/s]W0402 09:03:47.821000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.822000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.822000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.822000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.822000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.822000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.822000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.849000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.849000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.849000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.849000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:47.849000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:00,  3.05it/s]W0402 09:03:48.127000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:48.127000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:48.127000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:48.127000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:48.127000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:10<00:00,  3.07it/s]I0402 09:03:48.397838 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 26 in 1.5731158256530762s
 97%|█████████▋| 31/32 [00:11<00:00,  3.04it/s]I0402 09:03:48.969412 1509661 quantize_finetune_llama.py:159] layer 27 gpu 3
W0402 09:03:48.987000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:48.987000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:48.987000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:48.987000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:48.987000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:48.987000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:48.987000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:49.006000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:49.006000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:49.006000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:49.006000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:49.006000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
W0402 09:03:49.212000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:49.212000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:49.212000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:49.212000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:49.213000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.338000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.338000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.338000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.338000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.338000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.338000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.338000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.355000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.355000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.356000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.356000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:50.356000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
I0402 09:03:50.961126 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 27 in 1.444139003753662s
I0402 09:03:50.981556 1544940 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:03:50.981736 1544940 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:03:50.981854 1544940 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:03:51.201083 1544940 config.py:58] PyTorch version 2.4.0 available.
W0402 09:03:51.201000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.201000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.202000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.202000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.202000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.352000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.353000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.353000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.353000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.353000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.353000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.353000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.380000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.380000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.380000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.380000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.380000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
I0402 09:03:51.488467 1509661 quantize_finetune_llama.py:159] layer 28 gpu 0
W0402 09:03:51.676000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.676000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.676000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.677000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:51.677000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 09:03:52.538000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.538000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.538000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.538000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.538000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.539000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.539000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.557000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.557000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.557000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.557000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.558000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.764000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.764000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.764000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.764000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:52.764000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
I0402 09:03:53.442360 1544940 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 09:03:53.454605 1545102 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:03:53.454738 1545102 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:03:53.454803 1545102 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:03:53.649146 1545102 config.py:58] PyTorch version 2.4.0 available.
W0402 09:03:53.791063 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:03:53.902000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.903000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.903000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.903000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.903000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.903000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.903000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.921000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.921000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.921000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.921000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:53.922000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:03:54.810000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:03:54.810000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:03:54.810000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:03:54.810000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:03:54.810000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0402 09:03:55.776650 1545102 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0402 09:03:56.111805 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:51,  1.67s/it]  6%|▋         | 2/32 [00:02<00:26,  1.12it/s]  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:18,  1.54it/s]I0402 09:03:57.277965 1543699 finetune.py:45] layer 24_v initial loss 0.012516687624156475
W0402 09:03:57.278181 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.14it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s]W0402 09:03:58.299276 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:47,  1.52s/it] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s]  6%|▋         | 2/32 [00:01<00:24,  1.22it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.59it/s]  9%|▉         | 3/32 [00:02<00:17,  1.68it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s]24_v proxy err 0.045775167644023895 tr(WHW.T) 136.39785766601562
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s]I0402 09:04:00.723791 1543931 finetune.py:45] layer 25_v initial loss 0.01338131446391344
W0402 09:04:00.723997 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s]  3%|▎         | 1/32 [00:00<00:29,  1.05it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.88it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.85it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.92it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.90it/s]  9%|▉         | 3/32 [00:01<00:13,  2.10it/s]W0402 09:04:01.641511 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:06<00:05,  2.96it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.94it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.95it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.96it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.53it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.00it/s] 41%|████      | 13/32 [00:05<00:06,  2.98it/s]25_v proxy err 0.04013826698064804 tr(WHW.T) 164.04367065429688
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.03it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.99it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.70it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.04it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.00it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.03it/s]  3%|▎         | 1/32 [00:00<00:27,  1.13it/s] 50%|█████     | 16/32 [00:06<00:05,  3.01it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.03it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.02it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.04it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.02it/s]  9%|▉         | 3/32 [00:01<00:13,  2.14it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.06it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.03it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.38it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.07it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.86it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.02it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.55it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.07it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.03it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.08it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.03it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.09it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.88it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.04it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.08it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.04it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.08it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.04it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.07it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.89it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.03it/s]100%|██████████| 32/32 [00:12<00:00,  3.03it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.89it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.03it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.03it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.03it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.89it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.03it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.81it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.04it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.88it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  3.05it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
 75%|███████▌  | 24/32 [00:08<00:02,  2.89it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s]W0402 09:04:09.257000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.257000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.257000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.257000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.257000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.257000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.258000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.283000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.283000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.283000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.283000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.283000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s]W0402 09:04:09.574000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.574000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.574000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.574000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:09.574000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s]W0402 09:04:10.410000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.411000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.411000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.411000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.411000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.411000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.411000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.428000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.428000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.428000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.428000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.428000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:07<00:03,  2.84it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s]W0402 09:04:10.626000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.627000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.627000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.627000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:10.627000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.86it/s]W0402 09:04:11.081000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.081000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.081000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.082000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.082000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.082000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.082000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.107000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.107000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.107000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.107000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.107000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s]W0402 09:04:11.384000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.384000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.384000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.384000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.384000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:08<00:02,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
W0402 09:04:11.686000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.687000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.687000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.687000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.687000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.687000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.687000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.704000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.704000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.704000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.704000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:11.704000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.87it/s]W0402 09:04:12.209000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.209000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.209000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.209000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.209000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.209000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.209000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.227000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.227000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.227000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.227000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.227000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.82it/s]W0402 09:04:12.418000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.419000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.419000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.419000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.419000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.558000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.558000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.558000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.558000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:12.558000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 91%|█████████ | 29/32 [00:10<00:01,  2.82it/s]W0402 09:04:13.488000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.488000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.488000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.488000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.488000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.489000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.489000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.506000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.506000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.506000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.506000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:13.506000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s]W0402 09:04:14.336000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:14.336000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:14.336000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:14.336000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:14.336000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 09:04:16.684000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.684000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.684000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.684000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.684000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.684000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.685000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.716000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.716000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.716000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.716000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.716000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.892000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.892000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.892000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.892000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:16.892000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.121000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.121000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.121000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.121000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.121000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.121000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.121000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.144000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.144000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.144000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.144000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.144000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.212000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.212000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.212000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.212000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.213000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:17.965000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.288000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.289000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.289000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.289000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.289000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.289000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.289000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.311000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.311000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.311000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.311000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.311000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
I0402 09:04:18.312135 1544940 finetune.py:45] layer 26_v initial loss 0.017920145764946938
W0402 09:04:18.312307 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:04:18.552000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.552000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.553000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.553000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.553000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:18.895000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:19.225313 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_v proxy err 0.053091760724782944 tr(WHW.T) 123.81900024414062
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:04:20.258063 1545102 finetune.py:45] layer 27_v initial loss 0.02067391946911812
W0402 09:04:20.258384 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:04:20.611000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.612000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.612000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.612000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.612000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.613000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.613000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.644000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.644000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.644000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.644000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.644000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.818000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.818000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.818000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.818000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:20.818000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.032000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.032000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.032000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.032000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.032000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.032000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.032000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.053000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.053000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.053000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.053000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.053000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]W0402 09:04:21.116000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.116000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.116000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.116000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.116000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:21.197306 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]W0402 09:04:21.850000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:01<00:14,  2.01it/s]W0402 09:04:22.156000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.156000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.156000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.156000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.156000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.156000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.156000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.177000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.177000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.177000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.177000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.178000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
27_v proxy err 0.04227025434374809 tr(WHW.T) 203.18115234375
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s]W0402 09:04:22.427000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.427000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.427000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.427000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:22.427000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s]W0402 09:04:22.765000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s]  6%|▋         | 2/32 [00:01<00:17,  1.72it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.33it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.58it/s]I0402 09:04:24.958943 1543699 finetune.py:45] layer 24_q initial loss 0.01252448558807373
W0402 09:04:24.959215 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s]W0402 09:04:26.124213 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:04<00:07,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s]24_q proxy err 0.007459725718945265 tr(WHW.T) 6549.0634765625
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.73it/s]  3%|▎         | 1/32 [00:00<00:27,  1.14it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.13it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s]I0402 09:04:29.794850 1543931 finetune.py:45] layer 25_q initial loss 0.013380243442952633
W0402 09:04:29.795281 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s]W0402 09:04:31.231631 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:03<00:07,  2.87it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s]25_q proxy err 0.006397170480340719 tr(WHW.T) 7679.1796875
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:04<00:06,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.95it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s]  3%|▎         | 1/32 [00:00<00:27,  1.12it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.27it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.98it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.99it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.99it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.00it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.01it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.02it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.03it/s]W0402 09:04:37.220000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.221000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.221000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.221000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.221000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.221000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.221000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s]W0402 09:04:37.249000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.249000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.249000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.249000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.249000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  3.03it/s]W0402 09:04:37.412000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.412000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.412000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.412000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.412000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:05<00:07,  2.67it/s]W0402 09:04:37.624000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.625000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.625000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.625000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.625000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.625000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.625000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.644000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.644000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.644000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.644000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.644000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 29/32 [00:10<00:00,  3.03it/s]W0402 09:04:37.707000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.707000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.707000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.707000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:37.707000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.02it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.02it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s]W0402 09:04:38.414000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  3.03it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
W0402 09:04:38.712000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.712000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.713000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.713000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.713000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.713000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.713000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.733000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.733000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.733000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.733000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.733000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.67it/s]W0402 09:04:38.984000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.984000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.984000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.984000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:38.984000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.105000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.105000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.105000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.105000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.105000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.105000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.105000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s]W0402 09:04:39.135000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.135000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.135000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.135000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.135000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.301000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.301000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.301000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.301000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.301000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.336000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s]W0402 09:04:39.522000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.522000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.522000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.523000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.523000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.523000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.523000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.543000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.543000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.543000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.543000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.543000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.606000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.606000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.607000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.607000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:39.607000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.67it/s]W0402 09:04:40.316000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s]W0402 09:04:40.623000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.624000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.624000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.624000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.624000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.624000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.624000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.646000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.646000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.646000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.646000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.646000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.894000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.894000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.894000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.894000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:04:40.894000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s]W0402 09:04:41.235000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
I0402 09:04:44.911123 1543699 finetune.py:45] layer 24_k initial loss 0.012529607862234116
W0402 09:04:44.911394 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0402 09:04:45.406519 1544940 finetune.py:45] layer 26_q initial loss 0.01791112683713436
W0402 09:04:45.406708 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:04:45.918671 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:04:46.357744 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_k proxy err 0.00639828946441412 tr(WHW.T) 4140.12353515625
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:04:47.224026 1545102 finetune.py:45] layer 27_q initial loss 0.020658977329730988
W0402 09:04:47.224241 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

26_q proxy err 0.00745640741661191 tr(WHW.T) 6098.626953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:23,  1.34it/s]  6%|▋         | 2/32 [00:01<00:15,  1.99it/s]  3%|▎         | 1/32 [00:00<00:26,  1.17it/s]W0402 09:04:48.240368 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:01<00:12,  2.33it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.54it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.68it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.38it/s]27_q proxy err 0.007733277510851622 tr(WHW.T) 6387.7978515625
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.77it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.62it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.84it/s]  3%|▎         | 1/32 [00:00<00:27,  1.13it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.87it/s]  6%|▋         | 2/32 [00:01<00:17,  1.74it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.87it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s] 41%|████      | 13/32 [00:04<00:06,  2.91it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s]I0402 09:04:51.976059 1543931 finetune.py:45] layer 25_k initial loss 0.013375758193433285
W0402 09:04:51.976355 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 41%|████      | 13/32 [00:05<00:06,  2.85it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s] 50%|█████     | 16/32 [00:05<00:05,  2.92it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.69it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.83it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s]W0402 09:04:53.374453 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.92it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.77it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s]25_k proxy err 0.00640203757211566 tr(WHW.T) 4239.72509765625
  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.92it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.79it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.86it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s]  3%|▎         | 1/32 [00:00<00:24,  1.29it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s]  6%|▋         | 2/32 [00:01<00:16,  1.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.92it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.89it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.93it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.33it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.94it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.91it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.92it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.93it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.91it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s]I0402 09:05:04.872485 1543699 finetune.py:45] layer 24_o initial loss 0.012172103859484196
W0402 09:05:04.872682 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s]I0402 09:05:05.437562 1544940 finetune.py:45] layer 26_k initial loss 0.017883656546473503
W0402 09:05:05.437737 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:10<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s]W0402 09:05:05.928523 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:11<00:00,  2.59it/s]W0402 09:05:06.365777 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
24_o proxy err 0.04136400669813156 tr(WHW.T) 6.529373645782471
  0%|          | 0/32 [00:00<?, ?it/s]26_k proxy err 0.006029495969414711 tr(WHW.T) 4395.3486328125
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:05:07.422445 1545102 finetune.py:45] layer 27_k initial loss 0.020640991628170013
W0402 09:05:07.422706 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:00<00:22,  1.35it/s]W0402 09:05:08.352505 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:01<00:15,  1.96it/s]  9%|▉         | 3/32 [00:01<00:12,  2.30it/s]  3%|▎         | 1/32 [00:01<00:56,  1.83s/it] 12%|█▎        | 4/32 [00:01<00:11,  2.52it/s]27_k proxy err 0.00694070290774107 tr(WHW.T) 4200.8603515625
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.64it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.71it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.77it/s]  3%|▎         | 1/32 [00:00<00:24,  1.27it/s]  6%|▋         | 2/32 [00:03<00:47,  1.57s/it] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.85it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.33it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s]  9%|▉         | 3/32 [00:04<00:43,  1.49s/it] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.88it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 41%|████      | 13/32 [00:04<00:06,  2.88it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.87it/s] 12%|█▎        | 4/32 [00:05<00:40,  1.45s/it] 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 50%|█████     | 16/32 [00:05<00:05,  2.88it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.88it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.89it/s]I0402 09:05:14.187990 1543931 finetune.py:45] layer 25_o initial loss 0.0133292768150568
W0402 09:05:14.188240 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.90it/s] 16%|█▌        | 5/32 [00:07<00:38,  1.43s/it] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.90it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.89it/s]W0402 09:05:15.300791 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:07<00:03,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.75it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 19%|█▉        | 6/32 [00:08<00:36,  1.42s/it] 75%|███████▌  | 24/32 [00:08<00:02,  2.87it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.87it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s]25_o proxy err 0.033538538962602615 tr(WHW.T) 8.825087547302246
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.89it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 22%|██▏       | 7/32 [00:10<00:35,  1.41s/it] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.90it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.80it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 25%|██▌       | 8/32 [00:11<00:33,  1.41s/it]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 28%|██▊       | 9/32 [00:12<00:32,  1.41s/it] 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it]I0402 09:05:25.204410 1544940 finetune.py:45] layer 26_o initial loss 0.01757643185555935
W0402 09:05:25.204658 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:18<00:26,  1.42s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it]W0402 09:05:26.109124 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it]26_o proxy err 0.02540883980691433 tr(WHW.T) 16.217060089111328
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it]I0402 09:05:28.058107 1545102 finetune.py:45] layer 27_o initial loss 0.020396603271365166
W0402 09:05:28.058402 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it]W0402 09:05:28.965096 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:01<00:56,  1.84s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 50%|█████     | 16/32 [00:22<00:22,  1.42s/it]27_o proxy err 0.03019145503640175 tr(WHW.T) 16.17061424255371
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:46,  1.56s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it]  9%|▉         | 3/32 [00:04<00:42,  1.47s/it]  3%|▎         | 1/32 [00:01<00:57,  1.86s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.42s/it] 12%|█▎        | 4/32 [00:05<00:40,  1.44s/it]  6%|▋         | 2/32 [00:03<00:48,  1.60s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.41s/it]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.42s/it] 19%|█▉        | 6/32 [00:08<00:36,  1.40s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:34,  1.39s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.39s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.43s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.42s/it] 28%|██▊       | 9/32 [00:12<00:31,  1.38s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.42s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.38s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 34%|███▍      | 11/32 [00:15<00:28,  1.37s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.42s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 38%|███▊      | 12/32 [00:16<00:27,  1.37s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 41%|████      | 13/32 [00:18<00:26,  1.37s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.43s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.41s/it] 44%|████▍     | 14/32 [00:19<00:24,  1.37s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.43s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.37s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.43s/it] 41%|████      | 13/32 [00:18<00:26,  1.42s/it] 50%|█████     | 16/32 [00:22<00:21,  1.37s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.43s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 53%|█████▎    | 17/32 [00:23<00:20,  1.37s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.43s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.37s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.38s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 62%|██████▎   | 20/32 [00:27<00:16,  1.39s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.41s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 69%|██████▉   | 22/32 [00:30<00:14,  1.43s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.47s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.44s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it]I0402 09:06:00.298696 1543699 finetune.py:45] layer 24_up initial loss 0.011804645881056786
W0402 09:06:00.298919 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:33<00:11,  1.44s/it]W0402 09:06:01.128612 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.48s/it]24_up proxy err 0.048210300505161285 tr(WHW.T) 1342.69384765625
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:35<00:09,  1.42s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.41s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.41s/it]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 84%|████████▍ | 27/32 [00:37<00:06,  1.40s/it]  6%|▋         | 2/32 [00:03<00:47,  1.57s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.42s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.39s/it]  9%|▉         | 3/32 [00:04<00:43,  1.50s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.38s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.47s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.43s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.39s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.45s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.45s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.38s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.45s/it]100%|██████████| 32/32 [00:44<00:00,  1.38s/it]100%|██████████| 32/32 [00:44<00:00,  1.40s/it]
 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it]I0402 09:06:12.688483 1543931 finetune.py:45] layer 25_up initial loss 0.013122676871716976
W0402 09:06:12.688766 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:43<00:02,  1.46s/it]W0402 09:06:13.510079 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.44s/it]25_up proxy err 0.04843603074550629 tr(WHW.T) 1428.771728515625
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
  3%|▎         | 1/32 [00:01<00:58,  1.87s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.43s/it]I0402 09:06:19.301644 1544940 finetune.py:45] layer 26_up initial loss 0.01679021306335926
W0402 09:06:19.301814 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it]W0402 09:06:20.138734 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:18<00:27,  1.43s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it]26_up proxy err 0.047525644302368164 tr(WHW.T) 1584.181640625
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:20<00:25,  1.43s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it]I0402 09:06:23.612347 1545102 finetune.py:45] layer 27_up initial loss 0.019823415204882622
W0402 09:06:23.612568 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:21<00:24,  1.43s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it]  6%|▋         | 2/32 [00:03<00:46,  1.56s/it]W0402 09:06:24.491294 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:23<00:22,  1.43s/it]27_up proxy err 0.04430178180336952 tr(WHW.T) 1872.387451171875
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:43,  1.49s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.56s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.43s/it] 12%|█▎        | 4/32 [00:05<00:40,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.54s/it]  3%|▎         | 1/32 [00:01<00:56,  1.83s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.43s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.43s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it]  6%|▋         | 2/32 [00:03<00:48,  1.60s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.43s/it] 19%|█▉        | 6/32 [00:08<00:36,  1.42s/it]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 62%|██████▎   | 20/32 [00:28<00:17,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.40s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.43s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.40s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.44s/it] 28%|██▊       | 9/32 [00:12<00:32,  1.39s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.45s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.39s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.44s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.38s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 38%|███▊      | 12/32 [00:17<00:27,  1.38s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 41%|████      | 13/32 [00:18<00:26,  1.38s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 44%|████▍     | 14/32 [00:19<00:24,  1.38s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.38s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it] 50%|█████     | 16/32 [00:22<00:21,  1.37s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.44s/it] 41%|████      | 13/32 [00:18<00:27,  1.42s/it] 53%|█████▎    | 17/32 [00:23<00:20,  1.38s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.38s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.44s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.38s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 69%|██████▉   | 22/32 [00:33<00:15,  1.53s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.40s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.44s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 75%|███████▌  | 24/32 [00:33<00:11,  1.45s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it]I0402 09:06:56.034307 1543699 finetune.py:45] layer 24_gate initial loss 0.011319500394165516
W0402 09:06:56.034553 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it] 78%|███████▊  | 25/32 [00:35<00:10,  1.44s/it]W0402 09:06:56.815253 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:31<00:14,  1.42s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.43s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it]24_gate proxy err 0.024923451244831085 tr(WHW.T) 4752.3671875
  0%|          | 0/112 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it]  1%|          | 1/112 [00:00<01:26,  1.29it/s]  2%|▏         | 2/112 [00:01<00:57,  1.91it/s] 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.41s/it]  3%|▎         | 3/112 [00:01<00:49,  2.22it/s]  4%|▎         | 4/112 [00:01<00:44,  2.41it/s] 78%|███████▊  | 25/32 [00:35<00:10,  1.43s/it]  4%|▍         | 5/112 [00:02<00:41,  2.55it/s]  5%|▌         | 6/112 [00:02<00:40,  2.63it/s] 91%|█████████ | 29/32 [00:40<00:04,  1.40s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]  6%|▋         | 7/112 [00:02<00:39,  2.68it/s]  7%|▋         | 8/112 [00:03<00:38,  2.73it/s] 81%|████████▏ | 26/32 [00:37<00:08,  1.43s/it]  8%|▊         | 9/112 [00:03<00:37,  2.75it/s]  9%|▉         | 10/112 [00:03<00:36,  2.76it/s] 94%|█████████▍| 30/32 [00:42<00:02,  1.40s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
 10%|▉         | 11/112 [00:04<00:36,  2.78it/s] 11%|█         | 12/112 [00:04<00:35,  2.78it/s] 84%|████████▍ | 27/32 [00:38<00:07,  1.43s/it] 12%|█▏        | 13/112 [00:05<00:35,  2.79it/s] 97%|█████████▋| 31/32 [00:43<00:01,  1.39s/it] 12%|█▎        | 14/112 [00:05<00:34,  2.80it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.80it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.79it/s] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 15%|█▌        | 17/112 [00:06<00:33,  2.81it/s]100%|██████████| 32/32 [00:45<00:00,  1.39s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 16%|█▌        | 18/112 [00:06<00:33,  2.80it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.80it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.79it/s] 91%|█████████ | 29/32 [00:41<00:04,  1.45s/it] 19%|█▉        | 21/112 [00:07<00:32,  2.81it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s] 21%|██        | 23/112 [00:08<00:31,  2.79it/s] 21%|██▏       | 24/112 [00:08<00:31,  2.81it/s] 94%|█████████▍| 30/32 [00:43<00:02,  1.46s/it] 22%|██▏       | 25/112 [00:09<00:31,  2.79it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.78it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.79it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.80it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.79it/s] 97%|█████████▋| 31/32 [00:44<00:01,  1.46s/it] 27%|██▋       | 30/112 [00:11<00:29,  2.79it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.80it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.80it/s]I0402 09:07:11.671842 1543931 finetune.py:45] layer 25_gate initial loss 0.012732494622468948
W0402 09:07:11.672080 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 29%|██▉       | 33/112 [00:12<00:28,  2.79it/s]100%|██████████| 32/32 [00:46<00:00,  1.47s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 30%|███       | 34/112 [00:12<00:27,  2.79it/s]W0402 09:07:12.503325 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 31%|███▏      | 35/112 [00:12<00:27,  2.80it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.80it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.79it/s] 34%|███▍      | 38/112 [00:13<00:26,  2.79it/s]I0402 09:07:13.692884 1544940 finetune.py:45] layer 26_gate initial loss 0.01585199311375618
W0402 09:07:13.693110 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 35%|███▍      | 39/112 [00:14<00:26,  2.80it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.78it/s]W0402 09:07:14.366936 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 37%|███▋      | 41/112 [00:15<00:25,  2.79it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.79it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.77it/s]25_gate proxy err 0.024786636233329773 tr(WHW.T) 5091.62109375
  0%|          | 0/112 [00:00<?, ?it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.78it/s] 40%|████      | 45/112 [00:16<00:23,  2.80it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s] 41%|████      | 46/112 [00:16<00:23,  2.78it/s]  2%|▏         | 2/112 [00:01<01:04,  1.71it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s]26_gate proxy err 0.022877279669046402 tr(WHW.T) 5988.38623046875
  0%|          | 0/112 [00:00<?, ?it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.80it/s]  3%|▎         | 3/112 [00:01<00:54,  2.02it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.78it/s]  4%|▎         | 4/112 [00:02<00:48,  2.21it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.78it/s]  1%|          | 1/112 [00:00<01:27,  1.27it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.79it/s]  2%|▏         | 2/112 [00:01<00:58,  1.88it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.78it/s]  3%|▎         | 3/112 [00:01<00:48,  2.23it/s]  6%|▋         | 7/112 [00:03<00:42,  2.45it/s]  4%|▎         | 4/112 [00:01<00:43,  2.46it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.78it/s]  7%|▋         | 8/112 [00:03<00:41,  2.48it/s]  4%|▍         | 5/112 [00:02<00:41,  2.60it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.79it/s]  8%|▊         | 9/112 [00:03<00:41,  2.51it/s]I0402 09:07:19.561381 1545102 finetune.py:45] layer 27_gate initial loss 0.01927519217133522
W0402 09:07:19.561627 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  5%|▌         | 6/112 [00:02<00:39,  2.69it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.78it/s]  9%|▉         | 10/112 [00:04<00:40,  2.51it/s]  6%|▋         | 7/112 [00:02<00:38,  2.74it/s] 50%|█████     | 56/112 [00:20<00:20,  2.78it/s]W0402 09:07:20.312701 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 10%|▉         | 11/112 [00:04<00:40,  2.52it/s]  7%|▋         | 8/112 [00:03<00:37,  2.77it/s] 51%|█████     | 57/112 [00:20<00:19,  2.79it/s]  8%|▊         | 9/112 [00:03<00:36,  2.81it/s] 11%|█         | 12/112 [00:05<00:39,  2.55it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.78it/s]  9%|▉         | 10/112 [00:03<00:35,  2.84it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.55it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.78it/s] 10%|▉         | 11/112 [00:04<00:35,  2.86it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.79it/s] 11%|█         | 12/112 [00:04<00:34,  2.86it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.78it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.55it/s] 12%|█▏        | 13/112 [00:04<00:34,  2.88it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.78it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.55it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.88it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.78it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.58it/s] 13%|█▎        | 15/112 [00:05<00:33,  2.89it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.78it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 14%|█▍        | 16/112 [00:05<00:33,  2.89it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.78it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.62it/s]27_gate proxy err 0.020639605820178986 tr(WHW.T) 7248.01416015625
  0%|          | 0/112 [00:00<?, ?it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.86it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.78it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.64it/s] 16%|█▌        | 18/112 [00:06<00:32,  2.88it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.78it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.65it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.88it/s]  1%|          | 1/112 [00:00<01:29,  1.24it/s] 61%|██████    | 68/112 [00:24<00:15,  2.78it/s] 18%|█▊        | 20/112 [00:07<00:31,  2.88it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.66it/s]  2%|▏         | 2/112 [00:01<00:59,  1.84it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.79it/s] 19%|█▉        | 21/112 [00:07<00:31,  2.89it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s]  3%|▎         | 3/112 [00:01<00:49,  2.19it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.78it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.90it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.66it/s]  4%|▎         | 4/112 [00:01<00:45,  2.39it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.78it/s] 21%|██        | 23/112 [00:08<00:30,  2.88it/s] 22%|██▏       | 25/112 [00:10<00:32,  2.64it/s]  4%|▍         | 5/112 [00:02<00:42,  2.49it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.79it/s] 21%|██▏       | 24/112 [00:08<00:30,  2.89it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.66it/s]  5%|▌         | 6/112 [00:02<00:40,  2.60it/s] 65%|██████▌   | 73/112 [00:26<00:14,  2.78it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.90it/s]  6%|▋         | 7/112 [00:02<00:39,  2.67it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.66it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.78it/s] 23%|██▎       | 26/112 [00:09<00:29,  2.88it/s]  7%|▋         | 8/112 [00:03<00:38,  2.70it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.66it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.78it/s] 24%|██▍       | 27/112 [00:09<00:29,  2.85it/s]  8%|▊         | 9/112 [00:03<00:37,  2.73it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.67it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.78it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.87it/s]  9%|▉         | 10/112 [00:04<00:37,  2.75it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.66it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.78it/s] 26%|██▌       | 29/112 [00:10<00:28,  2.88it/s] 10%|▉         | 11/112 [00:04<00:36,  2.76it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.67it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.77it/s] 27%|██▋       | 30/112 [00:10<00:28,  2.88it/s] 11%|█         | 12/112 [00:04<00:36,  2.77it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.67it/s] 71%|███████   | 79/112 [00:28<00:11,  2.78it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.89it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.78it/s] 29%|██▉       | 33/112 [00:13<00:29,  2.66it/s] 29%|██▊       | 32/112 [00:11<00:27,  2.89it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.78it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.79it/s] 29%|██▉       | 33/112 [00:11<00:27,  2.89it/s] 30%|███       | 34/112 [00:13<00:29,  2.67it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.78it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.79it/s] 30%|███       | 34/112 [00:12<00:26,  2.90it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.68it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.78it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.80it/s] 31%|███▏      | 35/112 [00:12<00:26,  2.90it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.78it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.68it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.80it/s] 32%|███▏      | 36/112 [00:12<00:26,  2.89it/s] 75%|███████▌  | 84/112 [00:30<00:10,  2.78it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.68it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.80it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.88it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.78it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.68it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.79it/s] 34%|███▍      | 38/112 [00:13<00:25,  2.89it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.78it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.68it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.80it/s] 35%|███▍      | 39/112 [00:13<00:25,  2.90it/s] 78%|███████▊  | 87/112 [00:31<00:09,  2.78it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.68it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.80it/s] 36%|███▌      | 40/112 [00:14<00:24,  2.90it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.76it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.68it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s] 37%|███▋      | 41/112 [00:14<00:24,  2.91it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.78it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.67it/s] 21%|██        | 23/112 [00:08<00:32,  2.78it/s] 38%|███▊      | 42/112 [00:14<00:24,  2.90it/s] 80%|████████  | 90/112 [00:32<00:07,  2.77it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.66it/s] 38%|███▊      | 43/112 [00:15<00:23,  2.90it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.78it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.77it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.66it/s] 39%|███▉      | 44/112 [00:15<00:23,  2.90it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.77it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.77it/s] 40%|████      | 45/112 [00:17<00:25,  2.67it/s] 40%|████      | 45/112 [00:16<00:23,  2.89it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.78it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.77it/s] 41%|████      | 46/112 [00:16<00:22,  2.89it/s] 41%|████      | 46/112 [00:17<00:24,  2.67it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.78it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.77it/s] 42%|████▏     | 47/112 [00:16<00:22,  2.90it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.68it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.79it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.77it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.88it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.68it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.79it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.77it/s] 44%|████▍     | 49/112 [00:17<00:21,  2.89it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.79it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.68it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.77it/s] 45%|████▍     | 50/112 [00:17<00:21,  2.90it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.79it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.68it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.77it/s] 46%|████▌     | 51/112 [00:18<00:20,  2.91it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.79it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.69it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.77it/s] 46%|████▋     | 52/112 [00:18<00:20,  2.90it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.79it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.68it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.77it/s] 47%|████▋     | 53/112 [00:18<00:20,  2.91it/s] 30%|███       | 34/112 [00:12<00:28,  2.78it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.68it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.77it/s] 48%|████▊     | 54/112 [00:19<00:19,  2.90it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.79it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.68it/s] 49%|████▉     | 55/112 [00:19<00:19,  2.89it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.77it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.79it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.68it/s] 50%|█████     | 56/112 [00:19<00:19,  2.90it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.77it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.78it/s] 50%|█████     | 56/112 [00:21<00:20,  2.67it/s] 51%|█████     | 57/112 [00:20<00:18,  2.91it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.77it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.77it/s] 51%|█████     | 57/112 [00:22<00:20,  2.67it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.89it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.77it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.78it/s] 53%|█████▎    | 59/112 [00:20<00:18,  2.90it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.66it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.77it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.77it/s] 54%|█████▎    | 60/112 [00:21<00:17,  2.90it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.67it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.77it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.76it/s] 54%|█████▍    | 61/112 [00:21<00:17,  2.89it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.66it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.77it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.75it/s] 55%|█████▌    | 62/112 [00:21<00:17,  2.90it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.65it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.77it/s] 38%|███▊      | 43/112 [00:15<00:25,  2.76it/s] 56%|█████▋    | 63/112 [00:22<00:16,  2.90it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.66it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.77it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.76it/s] 57%|█████▋    | 64/112 [00:22<00:16,  2.88it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.77it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.67it/s] 40%|████      | 45/112 [00:16<00:24,  2.78it/s] 58%|█████▊    | 65/112 [00:22<00:16,  2.89it/s]100%|██████████| 112/112 [00:40<00:00,  2.77it/s]100%|██████████| 112/112 [00:40<00:00,  2.76it/s]
 57%|█████▋    | 64/112 [00:24<00:18,  2.66it/s] 41%|████      | 46/112 [00:16<00:24,  2.75it/s] 59%|█████▉    | 66/112 [00:23<00:15,  2.89it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.62it/s] 60%|█████▉    | 67/112 [00:23<00:15,  2.85it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.71it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.62it/s] 61%|██████    | 68/112 [00:24<00:15,  2.80it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.71it/s] 60%|█████▉    | 67/112 [00:25<00:17,  2.63it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.81it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.74it/s] 61%|██████    | 68/112 [00:26<00:16,  2.64it/s] 62%|██████▎   | 70/112 [00:24<00:14,  2.82it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.75it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.62it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.82it/s] 46%|████▌     | 51/112 [00:18<00:22,  2.71it/s] 64%|██████▍   | 72/112 [00:25<00:14,  2.84it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.61it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.72it/s] 65%|██████▌   | 73/112 [00:25<00:13,  2.81it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.63it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.75it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.85it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.64it/s] 48%|████▊     | 54/112 [00:19<00:21,  2.75it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.86it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.64it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.76it/s] 68%|██████▊   | 76/112 [00:26<00:12,  2.88it/s] 50%|█████     | 56/112 [00:20<00:20,  2.78it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.66it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.88it/s] 51%|█████     | 57/112 [00:20<00:19,  2.78it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.66it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.88it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.77it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.66it/s] 71%|███████   | 79/112 [00:27<00:11,  2.87it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.78it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.63it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.87it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.75it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.62it/s] 72%|███████▏  | 81/112 [00:28<00:10,  2.88it/s]W0402 09:07:45.837000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:07:45.838000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:45.838000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:07:45.838000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:07:45.838000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:07:45.838000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:07:45.838000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 54%|█████▍    | 61/112 [00:22<00:18,  2.76it/s]W0402 09:07:45.881000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:07:45.881000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:07:45.881000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:07:45.881000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:45.881000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:30<00:12,  2.64it/s] 73%|███████▎  | 82/112 [00:28<00:10,  2.87it/s]W0402 09:07:46.048000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.049000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.049000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.049000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.049000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 55%|█████▌    | 62/112 [00:22<00:18,  2.77it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.64it/s]W0402 09:07:46.346000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.346000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.346000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.346000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.346000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.346000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.346000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.379000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.379000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.379000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.379000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.379000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:29<00:10,  2.89it/s]W0402 09:07:46.445000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.445000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.445000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.445000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:07:46.445000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 63/112 [00:23<00:17,  2.78it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.65it/s] 75%|███████▌  | 84/112 [00:29<00:09,  2.89it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.79it/s] 76%|███████▌  | 85/112 [00:29<00:09,  2.90it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.65it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.79it/s]W0402 09:07:47.364000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.378000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.385000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.385000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:30<00:08,  2.90it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.65it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.77it/s] 78%|███████▊  | 87/112 [00:30<00:08,  2.88it/s]W0402 09:07:47.815000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.815000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.815000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.815000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.815000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.815000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.815000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.842000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.842000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.842000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.842000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:07:47.842000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:32<00:10,  2.65it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.78it/s] 79%|███████▊  | 88/112 [00:30<00:08,  2.89it/s]W0402 09:07:48.125000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.126000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.126000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.126000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.126000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.126000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.126000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.126000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 76%|███████▌  | 85/112 [00:32<00:10,  2.65it/s] 61%|██████    | 68/112 [00:24<00:15,  2.77it/s]W0402 09:07:48.415000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.415000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.415000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.415000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.415000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:31<00:07,  2.88it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.65it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.78it/s] 80%|████████  | 90/112 [00:31<00:07,  2.88it/s]W0402 09:07:48.819000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:07:48.824000 139904550426432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:33<00:09,  2.66it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.78it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.84it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.66it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.78it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.86it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.66it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.79it/s] 83%|████████▎ | 93/112 [00:32<00:06,  2.87it/s] 80%|████████  | 90/112 [00:34<00:08,  2.67it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.80it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.89it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.66it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.79it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.86it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.68it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.80it/s] 86%|████████▌ | 96/112 [00:33<00:05,  2.81it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.68it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.80it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.77it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.69it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.80it/s] 88%|████████▊ | 98/112 [00:34<00:05,  2.75it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.81it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.70it/s] 88%|████████▊ | 99/112 [00:34<00:04,  2.73it/s] 71%|███████   | 79/112 [00:28<00:11,  2.81it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.71it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.72it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.80it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.70it/s] 90%|█████████ | 101/112 [00:35<00:04,  2.71it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.80it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.70it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.71it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.80it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.70it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.70it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.80it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.70it/s] 93%|█████████▎| 104/112 [00:36<00:02,  2.70it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.81it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.70it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.70it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.81it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.71it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.70it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.81it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.71it/s] 96%|█████████▌| 107/112 [00:37<00:01,  2.70it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.81it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.70it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.70it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.80it/s]I0402 09:07:55.584948 1543699 finetune.py:45] layer 24_down initial loss 0.010962865315377712
W0402 09:07:55.585133 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 105/112 [00:40<00:02,  2.69it/s] 97%|█████████▋| 109/112 [00:38<00:01,  2.71it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.80it/s]W0402 09:07:56.018596 1543699 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 95%|█████████▍| 106/112 [00:40<00:02,  2.69it/s] 98%|█████████▊| 110/112 [00:38<00:00,  2.77it/s] 80%|████████  | 90/112 [00:32<00:07,  2.80it/s]24_down proxy err 0.04718134552240372 tr(WHW.T) 34.95817565917969
 96%|█████████▌| 107/112 [00:40<00:01,  2.69it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.81it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.80it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.68it/s]100%|██████████| 112/112 [00:39<00:00,  2.83it/s]100%|██████████| 112/112 [00:39<00:00,  2.83it/s]
 82%|████████▏ | 92/112 [00:33<00:07,  2.80it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.68it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.79it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.68it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.79it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.67it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.78it/s]100%|██████████| 112/112 [00:42<00:00,  2.64it/s]100%|██████████| 112/112 [00:42<00:00,  2.62it/s]
 86%|████████▌ | 96/112 [00:34<00:05,  2.77it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.74it/s] 88%|████████▊ | 98/112 [00:35<00:05,  2.70it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.65it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.70it/s] 90%|█████████ | 101/112 [00:36<00:04,  2.71it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.72it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.74it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.75it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.76it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.78it/s]W0402 09:08:02.399000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.400000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.400000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.400000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.400000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.400000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.400000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
 96%|█████████▌| 107/112 [00:38<00:01,  2.79it/s]W0402 09:08:02.440000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.441000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.441000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.441000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.441000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.610000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.611000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.611000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.611000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.611000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 96%|█████████▋| 108/112 [00:39<00:01,  2.78it/s]W0402 09:08:02.909000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.909000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.909000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.909000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.909000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.909000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.909000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.938000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.938000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.938000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.938000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:02.938000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:03.003000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:03.003000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:03.003000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:03.003000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:03.003000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:39<00:01,  2.79it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.79it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.79it/s]W0402 09:08:03.906000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:03.911000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:03.917000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:03.917000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.163000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.163000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.163000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.163000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.163000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.163000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.163000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:40<00:00,  2.80it/s]100%|██████████| 112/112 [00:40<00:00,  2.75it/s]
W0402 09:08:04.204000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.204000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.204000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.205000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.205000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.333000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.333000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.333000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.334000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.334000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.334000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.334000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.361000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.361000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.361000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.361000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.361000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.368000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.368000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.368000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.368000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.368000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.640000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.640000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.640000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.640000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.640000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.640000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.640000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.640000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.682000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.683000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.683000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.683000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.683000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.683000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.683000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.717000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.717000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.717000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.717000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.717000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.787000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.787000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.787000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.787000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.787000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.922000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.922000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.922000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.922000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:04.923000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:05.338000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:05.343000 139910042785600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:05.757000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:05.763000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:05.769000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:05.769000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.213000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.213000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.213000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.213000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.214000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.214000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.214000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.243000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.243000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.243000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.243000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.243000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.526000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.526000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.526000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.526000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.526000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.526000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.526000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.526000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.805000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.805000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.805000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.805000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:06.805000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:07.211000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:07.216000 140191791040320 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.097000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.098000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.098000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.098000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.098000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.098000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.098000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.141000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.141000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.141000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.141000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.141000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.316000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.316000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.316000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.317000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.317000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.638000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.638000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.638000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.638000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.638000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.639000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.639000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.671000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.671000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.671000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.671000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.672000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.742000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.742000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.742000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.742000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:10.742000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:11.715000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:11.721000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:11.727000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:11.727000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
I0402 09:08:11.923949 1544940 finetune.py:45] layer 26_down initial loss 0.01536200474947691
W0402 09:08:11.924119 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:08:12.185000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.185000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.186000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.186000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.186000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.186000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.186000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.219000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.219000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.219000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.219000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.219000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.398501 1544940 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0402 09:08:12.518000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.519000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.519000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.519000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.519000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.519000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.519000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.519000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
26_down proxy err 0.04815440624952316 tr(WHW.T) 43.263397216796875
W0402 09:08:12.817000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.817000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.817000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.817000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:12.817000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:13.249000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:13.255000 139738418657088 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 09:08:13.977373 1543931 finetune.py:45] layer 25_down initial loss 0.012415838427841663
W0402 09:08:13.977583 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:08:14.461055 1543931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

25_down proxy err 0.047567371279001236 tr(WHW.T) 37.92062759399414
I0402 09:08:18.653002 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 28 in 1.5219950675964355s
I0402 09:08:19.110614 1509661 quantize_finetune_llama.py:159] layer 29 gpu 1
I0402 09:08:20.150600 1545102 finetune.py:45] layer 27_down initial loss 0.018677109852433205
W0402 09:08:20.150890 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:08:20.634230 1545102 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

27_down proxy err 0.040469691157341 tr(WHW.T) 61.202301025390625
I0402 09:08:21.037260 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 29 in 1.290816307067871s
I0402 09:08:21.071532 1549154 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:08:21.071709 1549154 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:08:21.071857 1549154 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:08:21.273397 1549154 config.py:58] PyTorch version 2.4.0 available.
I0402 09:08:21.546951 1509661 quantize_finetune_llama.py:159] layer 30 gpu 2
I0402 09:08:23.406879 1549154 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0402 09:08:23.482191 1549316 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:08:23.482359 1549316 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:08:23.482421 1549316 utils.py:162] NumExpr defaulting to 16 threads.
W0402 09:08:23.723675 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:08:23.793539 1549316 config.py:58] PyTorch version 2.4.0 available.
I0402 09:08:24.560755 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 30 in 1.4962685108184814s
  0%|          | 0/32 [00:00<?, ?it/s]I0402 09:08:24.992010 1509661 quantize_finetune_llama.py:159] layer 31 gpu 3
I0402 09:08:26.174231 1549316 data_utils.py:336] using 256 training seqs, 128 validation seqs
  3%|▎         | 1/32 [00:01<00:44,  1.43s/it]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]W0402 09:08:26.641531 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  9%|▉         | 3/32 [00:02<00:16,  1.77it/s]I0402 09:08:26.972358 1549843 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:08:26.972483 1549843 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:08:26.972543 1549843 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:08:27.104764 1509661 quantize_finetune_llama.py:190] computed original embedding for layer 31 in 1.446061134338379s
 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s]I0402 09:08:27.263247 1549843 config.py:58] PyTorch version 2.4.0 available.
 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.56it/s]  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.70it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.84it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.98it/s] 31%|███▏      | 10/32 [00:04<00:07,  3.07it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.09it/s]  3%|▎         | 1/32 [00:01<00:50,  1.63s/it]I0402 09:08:29.591122 1550298 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:08:29.591255 1550298 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:08:29.591309 1550298 utils.py:162] NumExpr defaulting to 16 threads.
 38%|███▊      | 12/32 [00:04<00:06,  3.13it/s]I0402 09:08:29.787850 1550298 config.py:58] PyTorch version 2.4.0 available.
I0402 09:08:29.806303 1549843 data_utils.py:336] using 256 training seqs, 128 validation seqs
  6%|▋         | 2/32 [00:01<00:26,  1.15it/s] 41%|████      | 13/32 [00:05<00:06,  3.17it/s]  9%|▉         | 3/32 [00:02<00:18,  1.60it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.21it/s]W0402 09:08:30.386014 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.17it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.21it/s] 50%|█████     | 16/32 [00:06<00:05,  3.20it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.21it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.23it/s]  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.19it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.66it/s]I0402 09:08:31.975918 1550298 data_utils.py:336] using 256 training seqs, 128 validation seqs
 62%|██████▎   | 20/32 [00:07<00:03,  3.18it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s]W0402 09:08:32.299280 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:07<00:03,  3.18it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.20it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.23it/s]  3%|▎         | 1/32 [00:01<00:48,  1.56s/it] 38%|███▊      | 12/32 [00:05<00:06,  2.89it/s]  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.24it/s]  6%|▋         | 2/32 [00:01<00:25,  1.20it/s] 41%|████      | 13/32 [00:05<00:06,  2.91it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.24it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.93it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.26it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.03it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.97it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 50%|█████     | 16/32 [00:06<00:05,  2.98it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.26it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.52it/s]  3%|▎         | 1/32 [00:01<00:51,  1.66s/it] 91%|█████████ | 29/32 [00:10<00:00,  3.26it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.91it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.68it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.29it/s]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.31it/s]  9%|▉         | 3/32 [00:02<00:17,  1.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.93it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  3.30it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]
 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.97it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.25it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.95it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.47it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.83it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.93it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.97it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.93it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.83it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.96it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.90it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.01it/s]W0402 09:08:38.183000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.183000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.183000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.183000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.184000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.184000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.184000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.210000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.211000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.211000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.211000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.211000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:06<00:05,  2.96it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.93it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.98it/s]W0402 09:08:38.507000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.508000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.508000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.508000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:38.508000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.92it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.96it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.95it/s] 41%|████      | 13/32 [00:05<00:06,  2.96it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.98it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.98it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.00it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.02it/s]W0402 09:08:39.332000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.332000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.332000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.332000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.332000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.332000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.332000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.350000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.350000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.350000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.350000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.350000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.541000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.541000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.541000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.541000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:39.541000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:07<00:03,  3.01it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.03it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.05it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.02it/s] 50%|█████     | 16/32 [00:06<00:05,  3.05it/s]100%|██████████| 32/32 [00:12<00:00,  3.05it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
 69%|██████▉   | 22/32 [00:08<00:03,  3.02it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.05it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.01it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.98it/s]W0402 09:08:40.624000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.624000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.624000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.624000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.624000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.624000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.625000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.642000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.642000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.642000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.642000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:40.642000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 19/32 [00:07<00:04,  3.01it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.99it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.96it/s]W0402 09:08:41.490000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:41.490000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:41.490000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:41.490000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:41.490000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  3.03it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.02it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.94it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 72%|███████▏  | 23/32 [00:08<00:02,  3.04it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.99it/s]W0402 09:08:42.320000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.320000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.320000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.320000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.320000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.320000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.320000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.346000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.346000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.346000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.346000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.346000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  3.06it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.93it/s]W0402 09:08:42.623000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.623000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.624000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.624000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:42.624000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  3.03it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.99it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s]W0402 09:08:43.514000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.515000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.515000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.515000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.515000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.515000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.515000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.534000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.534000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.534000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.534000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.534000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
W0402 09:08:43.747000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.747000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.747000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.747000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:43.748000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:10<00:01,  2.93it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.95it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.93it/s]W0402 09:08:44.918000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:44.919000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:44.919000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:44.919000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:44.919000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:44.919000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:44.919000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.93it/s]W0402 09:08:44.937000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:44.937000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:44.937000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:44.938000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:44.938000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
W0402 09:08:45.827000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:45.828000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:45.828000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:45.828000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:45.828000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.492000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.492000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.493000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.493000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.493000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.493000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.493000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0402 09:08:46.520000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.520000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.520000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.520000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.520000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.821000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.822000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.822000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.822000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:46.822000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
I0402 09:08:47.377611 1549154 finetune.py:45] layer 28_v initial loss 0.0267803892493248
W0402 09:08:47.379366 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:08:47.720000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.721000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.721000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.721000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.721000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.721000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.721000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.740000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.740000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.740000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.740000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.740000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.877000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.877000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.877000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.877000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.878000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.878000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.878000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.905000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.905000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.906000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.906000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.906000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.956000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.957000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.957000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.957000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:47.957000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:48.215000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:48.215000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:48.216000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:48.216000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:48.216000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:48.956040 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:08:49.116000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.116000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.116000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.116000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.116000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.116000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.116000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.135000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.135000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.135000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.135000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.135000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.144000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.145000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.145000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.145000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.145000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.145000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.145000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.164000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.164000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.165000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.165000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.165000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.344000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.344000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.345000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.345000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:49.345000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.082000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.082000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.083000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.083000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.083000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
28_v proxy err 0.05049825832247734 tr(WHW.T) 175.40756225585938
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:08:50.523000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.523000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.524000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.524000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.524000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.524000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.524000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.542000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.542000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.543000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.543000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:50.543000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]W0402 09:08:51.455000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:08:51.455000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:08:51.455000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:08:51.455000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:08:51.455000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:18,  1.61it/s]I0402 09:08:51.848392 1549316 finetune.py:45] layer 29_v initial loss 0.033906955271959305
W0402 09:08:51.848716 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 3/32 [00:01<00:14,  1.98it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s]W0402 09:08:52.948551 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s]29_v proxy err 0.042129915207624435 tr(WHW.T) 249.68582153320312
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s]  6%|▋         | 2/32 [00:01<00:18,  1.58it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s]I0402 09:08:56.226484 1549843 finetune.py:45] layer 30_v initial loss 0.05447804182767868
W0402 09:08:56.226666 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:05<00:05,  2.83it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.15it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.87it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s]W0402 09:08:57.362936 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:06<00:04,  2.87it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s]I0402 09:08:57.788667 1550298 finetune.py:45] layer 31_v initial loss 0.027711542323231697
W0402 09:08:57.789175 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.60it/s]30_v proxy err 0.05229121819138527 tr(WHW.T) 252.81201171875
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s]W0402 09:08:58.814533 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s]31_v proxy err 0.021562164649367332 tr(WHW.T) 365.68487548828125
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.30it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s]  3%|▎         | 1/32 [00:00<00:28,  1.07it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s]  6%|▋         | 2/32 [00:01<00:17,  1.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.82it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.83it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.35it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.75it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.77it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.81it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.73it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s]W0402 09:09:07.580000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.580000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.580000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.580000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.580000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.580000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.580000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s]W0402 09:09:07.610000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.610000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.610000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.610000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.610000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.776000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.777000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.777000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.777000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.777000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s]W0402 09:09:07.991000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.991000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.991000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.991000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.992000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.992000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:07.992000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:08.011000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:08.011000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:08.011000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:08.011000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:08.011000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:08.076000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:08.076000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:08.076000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:08.076000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:08.076000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.81it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s]W0402 09:09:08.785000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s]W0402 09:09:09.087000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.087000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.088000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.088000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.088000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.088000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.088000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.110000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.110000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.110000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.110000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.110000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.83it/s]W0402 09:09:09.357000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.357000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.357000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.357000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:09.358000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s]W0402 09:09:09.699000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s]W0402 09:09:11.261000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.261000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.261000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.261000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.261000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.261000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.261000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.288000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.288000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.289000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.289000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.289000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s]W0402 09:09:11.465000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.466000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.466000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.466000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.466000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.700000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.700000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.700000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.701000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.701000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.701000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.701000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.722000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.722000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.722000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.722000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.723000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.790000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.790000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.790000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.790000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:11.791000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
W0402 09:09:12.549000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.875000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.875000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.875000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.875000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.875000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.876000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.876000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.897000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.897000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.897000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.898000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:12.898000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:13.161000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:13.162000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:13.162000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:13.162000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:13.162000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:13.523000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.476000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.476000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.476000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.476000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.477000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.477000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.477000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.509000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.509000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.509000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.509000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.509000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.685000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.686000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.686000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.686000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.686000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.917000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.917000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.918000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.918000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.918000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.918000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.918000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.941000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.941000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.941000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.941000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:15.941000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:16.008000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:16.008000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:16.009000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:16.009000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:16.009000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
I0402 09:09:16.354249 1549154 finetune.py:45] layer 28_q initial loss 0.026773158460855484
W0402 09:09:16.354640 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:09:16.758000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.035000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.035000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.035000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.035000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.035000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.036000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.036000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.067000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.067000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.067000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.067000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.067000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.083000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.083000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.083000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.083000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.083000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.084000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.084000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.107000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.107000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.107000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.107000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.107000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.245000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.245000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.245000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.245000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.245000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.367000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.367000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.367000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.367000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.367000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.475546 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0402 09:09:17.479000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.480000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.480000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.480000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.480000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.480000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.480000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.503000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.504000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.504000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.504000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.504000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.572000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.572000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.572000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.572000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.572000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:17.725000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.338000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
28_q proxy err 0.007032209075987339 tr(WHW.T) 6755.15625
  0%|          | 0/32 [00:00<?, ?it/s]W0402 09:09:18.673000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.674000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.674000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.674000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.674000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.674000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.674000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.697000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.697000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.697000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.697000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.697000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.964000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.964000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.964000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.964000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:09:18.964000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:09:19.336000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:27,  1.14it/s]I0402 09:09:19.502908 1549316 finetune.py:45] layer 29_q initial loss 0.033885858952999115
W0402 09:09:19.503457 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:01<00:17,  1.76it/s]  9%|▉         | 3/32 [00:01<00:13,  2.14it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.38it/s]W0402 09:09:20.649612 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.65it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.78it/s]29_q proxy err 0.008457104675471783 tr(WHW.T) 6063.78369140625
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s]  3%|▎         | 1/32 [00:00<00:29,  1.06it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.88it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s]  9%|▉         | 3/32 [00:01<00:14,  1.98it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.88it/s]I0402 09:09:24.286939 1549843 finetune.py:45] layer 30_q initial loss 0.05461248755455017
W0402 09:09:24.287317 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.88it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s]I0402 09:09:25.573684 1550298 finetune.py:45] layer 31_q initial loss 0.027690855786204338
W0402 09:09:25.573994 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:07<00:04,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s]W0402 09:09:25.991957 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.84it/s]W0402 09:09:26.560233 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s]31_q proxy err 0.003943952731788158 tr(WHW.T) 9335.720703125
  0%|          | 0/32 [00:00<?, ?it/s]30_q proxy err 0.0058038802817463875 tr(WHW.T) 7045.0302734375
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s]  3%|▎         | 1/32 [00:00<00:26,  1.16it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.82it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]  6%|▋         | 2/32 [00:01<00:16,  1.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.82it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s]  6%|▋         | 2/32 [00:01<00:17,  1.67it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s]  9%|▉         | 3/32 [00:01<00:14,  1.99it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.41it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.57it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:05,  2.92it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.88it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.80it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s]I0402 09:09:37.266642 1549154 finetune.py:45] layer 28_k initial loss 0.026775134727358818
W0402 09:09:37.266911 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.63it/s]W0402 09:09:38.316534 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:10<00:01,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s]28_k proxy err 0.005826752167195082 tr(WHW.T) 4372.6787109375
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s]100%|██████████| 32/32 [00:11<00:00,  2.64it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s]I0402 09:09:39.990199 1549316 finetune.py:45] layer 29_k initial loss 0.033847909420728683
W0402 09:09:39.990524 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
  3%|▎         | 1/32 [00:00<00:23,  1.32it/s]  6%|▋         | 2/32 [00:01<00:15,  1.93it/s]  9%|▉         | 3/32 [00:01<00:12,  2.26it/s]W0402 09:09:41.032005 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 4/32 [00:01<00:11,  2.46it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.59it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.68it/s]29_k proxy err 0.006307289935648441 tr(WHW.T) 4804.9951171875
  0%|          | 0/32 [00:00<?, ?it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.74it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.78it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.84it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.86it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 41%|████      | 13/32 [00:04<00:06,  2.86it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.86it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 50%|█████     | 16/32 [00:05<00:05,  2.87it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.88it/s]I0402 09:09:45.856313 1550298 finetune.py:45] layer 31_k initial loss 0.027621008455753326
W0402 09:09:45.856665 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:06<00:04,  2.87it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s]W0402 09:09:46.967125 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:07<00:03,  2.86it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s]I0402 09:09:47.530932 1549843 finetune.py:45] layer 30_k initial loss 0.054578784853219986
W0402 09:09:47.531476 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s]31_k proxy err 0.00439111515879631 tr(WHW.T) 4138.115234375
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.83it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s]  3%|▎         | 1/32 [00:00<00:24,  1.27it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.82it/s]W0402 09:09:49.056753 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s]  6%|▋         | 2/32 [00:01<00:15,  1.89it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s]  9%|▉         | 3/32 [00:01<00:13,  2.20it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.42it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s]30_k proxy err 0.005021163262426853 tr(WHW.T) 4109.9091796875
  0%|          | 0/32 [00:00<?, ?it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.57it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.67it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s]  3%|▎         | 1/32 [00:00<00:24,  1.29it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.73it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
 72%|███████▏  | 23/32 [00:08<00:03,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.31it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.61it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s]I0402 09:09:58.086522 1549154 finetune.py:45] layer 28_o initial loss 0.027152737602591515
W0402 09:09:58.086753 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.52it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.51it/s]W0402 09:09:59.132265 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.53it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s]28_o proxy err 0.028956077992916107 tr(WHW.T) 24.542434692382812
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s]I0402 09:10:00.996529 1549316 finetune.py:45] layer 29_o initial loss 0.0339939221739769
W0402 09:10:00.996862 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s]W0402 09:10:01.985964 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
29_o proxy err 0.018186936154961586 tr(WHW.T) 36.68419647216797
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:03<00:48,  1.61s/it]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it]  3%|▎         | 1/32 [00:02<01:03,  2.03s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.49s/it]I0402 09:10:06.562691 1550298 finetune.py:45] layer 31_o initial loss 0.028159135952591896
W0402 09:10:06.562948 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]W0402 09:10:07.608831 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it]31_o proxy err 0.010887173935770988 tr(WHW.T) 182.20681762695312
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it]I0402 09:10:10.044037 1549843 finetune.py:45] layer 30_o initial loss 0.055182237178087234
W0402 09:10:10.044465 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:56,  1.82s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it]W0402 09:10:11.232391 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it]  6%|▋         | 2/32 [00:03<00:47,  1.58s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it]30_o proxy err 0.016382837668061256 tr(WHW.T) 84.08126831054688
  0%|          | 0/32 [00:00<?, ?it/s] 19%|█▉        | 6/32 [00:09<00:38,  1.46s/it]  9%|▉         | 3/32 [00:04<00:43,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.42s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.45s/it]  3%|▎         | 1/32 [00:01<01:00,  1.97s/it] 12%|█▎        | 4/32 [00:05<00:40,  1.45s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.41s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.44s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 16%|█▌        | 5/32 [00:07<00:38,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.43s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 19%|█▉        | 6/32 [00:08<00:36,  1.40s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.43s/it] 22%|██▏       | 7/32 [00:10<00:34,  1.39s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 41%|████      | 13/32 [00:18<00:26,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.43s/it] 25%|██▌       | 8/32 [00:11<00:33,  1.39s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.41s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.43s/it] 28%|██▊       | 9/32 [00:12<00:31,  1.39s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.42s/it] 41%|████      | 13/32 [00:19<00:27,  1.42s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.38s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.52s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 34%|███▍      | 11/32 [00:15<00:28,  1.38s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.43s/it] 38%|███▊      | 12/32 [00:16<00:27,  1.38s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 50%|█████     | 16/32 [00:23<00:22,  1.42s/it] 41%|████      | 13/32 [00:18<00:26,  1.38s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.42s/it] 44%|████▍     | 14/32 [00:19<00:24,  1.38s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.42s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.38s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.42s/it] 50%|█████     | 16/32 [00:22<00:22,  1.38s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.42s/it] 53%|█████▎    | 17/32 [00:23<00:20,  1.38s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.41s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.42s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.38s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 59%|█████▉    | 19/32 [00:26<00:17,  1.37s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.41s/it] 62%|██████▎   | 20/32 [00:27<00:16,  1.37s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.42s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.38s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.42s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.42s/it] 69%|██████▉   | 22/32 [00:30<00:13,  1.38s/it] 78%|███████▊  | 25/32 [00:36<00:09,  1.43s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.37s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.42s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 75%|███████▌  | 24/32 [00:33<00:10,  1.37s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.42s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.42s/it] 78%|███████▊  | 25/32 [00:34<00:09,  1.37s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.42s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.37s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.42s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 84%|████████▍ | 27/32 [00:37<00:06,  1.37s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.42s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.51s/it] 88%|████████▊ | 28/32 [00:38<00:05,  1.38s/it] 97%|█████████▋| 31/32 [00:44<00:01,  1.42s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.38s/it]100%|██████████| 32/32 [00:46<00:00,  1.42s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]
 78%|███████▊  | 25/32 [00:37<00:10,  1.52s/it] 94%|█████████▍| 30/32 [00:41<00:02,  1.40s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.40s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.53s/it]100%|██████████| 32/32 [00:44<00:00,  1.41s/it]100%|██████████| 32/32 [00:44<00:00,  1.40s/it]
 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it]I0402 09:10:54.425733 1549154 finetune.py:45] layer 28_up initial loss 0.0260658860206604
W0402 09:10:54.426120 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it]W0402 09:10:55.312682 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_up proxy err 0.03722415119409561 tr(WHW.T) 2478.37158203125
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it]I0402 09:10:56.683698 1549316 finetune.py:45] layer 29_up initial loss 0.032147519290447235
W0402 09:10:56.684000 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:10:57.703271 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it]  3%|▎         | 1/32 [00:01<00:56,  1.84s/it]29_up proxy err 0.0304392222315073 tr(WHW.T) 3248.7060546875
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]  6%|▋         | 2/32 [00:03<00:47,  1.59s/it]I0402 09:11:00.734794 1550298 finetune.py:45] layer 31_up initial loss 0.02731897495687008
W0402 09:11:00.735132 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
  3%|▎         | 1/32 [00:01<00:59,  1.93s/it]  9%|▉         | 3/32 [00:04<00:43,  1.51s/it]W0402 09:11:01.574017 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:03<00:48,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it]31_up proxy err 0.0074659534730017185 tr(WHW.T) 12277.7177734375
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:04<00:44,  1.52s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it]  3%|▎         | 1/32 [00:01<00:55,  1.80s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.46s/it]  6%|▋         | 2/32 [00:03<00:46,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it]  9%|▉         | 3/32 [00:04<00:42,  1.48s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it] 25%|██▌       | 8/32 [00:11<00:35,  1.47s/it] 12%|█▎        | 4/32 [00:05<00:40,  1.45s/it]I0402 09:11:09.487905 1549843 finetune.py:45] layer 30_up initial loss 0.05364885553717613
W0402 09:11:09.488350 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:10<00:36,  1.46s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.48s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.47s/it]W0402 09:11:10.513392 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:11<00:34,  1.46s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.47s/it] 19%|█▉        | 6/32 [00:08<00:37,  1.44s/it]30_up proxy err 0.018770316615700722 tr(WHW.T) 5513.9267578125
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.43s/it]  3%|▎         | 1/32 [00:01<00:58,  1.89s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.44s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.45s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.42s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.43s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 28%|██▊       | 9/32 [00:13<00:32,  1.41s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.42s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:30,  1.40s/it] 41%|████      | 13/32 [00:19<00:26,  1.42s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 34%|███▍      | 11/32 [00:15<00:29,  1.40s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.45s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.42s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.52s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.40s/it] 50%|█████     | 16/32 [00:23<00:23,  1.44s/it] 47%|████▋     | 15/32 [00:21<00:24,  1.41s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 41%|████      | 13/32 [00:18<00:26,  1.40s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.44s/it] 50%|█████     | 16/32 [00:23<00:22,  1.41s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.44s/it] 53%|█████▎    | 17/32 [00:24<00:21,  1.41s/it] 25%|██▌       | 8/32 [00:12<00:34,  1.44s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.44s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.41s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 50%|█████     | 16/32 [00:22<00:22,  1.40s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.44s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.41s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.42s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.39s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.44s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.41s/it] 34%|███▍      | 11/32 [00:16<00:29,  1.42s/it] 56%|█████▋    | 18/32 [00:25<00:19,  1.39s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.43s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.41s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.41s/it] 59%|█████▉    | 19/32 [00:26<00:18,  1.39s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.43s/it] 69%|██████▉   | 22/32 [00:31<00:14,  1.41s/it] 41%|████      | 13/32 [00:19<00:26,  1.41s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.40s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.44s/it] 72%|███████▏  | 23/32 [00:33<00:12,  1.41s/it] 44%|████▍     | 14/32 [00:20<00:25,  1.40s/it] 66%|██████▌   | 21/32 [00:29<00:15,  1.40s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.44s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.41s/it] 47%|████▋     | 15/32 [00:21<00:23,  1.40s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.39s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.41s/it] 50%|█████     | 16/32 [00:23<00:22,  1.40s/it] 72%|███████▏  | 23/32 [00:32<00:12,  1.39s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.44s/it] 81%|████████▏ | 26/32 [00:37<00:08,  1.41s/it] 53%|█████▎    | 17/32 [00:24<00:20,  1.40s/it] 75%|███████▌  | 24/32 [00:33<00:11,  1.39s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.44s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.41s/it] 56%|█████▋    | 18/32 [00:26<00:19,  1.40s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.40s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.44s/it] 88%|████████▊ | 28/32 [00:40<00:05,  1.41s/it] 59%|█████▉    | 19/32 [00:27<00:18,  1.40s/it] 81%|████████▏ | 26/32 [00:36<00:08,  1.39s/it] 94%|█████████▍| 30/32 [00:43<00:02,  1.44s/it] 91%|█████████ | 29/32 [00:41<00:04,  1.41s/it] 62%|██████▎   | 20/32 [00:28<00:16,  1.39s/it] 84%|████████▍ | 27/32 [00:38<00:06,  1.39s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.44s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.40s/it] 66%|██████▌   | 21/32 [00:30<00:15,  1.39s/it] 88%|████████▊ | 28/32 [00:39<00:05,  1.39s/it]100%|██████████| 32/32 [00:46<00:00,  1.44s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
 97%|█████████▋| 31/32 [00:44<00:01,  1.40s/it] 69%|██████▉   | 22/32 [00:31<00:13,  1.39s/it] 91%|█████████ | 29/32 [00:40<00:04,  1.39s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.43s/it]
 72%|███████▏  | 23/32 [00:33<00:12,  1.39s/it] 94%|█████████▍| 30/32 [00:42<00:02,  1.39s/it] 75%|███████▌  | 24/32 [00:34<00:11,  1.40s/it] 97%|█████████▋| 31/32 [00:43<00:01,  1.41s/it] 78%|███████▊  | 25/32 [00:35<00:09,  1.41s/it]100%|██████████| 32/32 [00:45<00:00,  1.42s/it]100%|██████████| 32/32 [00:45<00:00,  1.41s/it]
 81%|████████▏ | 26/32 [00:37<00:08,  1.44s/it] 84%|████████▍ | 27/32 [00:38<00:07,  1.47s/it]I0402 09:11:51.115633 1549154 finetune.py:45] layer 28_gate initial loss 0.025112483650445938
W0402 09:11:51.115909 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:11:51.946755 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0402 09:11:52.107206 1549316 finetune.py:45] layer 29_gate initial loss 0.03125302121043205
W0402 09:11:52.107520 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:40<00:05,  1.47s/it]W0402 09:11:52.867275 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:41<00:04,  1.49s/it]28_gate proxy err 0.01816868968307972 tr(WHW.T) 8688.7451171875
  0%|          | 0/112 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:43<00:02,  1.48s/it]I0402 09:11:55.336142 1550298 finetune.py:45] layer 31_gate initial loss 0.02687997557222843
W0402 09:11:55.336382 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  1%|          | 1/112 [00:00<01:29,  1.24it/s]29_gate proxy err 0.016443472355604172 tr(WHW.T) 9685.263671875
  0%|          | 0/112 [00:00<?, ?it/s]W0402 09:11:56.074566 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  2%|▏         | 2/112 [00:01<00:59,  1.85it/s]  3%|▎         | 3/112 [00:01<00:49,  2.18it/s] 97%|█████████▋| 31/32 [00:44<00:01,  1.48s/it]  1%|          | 1/112 [00:00<01:29,  1.24it/s]  4%|▎         | 4/112 [00:01<00:45,  2.39it/s]  2%|▏         | 2/112 [00:01<01:00,  1.80it/s]  4%|▍         | 5/112 [00:02<00:42,  2.53it/s]  3%|▎         | 3/112 [00:01<00:51,  2.11it/s]  5%|▌         | 6/112 [00:02<00:40,  2.60it/s]  4%|▎         | 4/112 [00:01<00:46,  2.33it/s]  6%|▋         | 7/112 [00:02<00:39,  2.67it/s]100%|██████████| 32/32 [00:46<00:00,  1.48s/it]100%|██████████| 32/32 [00:46<00:00,  1.45s/it]
  4%|▍         | 5/112 [00:02<00:42,  2.49it/s]  7%|▋         | 8/112 [00:03<00:38,  2.71it/s]  5%|▌         | 6/112 [00:02<00:40,  2.59it/s]  8%|▊         | 9/112 [00:03<00:37,  2.71it/s]31_gate proxy err 0.005644633900374174 tr(WHW.T) 25696.34765625
  0%|          | 0/112 [00:00<?, ?it/s]  6%|▋         | 7/112 [00:02<00:39,  2.67it/s]  9%|▉         | 10/112 [00:04<00:37,  2.70it/s]  7%|▋         | 8/112 [00:03<00:38,  2.72it/s] 10%|▉         | 11/112 [00:04<00:37,  2.68it/s]  8%|▊         | 9/112 [00:03<00:37,  2.76it/s]  1%|          | 1/112 [00:00<01:27,  1.26it/s] 11%|█         | 12/112 [00:04<00:36,  2.72it/s]  9%|▉         | 10/112 [00:04<00:36,  2.78it/s]  2%|▏         | 2/112 [00:01<00:58,  1.89it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.75it/s] 10%|▉         | 11/112 [00:04<00:36,  2.79it/s]  3%|▎         | 3/112 [00:01<00:48,  2.24it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.74it/s]  4%|▎         | 4/112 [00:01<00:44,  2.45it/s] 11%|█         | 12/112 [00:04<00:35,  2.80it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.76it/s]  4%|▍         | 5/112 [00:02<00:42,  2.54it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.75it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.75it/s]  5%|▌         | 6/112 [00:02<00:40,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.77it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.74it/s]  6%|▋         | 7/112 [00:02<00:38,  2.72it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.80it/s] 16%|█▌        | 18/112 [00:06<00:34,  2.74it/s]  7%|▋         | 8/112 [00:03<00:37,  2.78it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.82it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.74it/s]  8%|▊         | 9/112 [00:03<00:36,  2.81it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.82it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.72it/s]  9%|▉         | 10/112 [00:03<00:35,  2.84it/s] 16%|█▌        | 18/112 [00:06<00:33,  2.83it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.72it/s] 10%|▉         | 11/112 [00:04<00:35,  2.85it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.84it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.72it/s] 11%|█         | 12/112 [00:04<00:35,  2.86it/s] 18%|█▊        | 20/112 [00:07<00:32,  2.83it/s] 21%|██        | 23/112 [00:08<00:32,  2.72it/s] 12%|█▏        | 13/112 [00:04<00:34,  2.84it/s] 19%|█▉        | 21/112 [00:07<00:32,  2.80it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.72it/s] 12%|█▎        | 14/112 [00:05<00:34,  2.83it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.80it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.71it/s] 13%|█▎        | 15/112 [00:05<00:34,  2.84it/s] 21%|██        | 23/112 [00:08<00:31,  2.80it/s] 14%|█▍        | 16/112 [00:06<00:33,  2.85it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.71it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.82it/s] 15%|█▌        | 17/112 [00:06<00:33,  2.87it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.71it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.83it/s] 16%|█▌        | 18/112 [00:06<00:32,  2.87it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.71it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.84it/s] 17%|█▋        | 19/112 [00:07<00:32,  2.87it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.72it/s] 24%|██▍       | 27/112 [00:10<00:29,  2.84it/s]I0402 09:12:06.270430 1549843 finetune.py:45] layer 30_gate initial loss 0.05216551944613457
W0402 09:12:06.270896 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 18%|█▊        | 20/112 [00:07<00:31,  2.88it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.84it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.72it/s] 19%|█▉        | 21/112 [00:07<00:31,  2.88it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.84it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.72it/s] 20%|█▉        | 22/112 [00:08<00:31,  2.86it/s] 27%|██▋       | 30/112 [00:11<00:28,  2.83it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.75it/s]W0402 09:12:07.123301 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 21%|██        | 23/112 [00:08<00:30,  2.88it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.84it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.72it/s] 21%|██▏       | 24/112 [00:08<00:30,  2.88it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.84it/s] 30%|███       | 34/112 [00:12<00:28,  2.70it/s] 22%|██▏       | 25/112 [00:09<00:30,  2.87it/s] 29%|██▉       | 33/112 [00:12<00:27,  2.82it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.69it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.86it/s] 30%|███       | 34/112 [00:12<00:27,  2.82it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.67it/s] 24%|██▍       | 27/112 [00:09<00:29,  2.86it/s] 31%|███▏      | 35/112 [00:12<00:27,  2.83it/s] 33%|███▎      | 37/112 [00:13<00:28,  2.67it/s] 25%|██▌       | 28/112 [00:10<00:29,  2.87it/s] 32%|███▏      | 36/112 [00:13<00:26,  2.84it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.71it/s] 26%|██▌       | 29/112 [00:10<00:28,  2.87it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.84it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.74it/s] 27%|██▋       | 30/112 [00:10<00:28,  2.88it/s] 34%|███▍      | 38/112 [00:13<00:26,  2.84it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.75it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.86it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.82it/s]30_gate proxy err 0.012876687571406364 tr(WHW.T) 13194.71484375
  0%|          | 0/112 [00:00<?, ?it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.77it/s] 29%|██▊       | 32/112 [00:11<00:28,  2.84it/s] 36%|███▌      | 40/112 [00:14<00:25,  2.80it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.77it/s] 29%|██▉       | 33/112 [00:11<00:27,  2.84it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.82it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.77it/s] 30%|███       | 34/112 [00:12<00:27,  2.85it/s]  1%|          | 1/112 [00:00<01:35,  1.17it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.83it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.78it/s] 31%|███▏      | 35/112 [00:12<00:26,  2.86it/s]  2%|▏         | 2/112 [00:01<01:02,  1.76it/s] 38%|███▊      | 43/112 [00:15<00:24,  2.82it/s] 40%|████      | 45/112 [00:16<00:23,  2.79it/s] 32%|███▏      | 36/112 [00:13<00:26,  2.86it/s]  3%|▎         | 3/112 [00:01<00:52,  2.10it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.83it/s] 41%|████      | 46/112 [00:17<00:23,  2.78it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.86it/s]  4%|▎         | 4/112 [00:01<00:47,  2.29it/s] 40%|████      | 45/112 [00:16<00:23,  2.84it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.78it/s] 34%|███▍      | 38/112 [00:13<00:25,  2.87it/s]  4%|▍         | 5/112 [00:02<00:44,  2.41it/s] 41%|████      | 46/112 [00:16<00:23,  2.84it/s] 35%|███▍      | 39/112 [00:14<00:25,  2.88it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.79it/s]  5%|▌         | 6/112 [00:02<00:42,  2.51it/s] 42%|████▏     | 47/112 [00:17<00:22,  2.84it/s] 36%|███▌      | 40/112 [00:14<00:24,  2.88it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.79it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.83it/s]  6%|▋         | 7/112 [00:03<00:40,  2.56it/s] 37%|███▋      | 41/112 [00:14<00:24,  2.86it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.75it/s] 44%|████▍     | 49/112 [00:17<00:22,  2.84it/s]  7%|▋         | 8/112 [00:03<00:40,  2.57it/s] 38%|███▊      | 42/112 [00:15<00:24,  2.87it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.74it/s] 45%|████▍     | 50/112 [00:18<00:21,  2.84it/s]  8%|▊         | 9/112 [00:03<00:39,  2.59it/s] 38%|███▊      | 43/112 [00:15<00:23,  2.88it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.75it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.83it/s]  9%|▉         | 10/112 [00:04<00:39,  2.60it/s] 39%|███▉      | 44/112 [00:15<00:23,  2.87it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.72it/s] 46%|████▋     | 52/112 [00:18<00:21,  2.84it/s] 10%|▉         | 11/112 [00:04<00:38,  2.61it/s] 40%|████      | 45/112 [00:16<00:23,  2.88it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.73it/s] 47%|████▋     | 53/112 [00:19<00:20,  2.82it/s] 41%|████      | 46/112 [00:16<00:23,  2.86it/s] 11%|█         | 12/112 [00:04<00:38,  2.60it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.70it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.82it/s] 42%|████▏     | 47/112 [00:16<00:22,  2.87it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.59it/s] 50%|█████     | 56/112 [00:20<00:20,  2.69it/s] 49%|████▉     | 55/112 [00:19<00:20,  2.83it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.87it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.59it/s] 51%|█████     | 57/112 [00:21<00:20,  2.68it/s] 50%|█████     | 56/112 [00:20<00:19,  2.84it/s] 44%|████▍     | 49/112 [00:17<00:21,  2.87it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 51%|█████     | 57/112 [00:20<00:19,  2.82it/s] 45%|████▍     | 50/112 [00:17<00:21,  2.86it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.73it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.82it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.83it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.62it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.79it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.71it/s] 46%|████▋     | 52/112 [00:18<00:21,  2.82it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.62it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.77it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.72it/s] 47%|████▋     | 53/112 [00:18<00:20,  2.85it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.64it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.80it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.74it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.87it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.66it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.81it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.76it/s] 49%|████▉     | 55/112 [00:19<00:19,  2.86it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.80it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.67it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.77it/s] 50%|█████     | 56/112 [00:20<00:19,  2.85it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.82it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.69it/s] 58%|█████▊    | 65/112 [00:24<00:16,  2.79it/s] 51%|█████     | 57/112 [00:20<00:19,  2.86it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.80it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.78it/s] 21%|██        | 23/112 [00:09<00:33,  2.69it/s] 52%|█████▏    | 58/112 [00:20<00:18,  2.85it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.82it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.79it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.71it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.85it/s] 60%|█████▉    | 67/112 [00:24<00:15,  2.82it/s] 61%|██████    | 68/112 [00:25<00:15,  2.80it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.70it/s] 54%|█████▎    | 60/112 [00:21<00:18,  2.87it/s] 61%|██████    | 68/112 [00:24<00:15,  2.82it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.79it/s] 54%|█████▍    | 61/112 [00:21<00:17,  2.86it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.70it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.81it/s] 62%|██████▎   | 70/112 [00:25<00:15,  2.78it/s] 55%|█████▌    | 62/112 [00:22<00:17,  2.86it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.70it/s] 62%|██████▎   | 70/112 [00:25<00:14,  2.83it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.80it/s] 56%|█████▋    | 63/112 [00:22<00:17,  2.88it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.70it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.83it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.80it/s] 57%|█████▋    | 64/112 [00:22<00:16,  2.87it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.71it/s] 64%|██████▍   | 72/112 [00:25<00:14,  2.84it/s] 58%|█████▊    | 65/112 [00:23<00:16,  2.88it/s] 65%|██████▌   | 73/112 [00:27<00:13,  2.80it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.72it/s] 65%|██████▌   | 73/112 [00:26<00:13,  2.83it/s] 59%|█████▉    | 66/112 [00:23<00:16,  2.87it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.79it/s] 28%|██▊       | 31/112 [00:12<00:29,  2.72it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.83it/s] 60%|█████▉    | 67/112 [00:23<00:15,  2.87it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.79it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.71it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.83it/s] 61%|██████    | 68/112 [00:24<00:15,  2.88it/s] 68%|██████▊   | 76/112 [00:28<00:12,  2.80it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.71it/s] 68%|██████▊   | 76/112 [00:27<00:12,  2.82it/s] 62%|██████▏   | 69/112 [00:24<00:15,  2.86it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.80it/s] 30%|███       | 34/112 [00:13<00:28,  2.72it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.80it/s] 62%|██████▎   | 70/112 [00:24<00:14,  2.84it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.80it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.72it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.81it/s] 63%|██████▎   | 71/112 [00:25<00:14,  2.85it/s] 71%|███████   | 79/112 [00:29<00:11,  2.80it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.73it/s] 71%|███████   | 79/112 [00:28<00:11,  2.81it/s] 64%|██████▍   | 72/112 [00:25<00:14,  2.86it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.80it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.74it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.82it/s] 65%|██████▌   | 73/112 [00:25<00:13,  2.87it/s] 72%|███████▏  | 81/112 [00:29<00:11,  2.79it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.74it/s] 72%|███████▏  | 81/112 [00:29<00:10,  2.84it/s] 66%|██████▌   | 74/112 [00:26<00:13,  2.88it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.80it/s] 35%|███▍      | 39/112 [00:15<00:26,  2.74it/s] 67%|██████▋   | 75/112 [00:26<00:12,  2.88it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.84it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.79it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.73it/s] 68%|██████▊   | 76/112 [00:26<00:12,  2.87it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.83it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.80it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.74it/s] 69%|██████▉   | 77/112 [00:27<00:12,  2.88it/s] 75%|███████▌  | 84/112 [00:30<00:09,  2.84it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.80it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.74it/s] 70%|██████▉   | 78/112 [00:27<00:11,  2.86it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.82it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.79it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.70it/s] 71%|███████   | 79/112 [00:28<00:11,  2.87it/s] 77%|███████▋  | 86/112 [00:30<00:09,  2.82it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.75it/s] 71%|███████▏  | 80/112 [00:28<00:11,  2.88it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.67it/s] 78%|███████▊  | 87/112 [00:31<00:08,  2.84it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.71it/s] 72%|███████▏  | 81/112 [00:28<00:10,  2.87it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.83it/s] 40%|████      | 45/112 [00:17<00:25,  2.64it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.70it/s] 73%|███████▎  | 82/112 [00:29<00:10,  2.88it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.84it/s] 41%|████      | 46/112 [00:17<00:24,  2.65it/s] 80%|████████  | 90/112 [00:33<00:08,  2.72it/s] 74%|███████▍  | 83/112 [00:29<00:10,  2.89it/s] 80%|████████  | 90/112 [00:32<00:07,  2.85it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.72it/s] 75%|███████▌  | 84/112 [00:29<00:09,  2.88it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.84it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.66it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.68it/s] 76%|███████▌  | 85/112 [00:30<00:09,  2.89it/s] 82%|████████▏ | 92/112 [00:33<00:07,  2.85it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.66it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.71it/s] 77%|███████▋  | 86/112 [00:30<00:08,  2.89it/s] 83%|████████▎ | 93/112 [00:33<00:06,  2.85it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.69it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.74it/s] 78%|███████▊  | 87/112 [00:30<00:08,  2.85it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.82it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.70it/s] 85%|████████▍ | 95/112 [00:34<00:06,  2.76it/s] 79%|███████▊  | 88/112 [00:31<00:08,  2.87it/s] 85%|████████▍ | 95/112 [00:34<00:05,  2.83it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.71it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.76it/s] 79%|███████▉  | 89/112 [00:31<00:08,  2.86it/s] 86%|████████▌ | 96/112 [00:34<00:05,  2.81it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.70it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.75it/s] 80%|████████  | 90/112 [00:31<00:07,  2.84it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.81it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.70it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.76it/s] 81%|████████▏ | 91/112 [00:32<00:07,  2.86it/s] 88%|████████▊ | 98/112 [00:35<00:04,  2.83it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.71it/s] 82%|████████▏ | 92/112 [00:32<00:06,  2.87it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.77it/s] 88%|████████▊ | 99/112 [00:35<00:04,  2.84it/s] 50%|█████     | 56/112 [00:21<00:20,  2.71it/s] 83%|████████▎ | 93/112 [00:32<00:06,  2.88it/s] 89%|████████▉ | 100/112 [00:36<00:04,  2.77it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.83it/s] 51%|█████     | 57/112 [00:21<00:20,  2.72it/s] 84%|████████▍ | 94/112 [00:33<00:06,  2.89it/s] 90%|█████████ | 101/112 [00:37<00:03,  2.78it/s] 90%|█████████ | 101/112 [00:36<00:03,  2.85it/s] 52%|█████▏    | 58/112 [00:22<00:19,  2.72it/s] 85%|████████▍ | 95/112 [00:33<00:05,  2.89it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.79it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.85it/s] 86%|████████▌ | 96/112 [00:33<00:05,  2.88it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.71it/s] 92%|█████████▏| 103/112 [00:37<00:03,  2.78it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.83it/s] 87%|████████▋ | 97/112 [00:34<00:05,  2.82it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.70it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.77it/s] 93%|█████████▎| 104/112 [00:37<00:02,  2.83it/s] 88%|████████▊ | 98/112 [00:34<00:04,  2.84it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.72it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.79it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.83it/s] 88%|████████▊ | 99/112 [00:34<00:04,  2.86it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.79it/s] 95%|█████████▍| 106/112 [00:38<00:02,  2.84it/s] 89%|████████▉ | 100/112 [00:35<00:04,  2.88it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.72it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.79it/s] 96%|█████████▌| 107/112 [00:38<00:01,  2.82it/s] 90%|█████████ | 101/112 [00:35<00:03,  2.88it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.74it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.79it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.83it/s] 91%|█████████ | 102/112 [00:36<00:03,  2.88it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.79it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.73it/s] 97%|█████████▋| 109/112 [00:39<00:01,  2.84it/s] 92%|█████████▏| 103/112 [00:36<00:03,  2.89it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.79it/s] 98%|█████████▊| 110/112 [00:39<00:00,  2.85it/s] 59%|█████▉    | 66/112 [00:25<00:16,  2.72it/s] 93%|█████████▎| 104/112 [00:36<00:02,  2.89it/s] 99%|█████████▉| 111/112 [00:40<00:00,  2.79it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.83it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.72it/s] 94%|█████████▍| 105/112 [00:37<00:02,  2.88it/s]100%|██████████| 112/112 [00:41<00:00,  2.80it/s]100%|██████████| 112/112 [00:41<00:00,  2.73it/s]
100%|██████████| 112/112 [00:40<00:00,  2.84it/s]100%|██████████| 112/112 [00:40<00:00,  2.79it/s]
 61%|██████    | 68/112 [00:25<00:16,  2.73it/s] 95%|█████████▍| 106/112 [00:37<00:02,  2.87it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.73it/s] 96%|█████████▌| 107/112 [00:37<00:01,  2.84it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.68it/s] 96%|█████████▋| 108/112 [00:38<00:01,  2.79it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.64it/s] 97%|█████████▋| 109/112 [00:38<00:01,  2.80it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.62it/s] 98%|█████████▊| 110/112 [00:38<00:00,  2.80it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.62it/s] 99%|█████████▉| 111/112 [00:39<00:00,  2.80it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.61it/s]100%|██████████| 112/112 [00:39<00:00,  2.83it/s]100%|██████████| 112/112 [00:39<00:00,  2.83it/s]
 67%|██████▋   | 75/112 [00:28<00:14,  2.61it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.59it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.63it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.66it/s] 71%|███████   | 79/112 [00:29<00:12,  2.68it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.70it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.71it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.71it/s]W0402 09:12:41.756000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.756000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.757000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.757000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.757000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.757000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.757000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:31<00:10,  2.72it/s]W0402 09:12:41.797000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.797000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.797000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.797000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.797000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.960000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.960000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.960000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.960000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:41.961000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.067000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.067000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.067000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.068000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.068000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.068000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.068000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.109000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.110000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.110000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.110000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.110000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:31<00:10,  2.72it/s]W0402 09:12:42.257000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.257000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.257000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.257000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.257000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.257000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.257000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.276000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.276000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.276000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.276000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.276000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.287000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.287000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.287000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.288000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.288000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.353000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.353000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.353000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.353000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.353000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 76%|███████▌  | 85/112 [00:32<00:09,  2.73it/s]W0402 09:12:42.578000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.578000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.579000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.579000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.579000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.579000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.579000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.609000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.609000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.609000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.610000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.610000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.675000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.675000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.675000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.676000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:42.676000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:32<00:09,  2.73it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.72it/s]W0402 09:12:43.263000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.274000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.282000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.282000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.601000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:33<00:08,  2.72it/s]W0402 09:12:43.615000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.623000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.624000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.720000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.720000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.720000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.720000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.720000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.720000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.721000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.751000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.751000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.751000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.751000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:43.751000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:33<00:08,  2.68it/s]W0402 09:12:44.034000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.034000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.034000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.035000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.035000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.035000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.035000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.035000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.077000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.078000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.078000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.078000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.078000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.078000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.078000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.092000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.092000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.092000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.093000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.093000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.093000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.093000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.118000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.119000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.119000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.119000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.119000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.123000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.124000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.124000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.124000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.124000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.283000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.284000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.284000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.284000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.284000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.309000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.309000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.309000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.309000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.309000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 90/112 [00:33<00:08,  2.65it/s]W0402 09:12:44.426000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.426000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.426000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.426000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.426000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.426000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.426000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.426000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.585000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.586000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.586000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.586000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.586000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.586000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.586000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.615000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.615000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.615000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.615000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.615000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.681000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.681000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.681000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.681000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.681000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.711000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.712000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.712000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.712000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.712000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.712000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:44.716000 140002518615872 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:34<00:07,  2.65it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.63it/s]W0402 09:12:45.147000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:45.152000 139789019932480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:35<00:07,  2.62it/s]W0402 09:12:45.595000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:45.601000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:45.607000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:45.607000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:35<00:06,  2.63it/s]W0402 09:12:46.041000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.041000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.041000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.042000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.042000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.042000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.042000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.068000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.068000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.068000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.068000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.068000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 95/112 [00:35<00:06,  2.62it/s]W0402 09:12:46.365000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.365000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.365000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.365000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.365000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.365000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.366000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.366000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 96/112 [00:36<00:06,  2.65it/s]W0402 09:12:46.671000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.671000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.671000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.671000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:46.672000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:36<00:05,  2.64it/s]W0402 09:12:47.114000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:47.121000 140642738739008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:37<00:05,  2.64it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.64it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.64it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.64it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.63it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.63it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.63it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.64it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.64it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.63it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.63it/s]I0402 09:12:51.259122 1549316 finetune.py:45] layer 29_down initial loss 0.03032146766781807
W0402 09:12:51.259425 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 97%|█████████▋| 109/112 [00:41<00:01,  2.63it/s]W0402 09:12:51.749353 1549316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 98%|█████████▊| 110/112 [00:41<00:00,  2.64it/s]29_down proxy err 0.03254582732915878 tr(WHW.T) 132.4029083251953
I0402 09:12:52.253217 1549154 finetune.py:45] layer 28_down initial loss 0.024090543389320374
W0402 09:12:52.253442 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 99%|█████████▉| 111/112 [00:41<00:00,  2.64it/s]W0402 09:12:52.728952 1549154 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

100%|██████████| 112/112 [00:42<00:00,  2.64it/s]100%|██████████| 112/112 [00:42<00:00,  2.65it/s]
28_down proxy err 0.03876182809472084 tr(WHW.T) 82.2515869140625
I0402 09:12:53.841740 1550298 finetune.py:45] layer 31_down initial loss 0.026613062247633934
W0402 09:12:53.841975 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:12:54.501710 1550298 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

31_down proxy err 0.005534423980861902 tr(WHW.T) 2762.897216796875
W0402 09:12:58.852000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.852000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.852000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.853000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.853000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.853000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.853000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.897000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:58.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.070000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.070000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.070000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.070000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.070000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.374000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.375000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.375000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.375000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.375000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.375000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.375000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.406000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.406000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.406000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.406000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.406000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.472000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.472000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.472000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.472000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0402 09:12:59.472000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.434000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.447000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.454000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.455000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.898000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.926000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.927000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.927000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.927000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:13:00.927000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.211000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.211000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.212000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.212000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.212000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.212000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.212000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.212000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.487000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.488000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.488000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.488000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.488000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.911000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0402 09:13:01.917000 140033969268544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0402 09:13:09.423173 1549843 finetune.py:45] layer 30_down initial loss 0.051070041954517365
W0402 09:13:09.423936 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0402 09:13:10.522286 1549843 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

30_down proxy err 0.017661165446043015 tr(WHW.T) 368.5620422363281
I0402 09:13:23.452397 1554709 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0402 09:13:23.452530 1554709 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0402 09:13:23.452573 1554709 utils.py:162] NumExpr defaulting to 16 threads.
I0402 09:13:23.639551 1554709 config.py:58] PyTorch version 2.4.0 available.
W0402 09:13:25.431050 1554709 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0402 09:13:25.431617 1554709 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.96it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  4.30it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  4.47it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  4.41it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  4.56it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.05it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.31it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.81it/s]
Some weights of the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:02,  2.75it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  3.16it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:01,  3.39it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.63it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.85it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.00it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.15it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.79it/s]
W0402 09:13:31.084591 1554709 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0402 09:13:31.086247 1554709 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0402 09:13:31.105959 1554709 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0402 09:13:31.119210 1554709 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0402 09:13:31.127170 1554709 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0402 09:13:31.142905 1554709 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0402 09:13:31.182099 1554709 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0402 09:13:31.221735 1554709 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0402 09:13:31.250275 1554709 hfize_llama.py:113] loaded layer 0
I0402 09:13:31.346712 1554709 hfize_llama.py:113] loaded layer 1
I0402 09:13:31.454861 1554709 hfize_llama.py:113] loaded layer 2
I0402 09:13:31.605575 1554709 hfize_llama.py:113] loaded layer 3
I0402 09:13:31.731930 1554709 hfize_llama.py:113] loaded layer 4
I0402 09:13:31.834468 1554709 hfize_llama.py:113] loaded layer 5
I0402 09:13:31.921138 1554709 hfize_llama.py:113] loaded layer 6
I0402 09:13:31.994340 1554709 hfize_llama.py:113] loaded layer 7
I0402 09:13:32.154604 1554709 hfize_llama.py:113] loaded layer 8
I0402 09:13:32.283481 1554709 hfize_llama.py:113] loaded layer 9
I0402 09:13:32.416249 1554709 hfize_llama.py:113] loaded layer 10
I0402 09:13:32.562855 1554709 hfize_llama.py:113] loaded layer 11
I0402 09:13:32.668054 1554709 hfize_llama.py:113] loaded layer 12
I0402 09:13:32.816803 1554709 hfize_llama.py:113] loaded layer 13
I0402 09:13:32.963167 1554709 hfize_llama.py:113] loaded layer 14
I0402 09:13:33.072791 1554709 hfize_llama.py:113] loaded layer 15
I0402 09:13:33.189725 1554709 hfize_llama.py:113] loaded layer 16
I0402 09:13:33.281964 1554709 hfize_llama.py:113] loaded layer 17
I0402 09:13:33.399625 1554709 hfize_llama.py:113] loaded layer 18
I0402 09:13:33.563558 1554709 hfize_llama.py:113] loaded layer 19
I0402 09:13:33.690346 1554709 hfize_llama.py:113] loaded layer 20
I0402 09:13:33.853186 1554709 hfize_llama.py:113] loaded layer 21
I0402 09:13:34.022829 1554709 hfize_llama.py:113] loaded layer 22
I0402 09:13:34.154219 1554709 hfize_llama.py:113] loaded layer 23
I0402 09:13:34.318557 1554709 hfize_llama.py:113] loaded layer 24
I0402 09:13:34.441568 1554709 hfize_llama.py:113] loaded layer 25
I0402 09:13:34.580399 1554709 hfize_llama.py:113] loaded layer 26
I0402 09:13:34.695868 1554709 hfize_llama.py:113] loaded layer 27
I0402 09:13:34.801164 1554709 hfize_llama.py:113] loaded layer 28
I0402 09:13:34.951964 1554709 hfize_llama.py:113] loaded layer 29
I0402 09:13:35.086442 1554709 hfize_llama.py:113] loaded layer 30
I0402 09:13:35.264886 1554709 hfize_llama.py:113] loaded layer 31
I0402 09:13:35.264976 1554709 hfize_llama.py:115] saving model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.53it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.14it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.83it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]
I0402 09:13:47.263278 1554709 hfize_llama.py:122] successfully loaded hfized model
