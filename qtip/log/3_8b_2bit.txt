I0314 11:58:25.434319 3667 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 11:58:25.434487 3667 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 11:58:25.434535 3667 utils.py:162] NumExpr defaulting to 16 threads.
I0314 11:58:25.593015 3667 config.py:58] PyTorch version 2.4.0 available.
W0314 11:58:28.214395 3667 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/4 [00:31<?, ?it/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 215, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 110, in main
    model = AutoModelForCausalLM.from_pretrained(args.base_model,
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py", line 3769, in from_pretrained
    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 1098, in get_checkpoint_shard_files
    cached_filename = cached_file(
                      ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1389, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1915, in _download_to_tmp_and_move
    http_get(
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 549, in http_get
    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):
  File "/opt/conda/lib/python3.11/site-packages/requests/models.py", line 820, in generate
    yield from self.raw.stream(chunk_size, decode_content=True)
  File "/opt/conda/lib/python3.11/site-packages/urllib3/response.py", line 1060, in stream
    data = self.read(amt=amt, decode_content=decode_content)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/urllib3/response.py", line 949, in read
    data = self._raw_read(amt)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/urllib3/response.py", line 873, in _raw_read
    data = self._fp_read(amt, read1=read1) if not fp_closed else b""
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/urllib3/response.py", line 856, in _fp_read
    return self._fp.read(amt) if amt is not None else self._fp.read()
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/http/client.py", line 473, in read
    s = self.fp.read(amt)
        ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/socket.py", line 718, in readinto
    return self._sock.recv_into(b)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/ssl.py", line 1314, in recv_into
    return self.read(nbytes, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/ssl.py", line 1166, in read
    return self._sslobj.read(len, buffer)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
I0314 12:00:23.857352 5314 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:00:23.857485 5314 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:00:23.857532 5314 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:00:23.975708 5314 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 15, in <module>
    from lib import utils
  File "/workspace/Weight_compression/qtip/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/workspace/Weight_compression/qtip/lib/utils/data_utils.py", line 10, in <module>
    from .matmul_had import matmul_hadU
  File "/workspace/Weight_compression/qtip/lib/utils/matmul_had.py", line 1, in <module>
    import fast_hadamard_transform
  File "/opt/conda/lib/python3.11/site-packages/fast_hadamard_transform-1.0.4.post1-py3.11-linux-x86_64.egg/fast_hadamard_transform/__init__.py", line 3, in <module>
    from fast_hadamard_transform.fast_hadamard_transform_interface import hadamard_transform
  File "/opt/conda/lib/python3.11/site-packages/fast_hadamard_transform-1.0.4.post1-py3.11-linux-x86_64.egg/fast_hadamard_transform/fast_hadamard_transform_interface.py", line 5, in <module>
    from scipy.linalg import hadamard
  File "/opt/conda/lib/python3.11/site-packages/scipy/linalg/__init__.py", line 203, in <module>
    from ._misc import *
  File "/opt/conda/lib/python3.11/site-packages/scipy/linalg/_misc.py", line 3, in <module>
    from .blas import get_blas_funcs
  File "<frozen importlib._bootstrap>", line 1176, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1147, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 690, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 936, in exec_module
  File "<frozen importlib._bootstrap_external>", line 1021, in get_code
  File "<frozen importlib._bootstrap_external>", line 468, in cache_from_source
KeyboardInterrupt
I0314 12:01:21.342613 5999 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:01:21.342759 5999 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:01:21.342799 5999 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:01:21.472306 5999 config.py:58] PyTorch version 2.4.0 available.
W0314 12:01:23.989218 5999 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:02,  2.89it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  3.24it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:01,  3.78it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.84it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  4.01it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.06it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.27it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.95it/s]
I0314 12:01:26.598608 5999 quantize_finetune_llama.py:135] loaded model
I0314 12:01:50.827085 5999 quantize_finetune_llama.py:139] loaded dataset and devset
I0314 12:02:00.296401 5999 quantize_finetune_llama.py:159] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 12:03:00.442919 5999 quantize_finetune_llama.py:186] computed original embedding for layer 0 in 59.880053997039795s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0314 12:03:26.293686 5999 quantize_finetune_llama.py:159] layer 1 gpu 1
I0314 12:03:28.286376 8164 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:03:28.286475 8164 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:03:28.286533 8164 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:03:28.474641 8164 config.py:58] PyTorch version 2.4.0 available.
I0314 12:03:30.936857 8164 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 12:03:31.439918 8164 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:12,  2.35s/it]  6%|▋         | 2/32 [00:02<00:34,  1.16s/it]  9%|▉         | 3/32 [00:02<00:22,  1.29it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.67it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.01it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.23it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:05<00:07,  2.86it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.92it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.00it/s] 41%|████      | 13/32 [00:06<00:06,  3.03it/s] 44%|████▍     | 14/32 [00:06<00:05,  3.03it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.06it/s] 50%|█████     | 16/32 [00:07<00:05,  3.08it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.06it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.08it/s] 59%|█████▉    | 19/32 [00:08<00:04,  3.10it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.11it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.08it/s] 69%|██████▉   | 22/32 [00:09<00:03,  3.10it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.09it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.10it/s] 78%|███████▊  | 25/32 [00:10<00:02,  3.12it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.13it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.13it/s] 88%|████████▊ | 28/32 [00:11<00:01,  3.14it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.10it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.09it/s] 97%|█████████▋| 31/32 [00:12<00:00,  3.12it/s]100%|██████████| 32/32 [00:12<00:00,  3.13it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
W0314 12:03:47.137000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.138000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.138000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.138000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.138000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.138000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.138000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.165000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.165000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.165000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.165000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.165000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.450000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.450000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.450000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.450000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:03:47.450000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.289000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.289000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.289000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.289000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.289000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.289000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.289000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.306000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.306000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.306000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.306000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.307000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.505000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.505000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.505000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.505000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:03:48.505000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.612000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.613000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.613000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.613000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.613000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.613000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.613000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.630000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.630000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.630000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.631000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:03:49.631000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:03:50.480000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:03:50.481000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:03:50.481000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:03:50.481000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:03:50.481000 140624353199936 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 12:03:57.456413 8164 finetune.py:45] layer 0_v initial loss 5.740832875744672e-07
W0314 12:03:57.456614 8164 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:04:49.926798 9966 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:04:49.926941 9966 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:04:49.926982 9966 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:04:50.109938 9966 config.py:58] PyTorch version 2.4.0 available.
W0314 12:04:52.306144 9966 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:02,  2.75it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  2.84it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:01,  3.22it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.40it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.73it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.06it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  3.42it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  3.41it/s]
I0314 12:04:55.221772 9966 quantize_finetune_llama.py:135] loaded model
I0314 12:05:19.580933 9966 quantize_finetune_llama.py:139] loaded dataset and devset
I0314 12:05:25.710403 9966 quantize_finetune_llama.py:159] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 12:06:30.821334 9966 quantize_finetune_llama.py:186] computed original embedding for layer 0 in 64.97268652915955s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0314 12:07:00.110140 9966 quantize_finetune_llama.py:159] layer 1 gpu 1
I0314 12:07:02.043945 12330 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:07:02.044058 12330 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:07:02.044116 12330 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:07:02.223548 12330 config.py:58] PyTorch version 2.4.0 available.
I0314 12:07:04.399217 12330 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 12:07:04.753974 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]  9%|▉         | 3/32 [00:01<00:16,  1.81it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.75it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.87it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.93it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.96it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.02it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.02it/s] 41%|████      | 13/32 [00:05<00:06,  3.00it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.04it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.06it/s] 50%|█████     | 16/32 [00:06<00:05,  3.06it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.07it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.08it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.12it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.10it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.07it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.06it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.07it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.09it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.09it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.11it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.10it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.06it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.06it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.08it/s]100%|██████████| 32/32 [00:11<00:00,  3.09it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]
W0314 12:07:19.420000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.420000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.420000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.420000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.420000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.420000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.421000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.445000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.445000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.446000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.446000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.446000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.726000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.726000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.727000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.727000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:07:19.727000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.565000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.565000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.565000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.565000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.566000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.566000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.566000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.583000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.583000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.583000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.583000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.583000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.776000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.777000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.777000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.777000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:07:20.777000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.880000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.880000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.881000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.881000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.881000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.881000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.881000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.899000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.899000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.899000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.899000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:07:21.899000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:07:22.751000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:07:22.751000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:07:22.751000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:07:22.751000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:07:22.751000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 12:07:28.901548 12330 finetune.py:45] layer 0_v initial loss 5.740832875744672e-07
W0314 12:07:28.901804 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:08:04.193086 12330 finetune.py:68] layer 0_v @ epoch 0 new loss 4.2221199691994116e-07 old loss 5.740832875744672e-07 BETTER
I0314 12:08:05.447352 9966 quantize_finetune_llama.py:186] computed original embedding for layer 1 in 65.179682970047s
I0314 12:08:14.972991 9966 quantize_finetune_llama.py:159] layer 2 gpu 2
I0314 12:08:16.990513 13837 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:08:16.990745 13837 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:08:16.990872 13837 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:08:17.188588 13837 config.py:58] PyTorch version 2.4.0 available.
I0314 12:08:19.604600 13837 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 12:08:20.073906 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:21,  2.62s/it]  6%|▋         | 2/32 [00:02<00:38,  1.28s/it]  9%|▉         | 3/32 [00:03<00:24,  1.17it/s] 12%|█▎        | 4/32 [00:03<00:18,  1.54it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.86it/s] 19%|█▉        | 6/32 [00:04<00:12,  2.11it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.31it/s] 25%|██▌       | 8/32 [00:05<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:05<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:06<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.80it/s] 41%|████      | 13/32 [00:06<00:06,  2.83it/s] 44%|████▍     | 14/32 [00:07<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:07<00:05,  2.90it/s] 50%|█████     | 16/32 [00:07<00:05,  2.89it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.88it/s] 56%|█████▋    | 18/32 [00:08<00:04,  2.88it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.89it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.90it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.95it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.95it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.94it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.93it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.92it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.94it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.93it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.93it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.97it/s]100%|██████████| 32/32 [00:13<00:00,  2.93it/s]100%|██████████| 32/32 [00:13<00:00,  2.42it/s]
W0314 12:08:36.867000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.867000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.867000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.867000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.868000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.868000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.868000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.895000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.895000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.895000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.895000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:08:36.895000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:08:37.195000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:08:37.196000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:08:37.196000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:08:37.196000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:08:37.196000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.083000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.083000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.083000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.083000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.083000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.083000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.083000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.102000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.102000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.102000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.102000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.102000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.311000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.311000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.312000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.312000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:08:38.312000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.471000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.472000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.472000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.472000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.472000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.472000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.472000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.492000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.492000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.492000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.492000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:08:39.492000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:08:40.390000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:08:40.390000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:08:40.391000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:08:40.391000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:08:40.391000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
I0314 12:08:40.888635 12330 finetune.py:68] layer 0_v @ epoch 1 new loss 3.864934683406318e-07 old loss 4.2221199691994116e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 12:08:47.117357 13837 finetune.py:45] layer 1_v initial loss 4.826856184081407e-06
W0314 12:08:47.117596 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:09:17.902449 12330 finetune.py:68] layer 0_v @ epoch 2 new loss 3.6969288430555025e-07 old loss 3.864934683406318e-07 BETTER
I0314 12:09:20.449734 9966 quantize_finetune_llama.py:186] computed original embedding for layer 2 in 65.3203284740448s
I0314 12:09:21.616744 13837 finetune.py:68] layer 1_v @ epoch 0 new loss 1.7355890804537921e-06 old loss 4.826856184081407e-06 BETTER
I0314 12:09:29.517791 9966 quantize_finetune_llama.py:159] layer 3 gpu 3
I0314 12:09:31.554811 15465 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:09:31.554934 15465 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:09:31.554997 15465 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:09:31.736132 15465 config.py:58] PyTorch version 2.4.0 available.
I0314 12:09:33.955028 15465 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 12:09:34.363958 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:32,  3.00s/it]  6%|▋         | 2/32 [00:03<00:43,  1.44s/it]  9%|▉         | 3/32 [00:03<00:27,  1.06it/s] 12%|█▎        | 4/32 [00:04<00:19,  1.41it/s] 16%|█▌        | 5/32 [00:04<00:15,  1.72it/s] 19%|█▉        | 6/32 [00:04<00:13,  2.00it/s] 22%|██▏       | 7/32 [00:05<00:11,  2.21it/s] 25%|██▌       | 8/32 [00:05<00:10,  2.38it/s] 28%|██▊       | 9/32 [00:05<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:06<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:06<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.74it/s] 41%|████      | 13/32 [00:07<00:06,  2.78it/s] 44%|████▍     | 14/32 [00:07<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.81it/s] 50%|█████     | 16/32 [00:08<00:05,  2.81it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:08<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:09<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:10<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:10<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:11<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:11<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:12<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:13<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.86it/s]100%|██████████| 32/32 [00:13<00:00,  2.86it/s]100%|██████████| 32/32 [00:13<00:00,  2.31it/s]
W0314 12:09:51.694000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.695000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.695000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.695000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.695000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.695000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.695000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.724000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.724000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.724000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.724000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:09:51.725000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.025000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.025000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.025000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.025000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.026000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.910000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.910000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.910000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.910000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.910000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.910000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.910000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.929000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.929000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.929000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.929000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:09:52.929000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:09:53.139000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:09:53.139000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:09:53.139000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:09:53.139000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:09:53.139000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.302000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.302000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.302000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.302000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.302000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.302000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.302000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.321000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.321000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.321000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.321000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:09:54.321000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:09:55.213000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:09:55.213000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:09:55.213000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:09:55.213000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:09:55.213000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
I0314 12:09:55.934505 12330 finetune.py:68] layer 0_v @ epoch 3 new loss 3.603609002311714e-07 old loss 3.6969288430555025e-07 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 12:09:58.066989 13837 finetune.py:68] layer 1_v @ epoch 1 new loss 1.2135409406255349e-06 old loss 1.7355890804537921e-06 BETTER
I0314 12:10:02.030092 15465 finetune.py:45] layer 2_v initial loss 2.779255373752676e-05
W0314 12:10:02.030544 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:10:33.895045 13837 finetune.py:68] layer 1_v @ epoch 2 new loss 1.082268681784626e-06 old loss 1.2135409406255349e-06 BETTER
I0314 12:10:33.929042 12330 finetune.py:68] layer 0_v @ epoch 4 new loss 3.5418230481809587e-07 old loss 3.603609002311714e-07 BETTER
I0314 12:10:34.940665 9966 quantize_finetune_llama.py:186] computed original embedding for layer 3 in 65.1897439956665s
W0314 12:10:35.952629 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_v proxy err 0.019532809033989906 tr(WHW.T) 1.3175290822982788
  0%|          | 0/32 [00:00<?, ?it/s]I0314 12:10:37.638050 15465 finetune.py:68] layer 2_v @ epoch 0 new loss 4.290189281164203e-06 old loss 2.779255373752676e-05 BETTER
  3%|▎         | 1/32 [00:02<01:23,  2.68s/it]  6%|▋         | 2/32 [00:03<00:41,  1.39s/it]  9%|▉         | 3/32 [00:03<00:29,  1.02s/it] 12%|█▎        | 4/32 [00:04<00:21,  1.31it/s] 16%|█▌        | 5/32 [00:04<00:16,  1.60it/s] 19%|█▉        | 6/32 [00:04<00:13,  1.86it/s] 22%|██▏       | 7/32 [00:05<00:12,  2.07it/s] 25%|██▌       | 8/32 [00:05<00:10,  2.20it/s] 28%|██▊       | 9/32 [00:06<00:10,  2.28it/s] 31%|███▏      | 10/32 [00:06<00:09,  2.34it/s] 34%|███▍      | 11/32 [00:06<00:08,  2.38it/s]I0314 12:10:44.575368 9966 quantize_finetune_llama.py:159] layer 4 gpu 0
 38%|███▊      | 12/32 [00:07<00:08,  2.39it/s] 41%|████      | 13/32 [00:07<00:07,  2.38it/s] 44%|████▍     | 14/32 [00:08<00:07,  2.37it/s] 47%|████▋     | 15/32 [00:08<00:07,  2.42it/s] 50%|█████     | 16/32 [00:08<00:06,  2.42it/s]I0314 12:10:46.610991 17150 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:10:46.611116 17150 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:10:46.611184 17150 utils.py:162] NumExpr defaulting to 16 threads.
 53%|█████▎    | 17/32 [00:09<00:06,  2.41it/s]I0314 12:10:46.814743 17150 config.py:58] PyTorch version 2.4.0 available.
 56%|█████▋    | 18/32 [00:09<00:05,  2.42it/s] 59%|█████▉    | 19/32 [00:10<00:05,  2.42it/s] 62%|██████▎   | 20/32 [00:10<00:04,  2.43it/s] 66%|██████▌   | 21/32 [00:10<00:04,  2.50it/s] 69%|██████▉   | 22/32 [00:11<00:03,  2.54it/s]I0314 12:10:49.040191 17150 data_utils.py:336] using 256 training seqs, 128 validation seqs
 72%|███████▏  | 23/32 [00:11<00:03,  2.57it/s]W0314 12:10:49.472408 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:12<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:12<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:12<00:02,  2.59it/s]  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:13<00:01,  2.54it/s] 88%|████████▊ | 28/32 [00:13<00:01,  2.50it/s] 91%|█████████ | 29/32 [00:14<00:01,  2.50it/s] 94%|█████████▍| 30/32 [00:14<00:00,  2.49it/s] 97%|█████████▋| 31/32 [00:14<00:00,  2.49it/s]100%|██████████| 32/32 [00:15<00:00,  2.49it/s]100%|██████████| 32/32 [00:15<00:00,  2.10it/s]
  3%|▎         | 1/32 [00:02<01:21,  2.62s/it]  6%|▋         | 2/32 [00:02<00:38,  1.29s/it]  9%|▉         | 3/32 [00:03<00:24,  1.16it/s] 12%|█▎        | 4/32 [00:03<00:18,  1.52it/s] 16%|█▌        | 5/32 [00:04<00:14,  1.83it/s] 19%|█▉        | 6/32 [00:04<00:12,  2.09it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.28it/s] 25%|██▌       | 8/32 [00:05<00:09,  2.44it/s] 28%|██▊       | 9/32 [00:05<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:06<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.75it/s] 41%|████      | 13/32 [00:06<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:07<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.83it/s]W0314 12:10:58.267000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.268000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.268000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.268000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.268000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.268000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.268000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.300000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.300000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.300000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.300000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.301000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:07<00:05,  2.84it/s]W0314 12:10:58.491000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.491000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.491000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.492000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.492000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:08<00:05,  2.85it/s]W0314 12:10:58.742000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.743000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.743000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.743000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.743000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.743000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.743000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.770000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.770000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.770000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.770000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.770000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.840000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.840000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.840000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.840000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:10:58.840000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:08<00:04,  2.86it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.86it/s]W0314 12:10:59.645000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 62%|██████▎   | 20/32 [00:09<00:04,  2.85it/s]W0314 12:10:59.988000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:10:59.988000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:10:59.988000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:10:59.988000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:10:59.988000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:10:59.989000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:10:59.989000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:00.013000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:00.013000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:00.013000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:00.013000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:00.013000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:09<00:03,  2.86it/s]W0314 12:11:00.291000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:00.292000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:00.292000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:00.292000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:00.292000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:09<00:03,  2.87it/s]W0314 12:11:00.672000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 72%|███████▏  | 23/32 [00:10<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:11<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.87it/s]100%|██████████| 32/32 [00:13<00:00,  2.87it/s]100%|██████████| 32/32 [00:13<00:00,  2.38it/s]
W0314 12:11:06.349000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.349000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.349000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.349000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.349000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.349000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.349000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.679000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.679000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.679000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.679000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:06.679000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.566000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.566000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.566000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.566000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.566000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.566000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.567000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.585000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.585000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.585000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.585000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.585000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.795000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.795000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.795000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.795000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:07.795000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
I0314 12:11:08.494279 12330 finetune.py:45] layer 0_q initial loss 3.6712359019475116e-07
W0314 12:11:08.494681 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0314 12:11:08.943000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.944000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.944000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.944000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.944000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.944000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.944000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.962000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.962000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.962000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.963000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:08.963000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:11:09.859000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:11:09.860000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:11:09.860000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:11:09.860000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:11:09.860000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
I0314 12:11:10.685471 13837 finetune.py:68] layer 1_v @ epoch 3 new loss 1.0299271480107564e-06 old loss 1.082268681784626e-06 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 12:11:14.541067 15465 finetune.py:68] layer 2_v @ epoch 1 new loss 2.3237423647515243e-06 old loss 4.290189281164203e-06 BETTER
I0314 12:11:17.900414 17150 finetune.py:45] layer 3_v initial loss 5.7784622185863554e-05
W0314 12:11:17.900605 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:11:46.238253 12330 finetune.py:68] layer 0_q @ epoch 0 new loss 3.5578565871219325e-07 old loss 3.6712359019475116e-07 BETTER
I0314 12:11:47.741694 13837 finetune.py:68] layer 1_v @ epoch 4 new loss 9.99018539005192e-07 old loss 1.0299271480107564e-06 BETTER
W0314 12:11:49.579214 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_v proxy err 0.01828262209892273 tr(WHW.T) 5.699800491333008
  0%|          | 0/32 [00:00<?, ?it/s]I0314 12:11:50.766829 15465 finetune.py:68] layer 2_v @ epoch 2 new loss 1.984096570595284e-06 old loss 2.3237423647515243e-06 BETTER
I0314 12:11:52.430242 17150 finetune.py:68] layer 3_v @ epoch 0 new loss 1.0077459592139348e-05 old loss 5.7784622185863554e-05 BETTER
  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:02<00:31,  1.03s/it]  9%|▉         | 3/32 [00:02<00:21,  1.37it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.70it/s] 16%|█▌        | 5/32 [00:03<00:13,  1.96it/s] 19%|█▉        | 6/32 [00:03<00:12,  2.16it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.31it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.42it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.62it/s] 41%|████      | 13/32 [00:06<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.65it/s] 50%|█████     | 16/32 [00:07<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.70it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.71it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.71it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.71it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.70it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.70it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.67it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.69it/s]100%|██████████| 32/32 [00:13<00:00,  2.69it/s]100%|██████████| 32/32 [00:13<00:00,  2.37it/s]
W0314 12:12:09.361000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.361000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.361000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.361000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.361000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.361000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.362000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.393000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.393000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.393000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.393000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.393000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.570000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.570000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.570000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.570000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.570000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.798000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.798000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.798000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.798000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.799000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.799000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.799000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.822000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.822000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.822000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.822000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.822000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.891000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.891000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.891000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.891000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:12:09.891000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.646000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.975000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.975000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.975000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.975000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.975000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.975000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.975000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.997000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.997000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.997000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.997000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:12:10.997000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:12:11.262000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:12:11.263000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:12:11.263000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:12:11.263000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:12:11.263000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:12:11.624000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 12:12:19.284659 13837 finetune.py:45] layer 1_q initial loss 1.1047864063584711e-06
W0314 12:12:19.285215 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:12:24.517677 12330 finetune.py:68] layer 0_q @ epoch 1 new loss 3.50920913660957e-07 old loss 3.5578565871219325e-07 BETTER
I0314 12:12:27.387333 15465 finetune.py:68] layer 2_v @ epoch 3 new loss 1.8631890270626172e-06 old loss 1.984096570595284e-06 BETTER
I0314 12:12:28.548689 17150 finetune.py:68] layer 3_v @ epoch 1 new loss 6.227874109754339e-06 old loss 1.0077459592139348e-05 BETTER
I0314 12:12:54.181887 13837 finetune.py:68] layer 1_q @ epoch 0 new loss 1.0719702459027758e-06 old loss 1.1047864063584711e-06 BETTER
I0314 12:13:02.766411 12330 finetune.py:68] layer 0_q @ epoch 2 new loss 3.470466936050798e-07 old loss 3.50920913660957e-07 BETTER
I0314 12:13:03.693523 15465 finetune.py:68] layer 2_v @ epoch 4 new loss 1.792943407963321e-06 old loss 1.8631890270626172e-06 BETTER
I0314 12:13:04.040107 17150 finetune.py:68] layer 3_v @ epoch 2 new loss 5.412936388893286e-06 old loss 6.227874109754339e-06 BETTER
W0314 12:13:05.571567 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_v proxy err 0.025783024728298187 tr(WHW.T) 26.839576721191406
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:05,  2.12s/it]  6%|▋         | 2/32 [00:02<00:32,  1.10s/it]  9%|▉         | 3/32 [00:02<00:22,  1.30it/s] 12%|█▎        | 4/32 [00:03<00:17,  1.63it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.89it/s] 19%|█▉        | 6/32 [00:04<00:12,  2.09it/s] 22%|██▏       | 7/32 [00:04<00:11,  2.24it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.35it/s] 28%|██▊       | 9/32 [00:05<00:09,  2.43it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.49it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.56it/s] 41%|████      | 13/32 [00:06<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:07<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.63it/s] 50%|█████     | 16/32 [00:07<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:10<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:11<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:13<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.64it/s]100%|██████████| 32/32 [00:13<00:00,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.30it/s]
W0314 12:13:25.760000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.760000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.760000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.760000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.760000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.760000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.761000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.792000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.792000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.792000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.792000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.792000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.972000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.972000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.972000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.972000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:13:25.972000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.206000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.207000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.207000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.207000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.207000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.207000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.207000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.229000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.229000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.229000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.229000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.229000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.297000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.297000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.297000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.298000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:13:26.298000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.061000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.390000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.390000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.390000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.390000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.390000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.390000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.390000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.412000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.412000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.412000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.412000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.413000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.681000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.681000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.681000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.681000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:13:27.681000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:13:28.054000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 12:13:30.237133 13837 finetune.py:68] layer 1_q @ epoch 1 new loss 1.0512351309444057e-06 old loss 1.0719702459027758e-06 BETTER
I0314 12:13:35.837985 15465 finetune.py:45] layer 2_q initial loss 3.2543521228944883e-06
W0314 12:13:35.838366 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:13:40.595859 17150 finetune.py:68] layer 3_v @ epoch 3 new loss 5.0574094530020375e-06 old loss 5.412936388893286e-06 BETTER
I0314 12:13:40.970016 12330 finetune.py:68] layer 0_q @ epoch 3 new loss 3.43810597769334e-07 old loss 3.470466936050798e-07 BETTER
I0314 12:14:06.279944 13837 finetune.py:68] layer 1_q @ epoch 2 new loss 1.034615138451045e-06 old loss 1.0512351309444057e-06 BETTER
I0314 12:14:11.193320 15465 finetune.py:68] layer 2_q @ epoch 0 new loss 3.07471418636851e-06 old loss 3.2543521228944883e-06 BETTER
I0314 12:14:16.217675 17150 finetune.py:68] layer 3_v @ epoch 4 new loss 4.839423581870506e-06 old loss 5.0574094530020375e-06 BETTER
W0314 12:14:18.108047 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 12:14:18.912074 12330 finetune.py:68] layer 0_q @ epoch 4 new loss 3.409752764582663e-07 old loss 3.43810597769334e-07 BETTER
3_v proxy err 0.03216426074504852 tr(WHW.T) 40.46714782714844
  0%|          | 0/32 [00:00<?, ?it/s]W0314 12:14:20.826670 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  3%|▎         | 1/32 [00:02<01:02,  2.02s/it]  6%|▋         | 2/32 [00:02<00:31,  1.05s/it]0_q proxy err 0.00030863468418829143 tr(WHW.T) 6233.57080078125
  0%|          | 0/32 [00:00<?, ?it/s]  9%|▉         | 3/32 [00:02<00:21,  1.34it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.66it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.91it/s] 19%|█▉        | 6/32 [00:03<00:12,  2.09it/s]  3%|▎         | 1/32 [00:01<00:37,  1.20s/it]  6%|▋         | 2/32 [00:01<00:21,  1.41it/s] 22%|██▏       | 7/32 [00:04<00:11,  2.24it/s]  9%|▉         | 3/32 [00:01<00:16,  1.81it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.35it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.10it/s] 28%|██▊       | 9/32 [00:05<00:09,  2.42it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.47it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 41%|████      | 13/32 [00:06<00:07,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 44%|████▍     | 14/32 [00:07<00:07,  2.56it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 50%|█████     | 16/32 [00:07<00:06,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.72it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.59it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.60it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.60it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.73it/s] 69%|██████▉   | 22/32 [00:10<00:03,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.76it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 78%|███████▊  | 25/32 [00:11<00:02,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.78it/s] 84%|████████▍ | 27/32 [00:12<00:01,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.59it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.77it/s] 94%|█████████▍| 30/32 [00:13<00:00,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.60it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s]100%|██████████| 32/32 [00:13<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.30it/s]
 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
W0314 12:14:38.427000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.428000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.428000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.428000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.428000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.428000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.428000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.460000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.460000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.460000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.460000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.460000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.637000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.637000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.637000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.638000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.638000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.870000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.870000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.870000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.870000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.870000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.870000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.871000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.893000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.893000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.893000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.893000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.894000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.962000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.962000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.962000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.962000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:14:38.962000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:14:39.723000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.050000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.050000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.050000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.050000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.050000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.050000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.050000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.072000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.072000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.073000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.073000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.073000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.337000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.337000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.337000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.337000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.338000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:14:40.700000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 12:14:42.771913 12330 finetune.py:45] layer 0_k initial loss 3.593216035824298e-07
W0314 12:14:42.772303 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:14:43.641062 13837 finetune.py:68] layer 1_q @ epoch 3 new loss 1.019856199491187e-06 old loss 1.034615138451045e-06 BETTER
I0314 12:14:47.454464 15465 finetune.py:68] layer 2_q @ epoch 1 new loss 2.991489736814401e-06 old loss 3.07471418636851e-06 BETTER
I0314 12:14:48.704568 17150 finetune.py:45] layer 3_q initial loss 8.258008165284991e-06
W0314 12:14:48.705008 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:15:20.979788 12330 finetune.py:68] layer 0_k @ epoch 0 new loss 3.523125542415073e-07 old loss 3.593216035824298e-07 BETTER
I0314 12:15:21.183359 13837 finetune.py:68] layer 1_q @ epoch 4 new loss 1.0078842933580745e-06 old loss 1.019856199491187e-06 BETTER
W0314 12:15:23.462512 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 12:15:24.209252 17150 finetune.py:68] layer 3_q @ epoch 0 new loss 7.876789823058061e-06 old loss 8.258008165284991e-06 BETTER
I0314 12:15:24.356975 15465 finetune.py:68] layer 2_q @ epoch 2 new loss 2.931378048742772e-06 old loss 2.991489736814401e-06 BETTER
1_q proxy err 0.0003239660582039505 tr(WHW.T) 7566.3056640625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:37,  1.20s/it]  6%|▋         | 2/32 [00:01<00:21,  1.41it/s]  9%|▉         | 3/32 [00:01<00:16,  1.81it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0314 12:15:44.880851 13837 finetune.py:45] layer 1_k initial loss 1.208215621772979e-06
W0314 12:15:44.881234 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:15:59.247807 12330 finetune.py:68] layer 0_k @ epoch 1 new loss 3.4946754112752387e-07 old loss 3.523125542415073e-07 BETTER
I0314 12:15:59.712368 17150 finetune.py:68] layer 3_q @ epoch 1 new loss 7.666156307095662e-06 old loss 7.876789823058061e-06 BETTER
I0314 12:16:01.150665 15465 finetune.py:68] layer 2_q @ epoch 3 new loss 2.884248033296899e-06 old loss 2.931378048742772e-06 BETTER
I0314 12:16:20.295782 13837 finetune.py:68] layer 1_k @ epoch 0 new loss 1.1202580481040059e-06 old loss 1.208215621772979e-06 BETTER
I0314 12:16:35.583693 17150 finetune.py:68] layer 3_q @ epoch 2 new loss 7.508433100156253e-06 old loss 7.666156307095662e-06 BETTER
I0314 12:16:36.985801 12330 finetune.py:68] layer 0_k @ epoch 2 new loss 3.4701676554504957e-07 old loss 3.4946754112752387e-07 BETTER
I0314 12:16:37.495512 15465 finetune.py:68] layer 2_q @ epoch 4 new loss 2.845716835508938e-06 old loss 2.884248033296899e-06 BETTER
W0314 12:16:39.352527 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_q proxy err 0.0028374656103551388 tr(WHW.T) 7140.18212890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:38,  1.23s/it]  6%|▋         | 2/32 [00:01<00:22,  1.36it/s]  9%|▉         | 3/32 [00:01<00:16,  1.75it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.01it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.19it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.60it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.46it/s]
I0314 12:16:56.675977 13837 finetune.py:68] layer 1_k @ epoch 1 new loss 1.1084628113167128e-06 old loss 1.1202580481040059e-06 BETTER
I0314 12:17:01.533215 15465 finetune.py:45] layer 2_k initial loss 4.430542503541801e-06
W0314 12:17:01.533688 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:17:12.520396 17150 finetune.py:68] layer 3_q @ epoch 3 new loss 7.380374427157221e-06 old loss 7.508433100156253e-06 BETTER
I0314 12:17:15.331973 12330 finetune.py:68] layer 0_k @ epoch 3 new loss 3.448076881795714e-07 old loss 3.4701676554504957e-07 BETTER
I0314 12:17:33.144552 13837 finetune.py:68] layer 1_k @ epoch 2 new loss 1.098071834348957e-06 old loss 1.1084628113167128e-06 BETTER
I0314 12:17:36.771421 15465 finetune.py:68] layer 2_k @ epoch 0 new loss 3.86127658202895e-06 old loss 4.430542503541801e-06 BETTER
I0314 12:17:48.148906 17150 finetune.py:68] layer 3_q @ epoch 4 new loss 7.272874427144416e-06 old loss 7.380374427157221e-06 BETTER
W0314 12:17:50.410957 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_q proxy err 0.004043624270707369 tr(WHW.T) 6653.330078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:37,  1.22s/it]I0314 12:17:53.103777 12330 finetune.py:68] layer 0_k @ epoch 4 new loss 3.428678780892369e-07 old loss 3.448076881795714e-07 BETTER
  6%|▋         | 2/32 [00:01<00:21,  1.37it/s]  9%|▉         | 3/32 [00:01<00:16,  1.75it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s]W0314 12:17:54.871111 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s]0_k proxy err 0.0003078522568102926 tr(WHW.T) 2167.68408203125
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s]  3%|▎         | 1/32 [00:00<00:24,  1.27it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.55it/s]  6%|▋         | 2/32 [00:01<00:16,  1.86it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.61it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.69it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.61it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.71it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s]100%|██████████| 32/32 [00:13<00:00,  2.65it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
 72%|███████▏  | 23/32 [00:08<00:03,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
I0314 12:18:10.169196 13837 finetune.py:68] layer 1_k @ epoch 3 new loss 1.0897639413087745e-06 old loss 1.098071834348957e-06 BETTER
I0314 12:18:12.942996 17150 finetune.py:45] layer 3_k initial loss 1.3533573110180441e-05
W0314 12:18:12.943436 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:18:14.006954 15465 finetune.py:68] layer 2_k @ epoch 1 new loss 3.808345354627818e-06 old loss 3.86127658202895e-06 BETTER
I0314 12:18:16.891845 12330 finetune.py:45] layer 0_o initial loss 7.505803409912915e-07
W0314 12:18:16.892259 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:18:47.144809 13837 finetune.py:68] layer 1_k @ epoch 4 new loss 1.0815366522365366e-06 old loss 1.0897639413087745e-06 BETTER
I0314 12:18:48.269497 17150 finetune.py:68] layer 3_k @ epoch 0 new loss 1.0472052963450551e-05 old loss 1.3533573110180441e-05 BETTER
W0314 12:18:49.273288 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_k proxy err 0.00032403875957243145 tr(WHW.T) 3946.5048828125
  0%|          | 0/32 [00:00<?, ?it/s]I0314 12:18:50.870473 15465 finetune.py:68] layer 2_k @ epoch 2 new loss 3.767311909541604e-06 old loss 3.808345354627818e-06 BETTER
  3%|▎         | 1/32 [00:00<00:24,  1.25it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s]  9%|▉         | 3/32 [00:01<00:13,  2.14it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.33it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s]I0314 12:18:54.076865 12330 finetune.py:68] layer 0_o @ epoch 0 new loss 7.256585945469851e-07 old loss 7.505803409912915e-07 BETTER
 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
I0314 12:19:10.642345 13837 finetune.py:45] layer 1_o initial loss 3.5538798783818493e-06
W0314 12:19:10.642692 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:19:24.565362 17150 finetune.py:68] layer 3_k @ epoch 1 new loss 1.008549406833481e-05 old loss 1.0472052963450551e-05 BETTER
I0314 12:19:27.151911 15465 finetune.py:68] layer 2_k @ epoch 3 new loss 3.733220637514023e-06 old loss 3.767311909541604e-06 BETTER
I0314 12:19:31.802209 12330 finetune.py:68] layer 0_o @ epoch 1 new loss 7.132806558729499e-07 old loss 7.256585945469851e-07 BETTER
I0314 12:19:46.330324 13837 finetune.py:68] layer 1_o @ epoch 0 new loss 3.3072287806135137e-06 old loss 3.5538798783818493e-06 BETTER
I0314 12:20:00.194952 17150 finetune.py:68] layer 3_k @ epoch 2 new loss 9.920328011503443e-06 old loss 1.008549406833481e-05 BETTER
I0314 12:20:03.768410 15465 finetune.py:68] layer 2_k @ epoch 4 new loss 3.7035165405541193e-06 old loss 3.733220637514023e-06 BETTER
W0314 12:20:05.839338 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_k proxy err 0.003112799022346735 tr(WHW.T) 3889.0419921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.19it/s]  6%|▋         | 2/32 [00:01<00:17,  1.75it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s]I0314 12:20:10.194083 12330 finetune.py:68] layer 0_o @ epoch 2 new loss 7.039980687295611e-07 old loss 7.132806558729499e-07 BETTER
 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
I0314 12:20:22.085308 13837 finetune.py:68] layer 1_o @ epoch 1 new loss 3.174268613292952e-06 old loss 3.3072287806135137e-06 BETTER
I0314 12:20:27.440019 15465 finetune.py:45] layer 2_o initial loss 8.263910785899498e-06
W0314 12:20:27.440385 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:20:36.391778 17150 finetune.py:68] layer 3_k @ epoch 3 new loss 9.809972652874421e-06 old loss 9.920328011503443e-06 BETTER
I0314 12:20:48.274900 12330 finetune.py:68] layer 0_o @ epoch 3 new loss 6.968989509914536e-07 old loss 7.039980687295611e-07 BETTER
I0314 12:20:58.269494 13837 finetune.py:68] layer 1_o @ epoch 2 new loss 3.085543312408845e-06 old loss 3.174268613292952e-06 BETTER
I0314 12:21:02.756103 15465 finetune.py:68] layer 2_o @ epoch 0 new loss 7.471724984497996e-06 old loss 8.263910785899498e-06 BETTER
I0314 12:21:11.605633 17150 finetune.py:68] layer 3_k @ epoch 4 new loss 9.71762892731931e-06 old loss 9.809972652874421e-06 BETTER
W0314 12:21:13.553660 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_k proxy err 0.0043467869982123375 tr(WHW.T) 3652.11328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.36it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.39it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.46it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.44it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.47it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.51it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.63it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s]I0314 12:21:25.926412 12330 finetune.py:68] layer 0_o @ epoch 4 new loss 6.912645744705515e-07 old loss 6.968989509914536e-07 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
W0314 12:21:27.736156 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_o proxy err 0.004265644587576389 tr(WHW.T) 0.23044952750205994
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:45,  1.59s/it]I0314 12:21:34.311515 13837 finetune.py:68] layer 1_o @ epoch 3 new loss 3.0229634830902796e-06 old loss 3.085543312408845e-06 BETTER
 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it]I0314 12:21:35.731771 17150 finetune.py:45] layer 3_o initial loss 1.998257175728213e-05
W0314 12:21:35.732160 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it]I0314 12:21:39.125269 15465 finetune.py:68] layer 2_o @ epoch 1 new loss 7.0922033046372235e-06 old loss 7.471724984497996e-06 BETTER
 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it]I0314 12:22:10.878708 13837 finetune.py:68] layer 1_o @ epoch 4 new loss 2.9766742954961956e-06 old loss 3.0229634830902796e-06 BETTER
I0314 12:22:11.055727 17150 finetune.py:68] layer 3_o @ epoch 0 new loss 1.772870382410474e-05 old loss 1.998257175728213e-05 BETTER
 88%|████████▊ | 28/32 [00:42<00:05,  1.50s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.50s/it]W0314 12:22:12.859710 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:45<00:02,  1.50s/it]1_o proxy err 0.015616835094988346 tr(WHW.T) 0.3137481212615967
  0%|          | 0/32 [00:00<?, ?it/s]I0314 12:22:15.606730 15465 finetune.py:68] layer 2_o @ epoch 2 new loss 6.877437044749968e-06 old loss 7.0922033046372235e-06 BETTER
 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]
  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.54s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it]I0314 12:22:25.704122 12330 finetune.py:45] layer 0_up initial loss 2.0712629975605523e-06
W0314 12:22:25.704533 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it]I0314 12:22:46.729360 17150 finetune.py:68] layer 3_o @ epoch 1 new loss 1.694997445156332e-05 old loss 1.772870382410474e-05 BETTER
 69%|██████▉   | 22/32 [00:32<00:14,  1.48s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it]I0314 12:22:51.822908 15465 finetune.py:68] layer 2_o @ epoch 3 new loss 6.7419423430692405e-06 old loss 6.877437044749968e-06 BETTER
 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it]I0314 12:23:01.727539 12330 finetune.py:68] layer 0_up @ epoch 0 new loss 2.0089726149308262e-06 old loss 2.0712629975605523e-06 BETTER
100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
I0314 12:23:10.189106 13837 finetune.py:45] layer 1_up initial loss 9.533918273518793e-06
W0314 12:23:10.189348 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:23:22.749506 17150 finetune.py:68] layer 3_o @ epoch 2 new loss 1.657259235798847e-05 old loss 1.694997445156332e-05 BETTER
I0314 12:23:28.638649 15465 finetune.py:68] layer 2_o @ epoch 4 new loss 6.645027042395668e-06 old loss 6.7419423430692405e-06 BETTER
W0314 12:23:30.624654 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_o proxy err 0.01472682598978281 tr(WHW.T) 0.5612744688987732
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.04s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]  9%|▉         | 3/32 [00:05<00:46,  1.62s/it]I0314 12:23:38.361051 12330 finetune.py:68] layer 0_up @ epoch 1 new loss 1.9955621155531844e-06 old loss 2.0089726149308262e-06 BETTER
 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.52s/it]I0314 12:23:44.095531 13837 finetune.py:68] layer 1_up @ epoch 0 new loss 6.14444707025541e-06 old loss 9.533918273518793e-06 BETTER
 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:25,  1.58s/it]I0314 12:23:58.401222 17150 finetune.py:68] layer 3_o @ epoch 3 new loss 1.632488965697121e-05 old loss 1.657259235798847e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.49s/it]I0314 12:24:14.906497 12330 finetune.py:68] layer 0_up @ epoch 2 new loss 1.9874221379723167e-06 old loss 1.9955621155531844e-06 BETTER
 88%|████████▊ | 28/32 [00:42<00:05,  1.50s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it]I0314 12:24:18.783462 13837 finetune.py:68] layer 1_up @ epoch 1 new loss 6.011660843796562e-06 old loss 6.14444707025541e-06 BETTER
 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
I0314 12:24:29.634325 15465 finetune.py:45] layer 2_up initial loss 1.4204509170667734e-05
W0314 12:24:29.635072 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:24:33.658922 17150 finetune.py:68] layer 3_o @ epoch 4 new loss 1.6142535969265737e-05 old loss 1.632488965697121e-05 BETTER
W0314 12:24:35.376721 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_o proxy err 0.02344236895442009 tr(WHW.T) 0.9373493194580078
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it]I0314 12:24:51.685395 12330 finetune.py:68] layer 0_up @ epoch 3 new loss 1.9817064185190247e-06 old loss 1.9874221379723167e-06 BETTER
 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it]I0314 12:24:53.562923 13837 finetune.py:68] layer 1_up @ epoch 2 new loss 5.980568403174402e-06 old loss 6.011660843796562e-06 BETTER
 34%|███▍      | 11/32 [00:16<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it]I0314 12:25:03.876873 15465 finetune.py:68] layer 2_up @ epoch 0 new loss 1.4014317457622383e-05 old loss 1.4204509170667734e-05 BETTER
 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
I0314 12:25:28.356100 13837 finetune.py:68] layer 1_up @ epoch 3 new loss 5.957603207207285e-06 old loss 5.980568403174402e-06 BETTER
I0314 12:25:28.444700 12330 finetune.py:68] layer 0_up @ epoch 4 new loss 1.9772498944803374e-06 old loss 1.9817064185190247e-06 BETTER
W0314 12:25:30.096708 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_up proxy err 0.033260829746723175 tr(WHW.T) 101.63304138183594
  0%|          | 0/32 [00:00<?, ?it/s]I0314 12:25:33.136103 17150 finetune.py:45] layer 3_up initial loss 3.262558311689645e-05
W0314 12:25:33.136409 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:59,  1.91s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it]I0314 12:25:39.043113 15465 finetune.py:68] layer 2_up @ epoch 1 new loss 1.3935492461314425e-05 old loss 1.4014317457622383e-05 BETTER
 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.47s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it]I0314 12:26:02.613323 13837 finetune.py:68] layer 1_up @ epoch 4 new loss 5.939838501944905e-06 old loss 5.957603207207285e-06 BETTER
 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it]W0314 12:26:04.004474 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it]1_up proxy err 0.039291687309741974 tr(WHW.T) 159.82254028320312
  0%|          | 0/32 [00:00<?, ?it/s] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it]I0314 12:26:05.845644 17150 finetune.py:68] layer 3_up @ epoch 0 new loss 3.228786226827651e-05 old loss 3.262558311689645e-05 BETTER
 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it]I0314 12:26:13.968652 15465 finetune.py:68] layer 2_up @ epoch 2 new loss 1.3877013770979829e-05 old loss 1.3935492461314425e-05 BETTER
 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.52s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 28%|██▊       | 9/32 [00:13<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:19<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it]I0314 12:26:27.036835 12330 finetune.py:45] layer 0_gate initial loss 3.111854539383785e-06
W0314 12:26:27.037210 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it]I0314 12:26:39.190262 17150 finetune.py:68] layer 3_up @ epoch 1 new loss 3.212967567378655e-05 old loss 3.228786226827651e-05 BETTER
 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it]I0314 12:26:48.503957 15465 finetune.py:68] layer 2_up @ epoch 3 new loss 1.3828904229740147e-05 old loss 1.3877013770979829e-05 BETTER
 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.48s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.47s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
I0314 12:27:01.272907 12330 finetune.py:68] layer 0_gate @ epoch 0 new loss 3.0260607672971673e-06 old loss 3.111854539383785e-06 BETTER
I0314 12:27:01.329323 13837 finetune.py:45] layer 1_gate initial loss 1.131142744270619e-05
W0314 12:27:01.329731 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:27:12.782624 17150 finetune.py:68] layer 3_up @ epoch 2 new loss 3.200575156370178e-05 old loss 3.212967567378655e-05 BETTER
I0314 12:27:22.912924 15465 finetune.py:68] layer 2_up @ epoch 4 new loss 1.3787156603939366e-05 old loss 1.3828904229740147e-05 BETTER
W0314 12:27:24.246088 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_up proxy err 0.045559585094451904 tr(WHW.T) 225.83253479003906
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it]I0314 12:27:33.771698 13837 finetune.py:68] layer 1_gate @ epoch 0 new loss 8.420470294367988e-06 old loss 1.131142744270619e-05 BETTER
 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it]I0314 12:27:36.062926 12330 finetune.py:68] layer 0_gate @ epoch 1 new loss 3.0081262138992315e-06 old loss 3.0260607672971673e-06 BETTER
 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it]I0314 12:27:46.550990 17150 finetune.py:68] layer 3_up @ epoch 3 new loss 3.189702329109423e-05 old loss 3.200575156370178e-05 BETTER
 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.50s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.51s/it]I0314 12:28:07.037502 13837 finetune.py:68] layer 1_gate @ epoch 1 new loss 8.391059964196756e-06 old loss 8.420470294367988e-06 BETTER
 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it]I0314 12:28:11.024660 12330 finetune.py:68] layer 0_gate @ epoch 2 new loss 2.998053332703421e-06 old loss 3.0081262138992315e-06 BETTER
 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
I0314 12:28:20.835055 17150 finetune.py:68] layer 3_up @ epoch 4 new loss 3.1796054827282205e-05 old loss 3.189702329109423e-05 BETTER
I0314 12:28:21.673552 15465 finetune.py:45] layer 2_gate initial loss 1.9889244867954403e-05
W0314 12:28:21.673897 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0314 12:28:22.390984 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_up proxy err 0.043810680508613586 tr(WHW.T) 315.7093505859375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.97s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it]I0314 12:28:40.591461 13837 finetune.py:68] layer 1_gate @ epoch 2 new loss 8.3744098446914e-06 old loss 8.391059964196756e-06 BETTER
 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.54s/it]I0314 12:28:46.186500 12330 finetune.py:68] layer 0_gate @ epoch 3 new loss 2.9909672321082326e-06 old loss 2.998053332703421e-06 BETTER
 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.54s/it]I0314 12:28:54.674055 15465 finetune.py:68] layer 2_gate @ epoch 0 new loss 1.9691831766976975e-05 old loss 1.9889244867954403e-05 BETTER
 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:35<00:14,  1.58s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.57s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.56s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0314 12:29:14.246792 13837 finetune.py:68] layer 1_gate @ epoch 3 new loss 8.362776497961022e-06 old loss 8.3744098446914e-06 BETTER
I0314 12:29:21.335563 17150 finetune.py:45] layer 3_gate initial loss 4.3214055040152743e-05
W0314 12:29:21.335924 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:29:21.447220 12330 finetune.py:68] layer 0_gate @ epoch 4 new loss 2.9854875265300507e-06 old loss 2.9909672321082326e-06 BETTER
W0314 12:29:22.619178 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_gate proxy err 0.023046869784593582 tr(WHW.T) 179.67465209960938
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s]I0314 12:29:27.873809 15465 finetune.py:68] layer 2_gate @ epoch 1 new loss 1.9633946067187935e-05 old loss 1.9691831766976975e-05 BETTER
  3%|▎         | 3/112 [00:01<00:53,  2.04it/s]  4%|▎         | 4/112 [00:02<00:47,  2.26it/s]  4%|▍         | 5/112 [00:02<00:44,  2.39it/s]  5%|▌         | 6/112 [00:02<00:42,  2.47it/s]  6%|▋         | 7/112 [00:03<00:41,  2.54it/s]  7%|▋         | 8/112 [00:03<00:40,  2.59it/s]  8%|▊         | 9/112 [00:03<00:39,  2.62it/s]  9%|▉         | 10/112 [00:04<00:38,  2.65it/s] 10%|▉         | 11/112 [00:04<00:37,  2.66it/s] 11%|█         | 12/112 [00:04<00:37,  2.67it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.68it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.68it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.69it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.69it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.68it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.68it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.68it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.68it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.63it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.65it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.67it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.67it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.68it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.69it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.69it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.69it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.69it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.69it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.69it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.69it/s] 30%|███       | 34/112 [00:13<00:29,  2.68it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.66it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.68it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.69it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.70it/s] 35%|███▍      | 39/112 [00:15<00:26,  2.71it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.71it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.72it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.72it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.71it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.71it/s] 40%|████      | 45/112 [00:17<00:24,  2.71it/s] 41%|████      | 46/112 [00:17<00:24,  2.70it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.69it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.69it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.65it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.66it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.67it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.68it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.69it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.70it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.70it/s]I0314 12:29:47.654838 13837 finetune.py:68] layer 1_gate @ epoch 4 new loss 8.350577445526142e-06 old loss 8.362776497961022e-06 BETTER
 50%|█████     | 56/112 [00:21<00:20,  2.70it/s] 51%|█████     | 57/112 [00:21<00:20,  2.70it/s] 52%|█████▏    | 58/112 [00:22<00:19,  2.71it/s]W0314 12:29:48.763677 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 53%|█████▎    | 59/112 [00:22<00:19,  2.70it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.70it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.71it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.71it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.68it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.69it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.69it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.70it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.70it/s] 61%|██████    | 68/112 [00:25<00:16,  2.71it/s]1_gate proxy err 0.028174880892038345 tr(WHW.T) 270.67803955078125
  0%|          | 0/112 [00:00<?, ?it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.71it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.71it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.71it/s]I0314 12:29:53.455776 17150 finetune.py:68] layer 3_gate @ epoch 0 new loss 4.288922718842514e-05 old loss 4.3214055040152743e-05 BETTER
  2%|▏         | 2/112 [00:01<01:03,  1.74it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.71it/s]  3%|▎         | 3/112 [00:01<00:52,  2.09it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.73it/s]  4%|▎         | 4/112 [00:01<00:47,  2.28it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.71it/s]  4%|▍         | 5/112 [00:02<00:44,  2.40it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.71it/s]  5%|▌         | 6/112 [00:02<00:42,  2.48it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.67it/s]  6%|▋         | 7/112 [00:03<00:41,  2.53it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.68it/s]  7%|▋         | 8/112 [00:03<00:40,  2.58it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.69it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s] 71%|███████   | 79/112 [00:29<00:12,  2.69it/s]  9%|▉         | 10/112 [00:04<00:38,  2.63it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.69it/s] 10%|▉         | 11/112 [00:04<00:38,  2.64it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.70it/s] 11%|█         | 12/112 [00:04<00:37,  2.66it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.70it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.67it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.70it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.68it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.70it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.68it/s] 76%|███████▌  | 85/112 [00:32<00:09,  2.70it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.69it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.70it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.69it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.70it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.69it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.70it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.69it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.69it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.68it/s] 80%|████████  | 90/112 [00:33<00:08,  2.66it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.69it/s]I0314 12:30:00.727840 15465 finetune.py:68] layer 2_gate @ epoch 2 new loss 1.9593557226471603e-05 old loss 1.9633946067187935e-05 BETTER
 81%|████████▏ | 91/112 [00:34<00:07,  2.68it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.70it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.70it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.70it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.68it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.70it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.69it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.70it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.69it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.70it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.70it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.70it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.70it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.70it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.71it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.69it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.72it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.70it/s] 28%|██▊       | 31/112 [00:12<00:29,  2.72it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.69it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.72it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.69it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.71it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.65it/s] 30%|███       | 34/112 [00:13<00:28,  2.71it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.68it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.72it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.70it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.72it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.68it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.67it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.68it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.68it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.69it/s] 35%|███▍      | 39/112 [00:14<00:27,  2.69it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.68it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.70it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.68it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.71it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.69it/s] 38%|███▊      | 42/112 [00:16<00:25,  2.70it/s]100%|██████████| 112/112 [00:42<00:00,  2.69it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]
 38%|███▊      | 43/112 [00:16<00:25,  2.70it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.70it/s] 40%|████      | 45/112 [00:17<00:24,  2.70it/s] 41%|████      | 46/112 [00:17<00:24,  2.70it/s] 42%|████▏     | 47/112 [00:17<00:24,  2.70it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.69it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.70it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.70it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.69it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.69it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.65it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.66it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.67it/s] 50%|█████     | 56/112 [00:21<00:20,  2.67it/s] 51%|█████     | 57/112 [00:21<00:20,  2.68it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.69it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.69it/s]W0314 12:30:14.810000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.810000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.810000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.811000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.811000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.811000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.811000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.857000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.858000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.858000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.858000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:14.858000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.037000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.038000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.038000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.038000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.038000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 54%|█████▎    | 60/112 [00:22<00:19,  2.69it/s]W0314 12:30:15.384000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.384000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.385000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.385000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.385000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.385000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.385000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.417000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.417000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.417000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.417000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.417000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 54%|█████▍    | 61/112 [00:23<00:18,  2.69it/s]W0314 12:30:15.488000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.488000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.488000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.488000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:15.488000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 55%|█████▌    | 62/112 [00:23<00:18,  2.69it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.69it/s]W0314 12:30:16.487000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:16.501000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:16.510000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:16.510000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 57%|█████▋    | 64/112 [00:24<00:17,  2.68it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.69it/s]W0314 12:30:16.977000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:16.977000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:16.977000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:16.977000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:30:16.977000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:16.977000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:16.977000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.006000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.006000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.006000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.006000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.006000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 66/112 [00:25<00:17,  2.68it/s]W0314 12:30:17.311000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.311000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.311000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.311000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.311000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.311000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.311000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.311000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.612000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.612000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.612000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.612000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:17.612000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 60%|█████▉    | 67/112 [00:25<00:17,  2.63it/s] 61%|██████    | 68/112 [00:25<00:16,  2.65it/s]W0314 12:30:18.053000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:18.060000 140459255453504 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 62%|██████▏   | 69/112 [00:26<00:16,  2.66it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.66it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.68it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.69it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.70it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.71it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.72it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.71it/s] 69%|██████▉   | 77/112 [00:29<00:12,  2.71it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s] 71%|███████   | 79/112 [00:29<00:12,  2.70it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.70it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.70it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.65it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.67it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.68it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.69it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.70it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.70it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.70it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.70it/s]I0314 12:30:26.146635 17150 finetune.py:68] layer 3_gate @ epoch 1 new loss 4.278012056602165e-05 old loss 4.288922718842514e-05 BETTER
 80%|████████  | 90/112 [00:33<00:08,  2.70it/s]I0314 12:30:26.359913 12330 finetune.py:45] layer 0_down initial loss 5.811262781207915e-06
W0314 12:30:26.360284 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 91/112 [00:34<00:07,  2.70it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.70it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.70it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.69it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.64it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.66it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.67it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.68it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.68it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.69it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.69it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.69it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.70it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.70it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.69it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.69it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.69it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.69it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.64it/s]I0314 12:30:33.754304 15465 finetune.py:68] layer 2_gate @ epoch 3 new loss 1.955948755494319e-05 old loss 1.9593557226471603e-05 BETTER
 99%|█████████▉| 111/112 [00:41<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.67it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]
W0314 12:30:40.889000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.889000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.890000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.890000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.890000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.890000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.890000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.936000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.936000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.936000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.936000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:40.936000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.115000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.115000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.115000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.115000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.115000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.440000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.441000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.441000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.441000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.441000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.441000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.441000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.476000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.476000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.476000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.476000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.476000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.548000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.548000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.548000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.548000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:41.549000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:42.535000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:42.548000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:42.556000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:42.556000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.024000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.024000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.024000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.024000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.025000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.025000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.025000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.055000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.055000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.056000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.056000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.056000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.362000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.362000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.362000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.362000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.363000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.363000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.363000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.363000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.664000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.664000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.664000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.664000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:30:43.664000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:30:44.106000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:30:44.111000 140207963330368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 12:30:52.436322 13837 finetune.py:45] layer 1_down initial loss 1.3652148481924087e-05
W0314 12:30:52.436719 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:30:59.342707 12330 finetune.py:68] layer 0_down @ epoch 0 new loss 5.799875907541718e-06 old loss 5.811262781207915e-06 BETTER
I0314 12:30:59.500379 17150 finetune.py:68] layer 3_gate @ epoch 2 new loss 4.269347118679434e-05 old loss 4.278012056602165e-05 BETTER
I0314 12:31:06.606563 15465 finetune.py:68] layer 2_gate @ epoch 4 new loss 1.9529910787241533e-05 old loss 1.955948755494319e-05 BETTER
W0314 12:31:07.742343 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_gate proxy err 0.0297212153673172 tr(WHW.T) 448.41424560546875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s]  2%|▏         | 2/112 [00:01<01:03,  1.72it/s]  3%|▎         | 3/112 [00:01<00:53,  2.05it/s]  4%|▎         | 4/112 [00:02<00:48,  2.25it/s]  4%|▍         | 5/112 [00:02<00:45,  2.38it/s]  5%|▌         | 6/112 [00:02<00:43,  2.46it/s]  6%|▋         | 7/112 [00:03<00:41,  2.53it/s]  7%|▋         | 8/112 [00:03<00:40,  2.56it/s]  8%|▊         | 9/112 [00:03<00:39,  2.59it/s]  9%|▉         | 10/112 [00:04<00:39,  2.61it/s] 10%|▉         | 11/112 [00:04<00:38,  2.63it/s] 11%|█         | 12/112 [00:05<00:37,  2.63it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.64it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.60it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.64it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.65it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.65it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.65it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.65it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.65it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.66it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.67it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.66it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.67it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.67it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.64it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.66it/s]I0314 12:31:23.398940 13837 finetune.py:68] layer 1_down @ epoch 0 new loss 1.363764749839902e-05 old loss 1.3652148481924087e-05 BETTER
 29%|██▊       | 32/112 [00:12<00:30,  2.66it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.67it/s] 30%|███       | 34/112 [00:13<00:29,  2.67it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.66it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.67it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.67it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.66it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.66it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.66it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.67it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.68it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.68it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.63it/s] 40%|████      | 45/112 [00:17<00:25,  2.64it/s] 41%|████      | 46/112 [00:17<00:24,  2.65it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.66it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.67it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.67it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.67it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.67it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.67it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.68it/s]I0314 12:31:32.016728 17150 finetune.py:68] layer 3_gate @ epoch 3 new loss 4.261700814822689e-05 old loss 4.269347118679434e-05 BETTER
I0314 12:31:32.219471 12330 finetune.py:68] layer 0_down @ epoch 1 new loss 5.796171990368748e-06 old loss 5.799875907541718e-06 BETTER
 49%|████▉     | 55/112 [00:21<00:21,  2.68it/s] 50%|█████     | 56/112 [00:21<00:20,  2.68it/s] 51%|█████     | 57/112 [00:21<00:20,  2.67it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.63it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.65it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.65it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.65it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.65it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.67it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.67it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.66it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.66it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.67it/s] 61%|██████    | 68/112 [00:26<00:16,  2.66it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.67it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.67it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.67it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.67it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.61it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.63it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.65it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.66it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.67it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.67it/s] 71%|███████   | 79/112 [00:30<00:12,  2.68it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.68it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.68it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.68it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.67it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.66it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.66it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.66it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.63it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.64it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.65it/s] 80%|████████  | 90/112 [00:34<00:08,  2.65it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.66it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.66it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.66it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.66it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.66it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.66it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.66it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.67it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.68it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.68it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.67it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.62it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.64it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.66it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.66it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.66it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.67it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.67it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.68it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.67it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
I0314 12:31:55.006817 13837 finetune.py:68] layer 1_down @ epoch 1 new loss 1.3635065442940686e-05 old loss 1.363764749839902e-05 BETTER
W0314 12:31:59.910000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.911000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.911000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.911000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.911000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.911000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.911000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.955000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.955000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.955000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.955000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:31:59.955000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.134000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.134000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.134000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.134000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.134000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.460000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.460000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.460000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.461000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.461000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.461000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.461000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.494000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.494000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.494000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.494000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.494000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.564000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.564000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.565000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.565000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:32:00.565000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:32:01.554000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:01.560000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:01.566000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:32:01.567000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.037000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.037000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.037000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.037000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.037000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.037000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.038000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.067000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.067000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.067000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.067000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.067000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.396000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.396000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.396000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.396000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.396000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.396000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.396000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.396000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.703000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.703000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.703000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.703000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:32:02.703000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:32:03.146000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:32:03.151000 140353842829120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 12:32:04.611594 17150 finetune.py:68] layer 3_gate @ epoch 4 new loss 4.254521263646893e-05 old loss 4.261700814822689e-05 BETTER
I0314 12:32:05.155487 12330 finetune.py:68] layer 0_down @ epoch 2 new loss 5.7940592341765296e-06 old loss 5.796171990368748e-06 BETTER
W0314 12:32:05.903765 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_gate proxy err 0.024059699848294258 tr(WHW.T) 873.7711181640625
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s]I0314 12:32:11.060374 15465 finetune.py:45] layer 2_down initial loss 3.0624225473729894e-05
W0314 12:32:11.061033 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  2%|▏         | 2/112 [00:01<01:05,  1.68it/s]  3%|▎         | 3/112 [00:01<00:54,  2.00it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s]  4%|▍         | 5/112 [00:02<00:45,  2.33it/s]  5%|▌         | 6/112 [00:02<00:43,  2.41it/s]  6%|▋         | 7/112 [00:03<00:42,  2.46it/s]  7%|▋         | 8/112 [00:03<00:41,  2.50it/s]  8%|▊         | 9/112 [00:03<00:40,  2.52it/s]  9%|▉         | 10/112 [00:04<00:40,  2.54it/s] 10%|▉         | 11/112 [00:04<00:39,  2.55it/s] 11%|█         | 12/112 [00:05<00:39,  2.54it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.56it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.58it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.58it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.58it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.58it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.58it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.57it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.57it/s] 21%|██        | 23/112 [00:09<00:35,  2.53it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.56it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.58it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.59it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.60it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.60it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.61it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.61it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.60it/s] 30%|███       | 34/112 [00:13<00:30,  2.60it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.59it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.58it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.58it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.54it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.55it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.56it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.57it/s]I0314 12:32:26.695471 13837 finetune.py:68] layer 1_down @ epoch 2 new loss 1.3628687156597152e-05 old loss 1.3635065442940686e-05 BETTER
 38%|███▊      | 42/112 [00:16<00:27,  2.58it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.60it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.61it/s] 40%|████      | 45/112 [00:17<00:25,  2.60it/s] 41%|████      | 46/112 [00:18<00:25,  2.61it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.61it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.61it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.59it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.60it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.58it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.59it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.62it/s] 48%|████▊     | 54/112 [00:21<00:21,  2.65it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.66it/s] 50%|█████     | 56/112 [00:22<00:21,  2.65it/s] 51%|█████     | 57/112 [00:22<00:20,  2.65it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.64it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.61it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.63it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.67it/s] 55%|█████▌    | 62/112 [00:24<00:18,  2.70it/s] 56%|█████▋    | 63/112 [00:24<00:17,  2.73it/s] 57%|█████▋    | 64/112 [00:25<00:17,  2.75it/s] 58%|█████▊    | 65/112 [00:25<00:16,  2.77it/s] 59%|█████▉    | 66/112 [00:25<00:16,  2.78it/s] 60%|█████▉    | 67/112 [00:26<00:16,  2.78it/s] 61%|██████    | 68/112 [00:26<00:15,  2.78it/s] 62%|██████▏   | 69/112 [00:26<00:15,  2.80it/s] 62%|██████▎   | 70/112 [00:27<00:14,  2.80it/s] 63%|██████▎   | 71/112 [00:27<00:14,  2.77it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.78it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.77it/s]I0314 12:32:38.293506 12330 finetune.py:68] layer 0_down @ epoch 3 new loss 5.792576303065289e-06 old loss 5.7940592341765296e-06 BETTER
 66%|██████▌   | 74/112 [00:28<00:13,  2.78it/s] 67%|██████▋   | 75/112 [00:29<00:13,  2.78it/s] 68%|██████▊   | 76/112 [00:29<00:12,  2.79it/s] 69%|██████▉   | 77/112 [00:29<00:12,  2.80it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.81it/s] 71%|███████   | 79/112 [00:30<00:11,  2.78it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.74it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.70it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.68it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.66it/s]I0314 12:32:42.118345 15465 finetune.py:68] layer 2_down @ epoch 0 new loss 3.058522270293906e-05 old loss 3.0624225473729894e-05 BETTER
 75%|███████▌  | 84/112 [00:32<00:10,  2.65it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.61it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.57it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.58it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.58it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.60it/s] 80%|████████  | 90/112 [00:34<00:08,  2.61it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.61it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.59it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.60it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.60it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.60it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.61it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.61it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.61it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.60it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.55it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.57it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.57it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.58it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.59it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.60it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.60it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.58it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.58it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.59it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.60it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.61it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]
I0314 12:32:58.431493 13837 finetune.py:68] layer 1_down @ epoch 3 new loss 1.3625353858515155e-05 old loss 1.3628687156597152e-05 BETTER
W0314 12:32:59.903000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.904000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.904000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.904000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.904000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.904000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.904000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.947000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.947000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.947000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.947000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:32:59.947000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.140000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.141000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.141000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.141000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.141000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.467000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.467000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.468000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.468000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.468000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.469000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.469000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.508000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.508000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.508000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.509000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.509000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.579000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.579000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.579000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.579000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:33:00.579000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:33:01.566000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:01.578000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:01.586000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:33:01.586000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.046000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.046000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.046000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.046000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.046000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.046000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.047000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.075000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.075000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.075000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.075000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.075000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.377000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.378000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.675000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.675000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.676000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.676000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:33:02.676000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:33:03.123000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:33:03.128000 140257452570432 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 12:33:11.899179 17150 finetune.py:45] layer 3_down initial loss 6.426459003705531e-05
W0314 12:33:11.899600 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:33:12.110292 12330 finetune.py:68] layer 0_down @ epoch 4 new loss 5.791197963844752e-06 old loss 5.792576303065289e-06 BETTER
W0314 12:33:12.911867 12330 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

0_down proxy err 0.02502826787531376 tr(WHW.T) 0.4819566607475281
I0314 12:33:13.927569 15465 finetune.py:68] layer 2_down @ epoch 1 new loss 3.057496724068187e-05 old loss 3.058522270293906e-05 BETTER
I0314 12:33:30.261075 13837 finetune.py:68] layer 1_down @ epoch 4 new loss 1.3620403478853405e-05 old loss 1.3625353858515155e-05 BETTER
W0314 12:33:31.001248 13837 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

1_down proxy err 0.0007305514300242066 tr(WHW.T) 67.1953125
I0314 12:33:42.598443 17150 finetune.py:68] layer 3_down @ epoch 0 new loss 6.419714918592945e-05 old loss 6.426459003705531e-05 BETTER
I0314 12:33:45.396178 15465 finetune.py:68] layer 2_down @ epoch 2 new loss 3.056306013604626e-05 old loss 3.057496724068187e-05 BETTER
I0314 12:34:13.846990 17150 finetune.py:68] layer 3_down @ epoch 1 new loss 6.417767144739628e-05 old loss 6.419714918592945e-05 BETTER
I0314 12:34:17.245026 15465 finetune.py:68] layer 2_down @ epoch 3 new loss 3.0555322155123577e-05 old loss 3.056306013604626e-05 BETTER
I0314 12:34:42.726222 9966 quantize_finetune_llama.py:186] computed original embedding for layer 4 in 66.47427558898926s
I0314 12:34:43.133543 9966 quantize_finetune_llama.py:159] layer 5 gpu 1
I0314 12:34:45.034626 17150 finetune.py:68] layer 3_down @ epoch 2 new loss 6.416648102458566e-05 old loss 6.417767144739628e-05 BETTER
I0314 12:34:45.267154 38451 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:34:45.267285 38451 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:34:45.267348 38451 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:34:45.497673 38451 config.py:58] PyTorch version 2.4.0 available.
I0314 12:34:47.813094 38451 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 12:34:48.231918 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 12:34:48.712540 15465 finetune.py:68] layer 2_down @ epoch 4 new loss 3.054854096262716e-05 old loss 3.0555322155123577e-05 BETTER
  0%|          | 0/32 [00:00<?, ?it/s]W0314 12:34:49.420567 15465 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

2_down proxy err 0.04178778827190399 tr(WHW.T) 1.20462965965271
  3%|▎         | 1/32 [00:01<00:44,  1.42s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:16,  1.74it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.54it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.82it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s] 41%|████      | 13/32 [00:05<00:06,  2.91it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.93it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.94it/s] 50%|█████     | 16/32 [00:06<00:05,  2.96it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.99it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.96it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.98it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.98it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.98it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.99it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.97it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.97it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.99it/s]100%|██████████| 32/32 [00:11<00:00,  2.99it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
W0314 12:35:03.402000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.402000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.402000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.402000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.402000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.402000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.403000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.429000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.429000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.429000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.429000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.429000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.728000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.728000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.728000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.728000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:35:03.728000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.602000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.602000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.602000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.602000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.602000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.603000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.603000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.620000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.620000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.620000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.620000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.620000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.828000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.829000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.829000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.829000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:35:04.829000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.955000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.956000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.956000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.956000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.956000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.956000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.956000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.974000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.974000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.974000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.974000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:35:05.975000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:35:06.859000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:35:06.860000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:35:06.860000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:35:06.860000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:35:06.860000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 12:35:12.905550 38451 finetune.py:45] layer 4_v initial loss 4.959826765116304e-05
W0314 12:35:12.905845 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:35:16.022281 17150 finetune.py:68] layer 3_down @ epoch 3 new loss 6.415350799215958e-05 old loss 6.416648102458566e-05 BETTER
I0314 12:35:47.107601 17150 finetune.py:68] layer 3_down @ epoch 4 new loss 6.414162635337561e-05 old loss 6.415350799215958e-05 BETTER
W0314 12:35:47.949105 17150 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

3_down proxy err 0.0461200587451458 tr(WHW.T) 2.132885217666626
I0314 12:35:48.994087 38451 finetune.py:68] layer 4_v @ epoch 0 new loss 1.0726856999099255e-05 old loss 4.959826765116304e-05 BETTER
I0314 12:35:55.398117 9966 quantize_finetune_llama.py:186] computed original embedding for layer 5 in 60.87198042869568s
I0314 12:35:55.804330 9966 quantize_finetune_llama.py:159] layer 6 gpu 2
I0314 12:35:57.795081 39917 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:35:57.795205 39917 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:35:57.795269 39917 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:35:57.985051 39917 config.py:58] PyTorch version 2.4.0 available.
I0314 12:36:00.170858 39917 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 12:36:00.580086 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:50,  1.63s/it]  6%|▋         | 2/32 [00:01<00:26,  1.14it/s]  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.87it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.14it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.73it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.80it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.80it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.82it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0314 12:36:16.656000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.656000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.656000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.657000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.657000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.657000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.657000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.686000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.686000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.686000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.686000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.686000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.980000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.980000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.980000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.980000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:36:16.980000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.842000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.842000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.842000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.842000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.842000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.842000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.843000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.859000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.860000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.860000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.860000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:36:17.860000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:36:18.051000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:36:18.051000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:36:18.051000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:36:18.051000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:36:18.051000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.178000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.178000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.178000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.178000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.178000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.178000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.178000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.197000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.197000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.197000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.197000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:36:19.197000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:36:20.056000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:36:20.056000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:36:20.056000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:36:20.056000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:36:20.056000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 12:36:26.424085 39917 finetune.py:45] layer 5_v initial loss 3.560849654604681e-05
W0314 12:36:26.424405 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:36:26.502168 38451 finetune.py:68] layer 4_v @ epoch 1 new loss 7.870897206885274e-06 old loss 1.0726856999099255e-05 BETTER
I0314 12:36:57.136698 9966 quantize_finetune_llama.py:186] computed original embedding for layer 6 in 60.88501739501953s
I0314 12:36:57.530868 9966 quantize_finetune_llama.py:159] layer 7 gpu 3
I0314 12:36:59.776020 41230 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:36:59.776187 41230 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:36:59.776276 41230 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:37:00.002575 41230 config.py:58] PyTorch version 2.4.0 available.
I0314 12:37:00.586322 39917 finetune.py:68] layer 5_v @ epoch 0 new loss 9.379727998748422e-06 old loss 3.560849654604681e-05 BETTER
I0314 12:37:02.361300 41230 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 12:37:02.693223 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0314 12:37:04.177499 38451 finetune.py:68] layer 4_v @ epoch 2 new loss 7.126587661332451e-06 old loss 7.870897206885274e-06 BETTER
  3%|▎         | 1/32 [00:01<00:51,  1.68s/it]  6%|▋         | 2/32 [00:02<00:26,  1.11it/s]  9%|▉         | 3/32 [00:02<00:18,  1.53it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.14it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.73it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 50%|█████     | 16/32 [00:07<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.80it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.80it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
W0314 12:37:18.953000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.954000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.954000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.954000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.954000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.954000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.954000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.982000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.982000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.982000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.982000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:37:18.982000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:37:19.279000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:37:19.280000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:37:19.280000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:37:19.280000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:37:19.280000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.174000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.174000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.174000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.174000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.174000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.174000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.175000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.193000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.193000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.194000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.194000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.194000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.402000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.402000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.403000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.403000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:37:20.403000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.556000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.556000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.557000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.557000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.557000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.557000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.557000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.575000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.575000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.575000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.576000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:37:21.576000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:37:22.494000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:37:22.494000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:37:22.494000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:37:22.494000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:37:22.494000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 12:37:28.979712 41230 finetune.py:45] layer 6_v initial loss 3.440797081566416e-05
W0314 12:37:28.979910 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:37:35.938970 39917 finetune.py:68] layer 5_v @ epoch 1 new loss 7.657379683223553e-06 old loss 9.379727998748422e-06 BETTER
I0314 12:37:41.820367 38451 finetune.py:68] layer 4_v @ epoch 3 new loss 6.783647222619038e-06 old loss 7.126587661332451e-06 BETTER
I0314 12:37:58.070575 9966 quantize_finetune_llama.py:186] computed original embedding for layer 7 in 60.02914500236511s
I0314 12:37:58.459629 9966 quantize_finetune_llama.py:159] layer 8 gpu 0
I0314 12:38:00.684756 42575 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 12:38:00.684889 42575 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 12:38:00.684975 42575 utils.py:162] NumExpr defaulting to 16 threads.
I0314 12:38:00.916732 42575 config.py:58] PyTorch version 2.4.0 available.
I0314 12:38:03.212232 42575 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0314 12:38:03.513659 41230 finetune.py:68] layer 6_v @ epoch 0 new loss 1.3035765732638538e-05 old loss 3.440797081566416e-05 BETTER
W0314 12:38:03.578613 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:54,  1.77s/it]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s]  9%|▉         | 3/32 [00:02<00:19,  1.48it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.82it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.08it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.72it/s] 41%|████      | 13/32 [00:06<00:06,  2.74it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s]I0314 12:38:11.510129 39917 finetune.py:68] layer 5_v @ epoch 2 new loss 7.120600002963329e-06 old loss 7.657379683223553e-06 BETTER
 50%|█████     | 16/32 [00:07<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.80it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.81it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.79it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0314 12:38:19.815322 38451 finetune.py:68] layer 4_v @ epoch 4 new loss 6.573012797161937e-06 old loss 6.783647222619038e-06 BETTER
W0314 12:38:20.089000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.089000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.089000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.090000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.090000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.090000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.090000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.117000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.117000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.117000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.118000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.118000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.414000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.414000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.414000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.414000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:20.414000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.284000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.284000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.284000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.284000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.284000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.284000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.285000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.303000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.303000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.303000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.303000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.303000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.388780 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0314 12:38:21.502000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.502000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.503000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.503000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:21.503000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
4_v proxy err 0.03257840499281883 tr(WHW.T) 38.379119873046875
  0%|          | 0/32 [00:00<?, ?it/s]W0314 12:38:22.629000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.629000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.629000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.629000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.630000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.630000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.630000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.648000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.648000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.648000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.648000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:22.648000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]W0314 12:38:23.516000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:23.516000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:23.516000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:23.516000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:23.516000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s]I0314 12:38:31.247839 42575 finetune.py:45] layer 7_v initial loss 3.3265256206505e-05
W0314 12:38:31.248121 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.75it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0314 12:38:40.047000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.048000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.048000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.048000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.048000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.048000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.048000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.079000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.079000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.080000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.080000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.080000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.257000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.257000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.257000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.257000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.257000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
I0314 12:38:40.378592 41230 finetune.py:68] layer 6_v @ epoch 1 new loss 1.1436284694354981e-05 old loss 1.3035765732638538e-05 BETTER
W0314 12:38:40.485000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.485000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.485000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.485000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.486000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.486000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.486000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.509000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.509000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.509000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.509000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.509000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.576000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.576000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.576000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.576000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:40.576000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.338000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.665000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.665000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.665000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.665000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.665000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.666000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.666000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.688000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.688000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.688000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.689000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.689000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.951000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.951000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.952000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.952000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:38:41.952000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:38:42.321000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 12:38:48.312378 39917 finetune.py:68] layer 5_v @ epoch 3 new loss 6.84833639752469e-06 old loss 7.120600002963329e-06 BETTER
I0314 12:38:49.535670 38451 finetune.py:45] layer 4_q initial loss 1.1599982826737687e-05
W0314 12:38:49.536101 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:39:04.887646 42575 finetune.py:68] layer 7_v @ epoch 0 new loss 1.5534047633991577e-05 old loss 3.3265256206505e-05 BETTER
I0314 12:39:16.586933 41230 finetune.py:68] layer 6_v @ epoch 2 new loss 1.0857533197849989e-05 old loss 1.1436284694354981e-05 BETTER
I0314 12:39:25.160094 39917 finetune.py:68] layer 5_v @ epoch 4 new loss 6.68388202029746e-06 old loss 6.84833639752469e-06 BETTER
I0314 12:39:26.417936 38451 finetune.py:68] layer 4_q @ epoch 0 new loss 1.0895155355683528e-05 old loss 1.1599982826737687e-05 BETTER
W0314 12:39:26.979667 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_v proxy err 0.030795998871326447 tr(WHW.T) 37.700050354003906
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.04it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s]I0314 12:39:39.711567 42575 finetune.py:68] layer 7_v @ epoch 1 new loss 1.4160387763695326e-05 old loss 1.5534047633991577e-05 BETTER
 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
W0314 12:39:46.407000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.408000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.408000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.408000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.408000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.408000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.408000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.437000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.437000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.438000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.438000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.438000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.604000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.604000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.604000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.604000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.604000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.836000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.837000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.837000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.837000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.837000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.837000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.837000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.860000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.860000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.860000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.860000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.860000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.928000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.928000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.928000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.928000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:39:46.928000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:39:47.677000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:39:47.996000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:39:47.996000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:39:47.996000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:39:47.996000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:39:47.996000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:39:47.996000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:39:47.997000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.018000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.018000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.018000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.018000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.018000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.281000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.281000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.281000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.281000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.281000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:39:48.643000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 12:39:53.104748 41230 finetune.py:68] layer 6_v @ epoch 3 new loss 1.053490359481657e-05 old loss 1.0857533197849989e-05 BETTER
I0314 12:39:55.512665 39917 finetune.py:45] layer 5_q initial loss 1.414486723660957e-05
W0314 12:39:55.512984 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:40:04.372561 38451 finetune.py:68] layer 4_q @ epoch 1 new loss 1.0628034942783415e-05 old loss 1.0895155355683528e-05 BETTER
I0314 12:40:15.054329 42575 finetune.py:68] layer 7_v @ epoch 2 new loss 1.3600172678707168e-05 old loss 1.4160387763695326e-05 BETTER
I0314 12:40:29.739791 41230 finetune.py:68] layer 6_v @ epoch 4 new loss 1.0312403901480138e-05 old loss 1.053490359481657e-05 BETTER
I0314 12:40:30.726871 39917 finetune.py:68] layer 5_q @ epoch 0 new loss 1.32492014017771e-05 old loss 1.414486723660957e-05 BETTER
W0314 12:40:31.595741 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_v proxy err 0.032362304627895355 tr(WHW.T) 42.837894439697266
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:14,  1.94it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s]I0314 12:40:42.130007 38451 finetune.py:68] layer 4_q @ epoch 2 new loss 1.0416006261948496e-05 old loss 1.0628034942783415e-05 BETTER
 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0314 12:40:50.347223 42575 finetune.py:68] layer 7_v @ epoch 3 new loss 1.3258718354336452e-05 old loss 1.3600172678707168e-05 BETTER
W0314 12:40:51.107000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.108000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.108000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.108000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.108000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.108000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.108000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.140000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.140000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.140000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.140000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.140000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.320000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.320000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.320000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.321000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.321000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.555000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.555000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.555000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.555000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.555000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.555000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.555000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.577000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.577000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.577000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.577000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.577000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.646000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.647000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.647000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.647000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:40:51.647000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.410000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.740000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.741000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.741000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.741000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.741000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.741000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.741000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.763000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.763000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.763000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.763000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:40:52.763000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:40:53.023000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:40:53.023000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:40:53.023000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:40:53.023000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:40:53.023000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:40:53.390000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 12:41:00.591764 41230 finetune.py:45] layer 6_q initial loss 2.0132232748437673e-05
W0314 12:41:00.592173 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:41:07.449544 39917 finetune.py:68] layer 5_q @ epoch 1 new loss 1.2914365470351186e-05 old loss 1.32492014017771e-05 BETTER
I0314 12:41:20.221812 38451 finetune.py:68] layer 4_q @ epoch 3 new loss 1.0255456800223328e-05 old loss 1.0416006261948496e-05 BETTER
I0314 12:41:25.421519 42575 finetune.py:68] layer 7_v @ epoch 4 new loss 1.30226135297562e-05 old loss 1.3258718354336452e-05 BETTER
W0314 12:41:27.035383 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_v proxy err 0.02701776660978794 tr(WHW.T) 53.14668655395508
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.00it/s]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:15,  1.89it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.10it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.24it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.55it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.55it/s]I0314 12:41:35.551745 41230 finetune.py:68] layer 6_q @ epoch 0 new loss 1.922525370900985e-05 old loss 2.0132232748437673e-05 BETTER
 56%|█████▋    | 18/32 [00:07<00:05,  2.56it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.55it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.56it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.55it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:13<00:00,  2.57it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
I0314 12:41:43.958443 39917 finetune.py:68] layer 5_q @ epoch 2 new loss 1.2664672794926446e-05 old loss 1.2914365470351186e-05 BETTER
W0314 12:41:46.522000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.522000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.523000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.523000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.523000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.523000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.523000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.555000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.555000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.555000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.555000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.555000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.728000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.728000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.728000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.728000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.728000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.961000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.961000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.961000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.962000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.962000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.962000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.962000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.985000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.985000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.985000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.985000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:41:46.985000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:41:47.053000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:41:47.053000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:41:47.053000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:41:47.053000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:41:47.053000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:41:47.800000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.123000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.123000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.123000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.123000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.123000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.123000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.123000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.146000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.146000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.146000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.146000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.146000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.409000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.409000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.409000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.410000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.410000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:41:48.766000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 12:41:56.182205 42575 finetune.py:45] layer 7_q initial loss 2.649192356329877e-05
W0314 12:41:56.182694 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:41:58.588611 38451 finetune.py:68] layer 4_q @ epoch 4 new loss 1.0097922313434538e-05 old loss 1.0255456800223328e-05 BETTER
W0314 12:42:00.659531 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_q proxy err 0.0032348402310162783 tr(WHW.T) 6743.53369140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s]  6%|▋         | 2/32 [00:01<00:17,  1.70it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.69it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.75it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s]I0314 12:42:11.661308 41230 finetune.py:68] layer 6_q @ epoch 1 new loss 1.8759314116323367e-05 old loss 1.922525370900985e-05 BETTER
 81%|████████▏ | 26/32 [00:09<00:02,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.78it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
I0314 12:42:20.931795 39917 finetune.py:68] layer 5_q @ epoch 3 new loss 1.2458331184461713e-05 old loss 1.2664672794926446e-05 BETTER
I0314 12:42:21.880341 38451 finetune.py:45] layer 4_k initial loss 1.6439906175946817e-05
W0314 12:42:21.880784 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:42:30.535350 42575 finetune.py:68] layer 7_q @ epoch 0 new loss 2.534437589929439e-05 old loss 2.649192356329877e-05 BETTER
I0314 12:42:47.818146 41230 finetune.py:68] layer 6_q @ epoch 2 new loss 1.8404769434710033e-05 old loss 1.8759314116323367e-05 BETTER
I0314 12:42:58.210110 39917 finetune.py:68] layer 5_q @ epoch 4 new loss 1.2287623576412443e-05 old loss 1.2458331184461713e-05 BETTER
I0314 12:42:59.601796 38451 finetune.py:68] layer 4_k @ epoch 0 new loss 1.3653434507432394e-05 old loss 1.6439906175946817e-05 BETTER
W0314 12:43:00.261068 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_q proxy err 0.004510234110057354 tr(WHW.T) 6497.2021484375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.56it/s]I0314 12:43:05.527825 42575 finetune.py:68] layer 7_q @ epoch 1 new loss 2.4800510800560005e-05 old loss 2.534437589929439e-05 BETTER
 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0314 12:43:22.716569 39917 finetune.py:45] layer 5_k initial loss 1.934031752170995e-05
W0314 12:43:22.717038 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:43:25.031995 41230 finetune.py:68] layer 6_q @ epoch 3 new loss 1.8109985830960795e-05 old loss 1.8404769434710033e-05 BETTER
I0314 12:43:37.330451 38451 finetune.py:68] layer 4_k @ epoch 1 new loss 1.3368907275435049e-05 old loss 1.3653434507432394e-05 BETTER
I0314 12:43:40.997168 42575 finetune.py:68] layer 7_q @ epoch 2 new loss 2.43320464505814e-05 old loss 2.4800510800560005e-05 BETTER
I0314 12:43:57.448003 39917 finetune.py:68] layer 5_k @ epoch 0 new loss 1.6391870303777978e-05 old loss 1.934031752170995e-05 BETTER
I0314 12:44:01.588852 41230 finetune.py:68] layer 6_q @ epoch 4 new loss 1.7863827451947145e-05 old loss 1.8109985830960795e-05 BETTER
W0314 12:44:03.472795 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_q proxy err 0.005073197185993195 tr(WHW.T) 6022.734375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.04it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:14,  1.94it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.15it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s]I0314 12:44:15.048856 38451 finetune.py:68] layer 4_k @ epoch 2 new loss 1.3190368918003514e-05 old loss 1.3368907275435049e-05 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s]I0314 12:44:16.222658 42575 finetune.py:68] layer 7_q @ epoch 3 new loss 2.4018709154915996e-05 old loss 2.43320464505814e-05 BETTER
 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0314 12:44:25.592275 41230 finetune.py:45] layer 6_k initial loss 2.565478098404128e-05
W0314 12:44:25.592627 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:44:34.038758 39917 finetune.py:68] layer 5_k @ epoch 1 new loss 1.6093861631816253e-05 old loss 1.6391870303777978e-05 BETTER
I0314 12:44:51.693442 42575 finetune.py:68] layer 7_q @ epoch 4 new loss 2.37709882640047e-05 old loss 2.4018709154915996e-05 BETTER
I0314 12:44:52.886107 38451 finetune.py:68] layer 4_k @ epoch 3 new loss 1.3052769645582885e-05 old loss 1.3190368918003514e-05 BETTER
W0314 12:44:53.445905 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_q proxy err 0.004899710416793823 tr(WHW.T) 6040.1171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:14,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.15it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.57it/s]I0314 12:45:00.771151 41230 finetune.py:68] layer 6_k @ epoch 0 new loss 2.347313056816347e-05 old loss 2.565478098404128e-05 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.57it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.31it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.38it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
I0314 12:45:10.357941 39917 finetune.py:68] layer 5_k @ epoch 2 new loss 1.5899784557404928e-05 old loss 1.6093861631816253e-05 BETTER
I0314 12:45:15.714640 42575 finetune.py:45] layer 7_k initial loss 3.456303966231644e-05
W0314 12:45:15.715132 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:45:31.308795 38451 finetune.py:68] layer 4_k @ epoch 4 new loss 1.2918778338644188e-05 old loss 1.3052769645582885e-05 BETTER
W0314 12:45:33.034310 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_k proxy err 0.003252229653298855 tr(WHW.T) 3931.695068359375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s]  6%|▋         | 2/32 [00:01<00:15,  1.90it/s]  9%|▉         | 3/32 [00:01<00:13,  2.22it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.41it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.53it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.60it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s]I0314 12:45:37.161216 41230 finetune.py:68] layer 6_k @ epoch 1 new loss 2.3123368009692058e-05 old loss 2.347313056816347e-05 BETTER
 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]
I0314 12:45:46.644994 39917 finetune.py:68] layer 5_k @ epoch 3 new loss 1.5737878129584715e-05 old loss 1.5899784557404928e-05 BETTER
I0314 12:45:49.824884 42575 finetune.py:68] layer 7_k @ epoch 0 new loss 3.129653850919567e-05 old loss 3.456303966231644e-05 BETTER
I0314 12:45:53.847929 38451 finetune.py:45] layer 4_o initial loss 2.5077875761780888e-05
W0314 12:45:53.848371 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:46:14.151309 41230 finetune.py:68] layer 6_k @ epoch 2 new loss 2.285902519361116e-05 old loss 2.3123368009692058e-05 BETTER
I0314 12:46:23.399191 39917 finetune.py:68] layer 5_k @ epoch 4 new loss 1.560089185659308e-05 old loss 1.5737878129584715e-05 BETTER
W0314 12:46:25.173681 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 12:46:25.337160 42575 finetune.py:68] layer 7_k @ epoch 1 new loss 3.0785638955421746e-05 old loss 3.129653850919567e-05 BETTER
5_k proxy err 0.004232431761920452 tr(WHW.T) 4141.0849609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s]I0314 12:46:30.881706 38451 finetune.py:68] layer 4_o @ epoch 0 new loss 2.2429887394537218e-05 old loss 2.5077875761780888e-05 BETTER
 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0314 12:46:46.322726 39917 finetune.py:45] layer 5_o initial loss 2.744971789070405e-05
W0314 12:46:46.323093 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:46:51.142099 41230 finetune.py:68] layer 6_k @ epoch 3 new loss 2.263992246298585e-05 old loss 2.285902519361116e-05 BETTER
I0314 12:47:00.379909 42575 finetune.py:68] layer 7_k @ epoch 2 new loss 3.0411396437557414e-05 old loss 3.0785638955421746e-05 BETTER
I0314 12:47:08.280933 38451 finetune.py:68] layer 4_o @ epoch 1 new loss 2.1783611373393796e-05 old loss 2.2429887394537218e-05 BETTER
I0314 12:47:21.252410 39917 finetune.py:68] layer 5_o @ epoch 0 new loss 2.566693910921458e-05 old loss 2.744971789070405e-05 BETTER
I0314 12:47:27.788627 41230 finetune.py:68] layer 6_k @ epoch 4 new loss 2.2457054001279175e-05 old loss 2.263992246298585e-05 BETTER
W0314 12:47:30.181126 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_k proxy err 0.003974436316639185 tr(WHW.T) 4403.6162109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.18it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s]I0314 12:47:35.461961 42575 finetune.py:68] layer 7_k @ epoch 3 new loss 3.014518915733788e-05 old loss 3.0411396437557414e-05 BETTER
 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0314 12:47:45.737227 38451 finetune.py:68] layer 4_o @ epoch 2 new loss 2.1414047296275385e-05 old loss 2.1783611373393796e-05 BETTER
I0314 12:47:52.413077 41230 finetune.py:45] layer 6_o initial loss 4.143774276599288e-05
W0314 12:47:52.413501 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:47:56.830122 39917 finetune.py:68] layer 5_o @ epoch 1 new loss 2.5195573471137322e-05 old loss 2.566693910921458e-05 BETTER
I0314 12:48:10.488981 42575 finetune.py:68] layer 7_k @ epoch 4 new loss 2.9958897357573733e-05 old loss 3.014518915733788e-05 BETTER
W0314 12:48:12.185370 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_k proxy err 0.004097911529242992 tr(WHW.T) 4595.63720703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.57it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s]I0314 12:48:23.388122 38451 finetune.py:68] layer 4_o @ epoch 3 new loss 2.1138821466593072e-05 old loss 2.1414047296275385e-05 BETTER
 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0314 12:48:27.747276 41230 finetune.py:68] layer 6_o @ epoch 0 new loss 3.9246206142706797e-05 old loss 4.143774276599288e-05 BETTER
I0314 12:48:32.804586 39917 finetune.py:68] layer 5_o @ epoch 2 new loss 2.48867927439278e-05 old loss 2.5195573471137322e-05 BETTER
I0314 12:48:33.540859 42575 finetune.py:45] layer 7_o initial loss 5.427376163424924e-05
W0314 12:48:33.541315 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:49:01.811476 38451 finetune.py:68] layer 4_o @ epoch 4 new loss 2.09248682949692e-05 old loss 2.1138821466593072e-05 BETTER
W0314 12:49:03.639159 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 12:49:03.990348 41230 finetune.py:68] layer 6_o @ epoch 1 new loss 3.860596189042553e-05 old loss 3.9246206142706797e-05 BETTER
4_o proxy err 0.02711663767695427 tr(WHW.T) 1.3764843940734863
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]I0314 12:49:07.815383 42575 finetune.py:68] layer 7_o @ epoch 0 new loss 5.2220439101802185e-05 old loss 5.427376163424924e-05 BETTER
  6%|▋         | 2/32 [00:03<00:50,  1.67s/it]I0314 12:49:08.912028 39917 finetune.py:68] layer 5_o @ epoch 3 new loss 2.4658027541590855e-05 old loss 2.48867927439278e-05 BETTER
  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.47s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.48s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it]I0314 12:49:40.384226 41230 finetune.py:68] layer 6_o @ epoch 2 new loss 3.81789323000703e-05 old loss 3.860596189042553e-05 BETTER
 75%|███████▌  | 24/32 [00:35<00:11,  1.48s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it]I0314 12:49:43.248976 42575 finetune.py:68] layer 7_o @ epoch 1 new loss 5.150366632733494e-05 old loss 5.2220439101802185e-05 BETTER
 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it]I0314 12:49:44.857114 39917 finetune.py:68] layer 5_o @ epoch 4 new loss 2.447129190841224e-05 old loss 2.4658027541590855e-05 BETTER
 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it]W0314 12:49:46.452487 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:41<00:05,  1.48s/it]5_o proxy err 0.026120854541659355 tr(WHW.T) 1.828094720840454
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it]  3%|▎         | 1/32 [00:02<01:03,  2.04s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it]I0314 12:50:00.996952 38451 finetune.py:45] layer 4_up initial loss 5.024879646953195e-05
W0314 12:50:00.997336 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it]I0314 12:50:16.764763 41230 finetune.py:68] layer 6_o @ epoch 3 new loss 3.7839068681932986e-05 old loss 3.81789323000703e-05 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.54s/it]I0314 12:50:18.415621 42575 finetune.py:68] layer 7_o @ epoch 2 new loss 5.097462417325005e-05 old loss 5.150366632733494e-05 BETTER
 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]I0314 12:50:37.121399 38451 finetune.py:68] layer 4_up @ epoch 0 new loss 4.959984289598651e-05 old loss 5.024879646953195e-05 BETTER
100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0314 12:50:45.563331 39917 finetune.py:45] layer 5_up initial loss 6.74627663102001e-05
W0314 12:50:45.563697 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:50:52.732730 41230 finetune.py:68] layer 6_o @ epoch 4 new loss 3.7560119380941615e-05 old loss 3.7839068681932986e-05 BETTER
I0314 12:50:53.780985 42575 finetune.py:68] layer 7_o @ epoch 3 new loss 5.05470743519254e-05 old loss 5.097462417325005e-05 BETTER
W0314 12:50:54.388799 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_o proxy err 0.03389386087656021 tr(WHW.T) 2.621384620666504
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]  9%|▉         | 3/32 [00:05<00:46,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it]I0314 12:51:13.950336 38451 finetune.py:68] layer 4_up @ epoch 1 new loss 4.928447742713615e-05 old loss 4.959984289598651e-05 BETTER
 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it]I0314 12:51:19.678772 39917 finetune.py:68] layer 5_up @ epoch 0 new loss 6.651708827121183e-05 old loss 6.74627663102001e-05 BETTER
 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it]I0314 12:51:28.565194 42575 finetune.py:68] layer 7_o @ epoch 4 new loss 5.0180056859971955e-05 old loss 5.05470743519254e-05 BETTER
 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it]W0314 12:51:30.152135 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it]7_o proxy err 0.03175436705350876 tr(WHW.T) 3.835618495941162
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it]  9%|▉         | 3/32 [00:05<00:47,  1.65s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.49s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.55s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 38%|███▊      | 12/32 [00:19<00:30,  1.55s/it]I0314 12:51:50.688969 38451 finetune.py:68] layer 4_up @ epoch 2 new loss 4.901720967609435e-05 old loss 4.928447742713615e-05 BETTER
 41%|████      | 13/32 [00:20<00:29,  1.54s/it]I0314 12:51:52.088806 41230 finetune.py:45] layer 6_up initial loss 9.226991824107245e-05
W0314 12:51:52.089221 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:22<00:27,  1.54s/it]I0314 12:51:54.352800 39917 finetune.py:68] layer 5_up @ epoch 1 new loss 6.602761277463287e-05 old loss 6.651708827121183e-05 BETTER
 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 50%|█████     | 16/32 [00:25<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0314 12:52:27.008144 41230 finetune.py:68] layer 6_up @ epoch 0 new loss 9.084901830647141e-05 old loss 9.226991824107245e-05 BETTER
I0314 12:52:27.669562 38451 finetune.py:68] layer 4_up @ epoch 3 new loss 4.878221079707146e-05 old loss 4.901720967609435e-05 BETTER
I0314 12:52:29.485581 42575 finetune.py:45] layer 7_up initial loss 0.00011113696382381022
W0314 12:52:29.485990 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:52:29.657230 39917 finetune.py:68] layer 5_up @ epoch 2 new loss 6.561213376699015e-05 old loss 6.602761277463287e-05 BETTER
I0314 12:53:02.150117 41230 finetune.py:68] layer 6_up @ epoch 1 new loss 9.008905908558518e-05 old loss 9.084901830647141e-05 BETTER
I0314 12:53:02.438473 42575 finetune.py:68] layer 7_up @ epoch 0 new loss 0.00010956619371427223 old loss 0.00011113696382381022 BETTER
I0314 12:53:04.324110 38451 finetune.py:68] layer 4_up @ epoch 4 new loss 4.8564106691628695e-05 old loss 4.878221079707146e-05 BETTER
I0314 12:53:04.428181 39917 finetune.py:68] layer 5_up @ epoch 3 new loss 6.523444608319551e-05 old loss 6.561213376699015e-05 BETTER
W0314 12:53:05.832497 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_up proxy err 0.042551055550575256 tr(WHW.T) 399.89312744140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.87s/it]  6%|▋         | 2/32 [00:03<00:48,  1.62s/it]  9%|▉         | 3/32 [00:04<00:44,  1.54s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.46s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.46s/it]I0314 12:53:36.385705 42575 finetune.py:68] layer 7_up @ epoch 1 new loss 0.00010863274656003341 old loss 0.00010956619371427223 BETTER
 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it]I0314 12:53:36.877046 41230 finetune.py:68] layer 6_up @ epoch 2 new loss 8.94350596354343e-05 old loss 9.008905908558518e-05 BETTER
 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it]I0314 12:53:39.212193 39917 finetune.py:68] layer 5_up @ epoch 4 new loss 6.487703649327159e-05 old loss 6.523444608319551e-05 BETTER
 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it]W0314 12:53:40.636356 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:33<00:13,  1.46s/it]5_up proxy err 0.041446615010499954 tr(WHW.T) 497.2673645019531
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.47s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.47s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]
 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it]I0314 12:54:02.286147 38451 finetune.py:45] layer 4_gate initial loss 6.667693378403783e-05
W0314 12:54:02.286721 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it]I0314 12:54:10.012675 42575 finetune.py:68] layer 7_up @ epoch 2 new loss 0.00010781353194033727 old loss 0.00010863274656003341 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it]I0314 12:54:11.493302 41230 finetune.py:68] layer 6_up @ epoch 3 new loss 8.884359704097733e-05 old loss 8.94350596354343e-05 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0314 12:54:36.669454 38451 finetune.py:68] layer 4_gate @ epoch 0 new loss 6.594121077796444e-05 old loss 6.667693378403783e-05 BETTER
I0314 12:54:39.516573 39917 finetune.py:45] layer 5_gate initial loss 9.094973211176693e-05
W0314 12:54:39.516996 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:54:43.740810 42575 finetune.py:68] layer 7_up @ epoch 3 new loss 0.00010709634807426482 old loss 0.00010781353194033727 BETTER
I0314 12:54:45.998094 41230 finetune.py:68] layer 6_up @ epoch 4 new loss 8.829296712065116e-05 old loss 8.884359704097733e-05 BETTER
W0314 12:54:47.497066 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_up proxy err 0.03919167071580887 tr(WHW.T) 571.4320678710938
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it]I0314 12:55:11.710010 38451 finetune.py:68] layer 4_gate @ epoch 1 new loss 6.569183460669592e-05 old loss 6.594121077796444e-05 BETTER
 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it]I0314 12:55:12.188204 39917 finetune.py:68] layer 5_gate @ epoch 0 new loss 8.988729678094387e-05 old loss 9.094973211176693e-05 BETTER
 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it]I0314 12:55:17.575654 42575 finetune.py:68] layer 7_up @ epoch 4 new loss 0.00010641558765200898 old loss 0.00010709634807426482 BETTER
 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it]W0314 12:55:18.974336 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it]7_up proxy err 0.03660183399915695 tr(WHW.T) 649.893798828125
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.49s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.55s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it]I0314 12:55:45.590382 41230 finetune.py:45] layer 6_gate initial loss 0.00011968528269790113
W0314 12:55:45.590869 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:55:46.413116 39917 finetune.py:68] layer 5_gate @ epoch 1 new loss 8.946179150370881e-05 old loss 8.988729678094387e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it]I0314 12:55:47.317426 38451 finetune.py:68] layer 4_gate @ epoch 2 new loss 6.54799077892676e-05 old loss 6.569183460669592e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it] 66%|██████▌   | 21/32 [00:32<00:17,  1.56s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.56s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.56s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.56s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.63s/it]100%|██████████| 32/32 [00:50<00:00,  1.61s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0314 12:56:18.811303 42575 finetune.py:45] layer 7_gate initial loss 0.0001431268610758707
W0314 12:56:18.811666 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:56:19.027263 41230 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.00011827424896182492 old loss 0.00011968528269790113 BETTER
I0314 12:56:20.311828 39917 finetune.py:68] layer 5_gate @ epoch 2 new loss 8.910298492992297e-05 old loss 8.946179150370881e-05 BETTER
I0314 12:56:22.963203 38451 finetune.py:68] layer 4_gate @ epoch 3 new loss 6.528657104354352e-05 old loss 6.54799077892676e-05 BETTER
I0314 12:56:50.855853 42575 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.0001416818267898634 old loss 0.0001431268610758707 BETTER
I0314 12:56:52.463063 41230 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.00011764674127334729 old loss 0.00011827424896182492 BETTER
I0314 12:56:53.693008 39917 finetune.py:68] layer 5_gate @ epoch 3 new loss 8.877387881511822e-05 old loss 8.910298492992297e-05 BETTER
I0314 12:56:57.966818 38451 finetune.py:68] layer 4_gate @ epoch 4 new loss 6.510569073725492e-05 old loss 6.528657104354352e-05 BETTER
W0314 12:56:59.251097 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_gate proxy err 0.01929858885705471 tr(WHW.T) 1572.283935546875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:35,  1.17it/s]  2%|▏         | 2/112 [00:01<01:02,  1.75it/s]  3%|▎         | 3/112 [00:01<00:51,  2.10it/s]  4%|▎         | 4/112 [00:01<00:46,  2.31it/s]  4%|▍         | 5/112 [00:02<00:43,  2.44it/s]  5%|▌         | 6/112 [00:02<00:41,  2.53it/s]  6%|▋         | 7/112 [00:03<00:40,  2.59it/s]  7%|▋         | 8/112 [00:03<00:39,  2.63it/s]  8%|▊         | 9/112 [00:03<00:38,  2.66it/s]  9%|▉         | 10/112 [00:04<00:38,  2.68it/s] 10%|▉         | 11/112 [00:04<00:37,  2.69it/s] 11%|█         | 12/112 [00:04<00:36,  2.70it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.71it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.72it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.72it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.72it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.73it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.73it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.74it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.74it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.74it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.74it/s] 21%|██        | 23/112 [00:08<00:32,  2.73it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.69it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.71it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.72it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.73it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.74it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.75it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.75it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.76it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.75it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.75it/s] 30%|███       | 34/112 [00:12<00:28,  2.75it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.74it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.74it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.73it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.70it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.71it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.72it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.73it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.74it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.75it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.75it/s] 40%|████      | 45/112 [00:16<00:24,  2.75it/s] 41%|████      | 46/112 [00:17<00:24,  2.75it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.75it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.75it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.75it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.74it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.70it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.71it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.72it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.72it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.73it/s]I0314 12:57:23.276696 42575 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.00014092451601754874 old loss 0.0001416818267898634 BETTER
 50%|█████     | 56/112 [00:20<00:20,  2.73it/s] 51%|█████     | 57/112 [00:21<00:20,  2.74it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.74it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.73it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.73it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.73it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.73it/s]I0314 12:57:25.912550 41230 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.00011711075785569847 old loss 0.00011764674127334729 BETTER
 56%|█████▋    | 63/112 [00:23<00:17,  2.73it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.70it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.72it/s]I0314 12:57:27.040210 39917 finetune.py:68] layer 5_gate @ epoch 4 new loss 8.846620039548725e-05 old loss 8.877387881511822e-05 BETTER
 59%|█████▉    | 66/112 [00:24<00:16,  2.73it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.73it/s] 61%|██████    | 68/112 [00:25<00:16,  2.73it/s]W0314 12:57:28.133512 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▏   | 69/112 [00:25<00:15,  2.74it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.74it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.74it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.74it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.73it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.73it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.73it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.71it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.72it/s]5_gate proxy err 0.018425235524773598 tr(WHW.T) 1969.3331298828125
  0%|          | 0/112 [00:00<?, ?it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.73it/s] 71%|███████   | 79/112 [00:29<00:12,  2.74it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s]  3%|▎         | 3/112 [00:01<00:53,  2.02it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.74it/s]  4%|▎         | 4/112 [00:02<00:48,  2.21it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.73it/s]  4%|▍         | 5/112 [00:02<00:45,  2.33it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.73it/s]  5%|▌         | 6/112 [00:02<00:44,  2.41it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.73it/s]  6%|▋         | 7/112 [00:03<00:42,  2.46it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.74it/s]  7%|▋         | 8/112 [00:03<00:41,  2.50it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.74it/s]  8%|▊         | 9/112 [00:03<00:40,  2.52it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.73it/s]  9%|▉         | 10/112 [00:04<00:40,  2.53it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.70it/s] 80%|████████  | 90/112 [00:33<00:08,  2.72it/s] 10%|▉         | 11/112 [00:04<00:39,  2.54it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.73it/s] 11%|█         | 12/112 [00:05<00:39,  2.54it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.72it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.55it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.73it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.54it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.73it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.55it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.73it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.54it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.73it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.54it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.74it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.56it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.72it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.57it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.72it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.57it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.72it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.56it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.73it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.57it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.71it/s] 21%|██        | 23/112 [00:09<00:34,  2.58it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.74it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.58it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.72it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.56it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.71it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.72it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.56it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.73it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.56it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.72it/s] 25%|██▌       | 28/112 [00:11<00:33,  2.53it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.73it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.55it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.73it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.56it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.73it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.56it/s]100%|██████████| 112/112 [00:41<00:00,  2.73it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
 29%|██▊       | 32/112 [00:12<00:31,  2.57it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.57it/s] 30%|███       | 34/112 [00:13<00:30,  2.56it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.56it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.55it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.55it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.54it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.55it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.55it/s] 37%|███▋      | 41/112 [00:16<00:28,  2.52it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.53it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.53it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.54it/s] 40%|████      | 45/112 [00:18<00:26,  2.55it/s] 41%|████      | 46/112 [00:18<00:25,  2.56it/s]W0314 12:57:50.171000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.171000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.172000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.172000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.172000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.172000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.172000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 42%|████▏     | 47/112 [00:18<00:25,  2.56it/s]W0314 12:57:50.215000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.215000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.215000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.215000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.215000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.396000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.396000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.396000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.396000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.396000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 43%|████▎     | 48/112 [00:19<00:24,  2.57it/s]W0314 12:57:50.723000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.723000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.723000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.724000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.724000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.724000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.724000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.761000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.761000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.762000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.762000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.762000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.832000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.833000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.833000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.833000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:57:50.833000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 49/112 [00:19<00:24,  2.57it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.56it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.56it/s]W0314 12:57:51.832000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:51.845000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:51.854000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:57:51.854000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 46%|████▋     | 52/112 [00:20<00:23,  2.56it/s]W0314 12:57:52.324000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.324000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.324000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.325000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.325000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.325000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.325000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.354000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.354000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.354000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.354000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.354000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 53/112 [00:21<00:22,  2.57it/s]W0314 12:57:52.655000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.655000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.655000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.655000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.655000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.655000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.656000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.656000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 48%|████▊     | 54/112 [00:21<00:22,  2.54it/s]W0314 12:57:52.954000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.954000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.954000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.954000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:57:52.954000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 49%|████▉     | 55/112 [00:21<00:22,  2.56it/s]W0314 12:57:53.395000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:57:53.401000 139939132868416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 56/112 [00:22<00:21,  2.56it/s] 51%|█████     | 57/112 [00:22<00:21,  2.57it/s] 52%|█████▏    | 58/112 [00:23<00:20,  2.58it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.58it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.59it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.57it/s]I0314 12:57:55.812280 42575 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.00014028105942998081 old loss 0.00014092451601754874 BETTER
 55%|█████▌    | 62/112 [00:24<00:19,  2.57it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.57it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.56it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.56it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.56it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.53it/s] 61%|██████    | 68/112 [00:27<00:17,  2.53it/s]I0314 12:57:58.769984 41230 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.0001166250731330365 old loss 0.00011711075785569847 BETTER
 62%|██████▏   | 69/112 [00:27<00:16,  2.54it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.54it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.54it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.54it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.55it/s]I0314 12:58:00.666776 38451 finetune.py:45] layer 4_down initial loss 0.00010143264080397785
W0314 12:58:00.667181 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 74/112 [00:29<00:14,  2.55it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.56it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.56it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.56it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.55it/s] 71%|███████   | 79/112 [00:31<00:12,  2.56it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.55it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.57it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.58it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.58it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.59it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.60it/s] 77%|███████▋  | 86/112 [00:34<00:09,  2.60it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.60it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.61it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.60it/s] 80%|████████  | 90/112 [00:35<00:08,  2.60it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.60it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.59it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.59it/s] 84%|████████▍ | 94/112 [00:37<00:06,  2.58it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.57it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.57it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.58it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.59it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.59it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.59it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.60it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.60it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.60it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.60it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.59it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.59it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.59it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.56it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.57it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.58it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:44<00:00,  2.59it/s]100%|██████████| 112/112 [00:44<00:00,  2.54it/s]
W0314 12:58:21.561000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.562000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.562000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.562000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.562000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.562000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.562000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.608000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.608000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.608000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.608000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.609000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.785000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.785000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.785000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.785000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:58:21.785000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.108000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.109000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.109000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.109000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.109000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.109000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.109000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.144000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.144000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.144000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.144000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.144000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.215000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.215000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.215000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.215000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:58:22.215000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.201000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.215000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.223000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.223000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.676000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.676000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.676000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.676000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.676000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.677000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.677000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.721000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.721000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.721000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.721000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:58:23.721000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.023000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.023000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.023000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.023000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.023000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.023000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.023000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.024000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.323000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.323000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.323000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.324000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.324000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.762000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:58:24.768000 140179864799040 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 12:58:27.967456 42575 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.00013968847633805126 old loss 0.00014028105942998081 BETTER
I0314 12:58:31.675020 41230 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.00011615314724622294 old loss 0.0001166250731330365 BETTER
I0314 12:58:32.156144 39917 finetune.py:45] layer 5_down initial loss 0.00013944300008006394
W0314 12:58:32.156592 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 12:58:32.899663 38451 finetune.py:68] layer 4_down @ epoch 0 new loss 0.00010129560541827232 old loss 0.00010143264080397785 BETTER
W0314 12:58:32.904589 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_gate proxy err 0.015523315407335758 tr(WHW.T) 2569.26318359375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s]  2%|▏         | 2/112 [00:01<01:04,  1.72it/s]  3%|▎         | 3/112 [00:01<00:53,  2.04it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s]  5%|▌         | 6/112 [00:02<00:43,  2.45it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s]  7%|▋         | 8/112 [00:03<00:40,  2.55it/s]  8%|▊         | 9/112 [00:03<00:39,  2.58it/s]  9%|▉         | 10/112 [00:04<00:39,  2.61it/s] 10%|▉         | 11/112 [00:04<00:38,  2.62it/s] 11%|█         | 12/112 [00:05<00:37,  2.64it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.65it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.65it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.65it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.65it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.65it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.62it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.64it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.65it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.65it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.65it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.66it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.66it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.66it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.66it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.65it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.65it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.65it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.65it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 29%|██▉       | 33/112 [00:12<00:30,  2.62it/s] 30%|███       | 34/112 [00:13<00:29,  2.63it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.65it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.65it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.66it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.66it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.66it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.66it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.66it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.66it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.66it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.66it/s] 40%|████      | 45/112 [00:17<00:25,  2.66it/s] 41%|████      | 46/112 [00:17<00:25,  2.63it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.64it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.66it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.66it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.67it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.67it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.67it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.67it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.67it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.67it/s] 50%|█████     | 56/112 [00:21<00:20,  2.67it/s] 51%|█████     | 57/112 [00:21<00:20,  2.66it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.66it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.66it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.63it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.64it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.65it/s]I0314 12:59:00.152793 42575 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.00013913019211031497 old loss 0.00013968847633805126 BETTER
 56%|█████▋    | 63/112 [00:24<00:18,  2.66it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.66it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.67it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.67it/s]W0314 12:59:01.491508 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 60%|█████▉    | 67/112 [00:25<00:16,  2.67it/s] 61%|██████    | 68/112 [00:26<00:16,  2.67it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.67it/s]I0314 12:59:02.681286 39917 finetune.py:68] layer 5_down @ epoch 0 new loss 0.0001392479462083429 old loss 0.00013944300008006394 BETTER
 62%|██████▎   | 70/112 [00:26<00:15,  2.67it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.67it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.66it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.66it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.61it/s] 67%|██████▋   | 75/112 [00:28<00:14,  2.63it/s]7_gate proxy err 0.015170090831816196 tr(WHW.T) 2613.78125
  0%|          | 0/112 [00:00<?, ?it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.64it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.64it/s]I0314 12:59:05.677548 38451 finetune.py:68] layer 4_down @ epoch 1 new loss 0.00010125336848432198 old loss 0.00010129560541827232 BETTER
  1%|          | 1/112 [00:00<01:38,  1.12it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.65it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s] 71%|███████   | 79/112 [00:30<00:12,  2.65it/s]  3%|▎         | 3/112 [00:01<00:54,  2.00it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.66it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.66it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.66it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.66it/s]  6%|▋         | 7/112 [00:03<00:42,  2.46it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.67it/s]  7%|▋         | 8/112 [00:03<00:41,  2.49it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.67it/s]  8%|▊         | 9/112 [00:03<00:40,  2.52it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.68it/s]  9%|▉         | 10/112 [00:04<00:40,  2.55it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.67it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.64it/s] 11%|█         | 12/112 [00:05<00:39,  2.56it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.66it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.58it/s] 80%|████████  | 90/112 [00:34<00:08,  2.68it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.59it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.68it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.59it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.68it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.68it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.68it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.68it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.68it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.60it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.67it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.60it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.67it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.59it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.67it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.60it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.67it/s] 21%|██        | 23/112 [00:09<00:34,  2.59it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.67it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.59it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.67it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.55it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.64it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.57it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.65it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.58it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.67it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.59it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.68it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.68it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.60it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.68it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.61it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.68it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.60it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.68it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.60it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.68it/s] 30%|███       | 34/112 [00:13<00:30,  2.59it/s]100%|██████████| 112/112 [00:42<00:00,  2.67it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
 31%|███▏      | 35/112 [00:14<00:29,  2.59it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.59it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.58it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.56it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.57it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.59it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.59it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.61it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.61it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.62it/s] 40%|████      | 45/112 [00:17<00:25,  2.62it/s] 41%|████      | 46/112 [00:18<00:25,  2.62it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.62it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.61it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.60it/s] 45%|████▍     | 50/112 [00:19<00:24,  2.57it/s]W0314 12:59:24.712000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.713000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.713000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.713000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.713000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.713000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.713000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.757000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.757000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.757000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.758000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.758000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.935000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.935000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.935000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.935000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:24.935000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 46%|████▌     | 51/112 [00:20<00:23,  2.57it/s]W0314 12:59:25.264000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.264000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.264000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.264000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.264000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.264000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.264000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.295000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.296000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.296000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.296000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.296000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.366000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.367000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.367000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.367000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:25.367000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 46%|████▋     | 52/112 [00:20<00:23,  2.58it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.58it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.59it/s]W0314 12:59:26.362000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.377000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.385000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.385000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 49%|████▉     | 55/112 [00:21<00:21,  2.60it/s]W0314 12:59:26.843000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.844000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.844000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.844000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.844000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.844000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.844000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.873000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.873000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.873000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.873000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:26.873000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 56/112 [00:22<00:21,  2.61it/s]W0314 12:59:27.185000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.185000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.186000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.186000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.186000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.186000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.186000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.186000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 51%|█████     | 57/112 [00:22<00:21,  2.61it/s]W0314 12:59:27.487000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.487000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.487000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.487000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.487000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 52%|█████▏    | 58/112 [00:22<00:20,  2.62it/s]W0314 12:59:27.923000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:27.928000 140587630114624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 59/112 [00:23<00:20,  2.61it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.62it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.63it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.59it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.59it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.59it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.59it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.59it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.59it/s] 61%|██████    | 68/112 [00:26<00:16,  2.59it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.59it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.59it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.59it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.55it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.57it/s]I0314 12:59:34.000055 39917 finetune.py:68] layer 5_down @ epoch 1 new loss 0.0001392100821249187 old loss 0.0001392479462083429 BETTER
 67%|██████▋   | 75/112 [00:29<00:14,  2.58it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.58it/s]I0314 12:59:35.023851 41230 finetune.py:45] layer 6_down initial loss 0.00017786529497243464
W0314 12:59:35.024314 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 77/112 [00:30<00:13,  2.58it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.59it/s] 71%|███████   | 79/112 [00:30<00:12,  2.59it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.59it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.60it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.60it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.60it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.59it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.58it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.56it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.57it/s]I0314 12:59:39.011279 38451 finetune.py:68] layer 4_down @ epoch 2 new loss 0.00010123051470145583 old loss 0.00010125336848432198 BETTER
 79%|███████▊  | 88/112 [00:34<00:09,  2.59it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.59it/s] 80%|████████  | 90/112 [00:35<00:08,  2.60it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.61it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.61it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.62it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.61it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.60it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.59it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.55it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.57it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.58it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.58it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.58it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.59it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.59it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.59it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.60it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.60it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.59it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.59it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.56it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.57it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.56it/s]
W0314 12:59:54.799000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.799000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.799000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.799000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.800000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.800000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.800000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.845000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.845000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.845000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.846000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:54.846000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.023000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.023000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.023000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.023000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.023000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.346000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.346000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.346000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.346000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.347000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.347000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.347000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.380000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.380000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.380000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.381000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.381000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.453000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.453000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.453000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.453000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:55.453000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.444000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.458000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.466000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.466000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.934000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.934000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.935000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.935000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.935000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.935000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.935000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.969000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.969000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.970000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.970000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:56.970000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.279000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.280000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.280000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.280000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.280000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.280000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.280000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.280000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.598000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.598000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.598000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.598000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 12:59:57.598000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 12:59:58.043000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 12:59:58.048000 140643854333760 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 13:00:04.979467 42575 finetune.py:45] layer 7_down initial loss 0.00020827203115914017
W0314 13:00:04.979935 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:00:05.479427 39917 finetune.py:68] layer 5_down @ epoch 2 new loss 0.00013917860633227974 old loss 0.0001392100821249187 BETTER
I0314 13:00:06.454318 41230 finetune.py:68] layer 6_down @ epoch 0 new loss 0.00017757715249899775 old loss 0.00017786529497243464 BETTER
I0314 13:00:12.576334 38451 finetune.py:68] layer 4_down @ epoch 3 new loss 0.00010121020022779703 old loss 0.00010123051470145583 BETTER
I0314 13:00:35.097451 42575 finetune.py:68] layer 7_down @ epoch 0 new loss 0.00020792045688722283 old loss 0.00020827203115914017 BETTER
I0314 13:00:37.022321 39917 finetune.py:68] layer 5_down @ epoch 3 new loss 0.00013915188901592046 old loss 0.00013917860633227974 BETTER
I0314 13:00:38.183748 41230 finetune.py:68] layer 6_down @ epoch 1 new loss 0.00017751842096913606 old loss 0.00017757715249899775 BETTER
I0314 13:00:45.992399 38451 finetune.py:68] layer 4_down @ epoch 4 new loss 0.00010119027137989178 old loss 0.00010121020022779703 BETTER
W0314 13:00:46.758215 38451 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

4_down proxy err 0.04770166054368019 tr(WHW.T) 3.444077253341675
I0314 13:01:05.994749 42575 finetune.py:68] layer 7_down @ epoch 1 new loss 0.00020784248772542924 old loss 0.00020792045688722283 BETTER
I0314 13:01:08.521641 39917 finetune.py:68] layer 5_down @ epoch 4 new loss 0.00013912926078774035 old loss 0.00013915188901592046 BETTER
W0314 13:01:09.284452 39917 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

5_down proxy err 0.046745091676712036 tr(WHW.T) 4.9540300369262695
I0314 13:01:09.995939 41230 finetune.py:68] layer 6_down @ epoch 2 new loss 0.00017747700621839613 old loss 0.00017751842096913606 BETTER
I0314 13:01:36.411302 42575 finetune.py:68] layer 7_down @ epoch 2 new loss 0.00020779878832399845 old loss 0.00020784248772542924 BETTER
I0314 13:01:41.186889 41230 finetune.py:68] layer 6_down @ epoch 3 new loss 0.00017744720389600843 old loss 0.00017747700621839613 BETTER
I0314 13:02:07.132115 42575 finetune.py:68] layer 7_down @ epoch 3 new loss 0.00020775533630512655 old loss 0.00020779878832399845 BETTER
I0314 13:02:12.538337 41230 finetune.py:68] layer 6_down @ epoch 4 new loss 0.000177413021447137 old loss 0.00017744720389600843 BETTER
W0314 13:02:13.292431 41230 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

6_down proxy err 0.04479295760393143 tr(WHW.T) 6.287062644958496
I0314 13:02:19.846386 9966 quantize_finetune_llama.py:186] computed original embedding for layer 8 in 66.25649452209473s
I0314 13:02:20.249679 9966 quantize_finetune_llama.py:159] layer 9 gpu 1
I0314 13:02:22.299841 63332 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:02:22.299973 63332 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:02:22.300034 63332 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:02:22.475858 63332 config.py:58] PyTorch version 2.4.0 available.
I0314 13:02:24.769271 63332 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:02:25.217672 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:02<00:16,  1.78it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.11it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.57it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.90it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.96it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.98it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.00it/s] 41%|████      | 13/32 [00:05<00:06,  3.02it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.04it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.05it/s] 50%|█████     | 16/32 [00:06<00:05,  3.06it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.07it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.07it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.07it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.07it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.07it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.07it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.06it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.06it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.06it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.06it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.04it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.04it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.05it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.05it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.05it/s]I0314 13:02:37.754870 42575 finetune.py:68] layer 7_down @ epoch 4 new loss 0.00020771972776856273 old loss 0.00020775533630512655 BETTER
100%|██████████| 32/32 [00:11<00:00,  3.06it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]
W0314 13:02:38.442916 42575 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

7_down proxy err 0.04616284742951393 tr(WHW.T) 6.860259056091309
W0314 13:02:40.224000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.224000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.224000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.225000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.225000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.225000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.225000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.250000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.250000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.250000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.250000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.250000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.531000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.531000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.531000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.531000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:02:40.531000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.389000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.389000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.389000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.389000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.390000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.390000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.390000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.407000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.407000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.407000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.408000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.408000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.604000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.604000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.604000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.604000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:02:41.604000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.686000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.687000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.687000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.687000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.687000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.687000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.687000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.704000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.705000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.705000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.705000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:02:42.705000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:02:43.567000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:02:43.567000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:02:43.567000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:02:43.567000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:02:43.567000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:02:50.237610 63332 finetune.py:45] layer 8_v initial loss 3.321204349049367e-05
W0314 13:02:50.237854 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:03:22.303296 9966 quantize_finetune_llama.py:186] computed original embedding for layer 9 in 61.63119578361511s
I0314 13:03:22.701057 9966 quantize_finetune_llama.py:159] layer 10 gpu 2
I0314 13:03:24.820664 64659 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:03:24.820787 64659 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:03:24.820848 64659 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:03:25.011517 64659 config.py:58] PyTorch version 2.4.0 available.
I0314 13:03:26.720975 63332 finetune.py:68] layer 8_v @ epoch 0 new loss 1.8794873540173285e-05 old loss 3.321204349049367e-05 BETTER
I0314 13:03:27.201094 64659 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:03:27.681938 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.60s/it]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.84it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.85it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.87it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.86it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.89it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.90it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.90it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.89it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.93it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.92it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.90it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.91it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.90it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
W0314 13:03:43.509000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.509000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.509000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.509000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.509000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.509000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.510000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.537000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.537000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.537000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.537000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.538000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.836000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.837000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.837000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.837000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:03:43.837000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.703000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.703000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.703000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.703000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.703000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.703000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.703000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.722000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.722000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.722000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.722000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.722000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.922000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.922000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.922000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.922000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:03:44.922000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.047000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.047000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.047000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.047000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.047000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.047000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.047000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.067000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.068000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.068000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.068000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.068000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.935000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.935000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.935000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.936000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:03:46.936000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:03:53.687439 64659 finetune.py:45] layer 9_v initial loss 5.3926491091260687e-05
W0314 13:03:53.687656 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:04:04.413330 63332 finetune.py:68] layer 8_v @ epoch 1 new loss 1.747984060784802e-05 old loss 1.8794873540173285e-05 BETTER
I0314 13:04:24.887287 9966 quantize_finetune_llama.py:186] computed original embedding for layer 10 in 61.76989769935608s
I0314 13:04:25.332987 9966 quantize_finetune_llama.py:159] layer 11 gpu 3
I0314 13:04:27.378249 66015 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:04:27.378365 66015 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:04:27.378468 66015 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:04:27.568054 66015 config.py:58] PyTorch version 2.4.0 available.
I0314 13:04:28.180848 64659 finetune.py:68] layer 9_v @ epoch 0 new loss 2.7018086257157847e-05 old loss 5.3926491091260687e-05 BETTER
I0314 13:04:29.832326 66015 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:04:30.209148 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.61s/it]  6%|▋         | 2/32 [00:01<00:26,  1.14it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.18it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.75it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.83it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s]I0314 13:04:42.114184 63332 finetune.py:68] layer 8_v @ epoch 2 new loss 1.6886211597011425e-05 old loss 1.747984060784802e-05 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
W0314 13:04:46.196000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.196000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.196000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.196000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.196000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.196000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.196000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.223000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.223000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.224000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.224000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.224000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.523000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.524000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.524000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.524000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:04:46.524000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.401000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.401000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.401000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.401000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.401000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.401000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.401000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.420000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.420000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.420000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.420000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.420000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.627000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.627000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.627000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.627000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:04:47.627000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.797000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.797000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.797000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.797000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.797000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.797000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.797000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.816000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.816000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.816000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.816000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:04:48.816000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:04:49.704000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:04:49.705000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:04:49.705000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:04:49.705000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:04:49.705000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:04:55.676665 66015 finetune.py:45] layer 10_v initial loss 4.7439982154173777e-05
W0314 13:04:55.677053 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:05:03.845598 64659 finetune.py:68] layer 9_v @ epoch 1 new loss 2.4559805751778185e-05 old loss 2.7018086257157847e-05 BETTER
I0314 13:05:19.887014 63332 finetune.py:68] layer 8_v @ epoch 3 new loss 1.651947786740493e-05 old loss 1.6886211597011425e-05 BETTER
I0314 13:05:25.781388 9966 quantize_finetune_llama.py:186] computed original embedding for layer 11 in 59.94286799430847s
I0314 13:05:26.176118 9966 quantize_finetune_llama.py:159] layer 12 gpu 0
I0314 13:05:28.600467 67368 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:05:28.600589 67368 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:05:28.600653 67368 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:05:28.855788 67368 config.py:58] PyTorch version 2.4.0 available.
I0314 13:05:30.631046 66015 finetune.py:68] layer 10_v @ epoch 0 new loss 2.548717566241976e-05 old loss 4.7439982154173777e-05 BETTER
I0314 13:05:31.201147 67368 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:05:31.780682 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:55,  1.78s/it]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s]  9%|▉         | 3/32 [00:02<00:19,  1.48it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.82it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.09it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 41%|████      | 13/32 [00:06<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s]I0314 13:05:39.776008 64659 finetune.py:68] layer 9_v @ epoch 2 new loss 2.3507289370172657e-05 old loss 2.4559805751778185e-05 BETTER
 50%|█████     | 16/32 [00:07<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.85it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.85it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0314 13:05:47.818000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.819000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.819000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.819000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.819000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.819000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.819000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.846000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.846000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.847000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.847000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:05:47.847000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:05:48.144000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:05:48.144000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:05:48.144000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:05:48.144000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:05:48.144000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.009000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.009000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.009000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.009000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.009000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.009000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.010000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.028000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.028000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.029000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.029000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.029000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.228000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.228000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.228000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.228000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:05:49.228000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.355000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.355000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.355000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.355000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.355000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.355000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.355000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.374000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.374000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.374000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.374000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:05:50.374000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:05:51.259000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:05:51.260000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:05:51.260000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:05:51.260000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:05:51.260000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:05:58.084028 63332 finetune.py:68] layer 8_v @ epoch 4 new loss 1.6234287613769993e-05 old loss 1.651947786740493e-05 BETTER
I0314 13:05:58.526626 67368 finetune.py:45] layer 11_v initial loss 3.44854015565943e-05
W0314 13:05:58.526848 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0314 13:05:59.963967 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_v proxy err 0.03488663211464882 tr(WHW.T) 53.42280197143555
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:31,  1.00s/it]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s]I0314 13:06:06.169640 66015 finetune.py:68] layer 10_v @ epoch 1 new loss 2.36260693782242e-05 old loss 2.548717566241976e-05 BETTER
 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
I0314 13:06:15.910709 64659 finetune.py:68] layer 9_v @ epoch 3 new loss 2.2842632461106405e-05 old loss 2.3507289370172657e-05 BETTER
W0314 13:06:19.016000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.016000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.016000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.016000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.016000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.016000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.017000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.047000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.047000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.048000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.048000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.048000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.224000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.224000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.225000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.225000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.225000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.457000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.457000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.457000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.457000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.457000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.457000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.457000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.481000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.481000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.481000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.481000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.481000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.548000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.548000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.548000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.548000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:06:19.548000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.315000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.644000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.644000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.644000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.644000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.644000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.644000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.644000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.669000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.669000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.669000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.669000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.669000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.932000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.932000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.932000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.932000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:06:20.932000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:06:21.301000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 13:06:28.141021 63332 finetune.py:45] layer 8_q initial loss 3.148120958940126e-05
W0314 13:06:28.141423 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:06:32.013881 67368 finetune.py:68] layer 11_v @ epoch 0 new loss 2.198578840761911e-05 old loss 3.44854015565943e-05 BETTER
I0314 13:06:42.351617 66015 finetune.py:68] layer 10_v @ epoch 2 new loss 2.2756761609343812e-05 old loss 2.36260693782242e-05 BETTER
I0314 13:06:52.461669 64659 finetune.py:68] layer 9_v @ epoch 4 new loss 2.237675835203845e-05 old loss 2.2842632461106405e-05 BETTER
W0314 13:06:54.157677 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_v proxy err 0.035444267094135284 tr(WHW.T) 72.69827270507812
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:14,  1.94it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s]I0314 13:07:04.873602 63332 finetune.py:68] layer 8_q @ epoch 0 new loss 3.0034179872018285e-05 old loss 3.148120958940126e-05 BETTER
 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s]I0314 13:07:06.895173 67368 finetune.py:68] layer 11_v @ epoch 1 new loss 2.0661480448325165e-05 old loss 2.198578840761911e-05 BETTER
 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
W0314 13:07:13.475000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.475000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.475000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.475000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.475000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.476000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.476000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.506000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.507000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.507000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.507000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.507000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.678000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.678000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.678000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.678000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.679000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.909000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.909000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.909000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.909000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.909000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.909000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.909000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.932000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.932000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.932000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.933000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:07:13.933000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:07:14.001000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:07:14.001000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:07:14.001000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:07:14.001000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:07:14.001000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:07:14.744000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.065000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.066000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.066000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.066000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.066000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.066000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.066000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.089000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.089000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.089000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.089000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.089000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.349000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.350000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.350000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.350000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.350000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:07:15.707000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 13:07:18.505485 66015 finetune.py:68] layer 10_v @ epoch 3 new loss 2.2236077711568214e-05 old loss 2.2756761609343812e-05 BETTER
I0314 13:07:22.489417 64659 finetune.py:45] layer 9_q initial loss 3.930459570256062e-05
W0314 13:07:22.489825 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:07:42.215052 67368 finetune.py:68] layer 11_v @ epoch 2 new loss 2.0017776478198357e-05 old loss 2.0661480448325165e-05 BETTER
I0314 13:07:42.561234 63332 finetune.py:68] layer 8_q @ epoch 1 new loss 2.9376069505815394e-05 old loss 3.0034179872018285e-05 BETTER
I0314 13:07:54.973386 66015 finetune.py:68] layer 10_v @ epoch 4 new loss 2.184078039135784e-05 old loss 2.2236077711568214e-05 BETTER
W0314 13:07:56.719393 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 13:07:57.662194 64659 finetune.py:68] layer 9_q @ epoch 0 new loss 3.7648995203198865e-05 old loss 3.930459570256062e-05 BETTER
10_v proxy err 0.033175814896821976 tr(WHW.T) 60.08286666870117
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.61it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0314 13:08:15.629000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.629000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.629000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.629000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.629000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.629000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.630000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.660000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.661000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.661000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.661000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.661000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.836000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.836000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.836000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.836000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:08:15.836000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.068000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.068000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.068000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.068000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.068000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.069000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.069000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.089000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.089000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.089000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.089000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.090000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.161000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.161000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.161000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.161000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.161000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:08:16.919000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.249000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.249000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.249000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.250000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.250000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.250000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.250000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.273000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.273000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.273000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.273000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.274000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
I0314 13:08:17.428969 67368 finetune.py:68] layer 11_v @ epoch 3 new loss 1.987355062738061e-05 old loss 2.0017776478198357e-05 BETTER
W0314 13:08:17.536000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.536000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.537000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.537000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.537000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:08:17.896000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 13:08:20.526083 63332 finetune.py:68] layer 8_q @ epoch 2 new loss 2.8833099349867553e-05 old loss 2.9376069505815394e-05 BETTER
I0314 13:08:24.481618 66015 finetune.py:45] layer 10_q initial loss 4.133199399802834e-05
W0314 13:08:24.482000 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:08:33.674291 64659 finetune.py:68] layer 9_q @ epoch 1 new loss 3.6775942135136575e-05 old loss 3.7648995203198865e-05 BETTER
I0314 13:08:52.807762 67368 finetune.py:68] layer 11_v @ epoch 4 new loss 1.9420798707869835e-05 old loss 1.987355062738061e-05 BETTER
W0314 13:08:54.591657 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_v proxy err 0.02733604609966278 tr(WHW.T) 74.2698974609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:31,  1.00s/it]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.15it/s]I0314 13:08:58.449699 63332 finetune.py:68] layer 8_q @ epoch 3 new loss 2.840871638909448e-05 old loss 2.8833099349867553e-05 BETTER
 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s]I0314 13:08:59.500428 66015 finetune.py:68] layer 10_q @ epoch 0 new loss 3.9284721424337476e-05 old loss 4.133199399802834e-05 BETTER
 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0314 13:09:09.339496 64659 finetune.py:68] layer 9_q @ epoch 2 new loss 3.61351776518859e-05 old loss 3.6775942135136575e-05 BETTER
W0314 13:09:14.114000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.115000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.115000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.115000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.115000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.115000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.115000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.147000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.147000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.147000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.147000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.147000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.319000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.320000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.320000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.320000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.320000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.558000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.559000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.559000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.559000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.559000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.559000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.559000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.582000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.583000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.583000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.583000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.583000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.656000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.656000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.656000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.656000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:09:14.656000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.414000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.743000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.743000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.743000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.743000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.743000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.743000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.744000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.766000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.766000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.766000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.766000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:09:15.766000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:09:16.032000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:09:16.032000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:09:16.032000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:09:16.033000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:09:16.033000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:09:16.395000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 13:09:22.774172 67368 finetune.py:45] layer 11_q initial loss 4.025843372801319e-05
W0314 13:09:22.774421 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:09:35.193983 66015 finetune.py:68] layer 10_q @ epoch 1 new loss 3.8351452531060204e-05 old loss 3.9284721424337476e-05 BETTER
I0314 13:09:36.392602 63332 finetune.py:68] layer 8_q @ epoch 4 new loss 2.8053464120603167e-05 old loss 2.840871638909448e-05 BETTER
W0314 13:09:37.991792 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_q proxy err 0.00637131417170167 tr(WHW.T) 5517.46875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s]I0314 13:09:45.375287 64659 finetune.py:68] layer 9_q @ epoch 3 new loss 3.5606401070253924e-05 old loss 3.61351776518859e-05 BETTER
 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.80it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
I0314 13:09:56.818149 67368 finetune.py:68] layer 11_q @ epoch 0 new loss 3.8263726310105994e-05 old loss 4.025843372801319e-05 BETTER
I0314 13:09:58.184085 63332 finetune.py:45] layer 8_k initial loss 3.872192246490158e-05
W0314 13:09:58.184475 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:10:11.116231 66015 finetune.py:68] layer 10_q @ epoch 2 new loss 3.762986307265237e-05 old loss 3.8351452531060204e-05 BETTER
I0314 13:10:21.223247 64659 finetune.py:68] layer 9_q @ epoch 4 new loss 3.526089494698681e-05 old loss 3.5606401070253924e-05 BETTER
W0314 13:10:23.031126 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_q proxy err 0.006729863118380308 tr(WHW.T) 5309.29931640625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]  9%|▉         | 3/32 [00:01<00:14,  1.99it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s]I0314 13:10:31.901761 67368 finetune.py:68] layer 11_q @ epoch 1 new loss 3.7355908716563135e-05 old loss 3.8263726310105994e-05 BETTER
 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:04,  2.26it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.36it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.44it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.49it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.53it/s]I0314 13:10:34.950931 63332 finetune.py:68] layer 8_k @ epoch 0 new loss 3.531372203724459e-05 old loss 3.872192246490158e-05 BETTER
 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0314 13:10:44.465707 64659 finetune.py:45] layer 9_k initial loss 4.607303344528191e-05
W0314 13:10:44.466064 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:10:47.371252 66015 finetune.py:68] layer 10_q @ epoch 3 new loss 3.707125870278105e-05 old loss 3.762986307265237e-05 BETTER
I0314 13:11:07.647891 67368 finetune.py:68] layer 11_q @ epoch 2 new loss 3.669424040708691e-05 old loss 3.7355908716563135e-05 BETTER
I0314 13:11:12.864552 63332 finetune.py:68] layer 8_k @ epoch 1 new loss 3.4800632420228794e-05 old loss 3.531372203724459e-05 BETTER
I0314 13:11:19.438632 64659 finetune.py:68] layer 9_k @ epoch 0 new loss 4.355487908469513e-05 old loss 4.607303344528191e-05 BETTER
I0314 13:11:23.704455 66015 finetune.py:68] layer 10_q @ epoch 4 new loss 3.660503352875821e-05 old loss 3.707125870278105e-05 BETTER
W0314 13:11:25.417269 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_q proxy err 0.007172081153839827 tr(WHW.T) 5571.84326171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
I0314 13:11:42.829688 67368 finetune.py:68] layer 11_q @ epoch 3 new loss 3.6131139495410025e-05 old loss 3.669424040708691e-05 BETTER
I0314 13:11:46.280420 66015 finetune.py:45] layer 10_k initial loss 4.608257586369291e-05
W0314 13:11:46.280870 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:11:50.868924 63332 finetune.py:68] layer 8_k @ epoch 2 new loss 3.445146285230294e-05 old loss 3.4800632420228794e-05 BETTER
I0314 13:11:55.517023 64659 finetune.py:68] layer 9_k @ epoch 1 new loss 4.30178661190439e-05 old loss 4.355487908469513e-05 BETTER
I0314 13:12:18.174746 67368 finetune.py:68] layer 11_q @ epoch 4 new loss 3.565237420843914e-05 old loss 3.6131139495410025e-05 BETTER
W0314 13:12:19.967694 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_q proxy err 0.007204624358564615 tr(WHW.T) 5161.2978515625
  0%|          | 0/32 [00:00<?, ?it/s]I0314 13:12:21.637216 66015 finetune.py:68] layer 10_k @ epoch 0 new loss 4.4717897253576666e-05 old loss 4.608257586369291e-05 BETTER
  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s]  9%|▉         | 3/32 [00:01<00:14,  1.99it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.60it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s]I0314 13:12:29.014725 63332 finetune.py:68] layer 8_k @ epoch 3 new loss 3.416087565710768e-05 old loss 3.445146285230294e-05 BETTER
 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s]I0314 13:12:31.950478 64659 finetune.py:68] layer 9_k @ epoch 2 new loss 4.2591018427629024e-05 old loss 4.30178661190439e-05 BETTER
 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0314 13:12:40.741113 67368 finetune.py:45] layer 11_k initial loss 4.666463428293355e-05
W0314 13:12:40.741473 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:12:57.869032 66015 finetune.py:68] layer 10_k @ epoch 1 new loss 4.419686592882499e-05 old loss 4.4717897253576666e-05 BETTER
I0314 13:13:07.405800 63332 finetune.py:68] layer 8_k @ epoch 4 new loss 3.3917818655027077e-05 old loss 3.416087565710768e-05 BETTER
I0314 13:13:08.133238 64659 finetune.py:68] layer 9_k @ epoch 3 new loss 4.221313429297879e-05 old loss 4.2591018427629024e-05 BETTER
W0314 13:13:09.167831 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_k proxy err 0.004701891914010048 tr(WHW.T) 4648.4248046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.28it/s]  6%|▋         | 2/32 [00:01<00:15,  1.88it/s]  9%|▉         | 3/32 [00:01<00:13,  2.21it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.40it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.60it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.66it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s]I0314 13:13:14.757482 67368 finetune.py:68] layer 11_k @ epoch 0 new loss 4.4274092942941934e-05 old loss 4.666463428293355e-05 BETTER
 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.75it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.75it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
I0314 13:13:29.810992 63332 finetune.py:45] layer 8_o initial loss 6.365236913552508e-05
W0314 13:13:29.811570 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:13:34.308607 66015 finetune.py:68] layer 10_k @ epoch 2 new loss 4.380087193567306e-05 old loss 4.419686592882499e-05 BETTER
I0314 13:13:44.220479 64659 finetune.py:68] layer 9_k @ epoch 4 new loss 4.192695632809773e-05 old loss 4.221313429297879e-05 BETTER
W0314 13:13:45.896441 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_k proxy err 0.005130130331963301 tr(WHW.T) 4316.0458984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s]I0314 13:13:49.821190 67368 finetune.py:68] layer 11_k @ epoch 1 new loss 4.3791733332909644e-05 old loss 4.4274092942941934e-05 BETTER
 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0314 13:14:06.707428 63332 finetune.py:68] layer 8_o @ epoch 0 new loss 6.140516779851168e-05 old loss 6.365236913552508e-05 BETTER
I0314 13:14:07.156976 64659 finetune.py:45] layer 9_o initial loss 7.821803592378274e-05
W0314 13:14:07.157262 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:14:10.603721 66015 finetune.py:68] layer 10_k @ epoch 3 new loss 4.348944639787078e-05 old loss 4.380087193567306e-05 BETTER
I0314 13:14:24.900372 67368 finetune.py:68] layer 11_k @ epoch 2 new loss 4.3336076487321407e-05 old loss 4.3791733332909644e-05 BETTER
I0314 13:14:42.359869 64659 finetune.py:68] layer 9_o @ epoch 0 new loss 7.485551032004878e-05 old loss 7.821803592378274e-05 BETTER
I0314 13:14:44.339875 63332 finetune.py:68] layer 8_o @ epoch 1 new loss 6.054087862139568e-05 old loss 6.140516779851168e-05 BETTER
I0314 13:14:46.974237 66015 finetune.py:68] layer 10_k @ epoch 4 new loss 4.322448876337148e-05 old loss 4.348944639787078e-05 BETTER
W0314 13:14:48.622685 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_k proxy err 0.005182192195206881 tr(WHW.T) 4696.7509765625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.19it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.64it/s]I0314 13:15:00.062674 67368 finetune.py:68] layer 11_k @ epoch 3 new loss 4.294897735235281e-05 old loss 4.3336076487321407e-05 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0314 13:15:09.729342 66015 finetune.py:45] layer 10_o initial loss 7.787797221681103e-05
W0314 13:15:09.729645 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:15:18.024637 64659 finetune.py:68] layer 9_o @ epoch 1 new loss 7.372495747404173e-05 old loss 7.485551032004878e-05 BETTER
I0314 13:15:21.998026 63332 finetune.py:68] layer 8_o @ epoch 2 new loss 5.990407589706592e-05 old loss 6.054087862139568e-05 BETTER
I0314 13:15:35.495303 67368 finetune.py:68] layer 11_k @ epoch 4 new loss 4.2740000935737044e-05 old loss 4.294897735235281e-05 BETTER
W0314 13:15:37.142077 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_k proxy err 0.005880304146558046 tr(WHW.T) 4171.14599609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.10it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.29it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.63it/s]I0314 13:15:45.671293 66015 finetune.py:68] layer 10_o @ epoch 0 new loss 7.472738798242062e-05 old loss 7.787797221681103e-05 BETTER
 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.13it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.26it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.37it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.45it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0314 13:15:54.734591 64659 finetune.py:68] layer 9_o @ epoch 2 new loss 7.291018118849024e-05 old loss 7.372495747404173e-05 BETTER
I0314 13:15:58.660549 67368 finetune.py:45] layer 11_o initial loss 7.879235636210069e-05
W0314 13:15:58.660985 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:15:59.673384 63332 finetune.py:68] layer 8_o @ epoch 3 new loss 5.938982212683186e-05 old loss 5.990407589706592e-05 BETTER
I0314 13:16:21.980398 66015 finetune.py:68] layer 10_o @ epoch 1 new loss 7.37067312002182e-05 old loss 7.472738798242062e-05 BETTER
I0314 13:16:31.002440 64659 finetune.py:68] layer 9_o @ epoch 3 new loss 7.22624099580571e-05 old loss 7.291018118849024e-05 BETTER
I0314 13:16:33.109175 67368 finetune.py:68] layer 11_o @ epoch 0 new loss 7.603615813422948e-05 old loss 7.879235636210069e-05 BETTER
I0314 13:16:37.512241 63332 finetune.py:68] layer 8_o @ epoch 4 new loss 5.8939778682542965e-05 old loss 5.938982212683186e-05 BETTER
W0314 13:16:39.234068 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_o proxy err 0.041482601314783096 tr(WHW.T) 3.7795610427856445
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.50s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it]I0314 13:16:58.341778 66015 finetune.py:68] layer 10_o @ epoch 2 new loss 7.291952351806685e-05 old loss 7.37067312002182e-05 BETTER
 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it]I0314 13:17:06.767835 64659 finetune.py:68] layer 9_o @ epoch 4 new loss 7.169952732510865e-05 old loss 7.22624099580571e-05 BETTER
 56%|█████▋    | 18/32 [00:27<00:20,  1.47s/it]W0314 13:17:08.383389 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 13:17:08.422935 67368 finetune.py:68] layer 11_o @ epoch 1 new loss 7.495332829421386e-05 old loss 7.603615813422948e-05 BETTER
 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it]9_o proxy err 0.04218559339642525 tr(WHW.T) 4.49445915222168
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it]  9%|▉         | 3/32 [00:05<00:46,  1.61s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.48s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.51s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it]I0314 13:17:34.145884 66015 finetune.py:68] layer 10_o @ epoch 3 new loss 7.229873153846711e-05 old loss 7.291952351806685e-05 BETTER
 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it]I0314 13:17:36.667835 63332 finetune.py:45] layer 8_up initial loss 0.00012602881179191172
W0314 13:17:36.668224 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it]I0314 13:17:43.507288 67368 finetune.py:68] layer 11_o @ epoch 2 new loss 7.415736035909504e-05 old loss 7.495332829421386e-05 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
I0314 13:18:06.457793 64659 finetune.py:45] layer 9_up initial loss 0.00014628902135882527
W0314 13:18:06.458095 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:18:10.133157 66015 finetune.py:68] layer 10_o @ epoch 4 new loss 7.17629591235891e-05 old loss 7.229873153846711e-05 BETTER
W0314 13:18:11.798763 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 13:18:12.686461 63332 finetune.py:68] layer 8_up @ epoch 0 new loss 0.0001242338039446622 old loss 0.00012602881179191172 BETTER
10_o proxy err 0.04138342663645744 tr(WHW.T) 4.258753299713135
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  2.00s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it]  9%|▉         | 3/32 [00:04<00:46,  1.61s/it]I0314 13:18:18.583463 67368 finetune.py:68] layer 11_o @ epoch 3 new loss 7.34906061552465e-05 old loss 7.415736035909504e-05 BETTER
 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it]I0314 13:18:40.283995 64659 finetune.py:68] layer 9_up @ epoch 0 new loss 0.000144158024340868 old loss 0.00014628902135882527 BETTER
 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it]I0314 13:18:49.317100 63332 finetune.py:68] layer 8_up @ epoch 1 new loss 0.0001231643691426143 old loss 0.0001242338039446622 BETTER
 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it]I0314 13:18:53.761279 67368 finetune.py:68] layer 11_o @ epoch 4 new loss 7.292998634511605e-05 old loss 7.34906061552465e-05 BETTER
 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it]W0314 13:18:55.304188 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_o proxy err 0.04287535697221756 tr(WHW.T) 4.536675453186035
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.50s/it]  3%|▎         | 1/32 [00:01<01:01,  2.00s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.49s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]
  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it]I0314 13:19:08.953505 66015 finetune.py:45] layer 10_up initial loss 0.00015018804697319865
W0314 13:19:08.953893 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it]I0314 13:19:15.112721 64659 finetune.py:68] layer 9_up @ epoch 1 new loss 0.0001428816613042727 old loss 0.000144158024340868 BETTER
 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.50s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.50s/it]I0314 13:19:26.175050 63332 finetune.py:68] layer 8_up @ epoch 2 new loss 0.00012224323290865868 old loss 0.0001231643691426143 BETTER
 62%|██████▎   | 20/32 [00:30<00:18,  1.50s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it]I0314 13:19:43.352574 66015 finetune.py:68] layer 10_up @ epoch 0 new loss 0.0001480483333580196 old loss 0.00015018804697319865 BETTER
 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0314 13:19:49.957858 64659 finetune.py:68] layer 9_up @ epoch 2 new loss 0.0001417985768057406 old loss 0.0001428816613042727 BETTER
I0314 13:19:53.283250 67368 finetune.py:45] layer 11_up initial loss 0.00015720234659966081
W0314 13:19:53.283620 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:20:02.683917 63332 finetune.py:68] layer 8_up @ epoch 3 new loss 0.00012139513273723423 old loss 0.00012224323290865868 BETTER
I0314 13:20:17.895234 66015 finetune.py:68] layer 10_up @ epoch 1 new loss 0.00014674040721729398 old loss 0.0001480483333580196 BETTER
I0314 13:20:24.082122 64659 finetune.py:68] layer 9_up @ epoch 3 new loss 0.0001408174284733832 old loss 0.0001417985768057406 BETTER
I0314 13:20:26.199974 67368 finetune.py:68] layer 11_up @ epoch 0 new loss 0.00015485282347071916 old loss 0.00015720234659966081 BETTER
I0314 13:20:38.912796 63332 finetune.py:68] layer 8_up @ epoch 4 new loss 0.00012062403402524069 old loss 0.00012139513273723423 BETTER
W0314 13:20:40.331902 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_up proxy err 0.037272609770298004 tr(WHW.T) 667.1614990234375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it]  9%|▉         | 3/32 [00:04<00:44,  1.55s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it]I0314 13:20:52.080213 66015 finetune.py:68] layer 10_up @ epoch 2 new loss 0.0001456358440918848 old loss 0.00014674040721729398 BETTER
 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it]I0314 13:20:58.506216 64659 finetune.py:68] layer 9_up @ epoch 4 new loss 0.00013991353625897318 old loss 0.0001408174284733832 BETTER
 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it]I0314 13:20:59.690488 67368 finetune.py:68] layer 11_up @ epoch 1 new loss 0.0001533970353193581 old loss 0.00015485282347071916 BETTER
W0314 13:20:59.875196 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:19<00:27,  1.47s/it]9_up proxy err 0.035841476172208786 tr(WHW.T) 722.2513427734375
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it]  9%|▉         | 3/32 [00:04<00:46,  1.62s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.46s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.46s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it]I0314 13:21:26.627580 66015 finetune.py:68] layer 10_up @ epoch 3 new loss 0.00014460616512224078 old loss 0.0001456358440918848 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.47s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it]I0314 13:21:33.153028 67368 finetune.py:68] layer 11_up @ epoch 2 new loss 0.00015213633014354855 old loss 0.0001533970353193581 BETTER
 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it]I0314 13:21:37.134373 63332 finetune.py:45] layer 8_gate initial loss 0.00016069976845756173
W0314 13:21:37.135243 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:36<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
I0314 13:21:58.058571 64659 finetune.py:45] layer 9_gate initial loss 0.00018406257731840014
W0314 13:21:58.058796 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:22:01.117886 66015 finetune.py:68] layer 10_up @ epoch 4 new loss 0.00014368121628649533 old loss 0.00014460616512224078 BETTER
W0314 13:22:02.669441 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_up proxy err 0.03634575009346008 tr(WHW.T) 747.0068969726562
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]I0314 13:22:06.887569 67368 finetune.py:68] layer 11_up @ epoch 3 new loss 0.00015099743905011564 old loss 0.00015213633014354855 BETTER
  6%|▋         | 2/32 [00:03<00:50,  1.69s/it]  9%|▉         | 3/32 [00:04<00:46,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it]I0314 13:22:11.389522 63332 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.0001590217580087483 old loss 0.00016069976845756173 BETTER
 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it]I0314 13:22:30.696765 64659 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0001822208141675219 old loss 0.00018406257731840014 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it]I0314 13:22:40.988858 67368 finetune.py:68] layer 11_up @ epoch 4 new loss 0.00014996578102000058 old loss 0.00015099743905011564 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it]W0314 13:22:42.412621 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it]11_up proxy err 0.0360957533121109 tr(WHW.T) 788.188720703125
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it]I0314 13:22:46.088338 63332 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.00015815859660506248 old loss 0.0001590217580087483 BETTER
 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it]I0314 13:23:00.556081 66015 finetune.py:45] layer 10_gate initial loss 0.00018991988326888531
W0314 13:23:00.556490 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:16<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:19<00:28,  1.51s/it]I0314 13:23:04.163564 64659 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.00018123943300452083 old loss 0.0001822208141675219 BETTER
 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it]I0314 13:23:21.383736 63332 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.000157406713697128 old loss 0.00015815859660506248 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0314 13:23:33.573677 66015 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.00018800779071170837 old loss 0.00018991988326888531 BETTER
I0314 13:23:37.667692 64659 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.0001803810300771147 old loss 0.00018123943300452083 BETTER
I0314 13:23:40.204389 67368 finetune.py:45] layer 11_gate initial loss 0.0001985214330488816
W0314 13:23:40.204761 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:23:56.564656 63332 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.00015672697918489575 old loss 0.000157406713697128 BETTER
I0314 13:24:06.996265 66015 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.00018700381042435765 old loss 0.00018800779071170837 BETTER
I0314 13:24:11.300217 64659 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.00017959623073693365 old loss 0.0001803810300771147 BETTER
I0314 13:24:12.401176 67368 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.00019665886065922678 old loss 0.0001985214330488816 BETTER
I0314 13:24:31.635802 63332 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.00015609357797075063 old loss 0.00015672697918489575 BETTER
W0314 13:24:32.918627 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_gate proxy err 0.014521438628435135 tr(WHW.T) 2908.92138671875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:35,  1.16it/s]  2%|▏         | 2/112 [00:01<01:02,  1.75it/s]  3%|▎         | 3/112 [00:01<00:52,  2.09it/s]  4%|▎         | 4/112 [00:01<00:46,  2.30it/s]  4%|▍         | 5/112 [00:02<00:43,  2.45it/s]  5%|▌         | 6/112 [00:02<00:41,  2.53it/s]  6%|▋         | 7/112 [00:03<00:40,  2.60it/s]  7%|▋         | 8/112 [00:03<00:39,  2.64it/s]  8%|▊         | 9/112 [00:03<00:38,  2.67it/s]I0314 13:24:40.565471 66015 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.00018613155407365412 old loss 0.00018700381042435765 BETTER
  9%|▉         | 10/112 [00:04<00:37,  2.70it/s] 10%|▉         | 11/112 [00:04<00:37,  2.72it/s] 11%|█         | 12/112 [00:04<00:36,  2.74it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.75it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.75it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.75it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.76it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.76it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.76it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.75it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.73it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.74it/s]I0314 13:24:44.601559 64659 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.0001788770459825173 old loss 0.00017959623073693365 BETTER
 20%|█▉        | 22/112 [00:08<00:32,  2.74it/s]I0314 13:24:45.063570 67368 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.00019557509222067893 old loss 0.00019665886065922678 BETTER
 21%|██        | 23/112 [00:08<00:32,  2.75it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.75it/s]W0314 13:24:45.739768 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 25/112 [00:09<00:31,  2.76it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.76it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.76it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.75it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.75it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.75it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.76it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.75it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.73it/s]9_gate proxy err 0.014022957533597946 tr(WHW.T) 3155.3232421875
  0%|          | 0/112 [00:00<?, ?it/s] 30%|███       | 34/112 [00:12<00:28,  2.75it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.75it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.75it/s]  1%|          | 1/112 [00:00<01:35,  1.16it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.76it/s]  2%|▏         | 2/112 [00:01<01:03,  1.73it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.75it/s]  3%|▎         | 3/112 [00:01<00:53,  2.05it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s]  4%|▎         | 4/112 [00:01<00:47,  2.25it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.75it/s]  4%|▍         | 5/112 [00:02<00:44,  2.38it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.75it/s]  5%|▌         | 6/112 [00:02<00:43,  2.47it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.74it/s]  6%|▋         | 7/112 [00:03<00:41,  2.52it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.75it/s]  7%|▋         | 8/112 [00:03<00:40,  2.56it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.74it/s]  8%|▊         | 9/112 [00:03<00:39,  2.58it/s] 40%|████      | 45/112 [00:16<00:24,  2.75it/s]  9%|▉         | 10/112 [00:04<00:39,  2.60it/s] 41%|████      | 46/112 [00:17<00:24,  2.73it/s] 10%|▉         | 11/112 [00:04<00:38,  2.60it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.73it/s] 11%|█         | 12/112 [00:05<00:38,  2.58it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.75it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.60it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.75it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.61it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.74it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.62it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.75it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.75it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.63it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.73it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.64it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.74it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.65it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.74it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.65it/s] 50%|█████     | 56/112 [00:20<00:20,  2.73it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.63it/s] 51%|█████     | 57/112 [00:21<00:20,  2.74it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.64it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 21%|██        | 23/112 [00:09<00:33,  2.63it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.73it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.74it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.62it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.74it/s] 22%|██▏       | 25/112 [00:09<00:33,  2.60it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.74it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.74it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.74it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.61it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.74it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.61it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.61it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.74it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.63it/s] 61%|██████    | 68/112 [00:25<00:16,  2.73it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.74it/s] 29%|██▉       | 33/112 [00:13<00:29,  2.64it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.73it/s] 30%|███       | 34/112 [00:13<00:29,  2.60it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.73it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.71it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.73it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.59it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.74it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.61it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.73it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.62it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.73it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.62it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.74it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.63it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.73it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.64it/s] 71%|███████   | 79/112 [00:29<00:12,  2.74it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.64it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.74it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.64it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 40%|████      | 45/112 [00:17<00:25,  2.64it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.73it/s] 41%|████      | 46/112 [00:17<00:25,  2.64it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.73it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.63it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.74it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.72it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.63it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.72it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.60it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.73it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.61it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.73it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.61it/s] 79%|███████▉  | 89/112 [00:32<00:08,  2.73it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.61it/s] 80%|████████  | 90/112 [00:33<00:08,  2.73it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.61it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.73it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.62it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.73it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.62it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.73it/s] 50%|█████     | 56/112 [00:21<00:21,  2.63it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.73it/s] 51%|█████     | 57/112 [00:22<00:20,  2.63it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.73it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.62it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.73it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.62it/s] 87%|████████▋ | 97/112 [00:35<00:05,  2.73it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.62it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.71it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.60it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.73it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.61it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.72it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.61it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.73it/s]I0314 13:25:13.920844 66015 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.0001853297872003168 old loss 0.00018613155407365412 BETTER
 57%|█████▋    | 64/112 [00:24<00:18,  2.62it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.74it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.62it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.73it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.63it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.73it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.63it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.73it/s] 61%|██████    | 68/112 [00:26<00:16,  2.63it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.73it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.64it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.73it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.64it/s] 96%|█████████▋| 108/112 [00:39<00:01,  2.73it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.64it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.73it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.73it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.62it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.70it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.62it/s]I0314 13:25:17.724895 67368 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.00019462908676359802 old loss 0.00019557509222067893 BETTER
100%|██████████| 112/112 [00:41<00:00,  2.73it/s]100%|██████████| 112/112 [00:41<00:00,  2.71it/s]
 66%|██████▌   | 74/112 [00:28<00:14,  2.60it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.61it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.62it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.63it/s] 71%|███████   | 79/112 [00:30<00:12,  2.64it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.64it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.65it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.66it/s] 74%|███████▍  | 83/112 [00:32<00:10,  2.66it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.66it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.65it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.64it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.61it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.63it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.64it/s] 80%|████████  | 90/112 [00:34<00:08,  2.65it/s]W0314 13:25:23.974000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:25:23.975000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:23.975000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:23.975000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:23.975000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:23.975000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:23.975000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.020000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.020000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.020000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.020000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.020000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.196000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.196000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.196000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.196000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.196000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:35<00:07,  2.65it/s]W0314 13:25:24.518000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.518000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.518000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.518000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.519000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.519000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.519000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.553000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.553000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.553000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.553000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.553000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.622000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.622000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.622000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.622000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:24.622000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:35<00:07,  2.65it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.65it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.65it/s]W0314 13:25:25.592000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:25.607000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:25.615000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:25.615000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 95/112 [00:36<00:06,  2.64it/s]W0314 13:25:26.061000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.061000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.061000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.061000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.061000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.061000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.061000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.093000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.093000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.093000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.094000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.094000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 96/112 [00:37<00:06,  2.64it/s]W0314 13:25:26.395000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.395000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.395000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.395000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.395000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.395000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.395000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.395000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:37<00:05,  2.63it/s]W0314 13:25:26.696000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.696000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.696000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.696000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:26.696000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:37<00:05,  2.63it/s]W0314 13:25:27.129000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:27.134000 140182040184640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:38<00:05,  2.60it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.61it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.61it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.62it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.62it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.62it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.63it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.63it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.63it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.62it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.62it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.59it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.61it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]
I0314 13:25:34.367093 63332 finetune.py:45] layer 8_down initial loss 0.00023014885664451867
W0314 13:25:34.368603 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0314 13:25:38.783000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.783000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.783000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.784000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.784000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.784000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.784000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.829000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.829000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.829000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.829000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:38.829000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.005000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.005000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.005000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.005000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.005000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.326000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.326000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.326000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.326000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.326000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.327000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.327000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.359000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.359000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.359000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.359000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.359000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.429000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.429000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.429000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.429000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:39.429000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.400000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.406000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.414000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.414000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.866000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.867000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.867000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.867000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.867000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.867000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.867000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.900000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.900000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.900000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.900000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:40.900000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.215000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.215000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.215000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.215000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.215000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.216000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.216000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.216000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.515000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.515000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.515000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.515000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.516000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.948000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:25:41.953000 139677320320832 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 13:25:47.452107 66015 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.00018459424609318376 old loss 0.0001853297872003168 BETTER
W0314 13:25:48.907332 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 13:25:49.393902 64659 finetune.py:45] layer 9_down initial loss 0.00025985768297687173
W0314 13:25:49.394330 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:25:50.584738 67368 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.00019375263946130872 old loss 0.00019462908676359802 BETTER
10_gate proxy err 0.014195493422448635 tr(WHW.T) 3014.98486328125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.11it/s]  2%|▏         | 2/112 [00:01<01:05,  1.67it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s]  4%|▍         | 5/112 [00:02<00:46,  2.33it/s]  5%|▌         | 6/112 [00:02<00:43,  2.41it/s]  6%|▋         | 7/112 [00:03<00:42,  2.48it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s]  9%|▉         | 10/112 [00:04<00:39,  2.58it/s] 10%|▉         | 11/112 [00:04<00:38,  2.59it/s] 11%|█         | 12/112 [00:05<00:38,  2.60it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.62it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.62it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.62it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.63it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.59it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.61it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.61it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.62it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.63it/s] 21%|██        | 23/112 [00:09<00:33,  2.63it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.64it/s] 22%|██▏       | 25/112 [00:10<00:32,  2.64it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.65it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.64it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.64it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.64it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.64it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.61it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.62it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.63it/s] 30%|███       | 34/112 [00:13<00:29,  2.63it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.64it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.65it/s]I0314 13:26:07.162381 63332 finetune.py:68] layer 8_down @ epoch 0 new loss 0.00022971353610046208 old loss 0.00023014885664451867 BETTER
 33%|███▎      | 37/112 [00:14<00:28,  2.65it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.65it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.65it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.64it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.64it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.64it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.60it/s] 40%|████      | 45/112 [00:17<00:25,  2.62it/s] 41%|████      | 46/112 [00:18<00:25,  2.63it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.63it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.64it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.64it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.64it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.64it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.65it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.64it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.63it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.63it/s] 50%|█████     | 56/112 [00:21<00:21,  2.59it/s] 51%|█████     | 57/112 [00:22<00:21,  2.61it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.62it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.63it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.64it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.65it/s] 55%|█████▌    | 62/112 [00:24<00:18,  2.65it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.65it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.65it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.64it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.65it/s] 60%|█████▉    | 67/112 [00:25<00:17,  2.64it/s] 61%|██████    | 68/112 [00:26<00:16,  2.64it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.59it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.61it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.62it/s]I0314 13:26:20.285939 64659 finetune.py:68] layer 9_down @ epoch 0 new loss 0.0002593528479337692 old loss 0.00025985768297687173 BETTER
 64%|██████▍   | 72/112 [00:27<00:15,  2.63it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.64it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.64it/s] 67%|██████▋   | 75/112 [00:29<00:13,  2.65it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.65it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.65it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.66it/s]I0314 13:26:23.019768 67368 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.00019293320656288415 old loss 0.00019375263946130872 BETTER
 71%|███████   | 79/112 [00:30<00:12,  2.66it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.65it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.65it/s]W0314 13:26:24.207454 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 73%|███████▎  | 82/112 [00:31<00:11,  2.60it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.62it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.63it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.64it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.65it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.64it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.64it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.64it/s] 80%|████████  | 90/112 [00:34<00:08,  2.65it/s]11_gate proxy err 0.0137671809643507 tr(WHW.T) 3134.45947265625
  0%|          | 0/112 [00:00<?, ?it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.66it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.65it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.64it/s]  1%|          | 1/112 [00:00<01:40,  1.10it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.64it/s]  2%|▏         | 2/112 [00:01<01:05,  1.67it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.61it/s]  3%|▎         | 3/112 [00:01<00:54,  2.01it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.62it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.63it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.63it/s]  5%|▌         | 6/112 [00:02<00:43,  2.45it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.64it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.65it/s]  7%|▋         | 8/112 [00:03<00:40,  2.56it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.64it/s]  8%|▊         | 9/112 [00:03<00:39,  2.58it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.64it/s]  9%|▉         | 10/112 [00:04<00:39,  2.60it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.64it/s] 10%|▉         | 11/112 [00:04<00:38,  2.61it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.65it/s] 11%|█         | 12/112 [00:05<00:38,  2.62it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.65it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.61it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.60it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.64it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.60it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.61it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.62it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.62it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.64it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.64it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.65it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.64it/s]100%|██████████| 112/112 [00:43<00:00,  2.65it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]
 18%|█▊        | 20/112 [00:08<00:34,  2.65it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.65it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.65it/s] 21%|██        | 23/112 [00:09<00:33,  2.65it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.65it/s] 22%|██▏       | 25/112 [00:10<00:32,  2.64it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.64it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.64it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.61it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.63it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.64it/s]I0314 13:26:40.346438 63332 finetune.py:68] layer 8_down @ epoch 1 new loss 0.00022963021183386445 old loss 0.00022971353610046208 BETTER
 29%|██▊       | 32/112 [00:12<00:30,  2.64it/s] 29%|██▉       | 33/112 [00:13<00:29,  2.65it/s] 30%|███       | 34/112 [00:13<00:29,  2.65it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.65it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.65it/s]W0314 13:26:42.053000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.054000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.054000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.054000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.054000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.055000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.055000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.101000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.101000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.101000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.102000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.102000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 33%|███▎      | 37/112 [00:14<00:28,  2.64it/s]W0314 13:26:42.279000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.279000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.279000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.279000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.279000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.605000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.605000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.605000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.606000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.606000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.606000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.606000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.640000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.640000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.640000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.640000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.640000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 38/112 [00:14<00:28,  2.64it/s]W0314 13:26:42.711000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.711000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.711000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.711000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:26:42.711000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 35%|███▍      | 39/112 [00:15<00:27,  2.64it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.59it/s]W0314 13:26:43.710000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:43.725000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:43.734000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:43.734000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 37%|███▋      | 41/112 [00:16<00:27,  2.61it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.62it/s]W0314 13:26:44.201000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.202000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.202000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.202000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.202000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.202000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.202000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.256000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.256000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.256000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.256000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.256000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.561000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.562000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.562000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.562000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.562000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.562000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.562000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.562000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 43/112 [00:16<00:26,  2.63it/s]W0314 13:26:44.857000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.857000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.857000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.858000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:26:44.858000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 39%|███▉      | 44/112 [00:17<00:25,  2.64it/s]W0314 13:26:45.295000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:26:45.300000 140377995519808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 40%|████      | 45/112 [00:17<00:25,  2.65it/s] 41%|████      | 46/112 [00:17<00:24,  2.64it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.66it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.65it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.64it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.63it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.57it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.58it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.59it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.60it/s] 50%|█████     | 56/112 [00:21<00:21,  2.61it/s] 51%|█████     | 57/112 [00:22<00:21,  2.61it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.61it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.62it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.63it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.63it/s] 55%|█████▌    | 62/112 [00:24<00:18,  2.64it/s]I0314 13:26:52.172915 64659 finetune.py:68] layer 9_down @ epoch 1 new loss 0.0002592625096440315 old loss 0.0002593528479337692 BETTER
 56%|█████▋    | 63/112 [00:24<00:18,  2.63it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.62it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.57it/s]I0314 13:26:53.018300 66015 finetune.py:45] layer 10_down initial loss 0.00027055153623223305
W0314 13:26:53.018709 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 66/112 [00:25<00:17,  2.58it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.59it/s] 61%|██████    | 68/112 [00:26<00:16,  2.60it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.60it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.61it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.61it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.62it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.61it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.61it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.61it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.60it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.56it/s] 71%|███████   | 79/112 [00:30<00:12,  2.58it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.59it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.60it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.61it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.62it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.63it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.62it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.62it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.62it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.63it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.63it/s] 80%|████████  | 90/112 [00:34<00:08,  2.62it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.58it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.59it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.60it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.60it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.61it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.62it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.62it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.63it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.63it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.63it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.63it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.63it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.62it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.59it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.60it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.61it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.61it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.62it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.63it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.64it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.64it/s]100%|██████████| 112/112 [00:43<00:00,  2.64it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]
I0314 13:27:13.954175 63332 finetune.py:68] layer 8_down @ epoch 2 new loss 0.00022957843611948192 old loss 0.00022963021183386445 BETTER
W0314 13:27:17.757000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.757000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.757000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.757000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.757000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.758000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.758000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.801000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.801000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.801000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.801000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.801000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.983000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.983000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.983000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.984000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:17.984000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.310000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.311000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.311000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.311000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.312000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.312000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.312000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.346000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.346000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.346000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.346000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.346000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.418000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.418000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.418000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.418000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:27:18.418000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.415000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.430000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.438000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.438000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.911000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.912000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.912000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.912000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.912000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.912000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.912000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.945000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.945000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.945000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.945000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:27:19.945000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.253000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.253000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.253000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.253000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.254000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.254000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.254000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.254000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.560000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.561000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.561000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.561000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:27:20.561000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:27:21.006000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:27:21.012000 139628988585792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 13:27:24.070429 64659 finetune.py:68] layer 9_down @ epoch 2 new loss 0.00025918500614352524 old loss 0.0002592625096440315 BETTER
I0314 13:27:24.430290 66015 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0002700425102375448 old loss 0.00027055153623223305 BETTER
I0314 13:27:28.364904 67368 finetune.py:45] layer 11_down initial loss 0.0002830384182743728
W0314 13:27:28.365255 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:27:47.351322 63332 finetune.py:68] layer 8_down @ epoch 3 new loss 0.00022953331063035876 old loss 0.00022957843611948192 BETTER
I0314 13:27:55.541929 64659 finetune.py:68] layer 9_down @ epoch 3 new loss 0.0002591529628261924 old loss 0.00025918500614352524 BETTER
I0314 13:27:56.029445 66015 finetune.py:68] layer 10_down @ epoch 1 new loss 0.00026995749794878066 old loss 0.0002700425102375448 BETTER
I0314 13:27:58.590576 67368 finetune.py:68] layer 11_down @ epoch 0 new loss 0.00028249507886357605 old loss 0.0002830384182743728 BETTER
I0314 13:28:20.839056 63332 finetune.py:68] layer 8_down @ epoch 4 new loss 0.0002294895821250975 old loss 0.00022953331063035876 BETTER
W0314 13:28:21.658114 63332 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

8_down proxy err 0.04655182734131813 tr(WHW.T) 7.316054344177246
I0314 13:28:27.770308 64659 finetune.py:68] layer 9_down @ epoch 4 new loss 0.00025910016847774386 old loss 0.0002591529628261924 BETTER
I0314 13:28:28.415447 66015 finetune.py:68] layer 10_down @ epoch 2 new loss 0.00026989690377376974 old loss 0.00026995749794878066 BETTER
W0314 13:28:28.655782 64659 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

9_down proxy err 0.04634438082575798 tr(WHW.T) 8.078441619873047
I0314 13:28:30.180279 67368 finetune.py:68] layer 11_down @ epoch 1 new loss 0.00028239571838639677 old loss 0.00028249507886357605 BETTER
I0314 13:28:59.926740 66015 finetune.py:68] layer 10_down @ epoch 3 new loss 0.0002698469324968755 old loss 0.00026989690377376974 BETTER
I0314 13:29:01.397931 67368 finetune.py:68] layer 11_down @ epoch 2 new loss 0.00028234493220224977 old loss 0.00028239571838639677 BETTER
I0314 13:29:31.281836 66015 finetune.py:68] layer 10_down @ epoch 4 new loss 0.00026980979600921273 old loss 0.0002698469324968755 BETTER
W0314 13:29:32.157847 66015 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0314 13:29:32.268482 67368 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0002822888782247901 old loss 0.00028234493220224977 BETTER
10_down proxy err 0.046319760382175446 tr(WHW.T) 8.4949951171875
I0314 13:29:39.620375 9966 quantize_finetune_llama.py:186] computed original embedding for layer 12 in 66.968181848526s
I0314 13:29:40.041633 9966 quantize_finetune_llama.py:159] layer 13 gpu 1
I0314 13:29:42.064507 87963 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:29:42.064624 87963 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:29:42.064688 87963 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:29:42.257905 87963 config.py:58] PyTorch version 2.4.0 available.
I0314 13:29:44.518669 87963 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:29:44.877303 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.39s/it]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]  9%|▉         | 3/32 [00:02<00:16,  1.75it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.82it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.89it/s] 41%|████      | 13/32 [00:05<00:06,  2.92it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.97it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.97it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.96it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.99it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.98it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.99it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.99it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.99it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.01it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.02it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.02it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
W0314 13:30:00.149000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.150000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.150000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.150000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.150000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.150000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.150000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.177000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.177000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.177000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.177000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.178000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.478000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.478000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.478000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.478000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:30:00.478000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.369000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.369000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.370000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.370000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.370000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.370000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.370000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.388000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.389000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.389000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.389000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.389000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.595000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.595000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.595000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.595000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:30:01.596000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.721000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.721000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.721000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.721000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.721000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.722000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.722000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.740000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.740000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.740000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.740000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:30:02.740000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
I0314 13:30:03.019726 67368 finetune.py:68] layer 11_down @ epoch 4 new loss 0.0002822427486535162 old loss 0.0002822888782247901 BETTER
W0314 13:30:03.610000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:30:03.610000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:30:03.610000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:30:03.610000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:30:03.610000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:30:03.779406 67368 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

11_down proxy err 0.04490332677960396 tr(WHW.T) 9.154268264770508
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:30:10.090177 87963 finetune.py:45] layer 12_v initial loss 5.282509664539248e-05
W0314 13:30:10.090575 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:30:41.862046 9966 quantize_finetune_llama.py:186] computed original embedding for layer 13 in 61.39024043083191s
I0314 13:30:42.229647 9966 quantize_finetune_llama.py:159] layer 14 gpu 2
I0314 13:30:44.369012 89269 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:30:44.369122 89269 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:30:44.369183 89269 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:30:44.566805 89269 config.py:58] PyTorch version 2.4.0 available.
I0314 13:30:46.459959 87963 finetune.py:68] layer 12_v @ epoch 0 new loss 3.426669354666956e-05 old loss 5.282509664539248e-05 BETTER
I0314 13:30:46.819278 89269 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:30:47.152320 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:47,  1.54s/it]  6%|▋         | 2/32 [00:01<00:25,  1.20it/s]  9%|▉         | 3/32 [00:02<00:17,  1.64it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.95it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.88it/s] 50%|█████     | 16/32 [00:06<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.88it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.91it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.91it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.89it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.89it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.89it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.90it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
W0314 13:31:02.675000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.675000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.675000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.675000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.675000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.675000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.675000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.702000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.702000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.702000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.702000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.703000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.990000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.990000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.990000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.990000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:31:02.990000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.845000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.846000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.846000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.846000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.846000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.846000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.846000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.864000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.864000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.864000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.864000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:31:03.864000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:31:04.069000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:31:04.069000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:31:04.069000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:31:04.069000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:31:04.070000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.199000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.199000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.199000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.199000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.199000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.199000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.199000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.217000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.217000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.217000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.217000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:31:05.217000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:31:06.097000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:31:06.098000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:31:06.098000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:31:06.098000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:31:06.098000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:31:11.896368 89269 finetune.py:45] layer 13_v initial loss 4.6263085096143186e-05
W0314 13:31:11.896633 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:31:23.537827 87963 finetune.py:68] layer 12_v @ epoch 1 new loss 3.2166874007089064e-05 old loss 3.426669354666956e-05 BETTER
I0314 13:31:43.570411 9966 quantize_finetune_llama.py:186] computed original embedding for layer 14 in 60.86161470413208s
I0314 13:31:43.962373 9966 quantize_finetune_llama.py:159] layer 15 gpu 3
I0314 13:31:45.764923 89269 finetune.py:68] layer 13_v @ epoch 0 new loss 3.112294143647887e-05 old loss 4.6263085096143186e-05 BETTER
I0314 13:31:46.071960 90617 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:31:46.072079 90617 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:31:46.072150 90617 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:31:46.262348 90617 config.py:58] PyTorch version 2.4.0 available.
I0314 13:31:48.470402 90617 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:31:48.876366 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.59s/it]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s]  9%|▉         | 3/32 [00:02<00:18,  1.58it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.65it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.83it/s] 41%|████      | 13/32 [00:05<00:06,  2.85it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.85it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.85it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.85it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.86it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.86it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.87it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s]I0314 13:32:00.719390 87963 finetune.py:68] layer 12_v @ epoch 2 new loss 3.116950756520964e-05 old loss 3.2166874007089064e-05 BETTER
 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
W0314 13:32:04.589000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.589000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.589000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.590000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.590000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.590000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.590000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.617000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.617000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.618000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.618000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.618000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.916000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.917000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.917000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.917000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:32:04.917000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.804000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.805000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.805000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.805000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.805000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.805000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.805000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.823000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.823000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.823000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.823000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:32:05.823000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:32:06.032000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:32:06.032000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:32:06.032000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:32:06.032000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:32:06.032000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.193000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.193000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.193000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.193000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.193000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.193000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.193000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.211000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.211000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.211000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.211000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:32:07.211000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:32:08.107000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:32:08.107000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:32:08.107000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:32:08.107000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:32:08.107000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:32:14.691650 90617 finetune.py:45] layer 14_v initial loss 4.604685818776488e-05
W0314 13:32:14.692002 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:32:21.023399 89269 finetune.py:68] layer 13_v @ epoch 1 new loss 2.9264998374856077e-05 old loss 3.112294143647887e-05 BETTER
I0314 13:32:38.208406 87963 finetune.py:68] layer 12_v @ epoch 3 new loss 3.04158529615961e-05 old loss 3.116950756520964e-05 BETTER
I0314 13:32:45.537357 9966 quantize_finetune_llama.py:186] computed original embedding for layer 15 in 61.11236548423767s
I0314 13:32:45.971387 9966 quantize_finetune_llama.py:159] layer 16 gpu 0
I0314 13:32:47.995981 91955 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:32:47.996094 91955 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:32:47.996188 91955 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:32:48.219959 91955 config.py:58] PyTorch version 2.4.0 available.
I0314 13:32:49.595451 90617 finetune.py:68] layer 14_v @ epoch 0 new loss 3.262520476710051e-05 old loss 4.604685818776488e-05 BETTER
I0314 13:32:50.489055 91955 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:32:50.999742 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:50,  1.63s/it]  6%|▋         | 2/32 [00:01<00:26,  1.14it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s]I0314 13:32:56.630572 89269 finetune.py:68] layer 13_v @ epoch 2 new loss 2.82960736512905e-05 old loss 2.9264998374856077e-05 BETTER
 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
W0314 13:33:07.048000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.048000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.048000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.048000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.048000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.048000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.048000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.075000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.075000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.075000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.075000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.075000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.371000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.372000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.372000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.372000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:07.372000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.242000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.243000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.243000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.243000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.243000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.243000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.243000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.261000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.261000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.261000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.261000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.261000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.468000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.468000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.469000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.469000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:08.469000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.600000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.600000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.600000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.600000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.600000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.600000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.600000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.618000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.618000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.618000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.619000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:09.619000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:10.500000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:10.500000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:10.500000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:10.500000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:10.500000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:33:16.416538 87963 finetune.py:76] layer 12_v @ epoch 4 new loss 3.087307413807139e-05 old loss 3.04158529615961e-05 WORSE
I0314 13:33:17.205978 91955 finetune.py:45] layer 15_v initial loss 8.506146696163341e-05
W0314 13:33:17.206307 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0314 13:33:17.652201 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_v proxy err 0.03501548990607262 tr(WHW.T) 68.45219421386719
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.04it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s]I0314 13:33:25.296598 90617 finetune.py:68] layer 14_v @ epoch 1 new loss 3.093591658398509e-05 old loss 3.262520476710051e-05 BETTER
 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
I0314 13:33:32.408623 89269 finetune.py:68] layer 13_v @ epoch 3 new loss 2.7621705157798715e-05 old loss 2.82960736512905e-05 BETTER
W0314 13:33:36.602000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.602000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.602000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.602000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.603000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.603000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.603000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.634000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.634000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.634000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.634000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.634000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.811000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.811000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.811000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.812000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:36.812000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.049000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.049000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.049000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.050000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.050000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.050000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.050000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.072000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.072000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.072000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.072000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.072000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.142000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.142000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.142000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.142000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.142000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:37.916000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.258000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.259000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.259000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.259000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.259000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.259000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.259000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.284000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.284000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.284000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.284000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.284000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.557000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.558000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.558000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.558000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.558000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:33:38.926000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 13:33:45.993480 87963 finetune.py:45] layer 12_q initial loss 4.738507777801715e-05
W0314 13:33:45.993913 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:33:51.132819 91955 finetune.py:68] layer 15_v @ epoch 0 new loss 4.435864684637636e-05 old loss 8.506146696163341e-05 BETTER
I0314 13:34:01.291981 90617 finetune.py:68] layer 14_v @ epoch 2 new loss 3.0003599022165872e-05 old loss 3.093591658398509e-05 BETTER
I0314 13:34:08.350213 89269 finetune.py:68] layer 13_v @ epoch 4 new loss 2.7615507860900834e-05 old loss 2.7621705157798715e-05 BETTER
W0314 13:34:10.129410 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_v proxy err 0.036613211035728455 tr(WHW.T) 71.04966735839844
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.05it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:14,  1.99it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s]I0314 13:34:22.652189 87963 finetune.py:68] layer 12_q @ epoch 0 new loss 4.57948335679248e-05 old loss 4.738507777801715e-05 BETTER
 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0314 13:34:26.081134 91955 finetune.py:68] layer 15_v @ epoch 1 new loss 4.082357554580085e-05 old loss 4.435864684637636e-05 BETTER
W0314 13:34:29.409000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.410000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.410000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.410000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.410000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.410000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.410000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.442000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.442000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.442000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.442000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.442000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.620000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.620000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.620000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.620000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.620000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.852000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.852000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.853000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.853000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.853000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.853000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.853000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.875000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.875000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.876000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.876000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.876000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.943000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.943000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.943000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.943000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:34:29.943000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:34:30.698000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.020000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.020000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.020000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.020000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.020000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.020000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.021000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.047000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.047000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.047000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.047000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.047000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.312000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.312000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.313000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.313000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.313000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:34:31.686000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 13:34:37.851560 90617 finetune.py:68] layer 14_v @ epoch 3 new loss 2.9511313186958432e-05 old loss 3.0003599022165872e-05 BETTER
I0314 13:34:38.680339 89269 finetune.py:45] layer 13_q initial loss 4.828585224458948e-05
W0314 13:34:38.680862 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:35:00.965521 87963 finetune.py:68] layer 12_q @ epoch 1 new loss 4.473531953408383e-05 old loss 4.57948335679248e-05 BETTER
I0314 13:35:01.649888 91955 finetune.py:68] layer 15_v @ epoch 2 new loss 3.90659442928154e-05 old loss 4.082357554580085e-05 BETTER
I0314 13:35:13.872853 89269 finetune.py:68] layer 13_q @ epoch 0 new loss 4.609900861396454e-05 old loss 4.828585224458948e-05 BETTER
I0314 13:35:14.167839 90617 finetune.py:68] layer 14_v @ epoch 4 new loss 2.8844873668276705e-05 old loss 2.9511313186958432e-05 BETTER
W0314 13:35:15.845867 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_v proxy err 0.03354126587510109 tr(WHW.T) 74.802978515625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:15,  1.91it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.60it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0314 13:35:35.114000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.115000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.115000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.115000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.115000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.115000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.115000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.146000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.146000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.146000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.147000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.147000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.323000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.323000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.323000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.323000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.323000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.554000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.554000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.554000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.554000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.554000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.554000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.554000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.575000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.575000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.576000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.576000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.576000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.644000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.644000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.644000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.644000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:35:35.644000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.399000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.724000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.725000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.725000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.725000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.725000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.725000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.725000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.748000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.748000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.748000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.748000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:35:36.748000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:35:37.012000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:35:37.012000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:35:37.012000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:35:37.012000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:35:37.012000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
I0314 13:35:37.061493 91955 finetune.py:68] layer 15_v @ epoch 3 new loss 3.796823148149997e-05 old loss 3.90659442928154e-05 BETTER
W0314 13:35:37.381000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 13:35:38.598086 87963 finetune.py:68] layer 12_q @ epoch 2 new loss 4.3872114474652335e-05 old loss 4.473531953408383e-05 BETTER
I0314 13:35:44.280321 90617 finetune.py:45] layer 14_q initial loss 5.558525663218461e-05
W0314 13:35:44.280709 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:35:49.682354 89269 finetune.py:68] layer 13_q @ epoch 1 new loss 4.522244489635341e-05 old loss 4.609900861396454e-05 BETTER
I0314 13:36:13.502396 91955 finetune.py:68] layer 15_v @ epoch 4 new loss 3.719913729582913e-05 old loss 3.796823148149997e-05 BETTER
W0314 13:36:15.447589 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 13:36:16.773570 87963 finetune.py:68] layer 12_q @ epoch 3 new loss 4.3147403630428016e-05 old loss 4.3872114474652335e-05 BETTER
15_v proxy err 0.04364098981022835 tr(WHW.T) 67.86465454101562
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]  6%|▋         | 2/32 [00:01<00:19,  1.58it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.15it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s]I0314 13:36:19.745512 90617 finetune.py:68] layer 14_q @ epoch 0 new loss 5.296933886711486e-05 old loss 5.558525663218461e-05 BETTER
 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s]I0314 13:36:25.769316 89269 finetune.py:68] layer 13_q @ epoch 2 new loss 4.466704194783233e-05 old loss 4.522244489635341e-05 BETTER
 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0314 13:36:34.982000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:36:34.983000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:36:34.983000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:36:34.983000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:36:34.983000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:36:34.983000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:36:34.983000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.012000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.012000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.012000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.012000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.012000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.193000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.193000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.194000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.194000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.194000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.424000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.424000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.424000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.424000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.424000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.424000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.424000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.445000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.445000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.445000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.445000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.445000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.511000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.511000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.511000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.512000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:36:35.512000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.257000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.576000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.576000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.576000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.576000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.576000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.576000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.576000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.598000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.599000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.599000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.599000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.599000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.861000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.862000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.862000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.862000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:36:36.862000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:36:37.216000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 13:36:43.831744 91955 finetune.py:45] layer 15_q initial loss 5.7215685956180096e-05
W0314 13:36:43.832104 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:36:55.096462 87963 finetune.py:68] layer 12_q @ epoch 4 new loss 4.2511474021011963e-05 old loss 4.3147403630428016e-05 BETTER
I0314 13:36:56.217458 90617 finetune.py:68] layer 14_q @ epoch 1 new loss 5.1645311032189056e-05 old loss 5.296933886711486e-05 BETTER
W0314 13:36:56.919908 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_q proxy err 0.005034853238612413 tr(WHW.T) 6425.7275390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:17,  1.67it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s]I0314 13:37:01.755398 89269 finetune.py:68] layer 13_q @ epoch 3 new loss 4.409744724398479e-05 old loss 4.466704194783233e-05 BETTER
 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.69it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
I0314 13:37:17.565591 87963 finetune.py:45] layer 12_k initial loss 5.77442224312108e-05
W0314 13:37:17.565960 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:37:18.437415 91955 finetune.py:68] layer 15_q @ epoch 0 new loss 5.522814535652287e-05 old loss 5.7215685956180096e-05 BETTER
I0314 13:37:32.328969 90617 finetune.py:68] layer 14_q @ epoch 2 new loss 5.066008088761009e-05 old loss 5.1645311032189056e-05 BETTER
I0314 13:37:37.594284 89269 finetune.py:68] layer 13_q @ epoch 4 new loss 4.388469096738845e-05 old loss 4.409744724398479e-05 BETTER
W0314 13:37:39.376142 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_q proxy err 0.00795564241707325 tr(WHW.T) 5300.705078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.11it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.70it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
I0314 13:37:53.913914 91955 finetune.py:68] layer 15_q @ epoch 1 new loss 5.413088365457952e-05 old loss 5.522814535652287e-05 BETTER
I0314 13:37:54.474087 87963 finetune.py:68] layer 12_k @ epoch 0 new loss 5.405620686360635e-05 old loss 5.77442224312108e-05 BETTER
I0314 13:37:59.773380 89269 finetune.py:45] layer 13_k initial loss 5.555687675951049e-05
W0314 13:37:59.773746 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:38:08.967380 90617 finetune.py:68] layer 14_q @ epoch 3 new loss 4.986430940334685e-05 old loss 5.066008088761009e-05 BETTER
I0314 13:38:29.777316 91955 finetune.py:68] layer 15_q @ epoch 2 new loss 5.338358096196316e-05 old loss 5.413088365457952e-05 BETTER
I0314 13:38:32.183468 87963 finetune.py:68] layer 12_k @ epoch 1 new loss 5.32640733581502e-05 old loss 5.405620686360635e-05 BETTER
I0314 13:38:34.788675 89269 finetune.py:68] layer 13_k @ epoch 0 new loss 5.358366252039559e-05 old loss 5.555687675951049e-05 BETTER
I0314 13:38:45.104769 90617 finetune.py:68] layer 14_q @ epoch 4 new loss 4.923656160826795e-05 old loss 4.986430940334685e-05 BETTER
W0314 13:38:46.946274 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_q proxy err 0.007158056832849979 tr(WHW.T) 5555.580078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0314 13:39:05.462510 91955 finetune.py:68] layer 15_q @ epoch 3 new loss 5.267747474135831e-05 old loss 5.338358096196316e-05 BETTER
I0314 13:39:08.199139 90617 finetune.py:45] layer 14_k initial loss 6.936849240446463e-05
W0314 13:39:08.199570 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:39:10.537074 87963 finetune.py:68] layer 12_k @ epoch 2 new loss 5.2497474825941026e-05 old loss 5.32640733581502e-05 BETTER
I0314 13:39:10.653862 89269 finetune.py:68] layer 13_k @ epoch 1 new loss 5.31544465047773e-05 old loss 5.358366252039559e-05 BETTER
I0314 13:39:41.210486 91955 finetune.py:68] layer 15_q @ epoch 4 new loss 5.215349665377289e-05 old loss 5.267747474135831e-05 BETTER
W0314 13:39:43.268788 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 13:39:43.745204 90617 finetune.py:68] layer 14_k @ epoch 0 new loss 6.521568138850853e-05 old loss 6.936849240446463e-05 BETTER
15_q proxy err 0.007208857219666243 tr(WHW.T) 6710.029296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s]I0314 13:39:46.964155 89269 finetune.py:68] layer 13_k @ epoch 2 new loss 5.272049020277336e-05 old loss 5.31544465047773e-05 BETTER
 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s]I0314 13:39:48.555463 87963 finetune.py:68] layer 12_k @ epoch 3 new loss 5.195978155825287e-05 old loss 5.2497474825941026e-05 BETTER
 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0314 13:40:04.891957 91955 finetune.py:45] layer 15_k initial loss 6.639897765126079e-05
W0314 13:40:04.892405 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:40:20.291796 90617 finetune.py:68] layer 14_k @ epoch 1 new loss 6.408301851479337e-05 old loss 6.521568138850853e-05 BETTER
I0314 13:40:23.492179 89269 finetune.py:68] layer 13_k @ epoch 3 new loss 5.2701048844028264e-05 old loss 5.272049020277336e-05 BETTER
I0314 13:40:26.611600 87963 finetune.py:68] layer 12_k @ epoch 4 new loss 5.153184611117467e-05 old loss 5.195978155825287e-05 BETTER
W0314 13:40:28.433471 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_k proxy err 0.004578557331115007 tr(WHW.T) 4319.49365234375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s]  6%|▋         | 2/32 [00:01<00:16,  1.85it/s]  9%|▉         | 3/32 [00:01<00:13,  2.19it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.39it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.51it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.60it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.75it/s]I0314 13:40:39.467161 91955 finetune.py:68] layer 15_k @ epoch 0 new loss 6.449354259530082e-05 old loss 6.639897765126079e-05 BETTER
 81%|████████▏ | 26/32 [00:09<00:02,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
I0314 13:40:49.175530 87963 finetune.py:45] layer 12_o initial loss 9.407561447005719e-05
W0314 13:40:49.176007 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:40:56.929059 90617 finetune.py:68] layer 14_k @ epoch 2 new loss 6.32190058240667e-05 old loss 6.408301851479337e-05 BETTER
I0314 13:40:59.845076 89269 finetune.py:68] layer 13_k @ epoch 4 new loss 5.2229293942218646e-05 old loss 5.2701048844028264e-05 BETTER
W0314 13:41:01.574320 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_k proxy err 0.006058482453227043 tr(WHW.T) 4499.2705078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s]  6%|▋         | 2/32 [00:01<00:16,  1.80it/s]  9%|▉         | 3/32 [00:01<00:13,  2.11it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0314 13:41:15.396155 91955 finetune.py:68] layer 15_k @ epoch 1 new loss 6.386813038261607e-05 old loss 6.449354259530082e-05 BETTER
I0314 13:41:22.300916 89269 finetune.py:45] layer 13_o initial loss 0.0001035442401189357
W0314 13:41:22.301285 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:41:26.285887 87963 finetune.py:68] layer 12_o @ epoch 0 new loss 9.075656998902559e-05 old loss 9.407561447005719e-05 BETTER
I0314 13:41:33.199004 90617 finetune.py:68] layer 14_k @ epoch 3 new loss 6.253282481338829e-05 old loss 6.32190058240667e-05 BETTER
I0314 13:41:51.012526 91955 finetune.py:68] layer 15_k @ epoch 2 new loss 6.338537787087262e-05 old loss 6.386813038261607e-05 BETTER
I0314 13:41:57.204898 89269 finetune.py:68] layer 13_o @ epoch 0 new loss 9.967193909687921e-05 old loss 0.0001035442401189357 BETTER
I0314 13:42:04.023405 87963 finetune.py:68] layer 12_o @ epoch 1 new loss 8.936932135839015e-05 old loss 9.075656998902559e-05 BETTER
I0314 13:42:09.565134 90617 finetune.py:68] layer 14_k @ epoch 4 new loss 6.20044011157006e-05 old loss 6.253282481338829e-05 BETTER
W0314 13:42:11.277561 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_k proxy err 0.005340421572327614 tr(WHW.T) 4930.099609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.26it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0314 13:42:26.455962 91955 finetune.py:68] layer 15_k @ epoch 3 new loss 6.297390791587532e-05 old loss 6.338537787087262e-05 BETTER
I0314 13:42:32.744608 90617 finetune.py:45] layer 14_o initial loss 0.00011473921767901629
W0314 13:42:32.745033 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:42:33.047354 89269 finetune.py:68] layer 13_o @ epoch 1 new loss 9.819769184105098e-05 old loss 9.967193909687921e-05 BETTER
I0314 13:42:41.871084 87963 finetune.py:68] layer 12_o @ epoch 2 new loss 8.830244041746482e-05 old loss 8.936932135839015e-05 BETTER
I0314 13:43:02.227484 91955 finetune.py:68] layer 15_k @ epoch 4 new loss 6.26098844804801e-05 old loss 6.297390791587532e-05 BETTER
W0314 13:43:04.009580 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_k proxy err 0.005658750422298908 tr(WHW.T) 4488.85302734375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.18it/s]  6%|▋         | 2/32 [00:01<00:17,  1.74it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s]I0314 13:43:08.214751 90617 finetune.py:68] layer 14_o @ epoch 0 new loss 0.0001103785980376415 old loss 0.00011473921767901629 BETTER
 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s]I0314 13:43:09.129143 89269 finetune.py:68] layer 13_o @ epoch 2 new loss 9.710034646559507e-05 old loss 9.819769184105098e-05 BETTER
 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0314 13:43:19.928191 87963 finetune.py:68] layer 12_o @ epoch 3 new loss 8.741508645471185e-05 old loss 8.830244041746482e-05 BETTER
I0314 13:43:25.737670 91955 finetune.py:45] layer 15_o initial loss 0.00012040606088703498
W0314 13:43:25.738070 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:43:44.643101 90617 finetune.py:68] layer 14_o @ epoch 1 new loss 0.00010851061961147934 old loss 0.0001103785980376415 BETTER
I0314 13:43:45.652514 89269 finetune.py:68] layer 13_o @ epoch 3 new loss 9.619866614229977e-05 old loss 9.710034646559507e-05 BETTER
I0314 13:43:57.700330 87963 finetune.py:68] layer 12_o @ epoch 4 new loss 8.664609049446881e-05 old loss 8.741508645471185e-05 BETTER
W0314 13:43:59.464011 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 13:44:00.739385 91955 finetune.py:68] layer 15_o @ epoch 0 new loss 0.0001148725496022962 old loss 0.00012040606088703498 BETTER
12_o proxy err 0.04076266288757324 tr(WHW.T) 5.691562652587891
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.46s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.45s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it]I0314 13:44:20.973477 90617 finetune.py:68] layer 14_o @ epoch 2 new loss 0.00010712193761719391 old loss 0.00010851061961147934 BETTER
 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it]I0314 13:44:21.866507 89269 finetune.py:68] layer 13_o @ epoch 4 new loss 9.544364002067596e-05 old loss 9.619866614229977e-05 BETTER
 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it]W0314 13:44:23.599076 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:23<00:23,  1.45s/it]13_o proxy err 0.04060903936624527 tr(WHW.T) 6.826041221618652
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it]  3%|▎         | 1/32 [00:02<01:02,  2.00s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.45s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it]  9%|▉         | 3/32 [00:05<00:46,  1.62s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.46s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it]I0314 13:44:36.202824 91955 finetune.py:68] layer 15_o @ epoch 1 new loss 0.00011298753088340163 old loss 0.0001148725496022962 BETTER
 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.46s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.46s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.50s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.46s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.46s/it] 41%|████      | 13/32 [00:20<00:28,  1.50s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.46s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.46s/it]100%|██████████| 32/32 [00:47<00:00,  1.46s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]
 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.50s/it]I0314 13:44:56.284105 87963 finetune.py:45] layer 12_up initial loss 0.00017828009731601924
W0314 13:44:56.284700 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it]I0314 13:44:57.146283 90617 finetune.py:68] layer 14_o @ epoch 3 new loss 0.00010596692300168797 old loss 0.00010712193761719391 BETTER
 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it]I0314 13:45:11.685135 91955 finetune.py:68] layer 15_o @ epoch 2 new loss 0.00011162948794662952 old loss 0.00011298753088340163 BETTER
 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
I0314 13:45:21.344295 89269 finetune.py:45] layer 13_up initial loss 0.00020005505939479917
W0314 13:45:21.344681 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:45:32.331024 87963 finetune.py:68] layer 12_up @ epoch 0 new loss 0.00017539280815981328 old loss 0.00017828009731601924 BETTER
I0314 13:45:33.549130 90617 finetune.py:68] layer 14_o @ epoch 4 new loss 0.00010500074131414294 old loss 0.00010596692300168797 BETTER
W0314 13:45:35.275864 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_o proxy err 0.04249947890639305 tr(WHW.T) 6.921741962432861
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:04,  2.07s/it]  6%|▋         | 2/32 [00:03<00:52,  1.76s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.61s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it]I0314 13:45:47.449677 91955 finetune.py:68] layer 15_o @ epoch 3 new loss 0.00011052691115764901 old loss 0.00011162948794662952 BETTER
 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it]I0314 13:45:55.330798 89269 finetune.py:68] layer 13_up @ epoch 0 new loss 0.00019681993580888957 old loss 0.00020005505939479917 BETTER
 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.52s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it]I0314 13:46:09.120131 87963 finetune.py:68] layer 12_up @ epoch 1 new loss 0.00017353787552565336 old loss 0.00017539280815981328 BETTER
 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it]I0314 13:46:23.057975 91955 finetune.py:68] layer 15_o @ epoch 4 new loss 0.00010958606435451657 old loss 0.00011052691115764901 BETTER
 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]W0314 13:46:24.721563 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
15_o proxy err 0.045329783111810684 tr(WHW.T) 6.818299293518066
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.06s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it]I0314 13:46:29.995899 89269 finetune.py:68] layer 13_up @ epoch 1 new loss 0.0001947478303918615 old loss 0.00019681993580888957 BETTER
  9%|▉         | 3/32 [00:05<00:47,  1.65s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it]I0314 13:46:34.140889 90617 finetune.py:45] layer 14_up initial loss 0.00023069721646606922
W0314 13:46:34.141250 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it]I0314 13:46:45.728186 87963 finetune.py:68] layer 12_up @ epoch 2 new loss 0.00017195635882671922 old loss 0.00017353787552565336 BETTER
 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it]I0314 13:47:04.887704 89269 finetune.py:68] layer 13_up @ epoch 2 new loss 0.00019297486869618297 old loss 0.0001947478303918615 BETTER
 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it]I0314 13:47:08.389492 90617 finetune.py:68] layer 14_up @ epoch 0 new loss 0.00022656693181488663 old loss 0.00023069721646606922 BETTER
 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:49<00:00,  1.51s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
I0314 13:47:23.006275 87963 finetune.py:68] layer 12_up @ epoch 3 new loss 0.00017052881594281644 old loss 0.00017195635882671922 BETTER
I0314 13:47:23.621283 91955 finetune.py:45] layer 15_up initial loss 0.00025821791496127844
W0314 13:47:23.621726 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:47:39.644687 89269 finetune.py:68] layer 13_up @ epoch 3 new loss 0.0001913765590870753 old loss 0.00019297486869618297 BETTER
I0314 13:47:43.334593 90617 finetune.py:68] layer 14_up @ epoch 1 new loss 0.00022385065676644444 old loss 0.00022656693181488663 BETTER
I0314 13:47:57.081078 91955 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0002529345510993153 old loss 0.00025821791496127844 BETTER
I0314 13:48:00.027762 87963 finetune.py:68] layer 12_up @ epoch 4 new loss 0.00016922428039833903 old loss 0.00017052881594281644 BETTER
W0314 13:48:01.547536 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_up proxy err 0.033749911934137344 tr(WHW.T) 853.1935424804688
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it]I0314 13:48:14.351098 89269 finetune.py:68] layer 13_up @ epoch 4 new loss 0.0001899280905490741 old loss 0.0001913765590870753 BETTER
 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it]W0314 13:48:15.866775 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it]13_up proxy err 0.033258434385061264 tr(WHW.T) 914.928466796875
  0%|          | 0/32 [00:00<?, ?it/s] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it]I0314 13:48:18.334787 90617 finetune.py:68] layer 14_up @ epoch 2 new loss 0.00022150532458908856 old loss 0.00022385065676644444 BETTER
  3%|▎         | 1/32 [00:01<01:01,  1.98s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.48s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.47s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it]I0314 13:48:31.179176 91955 finetune.py:68] layer 15_up @ epoch 1 new loss 0.0002496005326975137 old loss 0.0002529345510993153 BETTER
 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it]I0314 13:48:52.987096 90617 finetune.py:68] layer 14_up @ epoch 3 new loss 0.00021944283798802644 old loss 0.00022150532458908856 BETTER
 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it]I0314 13:48:58.901238 87963 finetune.py:45] layer 12_gate initial loss 0.0002232511033071205
W0314 13:48:58.901594 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:42<00:05,  1.50s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]I0314 13:49:05.213208 91955 finetune.py:68] layer 15_up @ epoch 2 new loss 0.0002468147431500256 old loss 0.0002496005326975137 BETTER
100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
I0314 13:49:13.610710 89269 finetune.py:45] layer 13_gate initial loss 0.00025017038569785655
W0314 13:49:13.611112 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:49:27.909621 90617 finetune.py:68] layer 14_up @ epoch 4 new loss 0.00021757901413366199 old loss 0.00021944283798802644 BETTER
W0314 13:49:29.537810 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_up proxy err 0.03578800708055496 tr(WHW.T) 929.6590576171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]I0314 13:49:33.707521 87963 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.0002209626545663923 old loss 0.0002232511033071205 BETTER
  6%|▋         | 2/32 [00:03<00:51,  1.70s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it]I0314 13:49:39.258834 91955 finetune.py:68] layer 15_up @ epoch 3 new loss 0.0002443121047690511 old loss 0.0002468147431500256 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it]I0314 13:49:46.102691 89269 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.00024750837474130094 old loss 0.00025017038569785655 BETTER
 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:28,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it]I0314 13:50:08.631030 87963 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.00021956235286779702 old loss 0.0002209626545663923 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it]I0314 13:50:13.152008 91955 finetune.py:68] layer 15_up @ epoch 4 new loss 0.00024203574866987765 old loss 0.0002443121047690511 BETTER
 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it]W0314 13:50:14.714752 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it]15_up proxy err 0.035919688642024994 tr(WHW.T) 987.2632446289062
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it]  3%|▎         | 1/32 [00:02<01:02,  2.01s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]I0314 13:50:19.160533 89269 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.0002459309180267155 old loss 0.00024750837474130094 BETTER
  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it]I0314 13:50:28.232904 90617 finetune.py:45] layer 14_gate initial loss 0.0002861115208361298
W0314 13:50:28.233290 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it]I0314 13:50:43.969091 87963 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.00021835303050465882 old loss 0.00021956235286779702 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it]I0314 13:50:52.776032 89269 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.0002445473801344633 old loss 0.0002459309180267155 BETTER
 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it]I0314 13:51:01.317433 90617 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.0002826083218678832 old loss 0.0002861115208361298 BETTER
 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
I0314 13:51:13.211940 91955 finetune.py:45] layer 15_gate initial loss 0.00032238123822025955
W0314 13:51:13.212346 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:51:19.086594 87963 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.0002172568056266755 old loss 0.00021835303050465882 BETTER
I0314 13:51:26.164542 89269 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.0002433286135783419 old loss 0.0002445473801344633 BETTER
I0314 13:51:34.553927 90617 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.0002805508556775749 old loss 0.0002826083218678832 BETTER
I0314 13:51:45.166733 91955 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.0003174544544890523 old loss 0.00032238123822025955 BETTER
I0314 13:51:54.148971 87963 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.0002162168239010498 old loss 0.0002172568056266755 BETTER
W0314 13:51:55.466038 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 13:51:59.202249 89269 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.00024215702433139086 old loss 0.0002433286135783419 BETTER
12_gate proxy err 0.01299167051911354 tr(WHW.T) 3159.3125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:35,  1.16it/s]W0314 13:52:00.373712 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  2%|▏         | 2/112 [00:01<01:03,  1.74it/s]  3%|▎         | 3/112 [00:01<00:52,  2.08it/s]  4%|▎         | 4/112 [00:01<00:46,  2.31it/s]  4%|▍         | 5/112 [00:02<00:43,  2.44it/s]  5%|▌         | 6/112 [00:02<00:41,  2.55it/s]  6%|▋         | 7/112 [00:03<00:40,  2.62it/s]  7%|▋         | 8/112 [00:03<00:39,  2.65it/s]  8%|▊         | 9/112 [00:03<00:38,  2.68it/s]  9%|▉         | 10/112 [00:04<00:37,  2.69it/s]13_gate proxy err 0.012271863408386707 tr(WHW.T) 3532.577392578125
  0%|          | 0/112 [00:00<?, ?it/s] 10%|▉         | 11/112 [00:04<00:37,  2.70it/s] 11%|█         | 12/112 [00:04<00:36,  2.71it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.72it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.71it/s]  2%|▏         | 2/112 [00:01<01:06,  1.67it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.70it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.71it/s]  4%|▎         | 4/112 [00:02<00:49,  2.19it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.71it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.72it/s]  5%|▌         | 6/112 [00:02<00:43,  2.42it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.72it/s]  6%|▋         | 7/112 [00:03<00:42,  2.49it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.73it/s]  7%|▋         | 8/112 [00:03<00:40,  2.54it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.74it/s]I0314 13:52:07.478962 90617 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.0002787576522678137 old loss 0.0002805508556775749 BETTER
  8%|▊         | 9/112 [00:03<00:39,  2.58it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.74it/s]  9%|▉         | 10/112 [00:04<00:39,  2.61it/s] 21%|██        | 23/112 [00:08<00:32,  2.74it/s] 10%|▉         | 11/112 [00:04<00:38,  2.63it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.74it/s] 11%|█         | 12/112 [00:05<00:37,  2.64it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.73it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.65it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.73it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.64it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.72it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.64it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.72it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.65it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.73it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.64it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.73it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.61it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.74it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.63it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.74it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.75it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.64it/s] 30%|███       | 34/112 [00:12<00:28,  2.74it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.64it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.74it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.64it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.75it/s] 21%|██        | 23/112 [00:09<00:33,  2.65it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.75it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.65it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.74it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.65it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.74it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.65it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.72it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.65it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.72it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.65it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.72it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.65it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.72it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.65it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.73it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.65it/s] 40%|████      | 45/112 [00:16<00:24,  2.73it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 41%|████      | 46/112 [00:17<00:24,  2.74it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.62it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.74it/s] 30%|███       | 34/112 [00:13<00:29,  2.63it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.74it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.64it/s]I0314 13:52:17.530795 91955 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.0003148479154333472 old loss 0.0003174544544890523 BETTER
 44%|████▍     | 49/112 [00:18<00:22,  2.74it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.66it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.74it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.66it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.75it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.67it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.74it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.69it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.71it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.68it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.73it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.68it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.74it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.68it/s] 50%|█████     | 56/112 [00:20<00:20,  2.73it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.67it/s] 51%|█████     | 57/112 [00:21<00:20,  2.74it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.69it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.74it/s] 40%|████      | 45/112 [00:17<00:25,  2.68it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.74it/s] 41%|████      | 46/112 [00:17<00:25,  2.64it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.74it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.74it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.67it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.74it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.68it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.74it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.68it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.73it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.68it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.74it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.68it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.71it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.68it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.73it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.69it/s] 61%|██████    | 68/112 [00:25<00:16,  2.73it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.69it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.73it/s] 50%|█████     | 56/112 [00:21<00:20,  2.68it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.73it/s] 63%|██████▎   | 71/112 [00:26<00:14,  2.74it/s] 51%|█████     | 57/112 [00:22<00:20,  2.67it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.73it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.67it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.73it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.67it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.73it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.63it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.74it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.65it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.73it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.65it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.73it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.66it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.74it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.67it/s] 71%|███████   | 79/112 [00:29<00:12,  2.72it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.66it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.72it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.67it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.67it/s] 73%|███████▎  | 82/112 [00:30<00:10,  2.73it/s] 61%|██████    | 68/112 [00:26<00:16,  2.67it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.73it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.68it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.74it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.67it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.73it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.68it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.73it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.68it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.73it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.63it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.73it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.65it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.73it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.67it/s] 80%|████████  | 90/112 [00:33<00:08,  2.72it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.68it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.73it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.68it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.70it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.67it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.72it/s] 71%|███████   | 79/112 [00:30<00:12,  2.68it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.72it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.68it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.72it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.69it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.72it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.70it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.73it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.68it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.72it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.68it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.72it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.67it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.73it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.67it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.41it/s] 78%|███████▊  | 87/112 [00:33<00:10,  2.33it/s] 91%|█████████ | 102/112 [00:38<00:04,  2.49it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.43it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.55it/s] 79%|███████▉  | 89/112 [00:34<00:09,  2.50it/s] 93%|█████████▎| 104/112 [00:38<00:03,  2.59it/s] 80%|████████  | 90/112 [00:34<00:08,  2.54it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.62it/s] 81%|████████▏ | 91/112 [00:34<00:08,  2.58it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.65it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.61it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.68it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.63it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.69it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.64it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.70it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.65it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.70it/s]I0314 13:52:40.232393 90617 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.00027718834462575614 old loss 0.0002787576522678137 BETTER
 86%|████████▌ | 96/112 [00:36<00:06,  2.66it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.72it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.66it/s]100%|██████████| 112/112 [00:41<00:00,  2.72it/s]100%|██████████| 112/112 [00:41<00:00,  2.69it/s]
 88%|████████▊ | 98/112 [00:37<00:05,  2.66it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.65it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.65it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.65it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.61it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.63it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.64it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.64it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.65it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.66it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.67it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.67it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.67it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.62it/s]
W0314 13:52:47.164000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.165000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.165000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.165000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.165000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.165000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.165000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.210000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.210000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.210000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.211000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.211000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.392000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.392000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.392000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.392000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.392000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.716000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.716000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.716000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.717000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.717000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.717000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.717000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.748000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.748000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.748000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.748000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.748000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.819000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.819000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.819000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.819000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:47.819000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:48.807000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:48.820000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:48.828000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:48.828000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.285000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.285000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.285000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.286000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.286000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.286000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.286000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.318000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.318000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.318000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.318000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.318000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.622000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.622000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.622000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.622000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.622000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.623000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.623000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.623000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.925000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.925000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.925000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.925000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:49.925000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
I0314 13:52:50.282612 91955 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.0003126224619336426 old loss 0.0003148479154333472 BETTER
W0314 13:52:50.372000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:50.377000 140311255807808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.696000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.696000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.696000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.696000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.696000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.696000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.697000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.742000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.742000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.742000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.742000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.743000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.925000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.926000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.926000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.926000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:52.926000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.254000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.254000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.254000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.255000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.255000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.255000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.255000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.291000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.291000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.291000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.291000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.291000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.363000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.364000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.364000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.364000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:53.364000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.384000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.390000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.396000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.397000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.870000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.870000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.870000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.870000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.870000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.871000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.871000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.903000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.903000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.903000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.903000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:54.903000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.214000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.214000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.214000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.214000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.215000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.215000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.215000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.215000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.522000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.523000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.523000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.523000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.523000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.969000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:52:55.975000 139898043877184 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 13:52:57.521602 87963 finetune.py:45] layer 12_down initial loss 0.0003134224971290678
W0314 13:52:57.522024 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:53:03.040731 89269 finetune.py:45] layer 13_down initial loss 0.0003578609030228108
W0314 13:53:03.041265 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:53:13.103124 90617 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0002756880421657115 old loss 0.00027718834462575614 BETTER
W0314 13:53:14.393561 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_gate proxy err 0.011936712078750134 tr(WHW.T) 4207.99951171875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:38,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s]  3%|▎         | 3/112 [00:01<00:54,  2.00it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s]  6%|▋         | 7/112 [00:03<00:42,  2.46it/s]  7%|▋         | 8/112 [00:03<00:41,  2.50it/s]  8%|▊         | 9/112 [00:03<00:40,  2.52it/s]  9%|▉         | 10/112 [00:04<00:40,  2.54it/s]I0314 13:53:22.621193 91955 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0003106372896581888 old loss 0.0003126224619336426 BETTER
 10%|▉         | 11/112 [00:04<00:39,  2.54it/s] 11%|█         | 12/112 [00:05<00:39,  2.55it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.57it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.57it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.57it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.57it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.57it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.58it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.59it/s] 21%|██        | 23/112 [00:09<00:34,  2.59it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.59it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.59it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.58it/s] 24%|██▍       | 27/112 [00:10<00:33,  2.56it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.57it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.58it/s]I0314 13:53:29.922138 87963 finetune.py:68] layer 12_down @ epoch 0 new loss 0.0003127864911220968 old loss 0.0003134224971290678 BETTER
 27%|██▋       | 30/112 [00:12<00:31,  2.58it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.59it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.59it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s] 30%|███       | 34/112 [00:13<00:30,  2.59it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.59it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.59it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.59it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.58it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.58it/s]I0314 13:53:33.765035 89269 finetune.py:68] layer 13_down @ epoch 0 new loss 0.00035710717202164233 old loss 0.0003578609030228108 BETTER
 36%|███▌      | 40/112 [00:16<00:28,  2.56it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.58it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.58it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.59it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.59it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.59it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.59it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.58it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.55it/s] 47%|████▋     | 53/112 [00:21<00:22,  2.57it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.58it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.58it/s] 50%|█████     | 56/112 [00:22<00:21,  2.59it/s] 51%|█████     | 57/112 [00:22<00:21,  2.59it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.59it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.59it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.59it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.59it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.59it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.58it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.59it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.56it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.58it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.58it/s] 61%|██████    | 68/112 [00:26<00:16,  2.59it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.60it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.60it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.60it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.60it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.60it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.60it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.60it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.60it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.55it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.56it/s] 71%|███████   | 79/112 [00:31<00:12,  2.58it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.58it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.59it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.60it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.60it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.60it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.60it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.60it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.60it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.59it/s] 80%|████████  | 90/112 [00:35<00:08,  2.56it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.57it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.58it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.59it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.60it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.60it/s]I0314 13:53:55.188440 91955 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.00030879577388986945 old loss 0.0003106372896581888 BETTER
 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.61it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.61it/s]W0314 13:53:56.382328 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 99/112 [00:38<00:04,  2.60it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.59it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.59it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.56it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.57it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.58it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.58it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.59it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.59it/s]15_gate proxy err 0.011340207420289516 tr(WHW.T) 5048.50732421875
  0%|          | 0/112 [00:00<?, ?it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.59it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.59it/s]  1%|          | 1/112 [00:00<01:38,  1.13it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.59it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.59it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.56it/s]
  4%|▎         | 4/112 [00:02<00:49,  2.19it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s]  5%|▌         | 6/112 [00:02<00:44,  2.39it/s]  6%|▋         | 7/112 [00:03<00:43,  2.44it/s]I0314 13:54:03.057319 87963 finetune.py:68] layer 12_down @ epoch 1 new loss 0.00031267167651094496 old loss 0.0003127864911220968 BETTER
  7%|▋         | 8/112 [00:03<00:42,  2.47it/s]  8%|▊         | 9/112 [00:04<00:41,  2.50it/s]  9%|▉         | 10/112 [00:04<00:40,  2.52it/s] 10%|▉         | 11/112 [00:04<00:39,  2.54it/s] 11%|█         | 12/112 [00:05<00:39,  2.54it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.55it/s]I0314 13:54:05.514437 89269 finetune.py:68] layer 13_down @ epoch 1 new loss 0.0003569636319298297 old loss 0.00035710717202164233 BETTER
 12%|█▎        | 14/112 [00:05<00:37,  2.58it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.58it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.60it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.60it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.59it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.59it/s]W0314 13:54:08.029000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.029000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.029000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.029000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.029000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.029000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.030000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.073000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.074000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.074000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.074000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.074000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s]W0314 13:54:08.253000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.253000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.253000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.253000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.253000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 21/112 [00:08<00:35,  2.57it/s]W0314 13:54:08.577000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.577000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.577000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.577000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.577000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.577000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.577000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.611000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.611000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.611000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.612000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.612000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.683000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.683000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.683000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.683000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:08.683000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 20%|█▉        | 22/112 [00:09<00:35,  2.54it/s] 21%|██        | 23/112 [00:09<00:34,  2.55it/s]W0314 13:54:09.668000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 21%|██▏       | 24/112 [00:09<00:34,  2.55it/s]W0314 13:54:09.682000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:09.690000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:09.690000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 25/112 [00:10<00:33,  2.56it/s]W0314 13:54:10.155000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.156000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.156000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.156000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.156000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.156000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.156000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.189000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.189000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.189000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.189000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.189000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 23%|██▎       | 26/112 [00:10<00:33,  2.58it/s]W0314 13:54:10.488000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.488000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.489000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.489000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.489000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.489000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.489000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.489000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.790000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.790000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.790000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.790000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:10.790000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 24%|██▍       | 27/112 [00:10<00:32,  2.59it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.59it/s]W0314 13:54:11.242000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:11.247000 139786240542528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.60it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.61it/s] 30%|███       | 34/112 [00:13<00:30,  2.57it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.58it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.59it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.58it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.58it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.58it/s] 36%|███▌      | 40/112 [00:16<00:27,  2.59it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.59it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.59it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.59it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s] 40%|████      | 45/112 [00:17<00:25,  2.58it/s] 41%|████      | 46/112 [00:18<00:25,  2.55it/s]I0314 13:54:18.284773 90617 finetune.py:45] layer 14_down initial loss 0.0004131730238441378
W0314 13:54:18.285201 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 42%|████▏     | 47/112 [00:18<00:25,  2.56it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.57it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.58it/s] 45%|████▍     | 50/112 [00:19<00:24,  2.58it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.58it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.58it/s] 47%|████▋     | 53/112 [00:21<00:22,  2.58it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.57it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.58it/s] 50%|█████     | 56/112 [00:22<00:21,  2.57it/s] 51%|█████     | 57/112 [00:22<00:21,  2.57it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.57it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.54it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.55it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.55it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.56it/s] 56%|█████▋    | 63/112 [00:24<00:19,  2.57it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.58it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.58it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.58it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.59it/s] 61%|██████    | 68/112 [00:26<00:17,  2.58it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.57it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.54it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.56it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.57it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.58it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.59it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.60it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.61it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.61it/s] 71%|███████   | 79/112 [00:31<00:12,  2.61it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.60it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.57it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.55it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.51it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.53it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.54it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.55it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.56it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.57it/s] 80%|████████  | 90/112 [00:35<00:08,  2.58it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.58it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.57it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.57it/s]I0314 13:54:36.514683 87963 finetune.py:68] layer 12_down @ epoch 2 new loss 0.0003126006049569696 old loss 0.00031267167651094496 BETTER
 84%|████████▍ | 94/112 [00:37<00:07,  2.57it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.57it/s]I0314 13:54:37.347410 89269 finetune.py:68] layer 13_down @ epoch 2 new loss 0.0003568931424524635 old loss 0.0003569636319298297 BETTER
 86%|████████▌ | 96/112 [00:37<00:06,  2.54it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.55it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.56it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.58it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.59it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.59it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.59it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.60it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.59it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.59it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.59it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.59it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.54it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.55it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.57it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.57it/s]100%|██████████| 112/112 [00:44<00:00,  2.56it/s]100%|██████████| 112/112 [00:44<00:00,  2.54it/s]
I0314 13:54:49.267032 90617 finetune.py:68] layer 14_down @ epoch 0 new loss 0.00041225773748010397 old loss 0.0004131730238441378 BETTER
W0314 13:54:50.288000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.289000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.289000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.289000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.289000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.289000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.289000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.337000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.337000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.337000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.337000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.337000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.516000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.516000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.516000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.516000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.516000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.840000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.841000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.841000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.841000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.841000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.841000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.841000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.874000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.874000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.874000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.875000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.875000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.946000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.946000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.946000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.946000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:50.946000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:51.934000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:51.954000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:51.962000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:51.962000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.437000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.437000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.438000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.438000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.438000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.438000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.438000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.470000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.470000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.470000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.470000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.470000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.772000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.772000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.773000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.773000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.773000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.773000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.773000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:52.773000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:53.070000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:53.070000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:54:53.070000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:54:53.070000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:54:53.070000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:54:53.508000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 13:54:53.514000 140485729060672 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 13:55:00.670769 91955 finetune.py:45] layer 15_down initial loss 0.00048432120820507407
W0314 13:55:00.671354 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:55:09.298277 89269 finetune.py:68] layer 13_down @ epoch 3 new loss 0.00035682308953255415 old loss 0.0003568931424524635 BETTER
I0314 13:55:09.891747 87963 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0003125423681922257 old loss 0.0003126006049569696 BETTER
I0314 13:55:20.997424 90617 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0004120719386264682 old loss 0.00041225773748010397 BETTER
I0314 13:55:31.241957 91955 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0004833562416024506 old loss 0.00048432120820507407 BETTER
I0314 13:55:41.159090 89269 finetune.py:68] layer 13_down @ epoch 4 new loss 0.00035676194238476455 old loss 0.00035682308953255415 BETTER
W0314 13:55:42.006755 89269 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

13_down proxy err 0.04454195126891136 tr(WHW.T) 11.799995422363281
I0314 13:55:43.217911 87963 finetune.py:68] layer 12_down @ epoch 4 new loss 0.00031249396852217615 old loss 0.0003125423681922257 BETTER
W0314 13:55:43.971812 87963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

12_down proxy err 0.04349987208843231 tr(WHW.T) 10.107794761657715
I0314 13:55:53.197244 90617 finetune.py:68] layer 14_down @ epoch 2 new loss 0.000411960412748158 old loss 0.0004120719386264682 BETTER
I0314 13:56:02.340470 91955 finetune.py:68] layer 15_down @ epoch 1 new loss 0.00048314162995666265 old loss 0.0004833562416024506 BETTER
I0314 13:56:24.630454 90617 finetune.py:68] layer 14_down @ epoch 3 new loss 0.00041188046452589333 old loss 0.000411960412748158 BETTER
I0314 13:56:33.116572 91955 finetune.py:68] layer 15_down @ epoch 2 new loss 0.0004829868266824633 old loss 0.00048314162995666265 BETTER
I0314 13:56:55.708382 9966 quantize_finetune_llama.py:186] computed original embedding for layer 16 in 66.70262932777405s
I0314 13:56:56.134924 9966 quantize_finetune_llama.py:159] layer 17 gpu 1
I0314 13:56:56.178335 90617 finetune.py:68] layer 14_down @ epoch 4 new loss 0.0004117853823117912 old loss 0.00041188046452589333 BETTER
W0314 13:56:57.015671 90617 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

14_down proxy err 0.046346813440322876 tr(WHW.T) 13.444437026977539
I0314 13:56:58.176202 112455 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:56:58.176348 112455 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:56:58.176405 112455 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:56:58.556066 112455 config.py:58] PyTorch version 2.4.0 available.
I0314 13:57:00.938898 112455 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:57:01.289709 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.42s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]I0314 13:57:04.150770 91955 finetune.py:68] layer 15_down @ epoch 3 new loss 0.00048287599929608405 old loss 0.0004829868266824633 BETTER
  9%|▉         | 3/32 [00:02<00:16,  1.75it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.09it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.70it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.88it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.93it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.96it/s] 41%|████      | 13/32 [00:05<00:06,  2.98it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.99it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.00it/s] 50%|█████     | 16/32 [00:06<00:05,  3.00it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.01it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.01it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.02it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.02it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.02it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.03it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.05it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.02it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.02it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.03it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.03it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.04it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.05it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
W0314 13:57:16.545000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.545000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.546000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.546000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.546000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.546000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.546000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.574000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.575000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.575000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.575000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.575000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.871000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.871000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.871000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.871000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:57:16.871000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.746000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.746000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.746000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.746000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.746000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.746000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.747000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.765000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.765000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.766000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.766000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.766000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.976000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.976000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.976000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.976000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:57:17.976000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.102000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.102000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.102000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.103000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.103000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.103000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.103000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.121000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.121000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.121000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.121000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.121000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.989000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.989000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.989000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.990000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:57:19.990000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:57:26.646597 112455 finetune.py:45] layer 16_v initial loss 7.53156200516969e-05
W0314 13:57:26.646852 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:57:34.979410 91955 finetune.py:68] layer 15_down @ epoch 4 new loss 0.00048276869347319007 old loss 0.00048287599929608405 BETTER
W0314 13:57:35.689398 91955 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

15_down proxy err 0.04639892652630806 tr(WHW.T) 17.035953521728516
I0314 13:58:02.846511 112455 finetune.py:68] layer 16_v @ epoch 0 new loss 4.1436083847656846e-05 old loss 7.53156200516969e-05 BETTER
I0314 13:58:03.257025 9966 quantize_finetune_llama.py:186] computed original embedding for layer 17 in 62.220757246017456s
I0314 13:58:03.669893 9966 quantize_finetune_llama.py:159] layer 18 gpu 2
I0314 13:58:05.685093 113854 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:58:05.685217 113854 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:58:05.685277 113854 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:58:05.881931 113854 config.py:58] PyTorch version 2.4.0 available.
I0314 13:58:08.248051 113854 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:58:08.709046 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.60s/it]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s]  9%|▉         | 3/32 [00:02<00:18,  1.60it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.95it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.24it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.86it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.85it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.86it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.86it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.87it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.88it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.88it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.88it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.88it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.90it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
W0314 13:58:24.953000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.953000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.953000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.953000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.953000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.953000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.954000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.980000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.980000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.980000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.981000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:58:24.981000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:58:25.278000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:58:25.278000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:58:25.279000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:58:25.279000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:58:25.279000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.140000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.140000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.140000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.140000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.140000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.140000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.140000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.159000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.159000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.159000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.159000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.159000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.356000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.356000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.356000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.356000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:58:26.356000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.479000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.479000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.479000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.479000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.480000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.480000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.480000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.498000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.498000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.498000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.498000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:58:27.498000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:58:28.361000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:58:28.361000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:58:28.361000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:58:28.361000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:58:28.361000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:58:34.961456 113854 finetune.py:45] layer 17_v initial loss 0.0001087945347535424
W0314 13:58:34.961754 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:58:40.431435 112455 finetune.py:68] layer 16_v @ epoch 1 new loss 3.827589898719452e-05 old loss 4.1436083847656846e-05 BETTER
I0314 13:59:05.832165 9966 quantize_finetune_llama.py:186] computed original embedding for layer 18 in 61.742515563964844s
I0314 13:59:06.198946 9966 quantize_finetune_llama.py:159] layer 19 gpu 3
I0314 13:59:08.317727 115184 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 13:59:08.317895 115184 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 13:59:08.317960 115184 utils.py:162] NumExpr defaulting to 16 threads.
I0314 13:59:08.582960 115184 config.py:58] PyTorch version 2.4.0 available.
I0314 13:59:09.387085 113854 finetune.py:68] layer 17_v @ epoch 0 new loss 4.8967696784529835e-05 old loss 0.0001087945347535424 BETTER
I0314 13:59:10.911185 115184 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 13:59:11.329497 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:55,  1.81s/it]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s]  9%|▉         | 3/32 [00:02<00:19,  1.49it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.84it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.12it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.33it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.79it/s]I0314 13:59:17.890871 112455 finetune.py:68] layer 16_v @ epoch 2 new loss 3.6622732295654714e-05 old loss 3.827589898719452e-05 BETTER
 38%|███▊      | 12/32 [00:05<00:07,  2.82it/s] 41%|████      | 13/32 [00:05<00:06,  2.85it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.88it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.86it/s] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.89it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.91it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.90it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.88it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.88it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.89it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.88it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
W0314 13:59:27.494000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.495000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.495000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.495000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.495000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.495000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.495000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.521000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.521000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.522000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.522000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.522000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.824000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.824000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.824000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.824000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 13:59:27.824000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.710000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.711000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.711000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.711000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.711000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.711000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.711000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.745000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.745000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.745000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.746000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.746000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.951000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.951000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.951000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.951000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 13:59:28.951000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.079000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.080000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.080000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.080000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.080000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.080000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.080000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.098000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.098000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.098000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.098000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.099000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.979000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.979000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.979000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.979000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 13:59:30.979000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 13:59:37.108776 115184 finetune.py:45] layer 18_v initial loss 0.00010700031998567283
W0314 13:59:37.109127 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 13:59:44.946311 113854 finetune.py:68] layer 17_v @ epoch 1 new loss 4.401242404128425e-05 old loss 4.8967696784529835e-05 BETTER
I0314 13:59:55.661762 112455 finetune.py:68] layer 16_v @ epoch 3 new loss 3.55015326931607e-05 old loss 3.6622732295654714e-05 BETTER
I0314 14:00:08.400002 9966 quantize_finetune_llama.py:186] computed original embedding for layer 19 in 61.71577572822571s
I0314 14:00:08.821014 9966 quantize_finetune_llama.py:159] layer 20 gpu 0
I0314 14:00:10.898771 116548 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 14:00:10.898932 116548 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 14:00:10.899008 116548 utils.py:162] NumExpr defaulting to 16 threads.
I0314 14:00:11.101312 116548 config.py:58] PyTorch version 2.4.0 available.
I0314 14:00:11.526702 115184 finetune.py:68] layer 18_v @ epoch 0 new loss 3.305294376332313e-05 old loss 0.00010700031998567283 BETTER
I0314 14:00:13.630222 116548 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 14:00:14.087525 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:55,  1.78s/it]  6%|▋         | 2/32 [00:02<00:28,  1.07it/s]  9%|▉         | 3/32 [00:02<00:19,  1.50it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.86it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.14it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s]I0314 14:00:20.564057 113854 finetune.py:68] layer 17_v @ epoch 2 new loss 4.176631409791298e-05 old loss 4.401242404128425e-05 BETTER
 34%|███▍      | 11/32 [00:05<00:07,  2.80it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.88it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.89it/s] 50%|█████     | 16/32 [00:06<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.88it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.90it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.91it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.91it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.91it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.91it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.93it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.93it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.94it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  2.93it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
W0314 14:00:30.298000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.298000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.298000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.298000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.298000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.298000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.299000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.325000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.326000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.326000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.326000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.326000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.624000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.624000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.624000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.625000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:30.625000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.499000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.500000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.500000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.500000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.500000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.500000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.500000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.518000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.518000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.518000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.518000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.518000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.721000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.721000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.722000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.722000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:31.722000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.855000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.855000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.855000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.855000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.855000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.855000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.855000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.873000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.873000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.874000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.874000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:32.874000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
I0314 14:00:33.434070 112455 finetune.py:68] layer 16_v @ epoch 4 new loss 3.470933370408602e-05 old loss 3.55015326931607e-05 BETTER
W0314 14:00:33.761000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:33.761000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:33.761000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:33.762000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:33.762000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0314 14:00:35.020360 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_v proxy err 0.03554375842213631 tr(WHW.T) 68.60269165039062
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.62it/s]  9%|▉         | 3/32 [00:01<00:14,  1.99it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s]I0314 14:00:39.606507 116548 finetune.py:45] layer 19_v initial loss 0.00014205006300471723
W0314 14:00:39.606749 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.70it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s]I0314 14:00:46.998895 115184 finetune.py:68] layer 18_v @ epoch 1 new loss 2.8908079912071116e-05 old loss 3.305294376332313e-05 BETTER
 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
W0314 14:00:53.747000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.748000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.748000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.748000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.748000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.748000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.748000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.779000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.779000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.779000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.779000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.779000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.955000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.955000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.955000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.955000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:53.955000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.186000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.186000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.186000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.186000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.186000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.186000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.186000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.209000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.209000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.209000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.209000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.209000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.276000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.276000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.277000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.277000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:54.277000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.029000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.354000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.354000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.354000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.354000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.354000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.354000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.354000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.377000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.377000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.377000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.377000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.377000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.641000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.641000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.641000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.641000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:00:55.641000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:00:56.002000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:00:56.734994 113854 finetune.py:68] layer 17_v @ epoch 3 new loss 4.034924859297462e-05 old loss 4.176631409791298e-05 BETTER
I0314 14:01:03.084160 112455 finetune.py:45] layer 16_q initial loss 5.6279313866980374e-05
W0314 14:01:03.084585 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:01:12.684828 116548 finetune.py:68] layer 19_v @ epoch 0 new loss 3.9018457755446434e-05 old loss 0.00014205006300471723 BETTER
I0314 14:01:23.031392 115184 finetune.py:68] layer 18_v @ epoch 2 new loss 2.7304367904434912e-05 old loss 2.8908079912071116e-05 BETTER
I0314 14:01:32.984866 113854 finetune.py:68] layer 17_v @ epoch 4 new loss 3.935478162020445e-05 old loss 4.034924859297462e-05 BETTER
W0314 14:01:34.751268 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_v proxy err 0.046600982546806335 tr(WHW.T) 69.1786880493164
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s]I0314 14:01:39.866937 112455 finetune.py:68] layer 16_q @ epoch 0 new loss 5.3722909797215834e-05 old loss 5.6279313866980374e-05 BETTER
 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.57it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.57it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s]I0314 14:01:47.159749 116548 finetune.py:68] layer 19_v @ epoch 1 new loss 3.380837006261572e-05 old loss 3.9018457755446434e-05 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
W0314 14:01:54.045000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.045000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.045000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.045000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.045000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.046000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.046000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.077000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.077000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.077000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.077000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.078000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.256000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.256000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.257000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.257000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.257000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.482000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.482000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.482000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.482000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.482000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.482000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.482000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.506000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.506000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.506000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.506000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.506000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.578000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.578000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.578000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.579000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:01:54.579000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.341000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.667000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.667000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.667000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.667000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.667000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.667000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.667000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.689000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.689000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.689000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.690000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.690000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.952000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.953000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.953000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.953000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:01:55.953000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:01:56.313000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:01:59.230505 115184 finetune.py:68] layer 18_v @ epoch 3 new loss 2.635290911712218e-05 old loss 2.7304367904434912e-05 BETTER
I0314 14:02:02.999771 113854 finetune.py:45] layer 17_q initial loss 5.8648620324674994e-05
W0314 14:02:03.000048 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:02:17.649267 112455 finetune.py:68] layer 16_q @ epoch 1 new loss 5.239315578364767e-05 old loss 5.3722909797215834e-05 BETTER
I0314 14:02:22.364640 116548 finetune.py:68] layer 19_v @ epoch 2 new loss 3.188004848198034e-05 old loss 3.380837006261572e-05 BETTER
I0314 14:02:35.799765 115184 finetune.py:68] layer 18_v @ epoch 4 new loss 2.570261131040752e-05 old loss 2.635290911712218e-05 BETTER
W0314 14:02:37.523454 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 14:02:38.136842 113854 finetune.py:68] layer 17_q @ epoch 0 new loss 5.660442911903374e-05 old loss 5.8648620324674994e-05 BETTER
18_v proxy err 0.0387101024389267 tr(WHW.T) 73.61924743652344
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:06,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0314 14:02:55.586418 112455 finetune.py:68] layer 16_q @ epoch 2 new loss 5.135043465998024e-05 old loss 5.239315578364767e-05 BETTER
W0314 14:02:56.423000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.424000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.424000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.424000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.424000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.424000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.424000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.456000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.456000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.456000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.456000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.456000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.636000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.636000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.636000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.637000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.637000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.865000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.865000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.865000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.865000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.865000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.865000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.865000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.885000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.885000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.885000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.885000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.885000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.955000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.955000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.956000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.956000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:02:56.956000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
I0314 14:02:57.324824 116548 finetune.py:68] layer 19_v @ epoch 3 new loss 3.080131864408031e-05 old loss 3.188004848198034e-05 BETTER
W0314 14:02:57.715000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.044000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.044000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.044000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.044000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.044000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.044000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.044000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.068000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.068000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.068000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.068000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.068000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.319000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.319000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.320000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.320000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.320000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:02:58.687000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:03:05.197666 115184 finetune.py:45] layer 18_q initial loss 4.4305026676738635e-05
W0314 14:03:05.197952 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:03:13.922022 113854 finetune.py:68] layer 17_q @ epoch 1 new loss 5.540877828025259e-05 old loss 5.660442911903374e-05 BETTER
I0314 14:03:32.492307 116548 finetune.py:68] layer 19_v @ epoch 4 new loss 3.0059058190090582e-05 old loss 3.080131864408031e-05 BETTER
I0314 14:03:33.353361 112455 finetune.py:68] layer 16_q @ epoch 3 new loss 5.0499074859544635e-05 old loss 5.135043465998024e-05 BETTER
W0314 14:03:34.471346 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_v proxy err 0.03766006976366043 tr(WHW.T) 87.24554443359375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:31,  1.01s/it]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:15,  1.90it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.50it/s]I0314 14:03:40.114195 115184 finetune.py:68] layer 18_q @ epoch 0 new loss 4.242404975229874e-05 old loss 4.4305026676738635e-05 BETTER
 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.57it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0314 14:03:50.147079 113854 finetune.py:68] layer 17_q @ epoch 2 new loss 5.4456635552924126e-05 old loss 5.540877828025259e-05 BETTER
W0314 14:03:54.077000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.077000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.077000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.077000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.077000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.078000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.078000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.109000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.109000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.109000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.109000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.109000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.283000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.284000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.284000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.284000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.284000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.520000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.520000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.521000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.521000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.521000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.521000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.521000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.545000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.545000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.545000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.545000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.545000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.613000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.613000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.613000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.613000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:03:54.613000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.379000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.709000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.709000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.709000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.709000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.710000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.710000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.710000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.732000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.732000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.732000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.733000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:03:55.733000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:03:56.002000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:03:56.003000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:03:56.003000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:03:56.003000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:03:56.003000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:03:56.361000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:04:02.847024 116548 finetune.py:45] layer 19_q initial loss 4.625160727300681e-05
W0314 14:04:02.847606 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:04:11.437290 112455 finetune.py:68] layer 16_q @ epoch 4 new loss 4.977354547008872e-05 old loss 5.0499074859544635e-05 BETTER
W0314 14:04:13.357671 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_q proxy err 0.006889969576150179 tr(WHW.T) 6127.1513671875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.07it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]I0314 14:04:16.309491 115184 finetune.py:68] layer 18_q @ epoch 1 new loss 4.147918298258446e-05 old loss 4.242404975229874e-05 BETTER
  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.69it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s]I0314 14:04:26.356957 113854 finetune.py:68] layer 17_q @ epoch 3 new loss 5.364829485188238e-05 old loss 5.4456635552924126e-05 BETTER
 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
I0314 14:04:34.120439 112455 finetune.py:45] layer 16_k initial loss 6.759374809917063e-05
W0314 14:04:34.120785 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:04:36.759265 116548 finetune.py:68] layer 19_q @ epoch 0 new loss 4.431292472872883e-05 old loss 4.625160727300681e-05 BETTER
I0314 14:04:52.607208 115184 finetune.py:68] layer 18_q @ epoch 2 new loss 4.0782288124319166e-05 old loss 4.147918298258446e-05 BETTER
I0314 14:05:02.983339 113854 finetune.py:68] layer 17_q @ epoch 4 new loss 5.300058546708897e-05 old loss 5.364829485188238e-05 BETTER
W0314 14:05:04.711699 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_q proxy err 0.006998797878623009 tr(WHW.T) 6714.853515625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s]I0314 14:05:11.534610 112455 finetune.py:68] layer 16_k @ epoch 0 new loss 6.190915155457333e-05 old loss 6.759374809917063e-05 BETTER
 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s]I0314 14:05:12.017226 116548 finetune.py:68] layer 19_q @ epoch 1 new loss 4.334237746661529e-05 old loss 4.431292472872883e-05 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0314 14:05:25.624625 113854 finetune.py:45] layer 17_k initial loss 6.848953489679843e-05
W0314 14:05:25.624925 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:05:29.057884 115184 finetune.py:68] layer 18_q @ epoch 3 new loss 4.019340485683642e-05 old loss 4.0782288124319166e-05 BETTER
I0314 14:05:46.872967 116548 finetune.py:68] layer 19_q @ epoch 2 new loss 4.2610317905200645e-05 old loss 4.334237746661529e-05 BETTER
I0314 14:05:50.122413 112455 finetune.py:68] layer 16_k @ epoch 1 new loss 6.100746395532042e-05 old loss 6.190915155457333e-05 BETTER
I0314 14:06:00.476800 113854 finetune.py:68] layer 17_k @ epoch 0 new loss 6.514284905279055e-05 old loss 6.848953489679843e-05 BETTER
I0314 14:06:05.289966 115184 finetune.py:68] layer 18_q @ epoch 4 new loss 3.971498517785221e-05 old loss 4.019340485683642e-05 BETTER
W0314 14:06:06.982856 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_q proxy err 0.008547969162464142 tr(WHW.T) 5736.748046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0314 14:06:21.953007 116548 finetune.py:68] layer 19_q @ epoch 3 new loss 4.202706986689009e-05 old loss 4.2610317905200645e-05 BETTER
I0314 14:06:27.556410 115184 finetune.py:45] layer 18_k initial loss 5.15399660798721e-05
W0314 14:06:27.556899 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:06:28.957091 112455 finetune.py:68] layer 16_k @ epoch 2 new loss 6.032438250258565e-05 old loss 6.100746395532042e-05 BETTER
I0314 14:06:36.643080 113854 finetune.py:68] layer 17_k @ epoch 1 new loss 6.437957927118987e-05 old loss 6.514284905279055e-05 BETTER
I0314 14:06:56.933094 116548 finetune.py:68] layer 19_q @ epoch 4 new loss 4.1540286474628374e-05 old loss 4.202706986689009e-05 BETTER
W0314 14:06:58.934396 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_q proxy err 0.008221857249736786 tr(WHW.T) 6152.84423828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.07it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s]I0314 14:07:02.917870 115184 finetune.py:68] layer 18_k @ epoch 0 new loss 4.9770900659495965e-05 old loss 5.15399660798721e-05 BETTER
 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s]I0314 14:07:07.833520 112455 finetune.py:68] layer 16_k @ epoch 3 new loss 5.978368062642403e-05 old loss 6.032438250258565e-05 BETTER
 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]I0314 14:07:12.990200 113854 finetune.py:68] layer 17_k @ epoch 2 new loss 6.382839637808502e-05 old loss 6.437957927118987e-05 BETTER
100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0314 14:07:19.894562 116548 finetune.py:45] layer 19_k initial loss 5.823042010888457e-05
W0314 14:07:19.894982 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:07:39.078509 115184 finetune.py:68] layer 18_k @ epoch 1 new loss 4.923928645439446e-05 old loss 4.9770900659495965e-05 BETTER
I0314 14:07:45.906793 112455 finetune.py:68] layer 16_k @ epoch 4 new loss 5.93321819906123e-05 old loss 5.978368062642403e-05 BETTER
W0314 14:07:47.636433 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_k proxy err 0.004963929299265146 tr(WHW.T) 4859.375
  0%|          | 0/32 [00:00<?, ?it/s]I0314 14:07:49.158405 113854 finetune.py:68] layer 17_k @ epoch 3 new loss 6.333208875730634e-05 old loss 6.382839637808502e-05 BETTER
  3%|▎         | 1/32 [00:00<00:24,  1.25it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s]  9%|▉         | 3/32 [00:01<00:13,  2.17it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.58it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s]I0314 14:07:54.010436 116548 finetune.py:68] layer 19_k @ epoch 0 new loss 5.274415161693469e-05 old loss 5.823042010888457e-05 BETTER
 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
I0314 14:08:08.727563 112455 finetune.py:45] layer 16_o initial loss 0.00011492616613395512
W0314 14:08:08.727952 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:08:15.421214 115184 finetune.py:68] layer 18_k @ epoch 2 new loss 4.877706305705942e-05 old loss 4.923928645439446e-05 BETTER
I0314 14:08:26.264069 113854 finetune.py:68] layer 17_k @ epoch 4 new loss 6.290227611316368e-05 old loss 6.333208875730634e-05 BETTER
W0314 14:08:28.056602 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 14:08:29.086769 116548 finetune.py:68] layer 19_k @ epoch 1 new loss 5.194013283471577e-05 old loss 5.274415161693469e-05 BETTER
17_k proxy err 0.006302420515567064 tr(WHW.T) 4225.625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.55it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0314 14:08:46.302442 112455 finetune.py:68] layer 16_o @ epoch 0 new loss 0.00011011675087502226 old loss 0.00011492616613395512 BETTER
I0314 14:08:49.472742 113854 finetune.py:45] layer 17_o initial loss 0.00011757813626900315
W0314 14:08:49.473179 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:08:51.945621 115184 finetune.py:68] layer 18_k @ epoch 3 new loss 4.842897760681808e-05 old loss 4.877706305705942e-05 BETTER
I0314 14:09:04.220984 116548 finetune.py:68] layer 19_k @ epoch 2 new loss 5.144229726283811e-05 old loss 5.194013283471577e-05 BETTER
I0314 14:09:24.564688 113854 finetune.py:68] layer 17_o @ epoch 0 new loss 0.00011050840112147853 old loss 0.00011757813626900315 BETTER
I0314 14:09:24.764974 112455 finetune.py:68] layer 16_o @ epoch 1 new loss 0.00010831421968759969 old loss 0.00011011675087502226 BETTER
I0314 14:09:28.314339 115184 finetune.py:68] layer 18_k @ epoch 4 new loss 4.810078826267272e-05 old loss 4.842897760681808e-05 BETTER
W0314 14:09:30.016500 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_k proxy err 0.006813694257289171 tr(WHW.T) 4440.98046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.26it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s]  9%|▉         | 3/32 [00:01<00:13,  2.16it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.35it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.46it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s]I0314 14:09:39.065317 116548 finetune.py:68] layer 19_k @ epoch 3 new loss 5.104942465550266e-05 old loss 5.144229726283811e-05 BETTER
 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
I0314 14:09:50.803511 115184 finetune.py:45] layer 18_o initial loss 9.267565474146977e-05
W0314 14:09:50.803928 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:10:00.447093 113854 finetune.py:68] layer 17_o @ epoch 1 new loss 0.00010874710278585553 old loss 0.00011050840112147853 BETTER
I0314 14:10:02.949816 112455 finetune.py:68] layer 16_o @ epoch 2 new loss 0.00010697630204958841 old loss 0.00010831421968759969 BETTER
I0314 14:10:14.045710 116548 finetune.py:68] layer 19_k @ epoch 4 new loss 5.072376370662823e-05 old loss 5.104942465550266e-05 BETTER
W0314 14:10:16.081344 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_k proxy err 0.007203943561762571 tr(WHW.T) 3966.10986328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.19it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.51it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.53it/s] 50%|█████     | 16/32 [00:06<00:06,  2.53it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.54it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.54it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.54it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s]I0314 14:10:26.406252 115184 finetune.py:68] layer 18_o @ epoch 0 new loss 8.499762770952657e-05 old loss 9.267565474146977e-05 BETTER
 69%|██████▉   | 22/32 [00:09<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.55it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.55it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.55it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.55it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.56it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]100%|██████████| 32/32 [00:12<00:00,  2.46it/s]
I0314 14:10:36.802304 113854 finetune.py:68] layer 17_o @ epoch 2 new loss 0.00010749084322014824 old loss 0.00010874710278585553 BETTER
I0314 14:10:37.740240 116548 finetune.py:45] layer 19_o initial loss 9.343789133708924e-05
W0314 14:10:37.740928 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:10:40.746512 112455 finetune.py:68] layer 16_o @ epoch 3 new loss 0.00010584732808638364 old loss 0.00010697630204958841 BETTER
I0314 14:11:02.316345 115184 finetune.py:68] layer 18_o @ epoch 1 new loss 8.368550334125757e-05 old loss 8.499762770952657e-05 BETTER
I0314 14:11:12.032499 116548 finetune.py:68] layer 19_o @ epoch 0 new loss 8.501969568897039e-05 old loss 9.343789133708924e-05 BETTER
I0314 14:11:12.686522 113854 finetune.py:68] layer 17_o @ epoch 3 new loss 0.00010649108298821375 old loss 0.00010749084322014824 BETTER
I0314 14:11:18.606465 112455 finetune.py:68] layer 16_o @ epoch 4 new loss 0.00010490553540876135 old loss 0.00010584732808638364 BETTER
W0314 14:11:20.259244 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_o proxy err 0.037917837500572205 tr(WHW.T) 7.442371368408203
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it]I0314 14:11:38.415050 115184 finetune.py:68] layer 18_o @ epoch 2 new loss 8.280932524939999e-05 old loss 8.368550334125757e-05 BETTER
 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it]I0314 14:11:46.991417 116548 finetune.py:68] layer 19_o @ epoch 1 new loss 8.379443897865713e-05 old loss 8.501969568897039e-05 BETTER
 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it]I0314 14:11:48.577760 113854 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0001056482651620172 old loss 0.00010649108298821375 BETTER
 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it]W0314 14:11:50.162335 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it]17_o proxy err 0.03949935361742973 tr(WHW.T) 6.440531253814697
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it]  6%|▋         | 2/32 [00:03<00:52,  1.74s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.47s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.47s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it]I0314 14:12:14.597684 115184 finetune.py:68] layer 18_o @ epoch 3 new loss 8.214137051254511e-05 old loss 8.280932524939999e-05 BETTER
 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it]I0314 14:12:17.192532 112455 finetune.py:45] layer 16_up initial loss 0.0002625025808811188
W0314 14:12:17.192927 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.54s/it]I0314 14:12:22.047361 116548 finetune.py:68] layer 19_o @ epoch 2 new loss 8.301417256006971e-05 old loss 8.379443897865713e-05 BETTER
 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.54s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.55s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.55s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0314 14:12:48.928385 113854 finetune.py:45] layer 17_up initial loss 0.000283070927252993
W0314 14:12:48.928752 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:12:50.636125 115184 finetune.py:68] layer 18_o @ epoch 4 new loss 8.158439595717937e-05 old loss 8.214137051254511e-05 BETTER
W0314 14:12:52.344307 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 14:12:53.002844 112455 finetune.py:68] layer 16_up @ epoch 0 new loss 0.00025726648163981736 old loss 0.0002625025808811188 BETTER
18_o proxy err 0.03812137246131897 tr(WHW.T) 4.759472370147705
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]I0314 14:12:56.672702 116548 finetune.py:68] layer 19_o @ epoch 3 new loss 8.241729665314779e-05 old loss 8.301417256006971e-05 BETTER
  6%|▋         | 2/32 [00:03<00:50,  1.69s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it]I0314 14:13:22.847451 113854 finetune.py:68] layer 17_up @ epoch 0 new loss 0.00027728243730962276 old loss 0.000283070927252993 BETTER
 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.53s/it]I0314 14:13:29.853761 112455 finetune.py:68] layer 16_up @ epoch 1 new loss 0.000253979378612712 old loss 0.00025726648163981736 BETTER
 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it]I0314 14:13:31.317075 116548 finetune.py:68] layer 19_o @ epoch 4 new loss 8.193174289772287e-05 old loss 8.241729665314779e-05 BETTER
 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it]W0314 14:13:32.893006 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_o proxy err 0.040403012186288834 tr(WHW.T) 3.9542078971862793
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it]  3%|▎         | 1/32 [00:01<01:01,  2.00s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it]100%|██████████| 32/32 [00:48<00:00,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]
 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it]I0314 14:13:49.345628 115184 finetune.py:45] layer 18_up initial loss 0.0002640825987327844
W0314 14:13:49.345835 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it]I0314 14:13:57.002237 113854 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0002736994356382638 old loss 0.00027728243730962276 BETTER
 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it]I0314 14:14:06.218481 112455 finetune.py:68] layer 16_up @ epoch 2 new loss 0.00025119807105511427 old loss 0.000253979378612712 BETTER
 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.54s/it]I0314 14:14:22.910673 115184 finetune.py:68] layer 18_up @ epoch 0 new loss 0.00025895293219946325 old loss 0.0002640825987327844 BETTER
100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0314 14:14:31.177642 116548 finetune.py:45] layer 19_up initial loss 0.00027740042423829436
W0314 14:14:31.178045 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:14:31.209942 113854 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0002707526437006891 old loss 0.0002736994356382638 BETTER
I0314 14:14:42.541113 112455 finetune.py:68] layer 16_up @ epoch 3 new loss 0.0002487232559360564 old loss 0.00025119807105511427 BETTER
I0314 14:14:57.543236 115184 finetune.py:68] layer 18_up @ epoch 1 new loss 0.00025571120204403996 old loss 0.00025895293219946325 BETTER
I0314 14:15:03.390080 116548 finetune.py:68] layer 19_up @ epoch 0 new loss 0.00027234407025389373 old loss 0.00027740042423829436 BETTER
I0314 14:15:05.241769 113854 finetune.py:68] layer 17_up @ epoch 3 new loss 0.00026809467817656696 old loss 0.0002707526437006891 BETTER
I0314 14:15:18.827149 112455 finetune.py:68] layer 16_up @ epoch 4 new loss 0.00024645382654853165 old loss 0.0002487232559360564 BETTER
W0314 14:15:20.343587 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_up proxy err 0.039521101862192154 tr(WHW.T) 980.6290283203125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it]  9%|▉         | 3/32 [00:04<00:45,  1.55s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it]I0314 14:15:32.003694 115184 finetune.py:68] layer 18_up @ epoch 2 new loss 0.0002530580386519432 old loss 0.00025571120204403996 BETTER
 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it]I0314 14:15:36.285492 116548 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0002692047564778477 old loss 0.00027234407025389373 BETTER
 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.47s/it]I0314 14:15:39.701843 113854 finetune.py:68] layer 17_up @ epoch 4 new loss 0.0002657559234648943 old loss 0.00026809467817656696 BETTER
 41%|████      | 13/32 [00:19<00:27,  1.47s/it]W0314 14:15:41.093311 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_up proxy err 0.038692690432071686 tr(WHW.T) 1062.5152587890625
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:20<00:26,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it]  9%|▉         | 3/32 [00:04<00:47,  1.62s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.48s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it]I0314 14:16:06.658335 115184 finetune.py:68] layer 18_up @ epoch 3 new loss 0.00025068179820664227 old loss 0.0002530580386519432 BETTER
 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.48s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
I0314 14:16:09.938350 116548 finetune.py:68] layer 19_up @ epoch 2 new loss 0.00026652502128854394 old loss 0.0002692047564778477 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it]I0314 14:16:17.185378 112455 finetune.py:45] layer 16_gate initial loss 0.0003375322849024087
W0314 14:16:17.185804 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.54s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.55s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.55s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0314 14:16:40.049740 113854 finetune.py:45] layer 17_gate initial loss 0.0003729792661033571
W0314 14:16:40.050190 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:16:41.507651 115184 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0002485517761670053 old loss 0.00025068179820664227 BETTER
W0314 14:16:42.950534 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 14:16:43.836476 116548 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0002641970640979707 old loss 0.00026652502128854394 BETTER
18_up proxy err 0.0420847125351429 tr(WHW.T) 1056.513427734375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it]I0314 14:16:51.822734 112455 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.00033264319063164294 old loss 0.0003375322849024087 BETTER
 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 22%|██▏       | 7/32 [00:10<00:38,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it]I0314 14:17:12.679835 113854 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.00036709991400130093 old loss 0.0003729792661033571 BETTER
 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it]I0314 14:17:17.599102 116548 finetune.py:68] layer 19_up @ epoch 4 new loss 0.0002620941086206585 old loss 0.0002641970640979707 BETTER
 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it]W0314 14:17:19.027204 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it]19_up proxy err 0.04423525556921959 tr(WHW.T) 1068.1630859375
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it]  3%|▎         | 1/32 [00:01<01:00,  1.97s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it]I0314 14:17:26.998409 112455 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.0003299421805422753 old loss 0.00033264319063164294 BETTER
 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.49s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it]100%|██████████| 32/32 [00:48<00:00,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it]I0314 14:17:40.288959 115184 finetune.py:45] layer 18_gate initial loss 0.0003644687822088599
W0314 14:17:40.289329 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it]I0314 14:17:45.970661 113854 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.00036412369809113443 old loss 0.00036709991400130093 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it]I0314 14:18:02.341253 112455 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.0003276860516052693 old loss 0.0003299421805422753 BETTER
 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0314 14:18:13.467700 115184 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0003594377194531262 old loss 0.0003644687822088599 BETTER
I0314 14:18:17.994664 116548 finetune.py:45] layer 19_gate initial loss 0.0003894231631420553
W0314 14:18:17.995083 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:18:19.225416 113854 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.0003616869216784835 old loss 0.00036412369809113443 BETTER
I0314 14:18:37.559495 112455 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.00032570588518865407 old loss 0.0003276860516052693 BETTER
I0314 14:18:46.728042 115184 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.00035669520730152726 old loss 0.0003594377194531262 BETTER
I0314 14:18:49.371930 116548 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.0003842813312076032 old loss 0.0003894231631420553 BETTER
I0314 14:18:52.218193 113854 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.0003595565212890506 old loss 0.0003616869216784835 BETTER
I0314 14:19:12.770252 112455 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.0003238765930291265 old loss 0.00032570588518865407 BETTER
W0314 14:19:13.907033 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_gate proxy err 0.013998528942465782 tr(WHW.T) 4807.3720703125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s]  2%|▏         | 2/112 [00:01<01:03,  1.74it/s]  3%|▎         | 3/112 [00:01<00:52,  2.08it/s]  4%|▎         | 4/112 [00:01<00:47,  2.29it/s]  4%|▍         | 5/112 [00:02<00:43,  2.44it/s]  5%|▌         | 6/112 [00:02<00:41,  2.53it/s]I0314 14:19:20.156119 115184 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.00035446733818389475 old loss 0.00035669520730152726 BETTER
  6%|▋         | 7/112 [00:03<00:40,  2.60it/s]  7%|▋         | 8/112 [00:03<00:39,  2.64it/s]  8%|▊         | 9/112 [00:03<00:38,  2.67it/s]  9%|▉         | 10/112 [00:04<00:37,  2.69it/s]I0314 14:19:21.349056 116548 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0003816041280515492 old loss 0.0003842813312076032 BETTER
 10%|▉         | 11/112 [00:04<00:37,  2.70it/s] 11%|█         | 12/112 [00:04<00:36,  2.71it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.71it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.71it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.68it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.70it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.71it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.72it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.72it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.73it/s]I0314 14:19:25.144469 113854 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.0003576131130103022 old loss 0.0003595565212890506 BETTER
 19%|█▉        | 21/112 [00:08<00:33,  2.73it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.73it/s] 21%|██        | 23/112 [00:08<00:32,  2.74it/s]W0314 14:19:26.251121 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 21%|██▏       | 24/112 [00:09<00:32,  2.73it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.73it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.72it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.70it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.71it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.72it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.72it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.72it/s]17_gate proxy err 0.014065169729292393 tr(WHW.T) 5218.6435546875
  0%|          | 0/112 [00:00<?, ?it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.73it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.73it/s]  1%|          | 1/112 [00:00<01:35,  1.16it/s] 30%|███       | 34/112 [00:12<00:28,  2.73it/s]  2%|▏         | 2/112 [00:01<01:04,  1.72it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.73it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.73it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.73it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.73it/s]  4%|▍         | 5/112 [00:02<00:45,  2.35it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.73it/s]  5%|▌         | 6/112 [00:02<00:43,  2.43it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.70it/s]  6%|▋         | 7/112 [00:03<00:42,  2.48it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.71it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.73it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.71it/s]  9%|▉         | 10/112 [00:04<00:39,  2.57it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.71it/s] 10%|▉         | 11/112 [00:04<00:39,  2.59it/s] 40%|████      | 45/112 [00:17<00:24,  2.73it/s] 11%|█         | 12/112 [00:05<00:38,  2.60it/s] 41%|████      | 46/112 [00:17<00:24,  2.72it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.60it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.72it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.62it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.72it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.62it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.72it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.62it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.71it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.62it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.72it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.62it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.72it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.59it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.70it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.60it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.72it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.60it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.72it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 50%|█████     | 56/112 [00:21<00:20,  2.71it/s] 21%|██        | 23/112 [00:09<00:34,  2.62it/s] 51%|█████     | 57/112 [00:21<00:20,  2.72it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.61it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.62it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.71it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.62it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.72it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.62it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.72it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.62it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.72it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.61it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.72it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.61it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.73it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 59%|█████▉    | 66/112 [00:24<00:17,  2.70it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.58it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.71it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.60it/s] 61%|██████    | 68/112 [00:25<00:16,  2.72it/s] 30%|███       | 34/112 [00:13<00:29,  2.60it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.70it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.61it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.71it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.72it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.70it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.62it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.71it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.63it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.72it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.62it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.71it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.62it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.71it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.62it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.71it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.58it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s] 71%|███████   | 79/112 [00:29<00:12,  2.69it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.71it/s] 41%|████      | 46/112 [00:18<00:25,  2.60it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.72it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.61it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.70it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.62it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.72it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.62it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.72it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.61it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.70it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.62it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.71it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.61it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.71it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.61it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.70it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.71it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.61it/s] 80%|████████  | 90/112 [00:33<00:08,  2.70it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.58it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.71it/s] 50%|█████     | 56/112 [00:21<00:21,  2.59it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.68it/s] 51%|█████     | 57/112 [00:22<00:21,  2.59it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.70it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.60it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.71it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.61it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.69it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.61it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.70it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.61it/s]I0314 14:19:53.147708 115184 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.00035257198032923043 old loss 0.00035446733818389475 BETTER
 87%|████████▋ | 97/112 [00:36<00:05,  2.71it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.61it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.70it/s]I0314 14:19:53.752689 116548 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.00037944159703329206 old loss 0.0003816041280515492 BETTER
 56%|█████▋    | 63/112 [00:24<00:18,  2.61it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.71it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.62it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.71it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.61it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.70it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.60it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.71it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.58it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.71it/s] 61%|██████    | 68/112 [00:26<00:16,  2.59it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.70it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.60it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.68it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.61it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.70it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.61it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.71it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.61it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.70it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.61it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.70it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.62it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.71it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.62it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.70it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.62it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]100%|██████████| 112/112 [00:41<00:00,  2.68it/s]
 69%|██████▉   | 77/112 [00:29<00:13,  2.62it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.60it/s] 71%|███████   | 79/112 [00:30<00:12,  2.59it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.56it/s] 72%|███████▏  | 81/112 [00:31<00:12,  2.58it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.59it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.60it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.60it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.62it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.62it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.62it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.62it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.62it/s] 80%|████████  | 90/112 [00:34<00:08,  2.60it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.61it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.58it/s]W0314 14:20:05.033000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.033000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.033000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.033000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.033000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.033000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.034000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.078000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.079000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.079000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.079000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.079000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.261000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.261000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.261000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.261000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.262000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:36<00:07,  2.58it/s]W0314 14:20:05.584000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.584000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.585000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.585000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.585000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.585000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.585000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.620000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.620000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.620000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.620000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.620000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.700000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.700000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.700000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.700000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:05.700000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:36<00:06,  2.59it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.60it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s]W0314 14:20:06.691000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:06.705000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:06.714000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:06.714000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:37<00:05,  2.61it/s]W0314 14:20:07.171000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.171000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.171000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.171000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.171000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.171000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.171000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.203000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.203000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.203000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.203000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.203000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:38<00:05,  2.61it/s]W0314 14:20:07.506000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.506000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.506000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.506000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.506000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.507000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.507000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.507000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:38<00:04,  2.61it/s]W0314 14:20:07.803000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.803000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.803000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.803000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:07.803000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:38<00:04,  2.61it/s]W0314 14:20:08.236000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:08.241000 140454700103488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 90%|█████████ | 101/112 [00:39<00:04,  2.60it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.60it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.59it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.57it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.57it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.58it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.59it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.60it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.60it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.60it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]
I0314 14:20:15.569820 112455 finetune.py:45] layer 16_down initial loss 0.0005129160708747804
W0314 14:20:15.570209 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0314 14:20:18.956000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:18.956000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:18.956000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:18.956000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:18.956000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:18.956000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:18.957000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.002000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.002000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.002000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.002000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.002000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.179000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.180000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.180000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.180000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.180000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.504000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.504000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.504000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.505000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.505000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.505000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.505000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.541000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.541000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.541000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.542000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.542000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.614000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.614000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.614000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.614000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:19.614000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:20.607000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:20.622000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:20.630000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:20.630000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.085000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.085000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.085000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.085000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.085000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.085000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.085000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.114000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.114000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.114000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.114000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.114000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.415000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.416000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.416000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.416000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.416000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.416000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.416000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.416000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.716000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.716000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.716000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.716000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:20:21.716000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:22.155000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:20:22.160000 140409646462784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 14:20:25.849696 116548 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.00037754682125523686 old loss 0.00037944159703329206 BETTER
I0314 14:20:26.219192 115184 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.0003508349182084203 old loss 0.00035257198032923043 BETTER
W0314 14:20:27.538835 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 14:20:29.356576 113854 finetune.py:45] layer 17_down initial loss 0.0005880886455997825
W0314 14:20:29.356970 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

18_gate proxy err 0.0173727348446846 tr(WHW.T) 4635.14697265625
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:38,  1.13it/s]  2%|▏         | 2/112 [00:01<01:05,  1.69it/s]  3%|▎         | 3/112 [00:01<00:53,  2.02it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s]  4%|▍         | 5/112 [00:02<00:45,  2.35it/s]  5%|▌         | 6/112 [00:02<00:43,  2.44it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s]  7%|▋         | 8/112 [00:03<00:40,  2.56it/s]  8%|▊         | 9/112 [00:03<00:39,  2.59it/s]  9%|▉         | 10/112 [00:04<00:39,  2.61it/s] 10%|▉         | 11/112 [00:04<00:38,  2.63it/s] 11%|█         | 12/112 [00:05<00:37,  2.64it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.65it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.65it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.65it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.65it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.65it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.64it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.63it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.64it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.64it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.66it/s] 21%|██        | 23/112 [00:09<00:33,  2.67it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.67it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.68it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.68it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.67it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.67it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.66it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.66it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.62it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.65it/s] 30%|███       | 34/112 [00:13<00:29,  2.66it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.67it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.67it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.68it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.68it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.68it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.68it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.69it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.68it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.67it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.63it/s] 40%|████      | 45/112 [00:17<00:25,  2.65it/s]I0314 14:20:48.335680 112455 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0005119523266330361 old loss 0.0005129160708747804 BETTER
 41%|████      | 46/112 [00:17<00:24,  2.66it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.67it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.67it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.68it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.68it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.69it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.69it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.69it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.69it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.68it/s] 50%|█████     | 56/112 [00:21<00:20,  2.68it/s] 51%|█████     | 57/112 [00:21<00:20,  2.65it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.66it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.66it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.67it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.68it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.69it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.69it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.69it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.68it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.69it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.68it/s] 61%|██████    | 68/112 [00:26<00:16,  2.69it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.68it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.65it/s]I0314 14:20:57.883734 116548 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.00037588278064504266 old loss 0.00037754682125523686 BETTER
 63%|██████▎   | 71/112 [00:27<00:15,  2.66it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.67it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.68it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.68it/s]W0314 14:20:59.088537 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 67%|██████▋   | 75/112 [00:28<00:13,  2.69it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.69it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.69it/s]I0314 14:21:00.301877 113854 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0005871477187611163 old loss 0.0005880886455997825 BETTER
 70%|██████▉   | 78/112 [00:29<00:12,  2.69it/s] 71%|███████   | 79/112 [00:30<00:12,  2.68it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.68it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.67it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.63it/s]19_gate proxy err 0.01914462074637413 tr(WHW.T) 4557.87646484375
  0%|          | 0/112 [00:00<?, ?it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.64it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.66it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.66it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.67it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.68it/s]  4%|▎         | 4/112 [00:02<00:49,  2.17it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.69it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.69it/s]  4%|▍         | 5/112 [00:02<00:46,  2.29it/s] 80%|████████  | 90/112 [00:34<00:08,  2.68it/s]  5%|▌         | 6/112 [00:02<00:44,  2.37it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.69it/s]  6%|▋         | 7/112 [00:03<00:43,  2.43it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.68it/s]  7%|▋         | 8/112 [00:03<00:42,  2.46it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.69it/s]  8%|▊         | 9/112 [00:04<00:41,  2.49it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.67it/s]  9%|▉         | 10/112 [00:04<00:40,  2.51it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.67it/s] 10%|▉         | 11/112 [00:04<00:40,  2.51it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.64it/s] 11%|█         | 12/112 [00:05<00:39,  2.52it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.65it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.54it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.65it/s] 12%|█▎        | 14/112 [00:06<00:39,  2.51it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.65it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.52it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.66it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.53it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.66it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.54it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.67it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.54it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.67it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.54it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.67it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.54it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.66it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.54it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.66it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.54it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.66it/s] 21%|██        | 23/112 [00:09<00:35,  2.54it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.65it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.54it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.63it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.55it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.64it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.65it/s] 23%|██▎       | 26/112 [00:10<00:34,  2.50it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
 24%|██▍       | 27/112 [00:11<00:33,  2.52it/s] 25%|██▌       | 28/112 [00:11<00:33,  2.54it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.54it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.54it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.55it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.56it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.57it/s] 30%|███       | 34/112 [00:13<00:30,  2.57it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.57it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.56it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.55it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.55it/s] 35%|███▍      | 39/112 [00:15<00:29,  2.51it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.53it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.55it/s] 38%|███▊      | 42/112 [00:17<00:27,  2.55it/s]W0314 14:21:19.366000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.366000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.367000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.367000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.367000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.367000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.367000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.413000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.413000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.413000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.413000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.413000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.595000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.595000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.595000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.595000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.595000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 43/112 [00:17<00:26,  2.56it/s]W0314 14:21:19.924000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.924000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.924000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.925000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.925000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.925000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.925000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.958000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.958000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.958000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.958000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:19.958000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 39%|███▉      | 44/112 [00:17<00:26,  2.56it/s]W0314 14:21:20.030000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:20.031000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:20.031000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:20.031000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:20.031000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 40%|████      | 45/112 [00:18<00:26,  2.56it/s] 41%|████      | 46/112 [00:18<00:25,  2.56it/s]W0314 14:21:21.040000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.056000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.065000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.065000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 42%|████▏     | 47/112 [00:18<00:25,  2.55it/s]W0314 14:21:21.547000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.547000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.547000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.547000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.547000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.547000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.548000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 43%|████▎     | 48/112 [00:19<00:25,  2.55it/s]W0314 14:21:21.576000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.577000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.577000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.577000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.577000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
I0314 14:21:21.668501 112455 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0005117079126648605 old loss 0.0005119523266330361 BETTER
W0314 14:21:21.889000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.889000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.889000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.889000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.889000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.889000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.889000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:21.890000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 49/112 [00:19<00:24,  2.55it/s]W0314 14:21:22.195000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:22.196000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:22.196000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:22.196000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:22.196000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 45%|████▍     | 50/112 [00:20<00:24,  2.54it/s]W0314 14:21:22.643000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:22.648000 140081856751424 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 46%|████▌     | 51/112 [00:20<00:24,  2.54it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.50it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.51it/s] 48%|████▊     | 54/112 [00:21<00:23,  2.52it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.52it/s] 50%|█████     | 56/112 [00:22<00:22,  2.54it/s] 51%|█████     | 57/112 [00:22<00:21,  2.55it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.56it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.57it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.57it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.56it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.56it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.53it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.54it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.55it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.56it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.57it/s] 61%|██████    | 68/112 [00:27<00:17,  2.57it/s]I0314 14:21:29.719548 115184 finetune.py:45] layer 18_down initial loss 0.0005832773167639971
W0314 14:21:29.719967 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▏   | 69/112 [00:27<00:16,  2.58it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.57it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.56it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.56it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.55it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.54it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.54it/s]I0314 14:21:32.173188 113854 finetune.py:68] layer 17_down @ epoch 1 new loss 0.0005869083106517792 old loss 0.0005871477187611163 BETTER
 68%|██████▊   | 76/112 [00:30<00:14,  2.51it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.53it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.54it/s] 71%|███████   | 79/112 [00:31<00:12,  2.54it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.55it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.56it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.55it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.55it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.56it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.57it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.57it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.56it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.56it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.53it/s] 80%|████████  | 90/112 [00:35<00:08,  2.54it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.55it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.55it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.54it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.55it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.56it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.56it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.57it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.56it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.55it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.55it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.50it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.52it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.53it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.53it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.54it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.54it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.55it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.55it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.56it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.55it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.56it/s]100%|██████████| 112/112 [00:44<00:00,  2.55it/s]100%|██████████| 112/112 [00:44<00:00,  2.52it/s]
W0314 14:21:52.875000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.876000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.876000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.876000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.876000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.876000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.876000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.921000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.921000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.922000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.922000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:52.922000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.099000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.099000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.099000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.099000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.099000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.421000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.421000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.421000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.422000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.422000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.422000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.422000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.456000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.456000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.456000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.456000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.457000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.527000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.527000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.527000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.527000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:53.528000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:54.509000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:54.523000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:54.531000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:54.531000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.007000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.007000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.007000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.007000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.007000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.007000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.007000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.038000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.039000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.039000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.039000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.039000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
I0314 14:21:55.171562 112455 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0005115519743412733 old loss 0.0005117079126648605 BETTER
W0314 14:21:55.339000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.339000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.340000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.340000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.340000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.340000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.340000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.340000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.638000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.639000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.639000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.639000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:21:55.639000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:21:56.073000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:21:56.078000 140020949276480 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 14:22:01.224965 115184 finetune.py:68] layer 18_down @ epoch 0 new loss 0.000582419685088098 old loss 0.0005832773167639971 BETTER
I0314 14:22:03.041334 116548 finetune.py:45] layer 19_down initial loss 0.0006163246580399573
W0314 14:22:03.041745 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:22:03.981141 113854 finetune.py:68] layer 17_down @ epoch 2 new loss 0.0005867318250238895 old loss 0.0005869083106517792 BETTER
I0314 14:22:28.563617 112455 finetune.py:68] layer 16_down @ epoch 3 new loss 0.0005114094237796962 old loss 0.0005115519743412733 BETTER
I0314 14:22:32.682007 115184 finetune.py:68] layer 18_down @ epoch 1 new loss 0.0005821638624183834 old loss 0.000582419685088098 BETTER
I0314 14:22:32.966262 116548 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0006154256989248097 old loss 0.0006163246580399573 BETTER
I0314 14:22:35.475011 113854 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0005866112187504768 old loss 0.0005867318250238895 BETTER
I0314 14:23:01.960106 112455 finetune.py:68] layer 16_down @ epoch 4 new loss 0.0005113070365041494 old loss 0.0005114094237796962 BETTER
W0314 14:23:02.866475 112455 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

16_down proxy err 0.04727327451109886 tr(WHW.T) 18.068477630615234
I0314 14:23:03.575136 116548 finetune.py:68] layer 19_down @ epoch 1 new loss 0.0006151541019789875 old loss 0.0006154256989248097 BETTER
I0314 14:23:04.456603 115184 finetune.py:68] layer 18_down @ epoch 2 new loss 0.0005819933721795678 old loss 0.0005821638624183834 BETTER
I0314 14:23:07.110375 113854 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0005864838021807373 old loss 0.0005866112187504768 BETTER
W0314 14:23:08.185066 113854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

17_down proxy err 0.04807882755994797 tr(WHW.T) 21.50391960144043
I0314 14:23:34.589539 116548 finetune.py:68] layer 19_down @ epoch 2 new loss 0.0006149813998490572 old loss 0.0006151541019789875 BETTER
I0314 14:23:35.926207 115184 finetune.py:68] layer 18_down @ epoch 3 new loss 0.000581850646995008 old loss 0.0005819933721795678 BETTER
I0314 14:24:05.328335 116548 finetune.py:68] layer 19_down @ epoch 3 new loss 0.0006148511311039329 old loss 0.0006149813998490572 BETTER
I0314 14:24:07.325453 115184 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0005817391211166978 old loss 0.000581850646995008 BETTER
W0314 14:24:08.095555 115184 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

18_down proxy err 0.04912715032696724 tr(WHW.T) 21.292560577392578
I0314 14:24:20.024865 9966 quantize_finetune_llama.py:186] computed original embedding for layer 20 in 67.41162300109863s
I0314 14:24:20.402775 9966 quantize_finetune_llama.py:159] layer 21 gpu 1
I0314 14:24:22.521497 137031 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 14:24:22.521618 137031 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 14:24:22.521679 137031 utils.py:162] NumExpr defaulting to 16 threads.
I0314 14:24:22.704041 137031 config.py:58] PyTorch version 2.4.0 available.
I0314 14:24:24.895653 137031 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 14:24:25.397219 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.41s/it]  6%|▋         | 2/32 [00:01<00:23,  1.29it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.54it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.84it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.90it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.94it/s] 41%|████      | 13/32 [00:05<00:06,  2.95it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.96it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.99it/s] 50%|█████     | 16/32 [00:06<00:05,  3.00it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.01it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.00it/s] 62%|██████▎   | 20/32 [00:07<00:04,  3.00it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.00it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.99it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.00it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.01it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.00it/s]I0314 14:24:36.182210 116548 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0006147366366349161 old loss 0.0006148511311039329 BETTER
 84%|████████▍ | 27/32 [00:10<00:01,  3.02it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.06it/s]W0314 14:24:36.884116 116548 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 91%|█████████ | 29/32 [00:10<00:00,  3.06it/s]19_down proxy err 0.04932845011353493 tr(WHW.T) 22.15447998046875
 94%|█████████▍| 30/32 [00:11<00:00,  3.07it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.08it/s]100%|██████████| 32/32 [00:11<00:00,  3.10it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
W0314 14:24:40.445000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.445000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.446000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.446000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.446000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.446000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.446000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.473000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.473000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.473000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.473000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.473000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.769000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.770000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.770000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.770000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:24:40.770000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.633000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.633000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.633000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.633000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.633000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.633000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.634000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.652000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.652000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.652000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.652000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.652000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.849000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.849000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.849000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.849000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:24:41.849000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.931000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.931000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.931000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.931000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.931000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.931000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.931000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.948000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.948000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.949000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.949000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:24:42.949000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:24:43.802000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:24:43.802000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:24:43.802000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:24:43.802000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:24:43.802000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 14:24:50.089621 137031 finetune.py:45] layer 20_v initial loss 0.00013592501636594534
W0314 14:24:50.089893 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:25:22.488545 9966 quantize_finetune_llama.py:186] computed original embedding for layer 21 in 61.63295936584473s
I0314 14:25:22.866930 9966 quantize_finetune_llama.py:159] layer 22 gpu 2
I0314 14:25:24.966804 138351 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 14:25:24.966960 138351 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 14:25:24.967024 138351 utils.py:162] NumExpr defaulting to 16 threads.
I0314 14:25:25.165137 138351 config.py:58] PyTorch version 2.4.0 available.
I0314 14:25:26.578915 137031 finetune.py:68] layer 20_v @ epoch 0 new loss 4.333300603320822e-05 old loss 0.00013592501636594534 BETTER
I0314 14:25:27.425838 138351 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 14:25:28.067202 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.59s/it]  6%|▋         | 2/32 [00:01<00:25,  1.17it/s]  9%|▉         | 3/32 [00:02<00:18,  1.58it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.21it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.82it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.88it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.89it/s] 50%|█████     | 16/32 [00:06<00:05,  2.90it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.87it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.88it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.93it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.94it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.91it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.93it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.95it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.94it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.96it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.90it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
W0314 14:25:43.677000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.677000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.678000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.678000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.678000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.678000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.678000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.705000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.705000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.705000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.705000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.705000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.996000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.996000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.996000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.996000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:25:43.997000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.851000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.851000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.851000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.851000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.851000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.851000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.851000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.869000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.869000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.869000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.869000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:25:44.869000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:25:45.068000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:25:45.068000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:25:45.068000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:25:45.068000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:25:45.068000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.221000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.221000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.221000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.222000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.222000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.222000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.222000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.240000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.241000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.241000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.241000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:25:46.241000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:25:47.115000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:25:47.115000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:25:47.115000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:25:47.115000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:25:47.115000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 14:25:53.240262 138351 finetune.py:45] layer 21_v initial loss 0.0001566704158904031
W0314 14:25:53.240441 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:26:03.872687 137031 finetune.py:68] layer 20_v @ epoch 1 new loss 3.8349837268469855e-05 old loss 4.333300603320822e-05 BETTER
I0314 14:26:24.643452 9966 quantize_finetune_llama.py:186] computed original embedding for layer 22 in 61.34586977958679s
I0314 14:26:25.028742 9966 quantize_finetune_llama.py:159] layer 23 gpu 3
I0314 14:26:27.210173 139698 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 14:26:27.210304 139698 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 14:26:27.210366 139698 utils.py:162] NumExpr defaulting to 16 threads.
I0314 14:26:27.402306 139698 config.py:58] PyTorch version 2.4.0 available.
I0314 14:26:27.424538 138351 finetune.py:68] layer 21_v @ epoch 0 new loss 5.567205880652182e-05 old loss 0.0001566704158904031 BETTER
I0314 14:26:29.607183 139698 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 14:26:30.035677 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:51,  1.65s/it]  6%|▋         | 2/32 [00:02<00:26,  1.13it/s]  9%|▉         | 3/32 [00:02<00:18,  1.56it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.91it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.17it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.69it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.75it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.87it/s]I0314 14:26:41.314125 137031 finetune.py:68] layer 20_v @ epoch 2 new loss 3.6499153793556616e-05 old loss 3.8349837268469855e-05 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.90it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
W0314 14:26:45.846000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.846000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.846000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.846000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.846000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.846000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.847000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.874000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.874000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.874000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.874000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:26:45.874000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:26:46.178000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:26:46.179000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:26:46.179000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:26:46.179000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:26:46.179000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.062000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.062000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.062000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.062000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.063000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.063000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.063000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.082000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.082000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.082000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.082000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.082000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.292000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.292000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.292000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.292000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:26:47.292000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.471000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.471000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.471000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.471000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:26:48.471000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:26:49.370000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:26:49.370000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:26:49.370000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:26:49.370000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:26:49.370000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 14:26:55.314053 139698 finetune.py:45] layer 22_v initial loss 0.00017234253755304962
W0314 14:26:55.314441 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:27:02.849126 138351 finetune.py:68] layer 21_v @ epoch 1 new loss 4.9248799768975005e-05 old loss 5.567205880652182e-05 BETTER
I0314 14:27:19.059586 137031 finetune.py:68] layer 20_v @ epoch 3 new loss 3.542232298059389e-05 old loss 3.6499153793556616e-05 BETTER
I0314 14:27:25.010811 9966 quantize_finetune_llama.py:186] computed original embedding for layer 23 in 59.479639530181885s
I0314 14:27:25.391639 9966 quantize_finetune_llama.py:159] layer 24 gpu 0
I0314 14:27:27.434563 141041 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 14:27:27.434697 141041 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 14:27:27.434777 141041 utils.py:162] NumExpr defaulting to 16 threads.
I0314 14:27:27.635943 141041 config.py:58] PyTorch version 2.4.0 available.
I0314 14:27:29.731985 139698 finetune.py:68] layer 22_v @ epoch 0 new loss 4.9123373173642904e-05 old loss 0.00017234253755304962 BETTER
I0314 14:27:29.829915 141041 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 14:27:30.269204 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.59s/it]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s]  9%|▉         | 3/32 [00:02<00:18,  1.60it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.95it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.21it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.80it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.83it/s] 41%|████      | 13/32 [00:05<00:06,  2.85it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.88it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.88it/s]I0314 14:27:38.797286 138351 finetune.py:68] layer 21_v @ epoch 2 new loss 4.67265781480819e-05 old loss 4.9248799768975005e-05 BETTER
 59%|█████▉    | 19/32 [00:07<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.89it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.89it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.88it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
W0314 14:27:46.038000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.039000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.039000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.039000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.039000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.039000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.039000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.067000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.067000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.067000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.067000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.067000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.361000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.361000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.361000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.361000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:27:46.361000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.225000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.225000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.225000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.225000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.225000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.225000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.225000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.244000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.244000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.244000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.244000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.244000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.446000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.446000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.446000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.446000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:27:47.447000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.569000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.569000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.569000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.569000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.569000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.569000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.569000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.588000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.588000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.588000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.588000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:27:48.588000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:27:49.465000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:27:49.465000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:27:49.465000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:27:49.466000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:27:49.466000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 14:27:57.776575 141041 finetune.py:45] layer 23_v initial loss 0.000251411838689819
W0314 14:27:57.776968 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:27:58.166991 137031 finetune.py:68] layer 20_v @ epoch 4 new loss 3.462872700765729e-05 old loss 3.542232298059389e-05 BETTER
W0314 14:28:00.194220 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_v proxy err 0.040878500789403915 tr(WHW.T) 90.61302185058594
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.04it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s]I0314 14:28:05.983139 139698 finetune.py:68] layer 22_v @ epoch 1 new loss 4.2749092244775966e-05 old loss 4.9123373173642904e-05 BETTER
 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
I0314 14:28:15.024249 138351 finetune.py:68] layer 21_v @ epoch 3 new loss 4.513385647442192e-05 old loss 4.67265781480819e-05 BETTER
W0314 14:28:19.177000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.177000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.177000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.177000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.177000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.178000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.178000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.209000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.209000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.209000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.209000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.209000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.388000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.388000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.388000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.388000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.388000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.618000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.618000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.618000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.618000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.618000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.618000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.619000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.641000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.641000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.641000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.641000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.641000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.709000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.709000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.709000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.709000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:28:19.710000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.470000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.801000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.801000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.801000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.801000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.801000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.801000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.802000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.823000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.823000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.824000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.824000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:28:20.824000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:28:21.085000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:28:21.085000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:28:21.085000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:28:21.085000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:28:21.085000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:28:21.452000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:28:29.059349 137031 finetune.py:45] layer 20_q initial loss 5.289676118991338e-05
W0314 14:28:29.059989 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:28:32.625232 141041 finetune.py:68] layer 23_v @ epoch 0 new loss 6.0583610320463777e-05 old loss 0.000251411838689819 BETTER
I0314 14:28:42.129380 139698 finetune.py:68] layer 22_v @ epoch 2 new loss 4.052663643960841e-05 old loss 4.2749092244775966e-05 BETTER
I0314 14:28:51.387326 138351 finetune.py:68] layer 21_v @ epoch 4 new loss 4.398713281261735e-05 old loss 4.513385647442192e-05 BETTER
W0314 14:28:53.191747 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_v proxy err 0.04035191610455513 tr(WHW.T) 95.41011047363281
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.61it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s]I0314 14:29:06.114818 137031 finetune.py:68] layer 20_q @ epoch 0 new loss 5.057705129729584e-05 old loss 5.289676118991338e-05 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0314 14:29:07.867143 141041 finetune.py:68] layer 23_v @ epoch 1 new loss 5.0132086471421644e-05 old loss 6.0583610320463777e-05 BETTER
W0314 14:29:12.101000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.102000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.102000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.102000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.102000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.102000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.102000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.134000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.134000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.134000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.134000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.134000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.308000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.308000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.308000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.308000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.308000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.535000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.536000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.536000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.536000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.536000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.536000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.536000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.558000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.559000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.559000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.559000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.559000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.627000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.627000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.627000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.627000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:29:12.627000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.369000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.700000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.700000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.700000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.700000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.700000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.700000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.700000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.724000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.724000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.724000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.724000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.724000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.987000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.987000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.988000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.988000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:29:13.988000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:29:14.361000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:29:18.862903 139698 finetune.py:68] layer 22_v @ epoch 3 new loss 3.924630436813459e-05 old loss 4.052663643960841e-05 BETTER
I0314 14:29:21.690842 138351 finetune.py:45] layer 21_q initial loss 7.194841600721702e-05
W0314 14:29:21.691212 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:29:43.321241 141041 finetune.py:68] layer 23_v @ epoch 2 new loss 4.679744961322285e-05 old loss 5.0132086471421644e-05 BETTER
I0314 14:29:43.884159 137031 finetune.py:68] layer 20_q @ epoch 1 new loss 4.9467107601230964e-05 old loss 5.057705129729584e-05 BETTER
I0314 14:29:55.680796 139698 finetune.py:68] layer 22_v @ epoch 4 new loss 3.836343967122957e-05 old loss 3.924630436813459e-05 BETTER
I0314 14:29:57.136765 138351 finetune.py:68] layer 21_q @ epoch 0 new loss 6.886777555337176e-05 old loss 7.194841600721702e-05 BETTER
W0314 14:29:57.863434 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_v proxy err 0.04406123235821724 tr(WHW.T) 101.29380798339844
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
W0314 14:30:17.451000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.452000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.483000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.483000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.483000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.483000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.483000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.663000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.664000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.664000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.664000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.664000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.901000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.901000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.901000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.902000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.902000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.902000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.902000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.924000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.924000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.924000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.924000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.924000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.995000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.995000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.995000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.995000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:30:17.995000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
I0314 14:30:18.752360 141041 finetune.py:68] layer 23_v @ epoch 3 new loss 4.5094431698089465e-05 old loss 4.679744961322285e-05 BETTER
W0314 14:30:18.766000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.094000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.094000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.094000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.094000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.094000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.094000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.094000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.116000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.116000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.116000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.116000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.116000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.382000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.382000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.383000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.383000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.383000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:30:19.756000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:30:21.728886 137031 finetune.py:68] layer 20_q @ epoch 2 new loss 4.859532418777235e-05 old loss 4.9467107601230964e-05 BETTER
I0314 14:30:27.092566 139698 finetune.py:45] layer 22_q initial loss 6.091223258408718e-05
W0314 14:30:27.092946 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:30:32.969419 138351 finetune.py:68] layer 21_q @ epoch 1 new loss 6.701378151774406e-05 old loss 6.886777555337176e-05 BETTER
I0314 14:30:54.599562 141041 finetune.py:68] layer 23_v @ epoch 4 new loss 4.3993601138936356e-05 old loss 4.5094431698089465e-05 BETTER
W0314 14:30:56.586319 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_v proxy err 0.04601619765162468 tr(WHW.T) 112.7859115600586
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]I0314 14:30:59.620206 137031 finetune.py:68] layer 20_q @ epoch 3 new loss 4.789653030456975e-05 old loss 4.859532418777235e-05 BETTER
  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s]I0314 14:31:02.259831 139698 finetune.py:68] layer 22_q @ epoch 0 new loss 5.858830627403222e-05 old loss 6.091223258408718e-05 BETTER
 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s]I0314 14:31:08.824824 138351 finetune.py:68] layer 21_q @ epoch 2 new loss 6.55568583169952e-05 old loss 6.701378151774406e-05 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
W0314 14:31:15.906000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.907000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.907000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.907000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.907000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.907000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.907000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.938000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.938000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.938000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.938000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:31:15.938000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.118000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.118000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.118000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.118000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.118000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.349000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.349000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.349000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.349000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.349000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.349000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.349000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.376000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.376000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.376000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.376000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.376000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.444000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.444000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.444000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.444000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:31:16.445000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.210000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.534000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.534000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.534000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.534000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.534000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.534000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.534000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.556000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.556000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.556000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.556000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.556000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.817000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.817000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.818000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.818000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:31:17.818000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:31:18.178000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:31:25.168998 141041 finetune.py:45] layer 23_q initial loss 6.499340088339522e-05
W0314 14:31:25.169381 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:31:38.350273 137031 finetune.py:68] layer 20_q @ epoch 4 new loss 4.73278560093604e-05 old loss 4.789653030456975e-05 BETTER
I0314 14:31:39.148280 139698 finetune.py:68] layer 22_q @ epoch 1 new loss 5.738823165302165e-05 old loss 5.858830627403222e-05 BETTER
W0314 14:31:40.377540 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_q proxy err 0.008898429572582245 tr(WHW.T) 5700.06787109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s]I0314 14:31:44.777638 138351 finetune.py:68] layer 21_q @ epoch 3 new loss 6.441138975787908e-05 old loss 6.55568583169952e-05 BETTER
 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 41%|████      | 13/32 [00:05<00:06,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.75it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.78it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
I0314 14:32:00.153213 141041 finetune.py:68] layer 23_q @ epoch 0 new loss 6.276113708736375e-05 old loss 6.499340088339522e-05 BETTER
I0314 14:32:02.132489 137031 finetune.py:45] layer 20_k initial loss 6.252624007174745e-05
W0314 14:32:02.133093 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:32:15.186963 139698 finetune.py:68] layer 22_q @ epoch 2 new loss 5.6454762670909986e-05 old loss 5.738823165302165e-05 BETTER
I0314 14:32:20.809200 138351 finetune.py:68] layer 21_q @ epoch 4 new loss 6.341059633996338e-05 old loss 6.441138975787908e-05 BETTER
W0314 14:32:22.656501 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_q proxy err 0.007089757826179266 tr(WHW.T) 6796.8525390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.07it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.69it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s]I0314 14:32:35.463778 141041 finetune.py:68] layer 23_q @ epoch 1 new loss 6.138093885965645e-05 old loss 6.276113708736375e-05 BETTER
 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
I0314 14:32:39.033479 137031 finetune.py:68] layer 20_k @ epoch 0 new loss 5.831208545714617e-05 old loss 6.252624007174745e-05 BETTER
I0314 14:32:43.661078 138351 finetune.py:45] layer 21_k initial loss 8.413497562287375e-05
W0314 14:32:43.661433 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:32:51.108125 139698 finetune.py:68] layer 22_q @ epoch 3 new loss 5.5717184295644984e-05 old loss 5.6454762670909986e-05 BETTER
I0314 14:33:10.508399 141041 finetune.py:68] layer 23_q @ epoch 2 new loss 6.0400641814339906e-05 old loss 6.138093885965645e-05 BETTER
I0314 14:33:17.032808 137031 finetune.py:68] layer 20_k @ epoch 1 new loss 5.765835885540582e-05 old loss 5.831208545714617e-05 BETTER
I0314 14:33:19.189219 138351 finetune.py:68] layer 21_k @ epoch 0 new loss 8.092516509350389e-05 old loss 8.413497562287375e-05 BETTER
I0314 14:33:27.019130 139698 finetune.py:68] layer 22_q @ epoch 4 new loss 5.50772892893292e-05 old loss 5.5717184295644984e-05 BETTER
W0314 14:33:29.035641 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_q proxy err 0.007998610846698284 tr(WHW.T) 5959.30419921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.07it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s]  9%|▉         | 3/32 [00:01<00:14,  1.99it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0314 14:33:45.925032 141041 finetune.py:68] layer 23_q @ epoch 3 new loss 5.961831266176887e-05 old loss 6.0400641814339906e-05 BETTER
I0314 14:33:50.285023 139698 finetune.py:45] layer 22_k initial loss 7.584668492199853e-05
W0314 14:33:50.285496 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:33:55.245079 138351 finetune.py:68] layer 21_k @ epoch 1 new loss 7.978486246429384e-05 old loss 8.092516509350389e-05 BETTER
I0314 14:33:56.053885 137031 finetune.py:68] layer 20_k @ epoch 2 new loss 5.713743667001836e-05 old loss 5.765835885540582e-05 BETTER
I0314 14:34:21.351894 141041 finetune.py:68] layer 23_q @ epoch 4 new loss 5.896945367567241e-05 old loss 5.961831266176887e-05 BETTER
W0314 14:34:23.417370 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_q proxy err 0.007868636399507523 tr(WHW.T) 6416.53369140625
  0%|          | 0/32 [00:00<?, ?it/s]I0314 14:34:25.217650 139698 finetune.py:68] layer 22_k @ epoch 0 new loss 6.999119068495929e-05 old loss 7.584668492199853e-05 BETTER
  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]  6%|▋         | 2/32 [00:01<00:17,  1.70it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.67it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s]I0314 14:34:31.291484 138351 finetune.py:68] layer 21_k @ epoch 2 new loss 7.890382403274998e-05 old loss 7.978486246429384e-05 BETTER
 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s]I0314 14:34:34.239459 137031 finetune.py:68] layer 20_k @ epoch 3 new loss 5.672129918821156e-05 old loss 5.713743667001836e-05 BETTER
 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0314 14:34:45.101342 141041 finetune.py:45] layer 23_k initial loss 8.094865916064009e-05
W0314 14:34:45.101734 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:35:02.429316 139698 finetune.py:68] layer 22_k @ epoch 1 new loss 6.933944678166881e-05 old loss 6.999119068495929e-05 BETTER
I0314 14:35:07.279975 138351 finetune.py:68] layer 21_k @ epoch 3 new loss 7.814390846760944e-05 old loss 7.890382403274998e-05 BETTER
I0314 14:35:12.408127 137031 finetune.py:68] layer 20_k @ epoch 4 new loss 5.63618668820709e-05 old loss 5.672129918821156e-05 BETTER
W0314 14:35:14.685534 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_k proxy err 0.006779121235013008 tr(WHW.T) 4215.1689453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:27,  1.14it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.31it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s]I0314 14:35:19.591121 141041 finetune.py:68] layer 23_k @ epoch 0 new loss 7.655772060388699e-05 old loss 8.094865916064009e-05 BETTER
 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.70it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.71it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
I0314 14:35:35.895938 137031 finetune.py:45] layer 20_o initial loss 0.00010411106632091105
W0314 14:35:35.896272 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:35:39.124456 139698 finetune.py:68] layer 22_k @ epoch 2 new loss 6.879219290567562e-05 old loss 6.933944678166881e-05 BETTER
I0314 14:35:44.231635 138351 finetune.py:68] layer 21_k @ epoch 4 new loss 7.748448115307838e-05 old loss 7.814390846760944e-05 BETTER
W0314 14:35:46.221397 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_k proxy err 0.006408117711544037 tr(WHW.T) 4396.5205078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.31it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s]I0314 14:35:55.694823 141041 finetune.py:68] layer 23_k @ epoch 1 new loss 7.574475603178144e-05 old loss 7.655772060388699e-05 BETTER
 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0314 14:36:07.651576 138351 finetune.py:45] layer 21_o initial loss 0.00013841671170666814
W0314 14:36:07.651930 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:36:13.453674 137031 finetune.py:68] layer 20_o @ epoch 0 new loss 9.511045936960727e-05 old loss 0.00010411106632091105 BETTER
I0314 14:36:15.983731 139698 finetune.py:68] layer 22_k @ epoch 3 new loss 6.836550892330706e-05 old loss 6.879219290567562e-05 BETTER
I0314 14:36:31.002245 141041 finetune.py:68] layer 23_k @ epoch 2 new loss 7.511887088185176e-05 old loss 7.574475603178144e-05 BETTER
I0314 14:36:43.358092 138351 finetune.py:68] layer 21_o @ epoch 0 new loss 0.00012839603004977107 old loss 0.00013841671170666814 BETTER
I0314 14:36:51.355348 137031 finetune.py:68] layer 20_o @ epoch 1 new loss 9.3667687906418e-05 old loss 9.511045936960727e-05 BETTER
I0314 14:36:52.136244 139698 finetune.py:68] layer 22_k @ epoch 4 new loss 6.798101094318554e-05 old loss 6.836550892330706e-05 BETTER
W0314 14:36:54.013385 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_k proxy err 0.006556948181241751 tr(WHW.T) 4296.7685546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s]  9%|▉         | 3/32 [00:01<00:13,  2.11it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.51it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s]I0314 14:37:06.653401 141041 finetune.py:68] layer 23_k @ epoch 3 new loss 7.461224595317617e-05 old loss 7.511887088185176e-05 BETTER
 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0314 14:37:15.283962 139698 finetune.py:45] layer 22_o initial loss 0.00013157789362594485
W0314 14:37:15.284394 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:37:19.820213 138351 finetune.py:68] layer 21_o @ epoch 1 new loss 0.0001262943260371685 old loss 0.00012839603004977107 BETTER
I0314 14:37:29.348202 137031 finetune.py:68] layer 20_o @ epoch 2 new loss 9.271551243728027e-05 old loss 9.3667687906418e-05 BETTER
I0314 14:37:41.975959 141041 finetune.py:68] layer 23_k @ epoch 4 new loss 7.417956658173352e-05 old loss 7.461224595317617e-05 BETTER
W0314 14:37:43.783317 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_k proxy err 0.006974430289119482 tr(WHW.T) 4200.158203125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.26it/s]  6%|▋         | 2/32 [00:01<00:16,  1.83it/s]  9%|▉         | 3/32 [00:01<00:13,  2.14it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.33it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s]I0314 14:37:50.371083 139698 finetune.py:68] layer 22_o @ epoch 0 new loss 0.00012140739272581413 old loss 0.00013157789362594485 BETTER
 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s]I0314 14:37:55.607769 138351 finetune.py:68] layer 21_o @ epoch 2 new loss 0.00012479811266530305 old loss 0.0001262943260371685 BETTER
 88%|████████▊ | 28/32 [00:10<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
I0314 14:38:05.234008 141041 finetune.py:45] layer 23_o initial loss 0.00014749617548659444
W0314 14:38:05.234416 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:38:08.427165 137031 finetune.py:68] layer 20_o @ epoch 3 new loss 9.194642916554585e-05 old loss 9.271551243728027e-05 BETTER
I0314 14:38:27.282315 139698 finetune.py:68] layer 22_o @ epoch 1 new loss 0.0001197937162942253 old loss 0.00012140739272581413 BETTER
I0314 14:38:31.981823 138351 finetune.py:68] layer 21_o @ epoch 3 new loss 0.00012365034490358084 old loss 0.00012479811266530305 BETTER
I0314 14:38:39.478842 141041 finetune.py:68] layer 23_o @ epoch 0 new loss 0.00013152648170944303 old loss 0.00014749617548659444 BETTER
I0314 14:38:46.574546 137031 finetune.py:68] layer 20_o @ epoch 4 new loss 9.135378786595538e-05 old loss 9.194642916554585e-05 BETTER
W0314 14:38:48.642463 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_o proxy err 0.04224501922726631 tr(WHW.T) 4.093571662902832
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it]  9%|▉         | 3/32 [00:04<00:45,  1.55s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it]I0314 14:39:03.405065 139698 finetune.py:68] layer 22_o @ epoch 2 new loss 0.00011879755038535222 old loss 0.0001197937162942253 BETTER
 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it]I0314 14:39:08.135872 138351 finetune.py:68] layer 21_o @ epoch 4 new loss 0.0001227113389177248 old loss 0.00012365034490358084 BETTER
 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it]W0314 14:39:09.775274 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it]21_o proxy err 0.030607834458351135 tr(WHW.T) 7.631941795349121
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it]  3%|▎         | 1/32 [00:02<01:08,  2.20s/it] 50%|█████     | 16/32 [00:23<00:24,  1.51s/it]  6%|▋         | 2/32 [00:03<00:53,  1.79s/it]I0314 14:39:14.924991 141041 finetune.py:68] layer 23_o @ epoch 1 new loss 0.00012942389003001153 old loss 0.00013152648170944303 BETTER
 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it]  9%|▉         | 3/32 [00:05<00:47,  1.65s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.51s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.50s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.50s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 41%|████      | 13/32 [00:20<00:28,  1.50s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.50s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it]I0314 14:39:39.657492 139698 finetune.py:68] layer 22_o @ epoch 3 new loss 0.00011801838991232216 old loss 0.00011879755038535222 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it]I0314 14:39:45.960983 137031 finetune.py:45] layer 20_up initial loss 0.00030282646184787154
W0314 14:39:45.961327 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 72%|███████▏  | 23/32 [00:35<00:13,  1.49s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it]I0314 14:39:50.044692 141041 finetune.py:68] layer 23_o @ epoch 2 new loss 0.00012822682037949562 old loss 0.00012942389003001153 BETTER
 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0314 14:40:07.816949 138351 finetune.py:45] layer 21_up initial loss 0.00036527542397379875
W0314 14:40:07.817367 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:40:15.832518 139698 finetune.py:68] layer 22_o @ epoch 4 new loss 0.00011737163731595501 old loss 0.00011801838991232216 BETTER
W0314 14:40:17.696760 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_o proxy err 0.04537361487746239 tr(WHW.T) 5.02524471282959
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.03s/it]I0314 14:40:22.008641 137031 finetune.py:68] layer 20_up @ epoch 0 new loss 0.0002975297684315592 old loss 0.00030282646184787154 BETTER
  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]  9%|▉         | 3/32 [00:05<00:46,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it]I0314 14:40:25.739971 141041 finetune.py:68] layer 23_o @ epoch 3 new loss 0.00012734212214127183 old loss 0.00012822682037949562 BETTER
 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.51s/it]I0314 14:40:41.971958 138351 finetune.py:68] layer 21_up @ epoch 0 new loss 0.00035907721030525863 old loss 0.00036527542397379875 BETTER
 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it]I0314 14:40:59.169149 137031 finetune.py:68] layer 20_up @ epoch 1 new loss 0.00029417267069220543 old loss 0.0002975297684315592 BETTER
 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it]I0314 14:41:01.019454 141041 finetune.py:68] layer 23_o @ epoch 4 new loss 0.00012666606926359236 old loss 0.00012734212214127183 BETTER
 88%|████████▊ | 28/32 [00:42<00:05,  1.50s/it]W0314 14:41:02.764721 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it]23_o proxy err 0.03865521773695946 tr(WHW.T) 6.177165985107422
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:45<00:02,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.49s/it]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]100%|██████████| 32/32 [00:48<00:00,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
  6%|▋         | 2/32 [00:03<00:49,  1.66s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.50s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it]I0314 14:41:15.641276 139698 finetune.py:45] layer 22_up initial loss 0.0003732606128323823
W0314 14:41:15.641642 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it]I0314 14:41:17.255420 138351 finetune.py:68] layer 21_up @ epoch 1 new loss 0.00035506486892700195 old loss 0.00035907721030525863 BETTER
 28%|██▊       | 9/32 [00:13<00:34,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.46s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it]I0314 14:41:36.214463 137031 finetune.py:68] layer 20_up @ epoch 2 new loss 0.00029141243430785835 old loss 0.00029417267069220543 BETTER
 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.46s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.46s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.46s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.46s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.46s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.48s/it]I0314 14:41:50.219905 139698 finetune.py:68] layer 22_up @ epoch 0 new loss 0.00036753437598235905 old loss 0.0003732606128323823 BETTER
100%|██████████| 32/32 [00:47<00:00,  1.50s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
I0314 14:41:52.396656 138351 finetune.py:68] layer 21_up @ epoch 2 new loss 0.00035179086262360215 old loss 0.00035506486892700195 BETTER
I0314 14:41:59.973038 141041 finetune.py:45] layer 23_up initial loss 0.0004060915089212358
W0314 14:41:59.973412 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:42:13.347857 137031 finetune.py:68] layer 20_up @ epoch 3 new loss 0.0002889484749175608 old loss 0.00029141243430785835 BETTER
I0314 14:42:25.771781 139698 finetune.py:68] layer 22_up @ epoch 1 new loss 0.00036382925463840365 old loss 0.00036753437598235905 BETTER
I0314 14:42:27.516406 138351 finetune.py:68] layer 21_up @ epoch 3 new loss 0.0003488914808258414 old loss 0.00035179086262360215 BETTER
I0314 14:42:32.788794 141041 finetune.py:68] layer 23_up @ epoch 0 new loss 0.00040036559221334755 old loss 0.0004060915089212358 BETTER
I0314 14:42:50.376665 137031 finetune.py:68] layer 20_up @ epoch 4 new loss 0.0002867656003218144 old loss 0.0002889484749175608 BETTER
W0314 14:42:51.667775 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_up proxy err 0.045214202255010605 tr(WHW.T) 1127.8660888671875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:56,  1.82s/it]  6%|▋         | 2/32 [00:03<00:47,  1.60s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.50s/it]I0314 14:43:00.140942 139698 finetune.py:68] layer 22_up @ epoch 2 new loss 0.00036076843389309943 old loss 0.00036382925463840365 BETTER
 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.47s/it]I0314 14:43:01.903410 138351 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0003462852328084409 old loss 0.0003488914808258414 BETTER
 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it]W0314 14:43:03.392734 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_up proxy err 0.043628010898828506 tr(WHW.T) 1238.297607421875
  0%|          | 0/32 [00:00<?, ?it/s] 25%|██▌       | 8/32 [00:11<00:35,  1.47s/it]I0314 14:43:06.148151 141041 finetune.py:68] layer 23_up @ epoch 1 new loss 0.0003967897209804505 old loss 0.00040036559221334755 BETTER
 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it]  6%|▋         | 2/32 [00:03<00:49,  1.66s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.47s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it]I0314 14:43:34.847874 139698 finetune.py:68] layer 22_up @ epoch 3 new loss 0.0003580993798095733 old loss 0.00036076843389309943 BETTER
 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.47s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it]I0314 14:43:39.540043 141041 finetune.py:68] layer 23_up @ epoch 2 new loss 0.0003938392037525773 old loss 0.0003967897209804505 BETTER
100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it]I0314 14:43:48.188328 137031 finetune.py:45] layer 20_gate initial loss 0.000431094435043633
W0314 14:43:48.188724 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:45<00:02,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]
I0314 14:44:00.572213 138351 finetune.py:45] layer 21_gate initial loss 0.0005126944161020219
W0314 14:44:00.572565 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:44:09.498779 139698 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0003556937735993415 old loss 0.0003580993798095733 BETTER
W0314 14:44:11.024207 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_up proxy err 0.04567021504044533 tr(WHW.T) 1249.4505615234375
  0%|          | 0/32 [00:00<?, ?it/s]I0314 14:44:13.318900 141041 finetune.py:68] layer 23_up @ epoch 3 new loss 0.0003913221589755267 old loss 0.0003938392037525773 BETTER
  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it]I0314 14:44:22.975058 137031 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.00042581019806675613 old loss 0.000431094435043633 BETTER
 22%|██▏       | 7/32 [00:10<00:37,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it]I0314 14:44:33.110559 138351 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.0005065042641945183 old loss 0.0005126944161020219 BETTER
 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.49s/it]I0314 14:44:46.638507 141041 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0003890442894771695 old loss 0.0003913221589755267 BETTER
 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it]W0314 14:44:48.039565 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it]23_up proxy err 0.04667145386338234 tr(WHW.T) 1299.1328125
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it]  3%|▎         | 1/32 [00:01<00:59,  1.92s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.53s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.47s/it]I0314 14:44:58.237558 137031 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.00042284588562324643 old loss 0.00042581019806675613 BETTER
 19%|█▉        | 6/32 [00:09<00:39,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.47s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]
 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it]I0314 14:45:06.539772 138351 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0005029771127738059 old loss 0.0005065042641945183 BETTER
 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it]I0314 14:45:09.050227 139698 finetune.py:45] layer 22_gate initial loss 0.0005351627478376031
W0314 14:45:09.050641 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 50%|█████     | 16/32 [00:24<00:24,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it]I0314 14:45:33.783571 137031 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.0004205149016343057 old loss 0.00042284588562324643 BETTER
 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
I0314 14:45:39.806196 138351 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0005002153338864446 old loss 0.0005029771127738059 BETTER
I0314 14:45:41.920459 139698 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0005293879075907171 old loss 0.0005351627478376031 BETTER
I0314 14:45:45.449931 141041 finetune.py:45] layer 23_gate initial loss 0.000590371317230165
W0314 14:45:45.450348 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:46:09.004043 137031 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.000418512528995052 old loss 0.0004205149016343057 BETTER
I0314 14:46:13.105383 138351 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.0004978355718776584 old loss 0.0005002153338864446 BETTER
I0314 14:46:15.700909 139698 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.0005260747857391834 old loss 0.0005293879075907171 BETTER
I0314 14:46:17.848739 141041 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0005848629516549408 old loss 0.000590371317230165 BETTER
I0314 14:46:44.496112 137031 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.0004166893777437508 old loss 0.000418512528995052 BETTER
W0314 14:46:45.704488 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 14:46:46.773150 138351 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.0004957486526109278 old loss 0.0004978355718776584 BETTER
W0314 14:46:47.894299 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_gate proxy err 0.020669015124440193 tr(WHW.T) 4524.6689453125
  0%|          | 0/112 [00:00<?, ?it/s]I0314 14:46:49.031567 139698 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0005235245334915817 old loss 0.0005260747857391834 BETTER
  1%|          | 1/112 [00:00<01:34,  1.18it/s]  2%|▏         | 2/112 [00:01<01:01,  1.77it/s]I0314 14:46:50.250042 141041 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.000581662985496223 old loss 0.0005848629516549408 BETTER
  3%|▎         | 3/112 [00:01<00:51,  2.12it/s]  4%|▎         | 4/112 [00:01<00:46,  2.33it/s]  4%|▍         | 5/112 [00:02<00:43,  2.49it/s]21_gate proxy err 0.020055117085576057 tr(WHW.T) 4991.24609375
  0%|          | 0/112 [00:00<?, ?it/s]  5%|▌         | 6/112 [00:02<00:41,  2.55it/s]  6%|▋         | 7/112 [00:03<00:39,  2.63it/s]  7%|▋         | 8/112 [00:03<00:38,  2.68it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s]  8%|▊         | 9/112 [00:03<00:38,  2.69it/s]  2%|▏         | 2/112 [00:01<01:04,  1.71it/s]  9%|▉         | 10/112 [00:04<00:37,  2.72it/s]  3%|▎         | 3/112 [00:01<00:53,  2.04it/s] 10%|▉         | 11/112 [00:04<00:36,  2.74it/s]  4%|▎         | 4/112 [00:02<00:48,  2.24it/s] 11%|█         | 12/112 [00:04<00:36,  2.74it/s]  4%|▍         | 5/112 [00:02<00:45,  2.37it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.75it/s]  5%|▌         | 6/112 [00:02<00:43,  2.46it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.76it/s]  6%|▋         | 7/112 [00:03<00:41,  2.53it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.76it/s]  7%|▋         | 8/112 [00:03<00:40,  2.57it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.76it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.77it/s]  9%|▉         | 10/112 [00:04<00:38,  2.62it/s] 16%|█▌        | 18/112 [00:06<00:34,  2.76it/s] 10%|▉         | 11/112 [00:04<00:38,  2.64it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.76it/s] 11%|█         | 12/112 [00:05<00:37,  2.64it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.77it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.64it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.76it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.65it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.72it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.65it/s] 21%|██        | 23/112 [00:08<00:32,  2.76it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.61it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.76it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.63it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.75it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.64it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.76it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.64it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.77it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.75it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.65it/s] 26%|██▌       | 29/112 [00:10<00:30,  2.76it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.66it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.76it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.66it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.75it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.76it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.66it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.75it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.67it/s] 30%|███       | 34/112 [00:12<00:28,  2.75it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.66it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.71it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.65it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.76it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.65it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.76it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.64it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.74it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.61it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.62it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.76it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.75it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.64it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.75it/s] 30%|███       | 34/112 [00:13<00:29,  2.65it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.76it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.65it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.75it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.66it/s] 40%|████      | 45/112 [00:16<00:24,  2.75it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.66it/s] 41%|████      | 46/112 [00:17<00:23,  2.75it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.67it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.76it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.66it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.75it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.66it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.74it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.66it/s] 45%|████▍     | 50/112 [00:18<00:23,  2.68it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.65it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.70it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.65it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.68it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.61it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.69it/s] 40%|████      | 45/112 [00:17<00:25,  2.62it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.70it/s] 41%|████      | 46/112 [00:17<00:25,  2.63it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.71it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.64it/s] 50%|█████     | 56/112 [00:20<00:20,  2.72it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.64it/s] 51%|█████     | 57/112 [00:21<00:20,  2.72it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.64it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.71it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.65it/s] 53%|█████▎    | 59/112 [00:21<00:19,  2.67it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.65it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.69it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.65it/s] 54%|█████▍    | 61/112 [00:22<00:19,  2.68it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.65it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.69it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.70it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.65it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.71it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.64it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.71it/s] 50%|█████     | 56/112 [00:21<00:21,  2.64it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.72it/s] 51%|█████     | 57/112 [00:22<00:21,  2.60it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.72it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.62it/s] 61%|██████    | 68/112 [00:25<00:16,  2.72it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.62it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.72it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.63it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.73it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.64it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.69it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.64it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.70it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.65it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.70it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.65it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.71it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.65it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.72it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.65it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.71it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.65it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.72it/s] 61%|██████    | 68/112 [00:26<00:16,  2.65it/s] 70%|██████▉   | 78/112 [00:28<00:12,  2.71it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.65it/s] 71%|███████   | 79/112 [00:29<00:12,  2.71it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.64it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.71it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.61it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.71it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.62it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.71it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.63it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.71it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.64it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.70it/s] 67%|██████▋   | 75/112 [00:28<00:14,  2.64it/s] 76%|███████▌  | 85/112 [00:31<00:10,  2.70it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.64it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.65it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.65it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.67it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.65it/s] 79%|███████▊  | 88/112 [00:32<00:09,  2.66it/s]I0314 14:47:21.676768 139698 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.000521377194672823 old loss 0.0005235245334915817 BETTER
 71%|███████   | 79/112 [00:30<00:12,  2.65it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.68it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.65it/s] 80%|████████  | 90/112 [00:33<00:08,  2.69it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.65it/s]I0314 14:47:22.623164 141041 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0005792318843305111 old loss 0.000581662985496223 BETTER
 81%|████████▏ | 91/112 [00:33<00:07,  2.71it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.66it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.72it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.65it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.72it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.65it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.72it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.65it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.73it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.62it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.73it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.64it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.73it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.64it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.73it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.64it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.73it/s] 80%|████████  | 90/112 [00:34<00:08,  2.65it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.73it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.66it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.67it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.67it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.70it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.68it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.72it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.68it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.70it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.67it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.72it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.68it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.72it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.72it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.67it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.72it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.67it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.72it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.66it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.73it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.62it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.73it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.64it/s]100%|██████████| 112/112 [00:41<00:00,  2.72it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]
 91%|█████████ | 102/112 [00:39<00:03,  2.64it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.65it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.65it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.65it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.66it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.66it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.67it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.66it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.67it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.67it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.62it/s]
W0314 14:47:37.150000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.151000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.151000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.151000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.151000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.151000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.151000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.201000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.201000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.201000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.202000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.202000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.383000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.383000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.383000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.384000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.384000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.703000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.718000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.718000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.718000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.719000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.719000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.719000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.753000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.753000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.753000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.754000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.754000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.825000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.825000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.825000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.825000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:37.825000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:38.823000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:38.837000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:38.846000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:38.847000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.318000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.319000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.319000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.319000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.319000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.319000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.320000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.354000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.354000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.354000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.354000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.354000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.653000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.653000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.653000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.653000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.653000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.653000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.653000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.653000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.956000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.957000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.957000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.957000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:39.957000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.390000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.390000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.391000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.391000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.391000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.391000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.391000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.392000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.398000 139923122513728 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.434000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.434000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.434000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.434000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.434000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.610000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.610000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.610000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.610000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.610000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.937000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.937000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.937000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.937000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.937000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.937000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.937000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.971000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.971000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.971000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.971000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:40.971000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:41.042000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:41.042000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:41.042000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:41.042000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:41.042000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.029000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.035000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.041000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.041000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.487000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.487000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.487000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.487000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.487000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.488000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.488000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.514000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.515000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.515000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.515000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.515000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.805000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.806000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.806000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.806000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.806000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.806000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.806000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:42.806000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:47:43.100000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:47:43.100000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:47:43.100000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:47:43.100000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:43.100000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:47:43.520000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:47:43.526000 139717019248448 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 14:47:48.037236 137031 finetune.py:45] layer 20_down initial loss 0.0006758881499990821
W0314 14:47:48.037726 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:47:50.801230 138351 finetune.py:45] layer 21_down initial loss 0.0007935729227028787
W0314 14:47:50.801611 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:47:54.844642 139698 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0005194462719373405 old loss 0.000521377194672823 BETTER
I0314 14:47:55.316563 141041 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.000577224069274962 old loss 0.0005792318843305111 BETTER
W0314 14:47:56.020825 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_gate proxy err 0.02159814164042473 tr(WHW.T) 4861.9248046875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.69it/s]  3%|▎         | 3/112 [00:01<00:54,  2.01it/s]  4%|▎         | 4/112 [00:02<00:48,  2.21it/s]  4%|▍         | 5/112 [00:02<00:45,  2.35it/s]  5%|▌         | 6/112 [00:02<00:43,  2.44it/s]  6%|▋         | 7/112 [00:03<00:41,  2.50it/s]  7%|▋         | 8/112 [00:03<00:40,  2.54it/s]  8%|▊         | 9/112 [00:03<00:39,  2.58it/s]  9%|▉         | 10/112 [00:04<00:39,  2.61it/s] 10%|▉         | 11/112 [00:04<00:38,  2.63it/s] 11%|█         | 12/112 [00:05<00:38,  2.63it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.64it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.63it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.60it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.62it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.62it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.63it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.64it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.64it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.65it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.67it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.67it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.68it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.68it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.68it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.63it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.65it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.66it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.67it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.68it/s] 30%|███       | 34/112 [00:13<00:29,  2.68it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.68it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.68it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.68it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.68it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.68it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.67it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.67it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.68it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.63it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.65it/s] 40%|████      | 45/112 [00:17<00:25,  2.66it/s] 41%|████      | 46/112 [00:17<00:24,  2.66it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.67it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.68it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.67it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.67it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.67it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.67it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.66it/s]I0314 14:48:20.759741 137031 finetune.py:68] layer 20_down @ epoch 0 new loss 0.0006748398300260305 old loss 0.0006758881499990821 BETTER
 49%|████▉     | 55/112 [00:21<00:21,  2.66it/s] 50%|█████     | 56/112 [00:21<00:21,  2.66it/s] 51%|█████     | 57/112 [00:21<00:20,  2.66it/s]I0314 14:48:21.750061 138351 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0007924783858470619 old loss 0.0007935729227028787 BETTER
 52%|█████▏    | 58/112 [00:22<00:20,  2.62it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.65it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.66it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.65it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.66it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.66it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.67it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.67it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.67it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.66it/s] 61%|██████    | 68/112 [00:26<00:16,  2.66it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.67it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.66it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.66it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.61it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.63it/s]I0314 14:48:27.923193 141041 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0005753539153374732 old loss 0.000577224069274962 BETTER
 66%|██████▌   | 74/112 [00:28<00:14,  2.64it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.67it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.68it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.69it/s]W0314 14:48:29.089247 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 70%|██████▉   | 78/112 [00:29<00:12,  2.70it/s] 71%|███████   | 79/112 [00:30<00:12,  2.70it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.70it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.67it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.63it/s] 74%|███████▍  | 83/112 [00:31<00:11,  2.63it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.60it/s]23_gate proxy err 0.023371001705527306 tr(WHW.T) 4750.58642578125
  0%|          | 0/112 [00:00<?, ?it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.61it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.63it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.64it/s]  1%|          | 1/112 [00:00<01:35,  1.16it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.66it/s]  2%|▏         | 2/112 [00:01<01:02,  1.75it/s] 80%|████████  | 90/112 [00:34<00:08,  2.67it/s]  3%|▎         | 3/112 [00:01<00:52,  2.08it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.66it/s]  4%|▎         | 4/112 [00:01<00:47,  2.27it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.63it/s]  4%|▍         | 5/112 [00:02<00:44,  2.40it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.64it/s]  5%|▌         | 6/112 [00:02<00:42,  2.49it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.66it/s]  6%|▋         | 7/112 [00:03<00:41,  2.54it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.66it/s]  7%|▋         | 8/112 [00:03<00:40,  2.58it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.66it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.67it/s]  9%|▉         | 10/112 [00:04<00:38,  2.62it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.68it/s] 10%|▉         | 11/112 [00:04<00:38,  2.65it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.68it/s] 11%|█         | 12/112 [00:04<00:37,  2.66it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.68it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.67it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.68it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.68it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.68it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.69it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.64it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.69it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.66it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.69it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.66it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.68it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.67it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.68it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.67it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.68it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.67it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.68it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.69it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.69it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.69it/s] 21%|██        | 23/112 [00:09<00:33,  2.65it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.69it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.66it/s]100%|██████████| 112/112 [00:42<00:00,  2.69it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
 22%|██▏       | 25/112 [00:09<00:32,  2.67it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.69it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.70it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.71it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.71it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.70it/s] 28%|██▊       | 31/112 [00:12<00:29,  2.70it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.71it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.71it/s] 30%|███       | 34/112 [00:13<00:28,  2.70it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.70it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.70it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.69it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.66it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.68it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.69it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.68it/s]W0314 14:48:48.389000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.389000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.389000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.390000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.390000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.390000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.390000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.436000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.436000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.436000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.436000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.436000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.618000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.618000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.619000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.619000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.619000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 42/112 [00:16<00:26,  2.68it/s]W0314 14:48:48.943000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.943000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.944000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.944000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.944000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.944000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.944000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.977000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.977000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.977000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.977000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:48.977000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 43/112 [00:16<00:25,  2.67it/s]W0314 14:48:49.048000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:48:49.048000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:48:49.048000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:48:49.048000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:49.048000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 39%|███▉      | 44/112 [00:16<00:25,  2.67it/s] 40%|████      | 45/112 [00:17<00:25,  2.67it/s]W0314 14:48:50.061000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.074000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.083000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.083000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 41%|████      | 46/112 [00:17<00:24,  2.67it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.67it/s]W0314 14:48:50.550000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.550000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.550000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.550000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.551000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.551000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.551000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.582000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.582000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.582000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.582000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.582000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 43%|████▎     | 48/112 [00:18<00:23,  2.67it/s]W0314 14:48:50.887000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.887000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.887000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.887000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.887000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.887000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.887000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:48:50.887000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:48:51.190000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:48:51.190000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:48:51.190000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:48:51.190000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:48:51.190000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 49/112 [00:18<00:23,  2.66it/s]W0314 14:48:51.626000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 45%|████▍     | 50/112 [00:19<00:23,  2.66it/s]W0314 14:48:51.632000 139790158112576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 46%|████▌     | 51/112 [00:19<00:22,  2.66it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.66it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.62it/s]I0314 14:48:52.979383 138351 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0007921611540950835 old loss 0.0007924783858470619 BETTER
 48%|████▊     | 54/112 [00:20<00:21,  2.66it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.68it/s] 50%|█████     | 56/112 [00:21<00:20,  2.68it/s]I0314 14:48:54.220082 137031 finetune.py:68] layer 20_down @ epoch 1 new loss 0.0006745712598785758 old loss 0.0006748398300260305 BETTER
 51%|█████     | 57/112 [00:21<00:20,  2.68it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.68it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.68it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.68it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.68it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.68it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.67it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.68it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.67it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.66it/s] 60%|█████▉    | 67/112 [00:25<00:17,  2.63it/s] 61%|██████    | 68/112 [00:25<00:16,  2.64it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.66it/s]I0314 14:48:58.942039 139698 finetune.py:45] layer 22_down initial loss 0.0008363040396943688
W0314 14:48:58.942428 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 70/112 [00:26<00:15,  2.66it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.67it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.68it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.69it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.69it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.69it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.69it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.68it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.68it/s] 71%|███████   | 79/112 [00:29<00:12,  2.68it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.68it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.63it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.65it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.65it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.66it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.67it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.68it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.69it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.69it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.70it/s] 80%|████████  | 90/112 [00:34<00:08,  2.69it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.70it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.69it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.69it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.68it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.64it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.66it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.67it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.68it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.68it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.68it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.68it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.69it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.69it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.70it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.70it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.69it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.69it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.65it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.67it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.68it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.69it/s]100%|██████████| 112/112 [00:42<00:00,  2.70it/s]100%|██████████| 112/112 [00:42<00:00,  2.65it/s]
W0314 14:49:21.033000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.033000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.033000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.034000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.034000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.034000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.034000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.081000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.081000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.081000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.081000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.081000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.263000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.263000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.263000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.263000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.264000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.592000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.592000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.593000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.593000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.593000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.593000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.593000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.629000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.629000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.629000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.629000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.629000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.700000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.701000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.701000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.701000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:49:21.701000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:49:22.702000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:22.717000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:22.725000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:22.725000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.195000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.196000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.196000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.196000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.196000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.196000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.196000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.230000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.230000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.230000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.230000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.230000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.540000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.540000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.540000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.540000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.540000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.540000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.540000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.541000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.845000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.845000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.845000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.845000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:49:23.845000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:49:24.287000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:49:24.292000 139715089753920 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 14:49:24.612133 138351 finetune.py:68] layer 21_down @ epoch 2 new loss 0.0007919743657112122 old loss 0.0007921611540950835 BETTER
I0314 14:49:27.894822 137031 finetune.py:68] layer 20_down @ epoch 2 new loss 0.0006743707926943898 old loss 0.0006745712598785758 BETTER
I0314 14:49:30.343900 139698 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0008350864518433809 old loss 0.0008363040396943688 BETTER
I0314 14:49:31.650804 141041 finetune.py:45] layer 23_down initial loss 0.0009136468288488686
W0314 14:49:31.651294 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:49:56.278614 138351 finetune.py:68] layer 21_down @ epoch 3 new loss 0.0007918315241113305 old loss 0.0007919743657112122 BETTER
I0314 14:50:01.991526 137031 finetune.py:68] layer 20_down @ epoch 3 new loss 0.0006742234108969569 old loss 0.0006743707926943898 BETTER
I0314 14:50:02.767804 141041 finetune.py:68] layer 23_down @ epoch 0 new loss 0.00091222592163831 old loss 0.0009136468288488686 BETTER
I0314 14:50:02.944517 139698 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0008347555412910879 old loss 0.0008350864518433809 BETTER
I0314 14:50:28.150296 138351 finetune.py:68] layer 21_down @ epoch 4 new loss 0.000791689264588058 old loss 0.0007918315241113305 BETTER
W0314 14:50:29.047158 138351 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

21_down proxy err 0.04616984724998474 tr(WHW.T) 29.35498046875
I0314 14:50:33.140462 141041 finetune.py:68] layer 23_down @ epoch 1 new loss 0.0009118751040659845 old loss 0.00091222592163831 BETTER
I0314 14:50:34.313915 139698 finetune.py:68] layer 22_down @ epoch 2 new loss 0.00083457853179425 old loss 0.0008347555412910879 BETTER
I0314 14:50:35.263011 137031 finetune.py:68] layer 20_down @ epoch 4 new loss 0.0006741152610629797 old loss 0.0006742234108969569 BETTER
W0314 14:50:35.973139 137031 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

20_down proxy err 0.04884245991706848 tr(WHW.T) 24.129791259765625
I0314 14:51:03.852481 141041 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0009116263245232403 old loss 0.0009118751040659845 BETTER
I0314 14:51:05.756164 139698 finetune.py:68] layer 22_down @ epoch 3 new loss 0.0008344181696884334 old loss 0.00083457853179425 BETTER
I0314 14:51:34.550463 141041 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0009114276617765427 old loss 0.0009116263245232403 BETTER
I0314 14:51:37.180902 139698 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0008342780638486147 old loss 0.0008344181696884334 BETTER
W0314 14:51:37.940916 139698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

22_down proxy err 0.046491120010614395 tr(WHW.T) 31.009613037109375
I0314 14:51:47.329233 9966 quantize_finetune_llama.py:186] computed original embedding for layer 24 in 66.37339925765991s
I0314 14:51:47.735611 9966 quantize_finetune_llama.py:159] layer 25 gpu 1
I0314 14:51:49.716019 161609 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 14:51:49.716143 161609 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 14:51:49.716235 161609 utils.py:162] NumExpr defaulting to 16 threads.
I0314 14:51:49.903216 161609 config.py:58] PyTorch version 2.4.0 available.
I0314 14:51:52.040267 161609 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 14:51:52.496556 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.39s/it]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]  9%|▉         | 3/32 [00:02<00:16,  1.76it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.09it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.85it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.89it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.94it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.96it/s] 41%|████      | 13/32 [00:05<00:06,  2.99it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.02it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.04it/s] 50%|█████     | 16/32 [00:06<00:05,  3.06it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.07it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.08it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.10it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.12it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.13it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.06it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.00it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.96it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.93it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
I0314 14:52:05.258213 141041 finetune.py:68] layer 23_down @ epoch 4 new loss 0.0009112813277170062 old loss 0.0009114276617765427 BETTER
W0314 14:52:06.035074 141041 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

23_down proxy err 0.046849578619003296 tr(WHW.T) 32.977970123291016
W0314 14:52:07.623000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.623000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.623000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.623000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.623000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.623000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.623000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.649000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.649000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.649000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.649000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.649000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.937000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.937000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.937000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.937000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:52:07.937000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.789000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.789000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.789000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.789000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.790000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.790000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.790000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.808000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.808000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.808000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.808000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:52:08.808000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:52:09.004000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:52:09.004000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:52:09.004000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:52:09.004000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:52:09.004000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.079000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.079000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.079000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.080000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.080000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.080000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.080000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.097000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.097000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.097000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.098000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.098000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.979000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.979000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.979000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.979000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:52:10.979000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 14:52:17.452858 161609 finetune.py:45] layer 24_v initial loss 0.00031332598882727325
W0314 14:52:17.453159 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:52:49.409731 9966 quantize_finetune_llama.py:186] computed original embedding for layer 25 in 61.24188446998596s
I0314 14:52:49.791576 9966 quantize_finetune_llama.py:159] layer 26 gpu 2
I0314 14:52:51.827568 162931 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 14:52:51.827682 162931 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 14:52:51.827744 162931 utils.py:162] NumExpr defaulting to 16 threads.
I0314 14:52:52.019635 162931 config.py:58] PyTorch version 2.4.0 available.
I0314 14:52:53.794339 161609 finetune.py:68] layer 24_v @ epoch 0 new loss 7.211846968857571e-05 old loss 0.00031332598882727325 BETTER
I0314 14:52:54.214406 162931 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 14:52:54.607628 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:48,  1.56s/it]  6%|▋         | 2/32 [00:01<00:25,  1.19it/s]  9%|▉         | 3/32 [00:02<00:17,  1.61it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.96it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.25it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.91it/s] 41%|████      | 13/32 [00:05<00:06,  2.92it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.93it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.96it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.97it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.98it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.97it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.00it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.99it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.98it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:12<00:00,  2.94it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
W0314 14:53:10.029000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.029000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.030000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.030000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.030000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.030000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.030000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.057000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.057000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.057000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.058000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.058000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.349000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.349000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.349000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.349000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:53:10.349000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.209000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.209000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.210000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.210000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.210000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.210000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.210000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.227000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.227000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.227000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.227000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.227000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.430000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.430000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.430000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.430000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:53:11.430000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.553000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.554000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.554000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.554000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.554000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.554000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.554000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.573000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.573000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.573000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.573000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:53:12.573000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:53:13.447000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:53:13.447000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:53:13.447000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:53:13.448000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:53:13.448000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 14:53:20.040760 162931 finetune.py:45] layer 25_v initial loss 0.0003673717728815973
W0314 14:53:20.041088 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:53:31.093041 161609 finetune.py:68] layer 24_v @ epoch 1 new loss 5.9737663832493126e-05 old loss 7.211846968857571e-05 BETTER
I0314 14:53:51.507911 9966 quantize_finetune_llama.py:186] computed original embedding for layer 26 in 61.23122787475586s
I0314 14:53:51.944824 9966 quantize_finetune_llama.py:159] layer 27 gpu 3
I0314 14:53:53.999288 164243 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 14:53:53.999395 164243 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 14:53:53.999462 164243 utils.py:162] NumExpr defaulting to 16 threads.
I0314 14:53:54.199178 164243 config.py:58] PyTorch version 2.4.0 available.
I0314 14:53:54.441694 162931 finetune.py:68] layer 25_v @ epoch 0 new loss 9.077113645616919e-05 old loss 0.0003673717728815973 BETTER
I0314 14:53:56.440949 164243 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 14:53:56.840084 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:56,  1.83s/it]  6%|▋         | 2/32 [00:02<00:28,  1.04it/s]  9%|▉         | 3/32 [00:02<00:19,  1.47it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.82it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.09it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 41%|████      | 13/32 [00:06<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 50%|█████     | 16/32 [00:07<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.79it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.76it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.77it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.77it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.78it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.79it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.79it/s]I0314 14:54:09.056485 161609 finetune.py:68] layer 24_v @ epoch 2 new loss 5.562031947192736e-05 old loss 5.9737663832493126e-05 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.83it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0314 14:54:13.113000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.114000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.114000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.114000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.114000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.114000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.114000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.142000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.143000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.143000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.143000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.143000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.446000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.446000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.446000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.446000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:54:13.446000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.343000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.343000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.344000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.344000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.344000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.344000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.344000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.362000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.362000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.363000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.363000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.363000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.573000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.573000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.573000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.574000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:54:14.574000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.722000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.722000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.722000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.722000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.722000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.723000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.723000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.741000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.741000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.741000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.741000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:54:15.741000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:54:16.644000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:54:16.644000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:54:16.644000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:54:16.644000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:54:16.645000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 14:54:22.720649 164243 finetune.py:45] layer 26_v initial loss 0.00040689416346140206
W0314 14:54:22.720981 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:54:30.709673 162931 finetune.py:68] layer 25_v @ epoch 1 new loss 7.469509728252888e-05 old loss 9.077113645616919e-05 BETTER
I0314 14:54:46.687988 161609 finetune.py:68] layer 24_v @ epoch 3 new loss 5.368076381273568e-05 old loss 5.562031947192736e-05 BETTER
I0314 14:54:53.293470 9966 quantize_finetune_llama.py:186] computed original embedding for layer 27 in 60.87706446647644s
I0314 14:54:53.735512 9966 quantize_finetune_llama.py:159] layer 28 gpu 0
I0314 14:54:55.935202 165588 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 14:54:55.935441 165588 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 14:54:55.935533 165588 utils.py:162] NumExpr defaulting to 16 threads.
I0314 14:54:56.192616 165588 config.py:58] PyTorch version 2.4.0 available.
I0314 14:54:58.196241 164243 finetune.py:68] layer 26_v @ epoch 0 new loss 0.00013136073539499193 old loss 0.00040689416346140206 BETTER
I0314 14:54:58.617469 165588 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 14:54:59.490097 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.06s/it]  6%|▋         | 2/32 [00:02<00:31,  1.06s/it]  9%|▉         | 3/32 [00:02<00:21,  1.36it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.70it/s] 16%|█▌        | 5/32 [00:03<00:13,  1.99it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.21it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.38it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.67it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.75it/s] 41%|████      | 13/32 [00:06<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.77it/s]I0314 14:55:07.545357 162931 finetune.py:68] layer 25_v @ epoch 2 new loss 6.944996857782826e-05 old loss 7.469509728252888e-05 BETTER
 47%|████▋     | 15/32 [00:07<00:06,  2.79it/s] 50%|█████     | 16/32 [00:07<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:08<00:04,  2.82it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.80it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.79it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.82it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:13<00:00,  2.82it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
W0314 14:55:16.472000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.472000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.472000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.473000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.473000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.473000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.473000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.500000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.500000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.500000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.500000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.500000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.803000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.803000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.803000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.803000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:16.804000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.687000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.687000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.687000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.687000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.687000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.687000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.687000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.706000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.706000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.706000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.706000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.706000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.912000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.912000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.913000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.913000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:17.913000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.056000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.057000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.057000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.057000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.057000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.057000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.057000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.075000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.075000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.075000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.075000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.076000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.971000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.971000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.971000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.971000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:19.971000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 14:55:24.929685 161609 finetune.py:68] layer 24_v @ epoch 4 new loss 5.239661913947202e-05 old loss 5.368076381273568e-05 BETTER
W0314 14:55:26.793097 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 14:55:27.138904 165588 finetune.py:45] layer 27_v initial loss 0.0005109509802423418
W0314 14:55:27.139173 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

24_v proxy err 0.045775167644023895 tr(WHW.T) 136.39785766601562
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:31,  1.01s/it]  6%|▋         | 2/32 [00:01<00:19,  1.58it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s]I0314 14:55:34.008915 164243 finetune.py:68] layer 26_v @ epoch 1 new loss 0.00011443920811871067 old loss 0.00013136073539499193 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0314 14:55:44.258507 162931 finetune.py:68] layer 25_v @ epoch 3 new loss 6.692769238725305e-05 old loss 6.944996857782826e-05 BETTER
W0314 14:55:46.075000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.076000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.076000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.076000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.076000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.076000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.076000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.107000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.107000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.107000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.107000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.108000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.284000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.284000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.284000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.284000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.284000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.516000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.516000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.516000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.516000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.516000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.516000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.516000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.540000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.540000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.540000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.540000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.540000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.607000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.607000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.607000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.607000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:46.607000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.366000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.694000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.694000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.694000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.694000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.695000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.695000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.695000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.719000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.719000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.719000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.719000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.719000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.988000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.989000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.989000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.989000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:55:47.989000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:55:48.354000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:55:56.778568 161609 finetune.py:45] layer 24_q initial loss 7.705726602580398e-05
W0314 14:55:56.779016 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:56:02.552780 165588 finetune.py:68] layer 27_v @ epoch 0 new loss 0.000138859759317711 old loss 0.0005109509802423418 BETTER
I0314 14:56:10.646118 164243 finetune.py:68] layer 26_v @ epoch 2 new loss 0.00010844224743777886 old loss 0.00011443920811871067 BETTER
I0314 14:56:21.164695 162931 finetune.py:68] layer 25_v @ epoch 4 new loss 6.522918556584045e-05 old loss 6.692769238725305e-05 BETTER
W0314 14:56:23.152912 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

25_v proxy err 0.04013826698064804 tr(WHW.T) 164.04367065429688
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s]I0314 14:56:33.497633 161609 finetune.py:68] layer 24_q @ epoch 0 new loss 7.409069803543389e-05 old loss 7.705726602580398e-05 BETTER
 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0314 14:56:37.410054 165588 finetune.py:68] layer 27_v @ epoch 1 new loss 0.00012191538553452119 old loss 0.000138859759317711 BETTER
W0314 14:56:42.442000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.442000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.442000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.442000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.442000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.442000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.442000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.472000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.473000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.473000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.473000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.473000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.649000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.649000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.649000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.649000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.649000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.883000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.883000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.883000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.883000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.883000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.883000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.883000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.907000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.907000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.907000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.907000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.907000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.975000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.975000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.975000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.975000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:56:42.975000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:56:43.729000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.058000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.058000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.058000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.058000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.058000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.058000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.058000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.080000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.080000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.080000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.081000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.081000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.348000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.348000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.348000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.348000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.348000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:56:44.712000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:56:47.207025 164243 finetune.py:68] layer 26_v @ epoch 3 new loss 0.00010509112325962633 old loss 0.00010844224743777886 BETTER
I0314 14:56:52.244323 162931 finetune.py:45] layer 25_q initial loss 0.00011152345541631803
W0314 14:56:52.244769 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:57:11.879444 161609 finetune.py:68] layer 24_q @ epoch 1 new loss 7.26640282664448e-05 old loss 7.409069803543389e-05 BETTER
I0314 14:57:12.684136 165588 finetune.py:68] layer 27_v @ epoch 2 new loss 0.00011632165114860982 old loss 0.00012191538553452119 BETTER
I0314 14:57:23.473589 164243 finetune.py:68] layer 26_v @ epoch 4 new loss 0.00010274921805830672 old loss 0.00010509112325962633 BETTER
W0314 14:57:25.421190 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_v proxy err 0.053091760724782944 tr(WHW.T) 123.81900024414062
  0%|          | 0/32 [00:00<?, ?it/s]I0314 14:57:27.192793 162931 finetune.py:68] layer 25_q @ epoch 0 new loss 0.00010531514271860942 old loss 0.00011152345541631803 BETTER
  3%|▎         | 1/32 [00:00<00:29,  1.05it/s]  6%|▋         | 2/32 [00:01<00:18,  1.63it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
W0314 14:57:44.434000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.435000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.435000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.435000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.435000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.435000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.435000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.465000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.465000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.465000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.465000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.466000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.640000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.640000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.640000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.640000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.640000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.873000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.873000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.873000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.873000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.873000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.873000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.873000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.895000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.895000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.895000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.895000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.895000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.963000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.963000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.963000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.963000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:57:44.963000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:57:45.724000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.049000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.049000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.050000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.050000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.050000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.050000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.050000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.072000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.072000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.072000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.072000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.072000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.333000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.334000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.334000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.334000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.334000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:57:46.699000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:57:47.386792 165588 finetune.py:68] layer 27_v @ epoch 3 new loss 0.00011333372094668448 old loss 0.00011632165114860982 BETTER
I0314 14:57:49.934186 161609 finetune.py:68] layer 24_q @ epoch 2 new loss 7.157803338486701e-05 old loss 7.26640282664448e-05 BETTER
I0314 14:57:53.717168 164243 finetune.py:45] layer 26_q initial loss 0.00013864171341992915
W0314 14:57:53.717525 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:58:03.374455 162931 finetune.py:68] layer 25_q @ epoch 1 new loss 0.00010229506005998701 old loss 0.00010531514271860942 BETTER
I0314 14:58:22.555888 165588 finetune.py:68] layer 27_v @ epoch 4 new loss 0.00011073119821958244 old loss 0.00011333372094668448 BETTER
W0314 14:58:25.521914 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

27_v proxy err 0.04227025434374809 tr(WHW.T) 203.18115234375
  0%|          | 0/32 [00:00<?, ?it/s]I0314 14:58:28.226416 161609 finetune.py:68] layer 24_q @ epoch 3 new loss 7.073663437040523e-05 old loss 7.157803338486701e-05 BETTER
I0314 14:58:28.523526 164243 finetune.py:68] layer 26_q @ epoch 0 new loss 0.00013404045603238046 old loss 0.00013864171341992915 BETTER
  3%|▎         | 1/32 [00:01<00:31,  1.02s/it]  6%|▋         | 2/32 [00:01<00:19,  1.55it/s]  9%|▉         | 3/32 [00:01<00:15,  1.90it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s]I0314 14:58:39.273295 162931 finetune.py:68] layer 25_q @ epoch 2 new loss 9.991839033318684e-05 old loss 0.00010229506005998701 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
W0314 14:58:46.717000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.717000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.717000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.717000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.717000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.717000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.717000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.750000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.750000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.750000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.750000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.750000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.931000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.931000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.931000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.932000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 14:58:46.932000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.163000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.163000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.163000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.163000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.163000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.163000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.163000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.186000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.186000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.186000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.186000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.186000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.256000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.256000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.256000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.256000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 14:58:47.256000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.032000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.367000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.367000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.367000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.367000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.368000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.368000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.368000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.392000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.392000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.392000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.392000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.392000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.656000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.656000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.656000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.656000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 14:58:48.657000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 14:58:49.032000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 14:58:56.147868 165588 finetune.py:45] layer 27_q initial loss 0.00016540513024665415
W0314 14:58:56.148385 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:59:04.997022 164243 finetune.py:68] layer 26_q @ epoch 1 new loss 0.00013138778740540147 old loss 0.00013404045603238046 BETTER
I0314 14:59:06.992361 161609 finetune.py:68] layer 24_q @ epoch 4 new loss 7.003971404628828e-05 old loss 7.073663437040523e-05 BETTER
W0314 14:59:08.937401 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_q proxy err 0.00744636869058013 tr(WHW.T) 6560.8134765625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s]  6%|▋         | 2/32 [00:01<00:17,  1.70it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.27it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.71it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s]I0314 14:59:15.450141 162931 finetune.py:68] layer 25_q @ epoch 3 new loss 9.814710938371718e-05 old loss 9.991839033318684e-05 BETTER
 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
I0314 14:59:29.383685 161609 finetune.py:45] layer 24_k initial loss 9.554956341162324e-05
W0314 14:59:29.384066 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 14:59:30.236711 165588 finetune.py:68] layer 27_q @ epoch 0 new loss 0.0001593380729900673 old loss 0.00016540513024665415 BETTER
I0314 14:59:41.617478 164243 finetune.py:68] layer 26_q @ epoch 2 new loss 0.0001292537635890767 old loss 0.00013138778740540147 BETTER
I0314 14:59:52.378178 162931 finetune.py:68] layer 25_q @ epoch 4 new loss 9.663488890510052e-05 old loss 9.814710938371718e-05 BETTER
W0314 14:59:54.676413 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

25_q proxy err 0.0063851154409348965 tr(WHW.T) 7693.73046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.62it/s]I0314 15:00:05.930095 165588 finetune.py:68] layer 27_q @ epoch 1 new loss 0.00015621983038727194 old loss 0.0001593380729900673 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s]I0314 15:00:07.034218 161609 finetune.py:68] layer 24_k @ epoch 0 new loss 9.228795533999801e-05 old loss 9.554956341162324e-05 BETTER
 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0314 15:00:16.257658 162931 finetune.py:45] layer 25_k initial loss 0.00014649367949459702
W0314 15:00:16.258070 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:00:18.297331 164243 finetune.py:68] layer 26_q @ epoch 3 new loss 0.0001275262766284868 old loss 0.0001292537635890767 BETTER
I0314 15:00:40.833476 165588 finetune.py:68] layer 27_q @ epoch 2 new loss 0.00015397227252833545 old loss 0.00015621983038727194 BETTER
I0314 15:00:45.225673 161609 finetune.py:68] layer 24_k @ epoch 1 new loss 9.146625961875543e-05 old loss 9.228795533999801e-05 BETTER
I0314 15:00:51.741178 162931 finetune.py:68] layer 25_k @ epoch 0 new loss 0.00013417593436315656 old loss 0.00014649367949459702 BETTER
I0314 15:00:54.754654 164243 finetune.py:68] layer 26_q @ epoch 4 new loss 0.00012598569446709007 old loss 0.0001275262766284868 BETTER
W0314 15:00:56.495348 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_q proxy err 0.007450043223798275 tr(WHW.T) 6101.39453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.61it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.69it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0314 15:01:15.865893 165588 finetune.py:68] layer 27_q @ epoch 3 new loss 0.0001521392841823399 old loss 0.00015397227252833545 BETTER
I0314 15:01:17.676252 164243 finetune.py:45] layer 26_k initial loss 0.0001581927645020187
W0314 15:01:17.676683 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:01:23.560636 161609 finetune.py:68] layer 24_k @ epoch 2 new loss 9.084666089620441e-05 old loss 9.146625961875543e-05 BETTER
I0314 15:01:28.584435 162931 finetune.py:68] layer 25_k @ epoch 1 new loss 0.0001322583993896842 old loss 0.00013417593436315656 BETTER
I0314 15:01:50.973336 165588 finetune.py:68] layer 27_q @ epoch 4 new loss 0.00015067608910612762 old loss 0.0001521392841823399 BETTER
I0314 15:01:52.704275 164243 finetune.py:68] layer 26_k @ epoch 0 new loss 0.00015301810344681144 old loss 0.0001581927645020187 BETTER
W0314 15:01:52.730336 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

27_q proxy err 0.007728990167379379 tr(WHW.T) 6392.35693359375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:27,  1.11it/s]  6%|▋         | 2/32 [00:01<00:17,  1.72it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s]I0314 15:02:01.920404 161609 finetune.py:68] layer 24_k @ epoch 3 new loss 9.034875256475061e-05 old loss 9.084666089620441e-05 BETTER
 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s]I0314 15:02:04.557490 162931 finetune.py:68] layer 25_k @ epoch 2 new loss 0.00013095444592181593 old loss 0.0001322583993896842 BETTER
 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
I0314 15:02:12.703865 165588 finetune.py:45] layer 27_k initial loss 0.00021000427659600973
W0314 15:02:12.704133 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:02:29.413701 164243 finetune.py:68] layer 26_k @ epoch 1 new loss 0.0001515303156338632 old loss 0.00015301810344681144 BETTER
I0314 15:02:40.307584 161609 finetune.py:68] layer 24_k @ epoch 4 new loss 8.989507477963343e-05 old loss 9.034875256475061e-05 BETTER
I0314 15:02:41.251920 162931 finetune.py:68] layer 25_k @ epoch 3 new loss 0.00012979800521861762 old loss 0.00013095444592181593 BETTER
W0314 15:02:42.196681 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_k proxy err 0.006420589983463287 tr(WHW.T) 4128.7314453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.27it/s]  6%|▋         | 2/32 [00:01<00:16,  1.86it/s]  9%|▉         | 3/32 [00:01<00:13,  2.17it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.37it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.50it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.62it/s]I0314 15:02:46.611204 165588 finetune.py:68] layer 27_k @ epoch 0 new loss 0.00019918351608794183 old loss 0.00021000427659600973 BETTER
 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.76it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
I0314 15:03:03.840180 161609 finetune.py:45] layer 24_o initial loss 0.00017378803750034422
W0314 15:03:03.840577 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:03:06.472751 164243 finetune.py:68] layer 26_k @ epoch 2 new loss 0.00015028125199023634 old loss 0.0001515303156338632 BETTER
I0314 15:03:18.134154 162931 finetune.py:68] layer 25_k @ epoch 4 new loss 0.00012881585280410945 old loss 0.00012979800521861762 BETTER
W0314 15:03:20.277982 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 15:03:21.283388 165588 finetune.py:68] layer 27_k @ epoch 1 new loss 0.0001973601756617427 old loss 0.00019918351608794183 BETTER
25_k proxy err 0.006425218656659126 tr(WHW.T) 4228.92919921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.26it/s]  6%|▋         | 2/32 [00:01<00:16,  1.83it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
I0314 15:03:42.266881 161609 finetune.py:68] layer 24_o @ epoch 0 new loss 0.00015536672435700893 old loss 0.00017378803750034422 BETTER
I0314 15:03:43.088194 162931 finetune.py:45] layer 25_o initial loss 0.0002249531535198912
W0314 15:03:43.088582 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:03:44.148995 164243 finetune.py:68] layer 26_k @ epoch 3 new loss 0.00014929292956367135 old loss 0.00015028125199023634 BETTER
I0314 15:03:56.362038 165588 finetune.py:68] layer 27_k @ epoch 2 new loss 0.00019612176402006298 old loss 0.0001973601756617427 BETTER
I0314 15:04:19.052969 162931 finetune.py:68] layer 25_o @ epoch 0 new loss 0.0002014419878832996 old loss 0.0002249531535198912 BETTER
I0314 15:04:20.768499 161609 finetune.py:68] layer 24_o @ epoch 1 new loss 0.00015331018948927522 old loss 0.00015536672435700893 BETTER
I0314 15:04:21.023721 164243 finetune.py:68] layer 26_k @ epoch 4 new loss 0.00014839360665064305 old loss 0.00014929292956367135 BETTER
W0314 15:04:22.823038 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_k proxy err 0.006055984180420637 tr(WHW.T) 4375.1318359375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.80it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s]I0314 15:04:31.403514 165588 finetune.py:68] layer 27_k @ epoch 3 new loss 0.00019498546316754073 old loss 0.00019612176402006298 BETTER
 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0314 15:04:44.394724 164243 finetune.py:45] layer 26_o initial loss 0.0002794030588120222
W0314 15:04:44.395138 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:04:55.915209 162931 finetune.py:68] layer 25_o @ epoch 1 new loss 0.00019858767336700112 old loss 0.0002014419878832996 BETTER
I0314 15:04:58.987890 161609 finetune.py:68] layer 24_o @ epoch 2 new loss 0.00015218612679746002 old loss 0.00015331018948927522 BETTER
I0314 15:05:06.211116 165588 finetune.py:68] layer 27_k @ epoch 4 new loss 0.00019409974629525095 old loss 0.00019498546316754073 BETTER
W0314 15:05:07.926697 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

27_k proxy err 0.006968270987272263 tr(WHW.T) 4185.5341796875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.26it/s]  6%|▋         | 2/32 [00:01<00:16,  1.85it/s]  9%|▉         | 3/32 [00:01<00:13,  2.18it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.36it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.62it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.69it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.62it/s]I0314 15:05:20.248018 164243 finetune.py:68] layer 26_o @ epoch 0 new loss 0.00025092755095101893 old loss 0.0002794030588120222 BETTER
 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0314 15:05:28.827217 165588 finetune.py:45] layer 27_o initial loss 0.00035226187901571393
W0314 15:05:28.827625 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:05:31.872298 162931 finetune.py:68] layer 25_o @ epoch 2 new loss 0.00019676715601235628 old loss 0.00019858767336700112 BETTER
I0314 15:05:37.774633 161609 finetune.py:68] layer 24_o @ epoch 3 new loss 0.00015134247951209545 old loss 0.00015218612679746002 BETTER
I0314 15:05:56.606330 164243 finetune.py:68] layer 26_o @ epoch 1 new loss 0.00024747487623244524 old loss 0.00025092755095101893 BETTER
I0314 15:06:03.191575 165588 finetune.py:68] layer 27_o @ epoch 0 new loss 0.0003123732458334416 old loss 0.00035226187901571393 BETTER
I0314 15:06:08.991302 162931 finetune.py:68] layer 25_o @ epoch 3 new loss 0.00019548081036191434 old loss 0.00019676715601235628 BETTER
I0314 15:06:16.092357 161609 finetune.py:68] layer 24_o @ epoch 4 new loss 0.0001506986009189859 old loss 0.00015134247951209545 BETTER
W0314 15:06:17.883128 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_o proxy err 0.041116874665021896 tr(WHW.T) 6.568134784698486
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.99s/it]  6%|▋         | 2/32 [00:03<00:50,  1.67s/it]  9%|▉         | 3/32 [00:04<00:45,  1.57s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.46s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it]I0314 15:06:32.896539 164243 finetune.py:68] layer 26_o @ epoch 2 new loss 0.0002453667111694813 old loss 0.00024747487623244524 BETTER
 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.46s/it]I0314 15:06:38.458548 165588 finetune.py:68] layer 27_o @ epoch 1 new loss 0.0003086624201387167 old loss 0.0003123732458334416 BETTER
 41%|████      | 13/32 [00:19<00:27,  1.46s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.45s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.46s/it] 50%|█████     | 16/32 [00:23<00:23,  1.46s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it]I0314 15:06:45.410761 162931 finetune.py:68] layer 25_o @ epoch 4 new loss 0.00019439507741481066 old loss 0.00019548081036191434 BETTER
 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it]W0314 15:06:47.374397 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:29<00:17,  1.46s/it]25_o proxy err 0.03349278122186661 tr(WHW.T) 8.83610725402832
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:31<00:16,  1.46s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.46s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 78%|███████▊  | 25/32 [00:36<00:10,  1.46s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.46s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.46s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 38%|███▊      | 12/32 [00:17<00:29,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it]I0314 15:07:09.099513 164243 finetune.py:68] layer 26_o @ epoch 3 new loss 0.00024375852080993354 old loss 0.0002453667111694813 BETTER
 44%|████▍     | 14/32 [00:20<00:26,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.47s/it] 50%|█████     | 16/32 [00:23<00:23,  1.48s/it]I0314 15:07:13.401765 165588 finetune.py:68] layer 27_o @ epoch 2 new loss 0.0003062824544031173 old loss 0.0003086624201387167 BETTER
 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it]I0314 15:07:14.805986 161609 finetune.py:45] layer 24_up initial loss 0.00045483995927497745
W0314 15:07:14.806331 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:26<00:20,  1.48s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
I0314 15:07:45.431444 162931 finetune.py:45] layer 25_up initial loss 0.0005414315965026617
W0314 15:07:45.431979 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:07:46.263189 164243 finetune.py:68] layer 26_o @ epoch 4 new loss 0.00024241459323093295 old loss 0.00024375852080993354 BETTER
I0314 15:07:48.450970 165588 finetune.py:68] layer 27_o @ epoch 3 new loss 0.00030456995591521263 old loss 0.0003062824544031173 BETTER
W0314 15:07:48.572615 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_o proxy err 0.025338610634207726 tr(WHW.T) 16.246623992919922
  0%|          | 0/32 [00:00<?, ?it/s]I0314 15:07:51.038085 161609 finetune.py:68] layer 24_up @ epoch 0 new loss 0.0004494093300309032 old loss 0.00045483995927497745 BETTER
  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it]  9%|▉         | 3/32 [00:04<00:46,  1.61s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.50s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it]I0314 15:08:19.490396 162931 finetune.py:68] layer 25_up @ epoch 0 new loss 0.0005349262501113117 old loss 0.0005414315965026617 BETTER
 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.49s/it]I0314 15:08:23.248313 165588 finetune.py:68] layer 27_o @ epoch 4 new loss 0.00030311266891658306 old loss 0.00030456995591521263 BETTER
 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it]W0314 15:08:24.761296 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it]27_o proxy err 0.03009643405675888 tr(WHW.T) 16.21500015258789
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it]I0314 15:08:27.697943 161609 finetune.py:68] layer 24_up @ epoch 1 new loss 0.0004458170442376286 old loss 0.0004494093300309032 BETTER
  3%|▎         | 1/32 [00:01<00:59,  1.92s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it]  9%|▉         | 3/32 [00:04<00:44,  1.53s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:41,  1.48s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.50s/it] 16%|█▌        | 5/32 [00:07<00:39,  1.46s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:37,  1.45s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it] 22%|██▏       | 7/32 [00:10<00:35,  1.44s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it] 25%|██▌       | 8/32 [00:11<00:34,  1.43s/it]100%|██████████| 32/32 [00:48<00:00,  1.49s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 28%|██▊       | 9/32 [00:13<00:33,  1.44s/it] 31%|███▏      | 10/32 [00:14<00:31,  1.45s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.45s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.49s/it]I0314 15:08:47.420347 164243 finetune.py:45] layer 26_up initial loss 0.0006542074261233211
W0314 15:08:47.420700 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 50%|█████     | 16/32 [00:23<00:23,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it]I0314 15:08:54.501930 162931 finetune.py:68] layer 25_up @ epoch 1 new loss 0.0005309212720021605 old loss 0.0005349262501113117 BETTER
 62%|██████▎   | 20/32 [00:29<00:17,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.49s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.50s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it]I0314 15:09:04.066556 161609 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0004429738619364798 old loss 0.0004458170442376286 BETTER
 81%|████████▏ | 26/32 [00:38<00:09,  1.50s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.50s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.50s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
I0314 15:09:21.820936 164243 finetune.py:68] layer 26_up @ epoch 0 new loss 0.000646130065433681 old loss 0.0006542074261233211 BETTER
I0314 15:09:22.450135 165588 finetune.py:45] layer 27_up initial loss 0.0007953143212944269
W0314 15:09:22.450659 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:09:28.893134 162931 finetune.py:68] layer 25_up @ epoch 2 new loss 0.0005275843432173133 old loss 0.0005309212720021605 BETTER
I0314 15:09:40.753071 161609 finetune.py:68] layer 24_up @ epoch 3 new loss 0.00044045443064533174 old loss 0.0004429738619364798 BETTER
I0314 15:09:54.687144 165588 finetune.py:68] layer 27_up @ epoch 0 new loss 0.0007854477153159678 old loss 0.0007953143212944269 BETTER
I0314 15:09:56.186581 164243 finetune.py:68] layer 26_up @ epoch 1 new loss 0.0006412570946849883 old loss 0.000646130065433681 BETTER
I0314 15:10:03.296947 162931 finetune.py:68] layer 25_up @ epoch 3 new loss 0.0005247730296105146 old loss 0.0005275843432173133 BETTER
I0314 15:10:17.355778 161609 finetune.py:68] layer 24_up @ epoch 4 new loss 0.00043817481491714716 old loss 0.00044045443064533174 BETTER
W0314 15:10:18.985865 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_up proxy err 0.04817662760615349 tr(WHW.T) 1343.7181396484375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it]  6%|▋         | 2/32 [00:03<00:49,  1.64s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it]I0314 15:10:27.787256 165588 finetune.py:68] layer 27_up @ epoch 1 new loss 0.0007793806144036353 old loss 0.0007854477153159678 BETTER
 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.49s/it]I0314 15:10:30.585085 164243 finetune.py:68] layer 26_up @ epoch 2 new loss 0.0006372973439283669 old loss 0.0006412570946849883 BETTER
 22%|██▏       | 7/32 [00:10<00:37,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it]I0314 15:10:37.730868 162931 finetune.py:68] layer 25_up @ epoch 4 new loss 0.0005221969913691282 old loss 0.0005247730296105146 BETTER
 38%|███▊      | 12/32 [00:18<00:29,  1.47s/it]W0314 15:10:39.498163 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:19<00:29,  1.55s/it]25_up proxy err 0.04838429391384125 tr(WHW.T) 1430.4195556640625
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it]  3%|▎         | 1/32 [00:01<00:58,  1.88s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.51s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.46s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 31%|███▏      | 10/32 [00:14<00:32,  1.46s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 38%|███▊      | 12/32 [00:17<00:28,  1.45s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 41%|████      | 13/32 [00:19<00:27,  1.45s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it]I0314 15:11:01.285483 165588 finetune.py:68] layer 27_up @ epoch 2 new loss 0.0007745790062472224 old loss 0.0007793806144036353 BETTER
 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it]I0314 15:11:05.160838 164243 finetune.py:68] layer 26_up @ epoch 3 new loss 0.0006338275852613151 old loss 0.0006372973439283669 BETTER
 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.46s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 59%|█████▉    | 19/32 [00:28<00:18,  1.46s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.45s/it] 66%|██████▌   | 21/32 [00:31<00:15,  1.45s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.46s/it] 72%|███████▏  | 23/32 [00:33<00:13,  1.46s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it]I0314 15:11:16.463724 161609 finetune.py:45] layer 24_gate initial loss 0.000667750951834023
W0314 15:11:16.464109 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 78%|███████▊  | 25/32 [00:36<00:10,  1.46s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]
I0314 15:11:35.208861 165588 finetune.py:68] layer 27_up @ epoch 3 new loss 0.0007703572046011686 old loss 0.0007745790062472224 BETTER
I0314 15:11:36.120651 162931 finetune.py:45] layer 25_gate initial loss 0.0007903928635641932
W0314 15:11:36.121047 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:11:39.977636 164243 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0006308085867203772 old loss 0.0006338275852613151 BETTER
W0314 15:11:41.563228 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_up proxy err 0.04748036339879036 tr(WHW.T) 1585.5166015625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.91s/it]  6%|▋         | 2/32 [00:03<00:49,  1.67s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it]I0314 15:11:51.128535 161609 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.0006624516681768 old loss 0.000667750951834023 BETTER
 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 22%|██▏       | 7/32 [00:10<00:38,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:19<00:28,  1.51s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it]I0314 15:12:09.080777 162931 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.0007846708176657557 old loss 0.0007903928635641932 BETTER
I0314 15:12:09.146299 165588 finetune.py:68] layer 27_up @ epoch 4 new loss 0.0007667380850762129 old loss 0.0007703572046011686 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it]W0314 15:12:11.120789 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 19/32 [00:29<00:19,  1.50s/it]27_up proxy err 0.0442500114440918 tr(WHW.T) 1874.459228515625
  0%|          | 0/32 [00:00<?, ?it/s] 62%|██████▎   | 20/32 [00:30<00:18,  1.50s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.49s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it]I0314 15:12:26.229506 161609 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.0006593260332010686 old loss 0.0006624516681768 BETTER
 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 38%|███▊      | 12/32 [00:18<00:30,  1.50s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:21,  1.46s/it]I0314 15:12:39.840113 164243 finetune.py:45] layer 26_gate initial loss 0.0009538459707982838
W0314 15:12:39.840342 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:27<00:20,  1.44s/it] 59%|█████▉    | 19/32 [00:28<00:18,  1.44s/it]I0314 15:12:42.659397 162931 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.0007810405804775655 old loss 0.0007846708176657557 BETTER
 62%|██████▎   | 20/32 [00:29<00:17,  1.43s/it] 66%|██████▌   | 21/32 [00:31<00:15,  1.44s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.44s/it] 72%|███████▏  | 23/32 [00:34<00:12,  1.44s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.46s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.46s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.49s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.49s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.49s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
I0314 15:13:01.280205 161609 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.000656927062664181 old loss 0.0006593260332010686 BETTER
I0314 15:13:08.417852 165588 finetune.py:45] layer 27_gate initial loss 0.0011727454839274287
W0314 15:13:08.418154 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:13:12.642130 164243 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0009464893373660743 old loss 0.0009538459707982838 BETTER
I0314 15:13:16.490874 162931 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.000778292422182858 old loss 0.0007810405804775655 BETTER
I0314 15:13:36.489846 161609 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.0006549538229592144 old loss 0.000656927062664181 BETTER
I0314 15:13:39.685400 165588 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.0011623637983575463 old loss 0.0011727454839274287 BETTER
I0314 15:13:46.017076 164243 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.0009420785936526954 old loss 0.0009464893373660743 BETTER
I0314 15:13:49.854596 162931 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.0007759456639178097 old loss 0.000778292422182858 BETTER
I0314 15:14:11.592274 165588 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.0011563068255782127 old loss 0.0011623637983575463 BETTER
I0314 15:14:11.832112 161609 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.0006531724357046187 old loss 0.0006549538229592144 BETTER
W0314 15:14:13.096252 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_gate proxy err 0.02504402957856655 tr(WHW.T) 4730.6728515625
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s]  2%|▏         | 2/112 [00:01<01:03,  1.74it/s]  3%|▎         | 3/112 [00:01<00:52,  2.09it/s]  4%|▎         | 4/112 [00:01<00:46,  2.31it/s]  4%|▍         | 5/112 [00:02<00:43,  2.45it/s]I0314 15:14:19.281678 164243 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0009387856116518378 old loss 0.0009420785936526954 BETTER
  5%|▌         | 6/112 [00:02<00:41,  2.56it/s]  6%|▋         | 7/112 [00:03<00:40,  2.62it/s]  7%|▋         | 8/112 [00:03<00:39,  2.66it/s]  8%|▊         | 9/112 [00:03<00:38,  2.70it/s]  9%|▉         | 10/112 [00:04<00:37,  2.72it/s] 10%|▉         | 11/112 [00:04<00:36,  2.73it/s] 11%|█         | 12/112 [00:04<00:36,  2.74it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.75it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.76it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.76it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.72it/s]I0314 15:14:22.998649 162931 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.0007739401771686971 old loss 0.0007759456639178097 BETTER
 15%|█▌        | 17/112 [00:06<00:34,  2.75it/s] 16%|█▌        | 18/112 [00:07<00:33,  2.77it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.74it/s]W0314 15:14:24.218969 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 18%|█▊        | 20/112 [00:07<00:33,  2.75it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.76it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.75it/s] 21%|██        | 23/112 [00:08<00:32,  2.75it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.76it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.75it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.76it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.75it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.76it/s]25_gate proxy err 0.024943388998508453 tr(WHW.T) 5060.69287109375
  0%|          | 0/112 [00:00<?, ?it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.76it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.74it/s]  1%|          | 1/112 [00:00<01:32,  1.20it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.70it/s]  2%|▏         | 2/112 [00:01<01:00,  1.80it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.74it/s]  3%|▎         | 3/112 [00:01<00:51,  2.12it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.75it/s]  4%|▎         | 4/112 [00:01<00:46,  2.33it/s] 30%|███       | 34/112 [00:12<00:28,  2.73it/s]  4%|▍         | 5/112 [00:02<00:42,  2.49it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.74it/s]  5%|▌         | 6/112 [00:02<00:41,  2.56it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.75it/s]  6%|▋         | 7/112 [00:03<00:39,  2.63it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.74it/s]  7%|▋         | 8/112 [00:03<00:38,  2.67it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.74it/s]  8%|▊         | 9/112 [00:03<00:38,  2.69it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s]  9%|▉         | 10/112 [00:04<00:37,  2.72it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.74it/s] 10%|▉         | 11/112 [00:04<00:36,  2.75it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.74it/s] 11%|█         | 12/112 [00:04<00:36,  2.74it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.75it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.76it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.74it/s] 12%|█▎        | 14/112 [00:05<00:35,  2.76it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.74it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.74it/s] 40%|████      | 45/112 [00:16<00:24,  2.74it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.71it/s] 41%|████      | 46/112 [00:17<00:24,  2.72it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.73it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.74it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.74it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.72it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.76it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.72it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.76it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.73it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.76it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.73it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.78it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.73it/s] 21%|██        | 23/112 [00:08<00:32,  2.78it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.74it/s] 21%|██▏       | 24/112 [00:09<00:31,  2.77it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.73it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.77it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.73it/s] 23%|██▎       | 26/112 [00:09<00:31,  2.76it/s] 50%|█████     | 56/112 [00:20<00:20,  2.74it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.75it/s] 51%|█████     | 57/112 [00:21<00:20,  2.73it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.74it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.73it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.72it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.68it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.73it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.72it/s] 28%|██▊       | 31/112 [00:11<00:30,  2.68it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.73it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.70it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.73it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.72it/s] 30%|███       | 34/112 [00:12<00:28,  2.72it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.73it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.72it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.72it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.72it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.72it/s] 33%|███▎      | 37/112 [00:13<00:27,  2.72it/s] 60%|█████▉    | 67/112 [00:24<00:16,  2.73it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.73it/s] 61%|██████    | 68/112 [00:25<00:16,  2.73it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.72it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.75it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.73it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.74it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.73it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.73it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.72it/s]I0314 15:14:43.509929 165588 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.0011518163373693824 old loss 0.0011563068255782127 BETTER
 38%|███▊      | 43/112 [00:16<00:25,  2.74it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.73it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.73it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.67it/s] 40%|████      | 45/112 [00:16<00:24,  2.74it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.70it/s] 41%|████      | 46/112 [00:17<00:24,  2.70it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.73it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.71it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.71it/s] 43%|████▎     | 48/112 [00:17<00:23,  2.71it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.71it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.72it/s] 71%|███████   | 79/112 [00:29<00:12,  2.72it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.73it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.72it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.73it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.72it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.74it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.72it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.74it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.72it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.73it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.72it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.72it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.72it/s] 50%|█████     | 56/112 [00:20<00:20,  2.71it/s] 77%|███████▋  | 86/112 [00:31<00:09,  2.71it/s] 51%|█████     | 57/112 [00:21<00:20,  2.70it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.72it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.70it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.72it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.69it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.67it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.64it/s] 80%|████████  | 90/112 [00:33<00:08,  2.69it/s] 54%|█████▍    | 61/112 [00:22<00:19,  2.65it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.70it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.67it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.70it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.67it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.71it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.68it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.71it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.68it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.71it/s] 59%|█████▉    | 66/112 [00:24<00:17,  2.69it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.72it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.69it/s]I0314 15:14:52.590589 164243 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.0009360117255710065 old loss 0.0009387856116518378 BETTER
 87%|████████▋ | 97/112 [00:36<00:05,  2.72it/s] 61%|██████    | 68/112 [00:25<00:16,  2.72it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.72it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.72it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.72it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.71it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.72it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.71it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.72it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.71it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.72it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.67it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.68it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.69it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.70it/s] 67%|██████▋   | 75/112 [00:27<00:13,  2.70it/s] 94%|█████████▍| 105/112 [00:38<00:02,  2.69it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.71it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.70it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.72it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.71it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.72it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.71it/s] 71%|███████   | 79/112 [00:29<00:12,  2.73it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.71it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.71it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.71it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.72it/s]100%|██████████| 112/112 [00:41<00:00,  2.70it/s]100%|██████████| 112/112 [00:41<00:00,  2.69it/s]
 74%|███████▍  | 83/112 [00:30<00:10,  2.73it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.74it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.72it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.70it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.68it/s] 79%|███████▊  | 88/112 [00:32<00:09,  2.64it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.66it/s] 80%|████████  | 90/112 [00:33<00:08,  2.69it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.71it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.73it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.73it/s] 84%|████████▍ | 94/112 [00:34<00:06,  2.73it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.73it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.74it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.74it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.74it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.74it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.73it/s]W0314 15:15:04.793000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.794000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.794000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.794000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.794000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.794000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.794000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.842000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.842000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.842000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.843000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:04.843000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.039000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.039000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.039000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.039000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.039000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 90%|█████████ | 101/112 [00:37<00:04,  2.73it/s]W0314 15:15:05.359000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.359000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.359000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.359000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.359000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.360000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.360000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.395000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.395000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.395000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.395000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.395000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 102/112 [00:37<00:03,  2.67it/s]W0314 15:15:05.466000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.466000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.466000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.466000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:05.466000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 92%|█████████▏| 103/112 [00:38<00:03,  2.69it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.71it/s]W0314 15:15:06.438000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.444000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.450000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.450000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 105/112 [00:39<00:02,  2.72it/s]W0314 15:15:06.903000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.904000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.904000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.904000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.904000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.904000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.904000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:39<00:02,  2.73it/s]W0314 15:15:06.935000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.936000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.936000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.936000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:06.936000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.237000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.237000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.237000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.237000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.238000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.238000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.238000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.238000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 96%|█████████▌| 107/112 [00:39<00:01,  2.73it/s]W0314 15:15:07.527000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.527000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.528000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.528000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.528000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 96%|█████████▋| 108/112 [00:40<00:01,  2.74it/s]W0314 15:15:07.956000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:07.961000 140023092180800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 109/112 [00:40<00:01,  2.74it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.73it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.74it/s]100%|██████████| 112/112 [00:41<00:00,  2.73it/s]100%|██████████| 112/112 [00:41<00:00,  2.69it/s]
I0314 15:15:15.188446 161609 finetune.py:45] layer 24_down initial loss 0.0010115534532815218
W0314 15:15:15.188941 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:15:15.420588 165588 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.0011480968678370118 old loss 0.0011518163373693824 BETTER
W0314 15:15:15.581000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.581000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.581000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.581000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.582000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.582000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.582000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.623000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.623000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.623000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.623000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.623000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.801000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.801000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.801000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.802000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:15.802000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.143000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.143000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.143000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.143000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.143000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.143000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.143000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.174000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.174000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.174000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.174000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.175000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.245000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.245000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.246000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.246000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:16.246000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.224000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.238000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.246000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.246000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.700000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.701000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.701000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.701000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.701000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.701000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.701000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.730000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.730000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.730000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.730000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:17.730000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.032000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.032000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.032000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.032000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.032000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.032000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.032000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.032000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.330000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.330000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.330000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.330000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.330000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.761000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:15:18.766000 139649275086656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 15:15:26.103667 162931 finetune.py:45] layer 25_down initial loss 0.0011650623055174947
W0314 15:15:26.104083 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:15:26.244812 164243 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.0009336184011772275 old loss 0.0009360117255710065 BETTER
W0314 15:15:27.509459 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_gate proxy err 0.022943343967199326 tr(WHW.T) 5971.2919921875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:40,  1.10it/s]  2%|▏         | 2/112 [00:01<01:06,  1.66it/s]  3%|▎         | 3/112 [00:01<00:55,  1.98it/s]  4%|▎         | 4/112 [00:02<00:49,  2.18it/s]  4%|▍         | 5/112 [00:02<00:46,  2.31it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s]  6%|▋         | 7/112 [00:03<00:42,  2.46it/s]  7%|▋         | 8/112 [00:03<00:41,  2.51it/s]  8%|▊         | 9/112 [00:03<00:40,  2.53it/s]  9%|▉         | 10/112 [00:04<00:39,  2.56it/s] 10%|▉         | 11/112 [00:04<00:39,  2.57it/s] 11%|█         | 12/112 [00:05<00:38,  2.58it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.59it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.59it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.59it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.58it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.59it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.55it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.57it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.59it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.59it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.62it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.61it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.62it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.62it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.62it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.61it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.61it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.60it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.56it/s] 30%|███       | 34/112 [00:13<00:30,  2.57it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.58it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.59it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.60it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.60it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.61it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.61it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.61it/s]I0314 15:15:47.624826 165588 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.0011449495796114206 old loss 0.0011480968678370118 BETTER
I0314 15:15:47.795557 161609 finetune.py:68] layer 24_down @ epoch 0 new loss 0.0010099528590217233 old loss 0.0010115534532815218 BETTER
 38%|███▊      | 42/112 [00:16<00:26,  2.61it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.61it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.61it/s]W0314 15:15:48.699458 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 40%|████      | 45/112 [00:17<00:25,  2.60it/s] 41%|████      | 46/112 [00:18<00:25,  2.56it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.58it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.59it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.61it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.63it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.64it/s]27_gate proxy err 0.020677609369158745 tr(WHW.T) 7235.84912109375
  0%|          | 0/112 [00:00<?, ?it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.64it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.66it/s] 48%|████▊     | 54/112 [00:21<00:21,  2.65it/s]  1%|          | 1/112 [00:00<01:33,  1.18it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.65it/s]  2%|▏         | 2/112 [00:01<01:02,  1.76it/s] 50%|█████     | 56/112 [00:22<00:21,  2.64it/s]  3%|▎         | 3/112 [00:01<00:52,  2.08it/s] 51%|█████     | 57/112 [00:22<00:20,  2.64it/s]  4%|▎         | 4/112 [00:01<00:47,  2.28it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.64it/s]  4%|▍         | 5/112 [00:02<00:44,  2.41it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.64it/s]  5%|▌         | 6/112 [00:02<00:42,  2.51it/s]  6%|▋         | 7/112 [00:03<00:40,  2.57it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.59it/s]  7%|▋         | 8/112 [00:03<00:39,  2.65it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.60it/s]  8%|▊         | 9/112 [00:03<00:38,  2.68it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.61it/s]  9%|▉         | 10/112 [00:04<00:37,  2.71it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s] 10%|▉         | 11/112 [00:04<00:36,  2.74it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.63it/s] 11%|█         | 12/112 [00:04<00:36,  2.76it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.63it/s] 12%|█▏        | 13/112 [00:05<00:35,  2.77it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.64it/s]I0314 15:15:57.174893 162931 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0011632096720859408 old loss 0.0011650623055174947 BETTER
 12%|█▎        | 14/112 [00:05<00:35,  2.76it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.64it/s] 13%|█▎        | 15/112 [00:05<00:35,  2.76it/s] 61%|██████    | 68/112 [00:26<00:16,  2.64it/s] 14%|█▍        | 16/112 [00:06<00:34,  2.76it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.64it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.77it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.65it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.75it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.64it/s] 17%|█▋        | 19/112 [00:07<00:33,  2.76it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.65it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.76it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.65it/s] 19%|█▉        | 21/112 [00:08<00:32,  2.77it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.59it/s] 20%|█▉        | 22/112 [00:08<00:32,  2.78it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s] 21%|██        | 23/112 [00:08<00:32,  2.73it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.62it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.75it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.63it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.77it/s] 23%|██▎       | 26/112 [00:09<00:30,  2.78it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.63it/s] 24%|██▍       | 27/112 [00:10<00:30,  2.78it/s] 71%|███████   | 79/112 [00:30<00:12,  2.63it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.80it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.63it/s] 26%|██▌       | 29/112 [00:10<00:29,  2.80it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.64it/s] 27%|██▋       | 30/112 [00:11<00:29,  2.80it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.64it/s] 28%|██▊       | 31/112 [00:11<00:28,  2.81it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.63it/s] 29%|██▊       | 32/112 [00:12<00:28,  2.79it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.63it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.79it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.63it/s] 30%|███       | 34/112 [00:12<00:27,  2.79it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.64it/s] 31%|███▏      | 35/112 [00:13<00:27,  2.78it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.63it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.80it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.59it/s] 33%|███▎      | 37/112 [00:13<00:26,  2.79it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.60it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.72it/s] 80%|████████  | 90/112 [00:34<00:08,  2.61it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.75it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.62it/s] 36%|███▌      | 40/112 [00:14<00:26,  2.76it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.64it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.76it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.64it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.77it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.64it/s] 38%|███▊      | 43/112 [00:16<00:24,  2.78it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.64it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.77it/s] 40%|████      | 45/112 [00:16<00:24,  2.76it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.64it/s] 41%|████      | 46/112 [00:17<00:23,  2.77it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.65it/s] 42%|████▏     | 47/112 [00:17<00:23,  2.79it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.64it/s] 43%|████▎     | 48/112 [00:17<00:22,  2.80it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.64it/s] 44%|████▍     | 49/112 [00:18<00:22,  2.80it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.63it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.81it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.63it/s] 46%|████▌     | 51/112 [00:18<00:21,  2.79it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.63it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.79it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.59it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.79it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.60it/s] 48%|████▊     | 54/112 [00:19<00:20,  2.80it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.61it/s] 49%|████▉     | 55/112 [00:20<00:20,  2.74it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.62it/s] 50%|█████     | 56/112 [00:20<00:20,  2.76it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.62it/s] 51%|█████     | 57/112 [00:21<00:19,  2.77it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.62it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.79it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.63it/s] 53%|█████▎    | 59/112 [00:21<00:18,  2.80it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.63it/s] 54%|█████▎    | 60/112 [00:22<00:18,  2.79it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.63it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.75it/s]100%|██████████| 112/112 [00:43<00:00,  2.63it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]
 55%|█████▌    | 62/112 [00:22<00:18,  2.71it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.69it/s] 57%|█████▋    | 64/112 [00:23<00:18,  2.67it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.62it/s] 59%|█████▉    | 66/112 [00:24<00:17,  2.61it/s] 60%|█████▉    | 67/112 [00:24<00:17,  2.54it/s] 61%|██████    | 68/112 [00:25<00:17,  2.57it/s] 62%|██████▏   | 69/112 [00:25<00:16,  2.56it/s] 62%|██████▎   | 70/112 [00:26<00:16,  2.57it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.58it/s] 64%|██████▍   | 72/112 [00:26<00:15,  2.56it/s] 65%|██████▌   | 73/112 [00:27<00:15,  2.56it/s] 66%|██████▌   | 74/112 [00:27<00:14,  2.55it/s] 67%|██████▋   | 75/112 [00:27<00:14,  2.53it/s] 68%|██████▊   | 76/112 [00:28<00:14,  2.53it/s] 69%|██████▉   | 77/112 [00:28<00:13,  2.54it/s] 70%|██████▉   | 78/112 [00:29<00:13,  2.55it/s]W0314 15:16:20.862000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.862000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.862000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.862000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.862000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.862000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.863000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.909000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.909000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.909000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.909000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:20.909000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
I0314 15:16:21.075235 161609 finetune.py:68] layer 24_down @ epoch 1 new loss 0.0010095475008711219 old loss 0.0010099528590217233 BETTER
W0314 15:16:21.086000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.086000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.086000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.086000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.086000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:29<00:12,  2.56it/s]W0314 15:16:21.421000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.422000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.422000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.422000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.422000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.422000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.422000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.454000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.454000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.454000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.454000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.455000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.525000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.525000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.526000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.526000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:21.526000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 71%|███████▏  | 80/112 [00:29<00:12,  2.57it/s] 72%|███████▏  | 81/112 [00:30<00:12,  2.58it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.59it/s]W0314 15:16:22.502000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:22.516000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:22.524000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:22.524000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 83/112 [00:31<00:11,  2.60it/s]W0314 15:16:22.980000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:22.980000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:22.980000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:22.980000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:16:22.980000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:22.980000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:22.980000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.009000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.009000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.009000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.009000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.009000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 84/112 [00:31<00:10,  2.60it/s]W0314 15:16:23.311000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.311000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.311000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.311000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.311000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.311000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.311000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.311000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 76%|███████▌  | 85/112 [00:31<00:10,  2.60it/s]W0314 15:16:23.604000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.604000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.605000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.605000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:23.605000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:32<00:10,  2.60it/s]W0314 15:16:24.032000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:24.037000 140679862908736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:32<00:09,  2.57it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.57it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.60it/s] 80%|████████  | 90/112 [00:33<00:08,  2.61it/s] 81%|████████▏ | 91/112 [00:34<00:08,  2.61it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.61it/s] 83%|████████▎ | 93/112 [00:34<00:07,  2.61it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.61it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.56it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.57it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.58it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.60it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.60it/s]I0314 15:16:29.083432 162931 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0011626430787146091 old loss 0.0011632096720859408 BETTER
 89%|████████▉ | 100/112 [00:37<00:04,  2.61it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.62it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.63it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.63it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.62it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.61it/s]I0314 15:16:31.489067 164243 finetune.py:45] layer 26_down initial loss 0.0013855560682713985
W0314 15:16:31.489492 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 95%|█████████▍| 106/112 [00:39<00:02,  2.56it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.58it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.60it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.61it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.62it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.62it/s]100%|██████████| 112/112 [00:42<00:00,  2.62it/s]100%|██████████| 112/112 [00:42<00:00,  2.65it/s]
W0314 15:16:40.047000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.048000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.048000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.048000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.048000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.048000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.048000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.092000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.092000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.092000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.092000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.092000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.271000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.271000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.271000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.271000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.271000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.596000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.596000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.596000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.596000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.596000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.596000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.596000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.635000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.636000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.636000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.636000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.636000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.706000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.707000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.707000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.707000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:40.707000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:41.691000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:41.706000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:41.714000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:41.714000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.165000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.165000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.165000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.165000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.166000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.166000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.166000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.198000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.198000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.198000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.198000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.198000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.503000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.503000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.503000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.503000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.503000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.503000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.504000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.504000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.805000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.806000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.806000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.806000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:16:42.806000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:16:43.248000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:16:43.254000 140427357148992 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 15:16:50.495013 165588 finetune.py:45] layer 27_down initial loss 0.0016786210471764207
W0314 15:16:50.495409 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:16:54.643254 161609 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0010093423770740628 old loss 0.0010095475008711219 BETTER
I0314 15:17:01.755129 162931 finetune.py:68] layer 25_down @ epoch 2 new loss 0.001162352622486651 old loss 0.0011626430787146091 BETTER
I0314 15:17:03.028278 164243 finetune.py:68] layer 26_down @ epoch 0 new loss 0.0013832617551088333 old loss 0.0013855560682713985 BETTER
I0314 15:17:20.208579 165588 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0016754366224631667 old loss 0.0016786210471764207 BETTER
I0314 15:17:27.691079 161609 finetune.py:68] layer 24_down @ epoch 3 new loss 0.0010091555304825306 old loss 0.0010093423770740628 BETTER
I0314 15:17:33.598859 162931 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0011621465673670173 old loss 0.001162352622486651 BETTER
I0314 15:17:34.942147 164243 finetune.py:68] layer 26_down @ epoch 1 new loss 0.0013825786300003529 old loss 0.0013832617551088333 BETTER
I0314 15:17:50.470790 165588 finetune.py:68] layer 27_down @ epoch 1 new loss 0.001674542436376214 old loss 0.0016754366224631667 BETTER
I0314 15:18:01.011290 161609 finetune.py:68] layer 24_down @ epoch 4 new loss 0.0010090082651004195 old loss 0.0010091555304825306 BETTER
W0314 15:18:01.797248 161609 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

24_down proxy err 0.04683167114853859 tr(WHW.T) 35.231956481933594
I0314 15:18:05.300277 162931 finetune.py:68] layer 25_down @ epoch 4 new loss 0.0011619635624811053 old loss 0.0011621465673670173 BETTER
I0314 15:18:06.858096 164243 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0013822445180267096 old loss 0.0013825786300003529 BETTER
W0314 15:18:06.946595 162931 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

25_down proxy err 0.04719856008887291 tr(WHW.T) 38.22883605957031
I0314 15:18:20.895945 165588 finetune.py:68] layer 27_down @ epoch 2 new loss 0.0016740584978833795 old loss 0.001674542436376214 BETTER
I0314 15:18:38.234607 164243 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0013820070307701826 old loss 0.0013822445180267096 BETTER
I0314 15:18:51.478112 165588 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0016737357946112752 old loss 0.0016740584978833795 BETTER
I0314 15:19:09.964358 164243 finetune.py:68] layer 26_down @ epoch 4 new loss 0.00138179212808609 old loss 0.0013820070307701826 BETTER
W0314 15:19:10.720531 164243 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

26_down proxy err 0.0478275790810585 tr(WHW.T) 43.5615348815918
I0314 15:19:18.913060 9966 quantize_finetune_llama.py:186] computed original embedding for layer 28 in 66.57708930969238s
I0314 15:19:19.313840 9966 quantize_finetune_llama.py:159] layer 29 gpu 1
I0314 15:19:21.442608 186346 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 15:19:21.442725 186346 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 15:19:21.442787 186346 utils.py:162] NumExpr defaulting to 16 threads.
I0314 15:19:21.629100 186346 config.py:58] PyTorch version 2.4.0 available.
I0314 15:19:22.228511 165588 finetune.py:68] layer 27_down @ epoch 4 new loss 0.0016734849195927382 old loss 0.0016737357946112752 BETTER
W0314 15:19:23.163985 165588 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

27_down proxy err 0.04004055634140968 tr(WHW.T) 61.85791778564453
I0314 15:19:23.796597 186346 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 15:19:24.157575 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.34s/it]  6%|▋         | 2/32 [00:01<00:22,  1.36it/s]  9%|▉         | 3/32 [00:01<00:15,  1.83it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.88it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.95it/s] 31%|███▏      | 10/32 [00:04<00:07,  3.00it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.06it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.00it/s] 41%|████      | 13/32 [00:05<00:06,  3.03it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.05it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.07it/s] 50%|█████     | 16/32 [00:06<00:05,  3.08it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.08it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.08it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.09it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.06it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.08it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.08it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.10it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.11it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.10it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.11it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.08it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.08it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.12it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.11it/s]100%|██████████| 32/32 [00:11<00:00,  3.11it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
W0314 15:19:38.806000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.806000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.806000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.806000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.807000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.807000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.807000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.832000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.832000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.832000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.832000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:19:38.832000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.114000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.114000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.115000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.115000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.115000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.967000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.967000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.968000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.968000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.968000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.968000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.968000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.986000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.986000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.987000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.987000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:19:39.987000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:19:40.186000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:19:40.186000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:19:40.186000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:19:40.186000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:19:40.186000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.270000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.270000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.270000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.270000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.271000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.271000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.271000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.289000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.289000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.289000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.289000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:19:41.289000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:19:42.127000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:19:42.127000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:19:42.128000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:19:42.128000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:19:42.128000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 15:19:48.935529 186346 finetune.py:45] layer 28_v initial loss 0.0006410616333596408
W0314 15:19:48.935940 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:20:20.814069 9966 quantize_finetune_llama.py:186] computed original embedding for layer 29 in 61.07191729545593s
I0314 15:20:21.193196 9966 quantize_finetune_llama.py:159] layer 30 gpu 2
I0314 15:20:23.275538 187649 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 15:20:23.275668 187649 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 15:20:23.275730 187649 utils.py:162] NumExpr defaulting to 16 threads.
I0314 15:20:23.479052 187649 config.py:58] PyTorch version 2.4.0 available.
I0314 15:20:25.057234 186346 finetune.py:68] layer 28_v @ epoch 0 new loss 0.00020501937251538038 old loss 0.0006410616333596408 BETTER
I0314 15:20:25.696637 187649 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 15:20:26.087886 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.59s/it]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s]  9%|▉         | 3/32 [00:02<00:18,  1.58it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.19it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.87it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.88it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.88it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
W0314 15:20:41.863000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.863000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.863000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.863000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.863000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.863000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.863000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.889000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.889000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.889000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.889000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:20:41.889000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:20:42.177000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:20:42.177000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:20:42.177000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:20:42.177000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:20:42.177000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.028000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.028000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.028000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.028000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.028000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.028000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.029000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.046000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.046000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.046000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.046000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.046000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.252000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.252000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.252000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.253000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:20:43.253000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.371000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.371000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.371000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.371000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.371000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.371000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.372000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.389000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.389000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.389000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.389000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:20:44.389000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:20:45.254000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:20:45.254000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:20:45.254000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:20:45.254000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:20:45.255000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 15:20:51.418045 187649 finetune.py:45] layer 29_v initial loss 0.0009423185838386416
W0314 15:20:51.418387 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:21:02.343027 186346 finetune.py:68] layer 28_v @ epoch 1 new loss 0.0001871348504209891 old loss 0.00020501937251538038 BETTER
I0314 15:21:21.652022 9966 quantize_finetune_llama.py:186] computed original embedding for layer 30 in 59.96938157081604s
I0314 15:21:22.008952 9966 quantize_finetune_llama.py:159] layer 31 gpu 3
I0314 15:21:24.123322 188981 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 15:21:24.123490 188981 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 15:21:24.123576 188981 utils.py:162] NumExpr defaulting to 16 threads.
I0314 15:21:24.321029 188981 config.py:58] PyTorch version 2.4.0 available.
I0314 15:21:25.741472 187649 finetune.py:68] layer 29_v @ epoch 0 new loss 0.0002547339245211333 old loss 0.0009423185838386416 BETTER
I0314 15:21:26.790153 188981 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 15:21:27.122827 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:48,  1.58s/it]  6%|▋         | 2/32 [00:01<00:25,  1.17it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.61it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s]I0314 15:21:40.226732 186346 finetune.py:68] layer 28_v @ epoch 2 new loss 0.00017992068023886532 old loss 0.0001871348504209891 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
W0314 15:21:43.072000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.072000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.072000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.072000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.072000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.072000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.072000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.100000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.100000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.100000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.100000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.100000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.395000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.395000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.396000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.396000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:21:43.396000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.275000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.275000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.275000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.275000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.275000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.275000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.275000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.293000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.293000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.293000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.294000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.294000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.499000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.499000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.499000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.499000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:21:44.499000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.646000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.646000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.647000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.647000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.647000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.647000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.647000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.665000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.665000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.665000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.665000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:21:45.665000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:21:46.551000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:21:46.552000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:21:46.552000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:21:46.552000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:21:46.552000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 15:21:54.616993 188981 finetune.py:45] layer 30_v initial loss 0.0011990757193416357
W0314 15:21:54.617328 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:22:02.218880 187649 finetune.py:68] layer 29_v @ epoch 1 new loss 0.00023448355204891413 old loss 0.0002547339245211333 BETTER
I0314 15:22:18.489470 186346 finetune.py:68] layer 28_v @ epoch 3 new loss 0.00017554615624248981 old loss 0.00017992068023886532 BETTER
I0314 15:22:25.823018 9966 quantize_finetune_llama.py:186] computed original embedding for layer 31 in 63.34830093383789s
I0314 15:22:28.418981 190381 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 15:22:28.419140 190381 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 15:22:28.419219 190381 utils.py:162] NumExpr defaulting to 16 threads.
I0314 15:22:28.734441 190381 config.py:58] PyTorch version 2.4.0 available.
I0314 15:22:29.694105 188981 finetune.py:68] layer 30_v @ epoch 0 new loss 0.0005579220596700907 old loss 0.0011990757193416357 BETTER
I0314 15:22:31.285854 190381 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0314 15:22:31.776983 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:53,  1.74s/it]  6%|▋         | 2/32 [00:02<00:27,  1.10it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.25it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.77it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.91it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.94it/s]I0314 15:22:38.446970 187649 finetune.py:68] layer 29_v @ epoch 2 new loss 0.00022675147920381278 old loss 0.00023448355204891413 BETTER
 38%|███▊      | 12/32 [00:05<00:06,  2.97it/s] 41%|████      | 13/32 [00:05<00:06,  3.00it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.02it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.05it/s] 50%|█████     | 16/32 [00:06<00:05,  3.05it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.06it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.08it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.07it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.09it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.09it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.07it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.08it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.09it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.09it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.09it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.09it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.08it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.09it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.09it/s]100%|██████████| 32/32 [00:11<00:00,  3.07it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
W0314 15:22:47.618000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.618000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.619000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.619000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.619000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.619000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.619000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.646000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.646000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.646000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.646000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.646000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.940000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.940000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.940000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.940000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:22:47.941000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.806000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.807000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.807000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.807000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.807000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.807000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.807000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.825000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.825000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.825000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.825000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:22:48.825000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:22:49.028000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:22:49.028000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:22:49.028000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:22:49.028000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:22:49.028000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.117000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.117000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.117000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.117000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.117000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.118000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.118000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.135000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.135000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.135000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.135000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.135000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.980000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.981000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.981000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.981000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:22:50.981000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0314 15:22:56.925760 186346 finetune.py:68] layer 28_v @ epoch 4 new loss 0.00017226769705303013 old loss 0.00017554615624248981 BETTER
I0314 15:22:58.410953 190381 finetune.py:45] layer 31_v initial loss 0.0014309812104329467
W0314 15:22:58.411235 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0314 15:22:59.103734 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_v proxy err 0.05049825832247734 tr(WHW.T) 175.40756225585938
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s]I0314 15:23:05.232868 188981 finetune.py:68] layer 30_v @ epoch 1 new loss 0.0005260643665678799 old loss 0.0005579220596700907 BETTER
 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
I0314 15:23:14.663945 187649 finetune.py:68] layer 29_v @ epoch 3 new loss 0.00022144145623315126 old loss 0.00022675147920381278 BETTER
W0314 15:23:17.704000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.704000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.704000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.704000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.704000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.705000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.705000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.736000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.736000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.737000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.737000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.737000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.915000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.915000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.915000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.916000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:23:17.916000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.150000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.150000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.150000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.150000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.150000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.150000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.150000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.173000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.173000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.173000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.173000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.173000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.240000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.240000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.241000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.241000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:23:18.241000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.011000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.348000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.348000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.348000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.348000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.348000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.348000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.348000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.372000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.372000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.372000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.372000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.372000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.633000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.633000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.633000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.633000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:23:19.633000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:23:20.007000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 15:23:27.999937 186346 finetune.py:45] layer 28_q initial loss 0.00023936043726280332
W0314 15:23:28.000397 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:23:33.210579 190381 finetune.py:68] layer 31_v @ epoch 0 new loss 0.0008121130522340536 old loss 0.0014309812104329467 BETTER
I0314 15:23:41.934613 188981 finetune.py:68] layer 30_v @ epoch 2 new loss 0.0005126348114572465 old loss 0.0005260643665678799 BETTER
I0314 15:23:50.663650 187649 finetune.py:68] layer 29_v @ epoch 4 new loss 0.00021809368627145886 old loss 0.00022144145623315126 BETTER
W0314 15:23:52.516705 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

29_v proxy err 0.042129915207624435 tr(WHW.T) 249.68582153320312
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.04it/s]  6%|▋         | 2/32 [00:01<00:18,  1.62it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s]I0314 15:24:04.824782 186346 finetune.py:68] layer 28_q @ epoch 0 new loss 0.0002320992643944919 old loss 0.00023936043726280332 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.65it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0314 15:24:07.954684 190381 finetune.py:68] layer 31_v @ epoch 1 new loss 0.0007672299398109317 old loss 0.0008121130522340536 BETTER
W0314 15:24:11.725000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.726000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.726000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.726000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.726000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.726000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.726000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.759000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.759000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.759000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.759000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.759000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.938000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.938000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.938000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.938000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:24:11.938000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.167000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.167000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.167000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.167000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.168000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.168000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.168000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.192000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.192000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.192000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.192000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.192000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.267000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.267000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.267000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.267000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:24:12.267000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.017000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.342000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.342000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.342000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.342000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.342000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.342000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.342000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.366000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.366000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.366000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.366000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.366000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.630000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.630000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.630000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.630000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.630000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:24:13.993000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 15:24:18.532522 188981 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0005034638452343643 old loss 0.0005126348114572465 BETTER
I0314 15:24:21.492324 187649 finetune.py:45] layer 29_q initial loss 0.0004019110929220915
W0314 15:24:21.492628 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:24:43.072273 186346 finetune.py:68] layer 28_q @ epoch 1 new loss 0.00022757500119041651 old loss 0.0002320992643944919 BETTER
I0314 15:24:43.347116 190381 finetune.py:68] layer 31_v @ epoch 2 new loss 0.0007317144772969186 old loss 0.0007672299398109317 BETTER
I0314 15:24:54.669664 188981 finetune.py:68] layer 30_v @ epoch 4 new loss 0.0004995261551812291 old loss 0.0005034638452343643 BETTER
I0314 15:24:56.282061 187649 finetune.py:68] layer 29_q @ epoch 0 new loss 0.0003710658638738096 old loss 0.0004019110929220915 BETTER
W0314 15:24:56.577425 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_v proxy err 0.05229121819138527 tr(WHW.T) 252.81201171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:14,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.15it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
W0314 15:25:15.753000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.753000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.753000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.753000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.753000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.753000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.753000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.785000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.785000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.785000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.785000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.785000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.961000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.961000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.961000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.961000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:25:15.961000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.194000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.194000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.194000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.195000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.195000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.195000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.195000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.216000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.217000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.217000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.217000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.217000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.284000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.284000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.284000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.285000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:25:16.285000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.032000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.353000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.353000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.353000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.353000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.353000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.353000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.353000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.375000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.375000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.375000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.375000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.375000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.636000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.636000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.636000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.636000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.636000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:25:17.996000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 15:25:18.698099 190381 finetune.py:76] layer 31_v @ epoch 3 new loss 0.0007753186509944499 old loss 0.0007317144772969186 WORSE
I0314 15:25:21.571628 186346 finetune.py:68] layer 28_q @ epoch 2 new loss 0.00022429604723583907 old loss 0.00022757500119041651 BETTER
I0314 15:25:25.465017 188981 finetune.py:45] layer 30_q initial loss 0.0006052803946658969
W0314 15:25:25.465262 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:25:32.493195 187649 finetune.py:68] layer 29_q @ epoch 1 new loss 0.0003595320740714669 old loss 0.0003710658638738096 BETTER
I0314 15:25:53.366572 190381 finetune.py:68] layer 31_v @ epoch 4 new loss 0.0007246712339110672 old loss 0.0007317144772969186 BETTER
W0314 15:25:55.485193 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

31_v proxy err 0.021562164649367332 tr(WHW.T) 365.68487548828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:19,  1.58it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s]I0314 15:25:59.476352 186346 finetune.py:68] layer 28_q @ epoch 3 new loss 0.00022142083616927266 old loss 0.00022429604723583907 BETTER
 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s]I0314 15:26:00.402897 188981 finetune.py:68] layer 30_q @ epoch 0 new loss 0.0005864114500582218 old loss 0.0006052803946658969 BETTER
 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.75it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s]I0314 15:26:08.869402 187649 finetune.py:68] layer 29_q @ epoch 2 new loss 0.0003517644654493779 old loss 0.0003595320740714669 BETTER
100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
W0314 15:26:14.237000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.238000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.238000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.238000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.238000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.238000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.239000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.270000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.270000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.270000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.270000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.270000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.436000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.436000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.436000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.437000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.437000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.664000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.664000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.664000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.664000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.664000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.664000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.665000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.688000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.688000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.688000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.688000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.688000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.755000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.755000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.755000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.756000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:26:14.756000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.480000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.800000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.800000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.800000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.801000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.801000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.801000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.801000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.821000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.821000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.821000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.821000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:26:15.821000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:26:16.088000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:26:16.088000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:26:16.088000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:26:16.088000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:26:16.089000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:26:16.462000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0314 15:26:23.300044 190381 finetune.py:45] layer 31_q initial loss 0.0010870868572965264
W0314 15:26:23.300464 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:26:36.952892 188981 finetune.py:68] layer 30_q @ epoch 1 new loss 0.0005771023570559919 old loss 0.0005864114500582218 BETTER
I0314 15:26:37.563259 186346 finetune.py:68] layer 28_q @ epoch 4 new loss 0.00021902320440858603 old loss 0.00022142083616927266 BETTER
W0314 15:26:39.425403 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_q proxy err 0.007024297025054693 tr(WHW.T) 6763.98046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.27it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s]I0314 15:26:45.382215 187649 finetune.py:68] layer 29_q @ epoch 3 new loss 0.0003455231781117618 old loss 0.0003517644654493779 BETTER
 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
I0314 15:26:57.686457 190381 finetune.py:68] layer 31_q @ epoch 0 new loss 0.0009778935927897692 old loss 0.0010870868572965264 BETTER
I0314 15:27:00.176639 186346 finetune.py:45] layer 28_k initial loss 0.0002777123881969601
W0314 15:27:00.176971 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:27:13.613350 188981 finetune.py:68] layer 30_q @ epoch 2 new loss 0.0005698676104657352 old loss 0.0005771023570559919 BETTER
I0314 15:27:22.599714 187649 finetune.py:68] layer 29_q @ epoch 4 new loss 0.00033964638714678586 old loss 0.0003455231781117618 BETTER
W0314 15:27:24.641220 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

29_q proxy err 0.008447946049273014 tr(WHW.T) 6071.55224609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s]I0314 15:27:32.777164 190381 finetune.py:68] layer 31_q @ epoch 1 new loss 0.0009438623674213886 old loss 0.0009778935927897692 BETTER
 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s]I0314 15:27:37.302201 186346 finetune.py:68] layer 28_k @ epoch 0 new loss 0.00026685037300921977 old loss 0.0002777123881969601 BETTER
 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
I0314 15:27:46.486602 187649 finetune.py:45] layer 29_k initial loss 0.00043933949200436473
W0314 15:27:46.486975 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:27:50.470137 188981 finetune.py:68] layer 30_q @ epoch 3 new loss 0.0005666350480169058 old loss 0.0005698676104657352 BETTER
I0314 15:28:07.825718 190381 finetune.py:68] layer 31_q @ epoch 2 new loss 0.0009230159921571612 old loss 0.0009438623674213886 BETTER
I0314 15:28:15.318909 186346 finetune.py:68] layer 28_k @ epoch 1 new loss 0.00026444520335644484 old loss 0.00026685037300921977 BETTER
I0314 15:28:22.294922 187649 finetune.py:68] layer 29_k @ epoch 0 new loss 0.00041546334978193045 old loss 0.00043933949200436473 BETTER
I0314 15:28:26.882326 188981 finetune.py:68] layer 30_q @ epoch 4 new loss 0.0005618976429104805 old loss 0.0005666350480169058 BETTER
W0314 15:28:28.901044 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_q proxy err 0.005794048774987459 tr(WHW.T) 7057.20849609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
I0314 15:28:42.960068 190381 finetune.py:68] layer 31_q @ epoch 3 new loss 0.0009119275491684675 old loss 0.0009230159921571612 BETTER
I0314 15:28:50.778691 188981 finetune.py:45] layer 30_k initial loss 0.0006732527981512249
W0314 15:28:50.779076 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:28:54.054607 186346 finetune.py:68] layer 28_k @ epoch 2 new loss 0.0002623988257255405 old loss 0.00026444520335644484 BETTER
I0314 15:28:58.509539 187649 finetune.py:68] layer 29_k @ epoch 1 new loss 0.0004099518118891865 old loss 0.00041546334978193045 BETTER
I0314 15:29:18.232444 190381 finetune.py:68] layer 31_q @ epoch 4 new loss 0.00087238714331761 old loss 0.0009119275491684675 BETTER
W0314 15:29:19.879621 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

31_q proxy err 0.00394033407792449 tr(WHW.T) 9342.0673828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]  6%|▋         | 2/32 [00:01<00:18,  1.62it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s]I0314 15:29:25.762340 188981 finetune.py:68] layer 30_k @ epoch 0 new loss 0.000653357186820358 old loss 0.0006732527981512249 BETTER
 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.64it/s]I0314 15:29:33.080187 186346 finetune.py:68] layer 28_k @ epoch 3 new loss 0.00026086438447237015 old loss 0.0002623988257255405 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0314 15:29:35.927803 187649 finetune.py:68] layer 29_k @ epoch 2 new loss 0.0004052927833981812 old loss 0.0004099518118891865 BETTER
I0314 15:29:40.837622 190381 finetune.py:45] layer 31_k initial loss 0.001194435521028936
W0314 15:29:40.838097 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:30:02.278674 188981 finetune.py:68] layer 30_k @ epoch 1 new loss 0.0006469125510193408 old loss 0.000653357186820358 BETTER
I0314 15:30:11.688295 186346 finetune.py:68] layer 28_k @ epoch 4 new loss 0.0002594761026557535 old loss 0.00026086438447237015 BETTER
I0314 15:30:12.348546 187649 finetune.py:68] layer 29_k @ epoch 3 new loss 0.00040099030593410134 old loss 0.0004052927833981812 BETTER
W0314 15:30:13.409621 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_k proxy err 0.005857706069946289 tr(WHW.T) 4351.22802734375
  0%|          | 0/32 [00:00<?, ?it/s]I0314 15:30:14.637953 190381 finetune.py:68] layer 31_k @ epoch 0 new loss 0.001136368722654879 old loss 0.001194435521028936 BETTER
  3%|▎         | 1/32 [00:00<00:23,  1.30it/s]  6%|▋         | 2/32 [00:01<00:15,  1.91it/s]  9%|▉         | 3/32 [00:01<00:13,  2.21it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.40it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.61it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.73it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.75it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
I0314 15:30:33.850855 186346 finetune.py:45] layer 28_o initial loss 0.0004974391194991767
W0314 15:30:33.851187 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:30:38.454232 188981 finetune.py:68] layer 30_k @ epoch 2 new loss 0.0006456882692873478 old loss 0.0006469125510193408 BETTER
I0314 15:30:48.569406 187649 finetune.py:68] layer 29_k @ epoch 4 new loss 0.0003969326207879931 old loss 0.00040099030593410134 BETTER
I0314 15:30:49.406202 190381 finetune.py:68] layer 31_k @ epoch 1 new loss 0.0011135662207379937 old loss 0.001136368722654879 BETTER
W0314 15:30:50.097982 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

29_k proxy err 0.0063330200500786304 tr(WHW.T) 4786.84619140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.27it/s]  6%|▋         | 2/32 [00:01<00:16,  1.82it/s]  9%|▉         | 3/32 [00:01<00:13,  2.12it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.50it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.50it/s] 41%|████      | 13/32 [00:05<00:07,  2.51it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.52it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.52it/s] 50%|█████     | 16/32 [00:06<00:06,  2.55it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0314 15:31:10.760699 186346 finetune.py:68] layer 28_o @ epoch 0 new loss 0.00043902124161832035 old loss 0.0004974391194991767 BETTER
I0314 15:31:11.049168 187649 finetune.py:45] layer 29_o initial loss 0.0006290962337516248
W0314 15:31:11.049400 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:31:14.631095 188981 finetune.py:68] layer 30_k @ epoch 3 new loss 0.0006412166985683143 old loss 0.0006456882692873478 BETTER
I0314 15:31:24.408123 190381 finetune.py:68] layer 31_k @ epoch 2 new loss 0.001096196472644806 old loss 0.0011135662207379937 BETTER
I0314 15:31:46.095930 187649 finetune.py:68] layer 29_o @ epoch 0 new loss 0.0005631655221804976 old loss 0.0006290962337516248 BETTER
I0314 15:31:48.617714 186346 finetune.py:68] layer 28_o @ epoch 1 new loss 0.00043327914318069816 old loss 0.00043902124161832035 BETTER
I0314 15:31:50.676929 188981 finetune.py:68] layer 30_k @ epoch 4 new loss 0.000639388628769666 old loss 0.0006412166985683143 BETTER
W0314 15:31:52.379783 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_k proxy err 0.00505226431414485 tr(WHW.T) 4086.073974609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.51it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.51it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.57it/s]I0314 15:31:59.695990 190381 finetune.py:68] layer 31_k @ epoch 3 new loss 0.0010727477492764592 old loss 0.001096196472644806 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.55it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.54it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.52it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.50it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.52it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.51it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.50it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.52it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.52it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.53it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.52it/s]100%|██████████| 32/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
I0314 15:32:13.818432 188981 finetune.py:45] layer 30_o initial loss 0.001092153717763722
W0314 15:32:13.818701 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:32:21.925616 187649 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0005564351449720562 old loss 0.0005631655221804976 BETTER
I0314 15:32:26.539582 186346 finetune.py:68] layer 28_o @ epoch 2 new loss 0.0004292869125492871 old loss 0.00043327914318069816 BETTER
I0314 15:32:34.610864 190381 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0010491616558283567 old loss 0.0010727477492764592 BETTER
W0314 15:32:36.103448 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

31_k proxy err 0.004408981651067734 tr(WHW.T) 4121.5771484375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:23,  1.31it/s]  6%|▋         | 2/32 [00:01<00:15,  1.92it/s]  9%|▉         | 3/32 [00:01<00:12,  2.24it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.45it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.56it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.62it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.78it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 41%|████      | 13/32 [00:04<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.86it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.85it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.86it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.88it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.86it/s]I0314 15:32:48.776328 188981 finetune.py:68] layer 30_o @ epoch 0 new loss 0.001003333250992 old loss 0.001092153717763722 BETTER
100%|██████████| 32/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
I0314 15:32:55.467118 190381 finetune.py:45] layer 31_o initial loss 0.0017304746434092522
W0314 15:32:55.467457 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:32:57.652316 187649 finetune.py:68] layer 29_o @ epoch 2 new loss 0.0005516420351341367 old loss 0.0005564351449720562 BETTER
I0314 15:33:04.171166 186346 finetune.py:68] layer 28_o @ epoch 3 new loss 0.0004263246082700789 old loss 0.0004292869125492871 BETTER
I0314 15:33:24.381740 188981 finetune.py:68] layer 30_o @ epoch 1 new loss 0.000994401634670794 old loss 0.001003333250992 BETTER
I0314 15:33:29.344976 190381 finetune.py:68] layer 31_o @ epoch 0 new loss 0.0015673128655180335 old loss 0.0017304746434092522 BETTER
I0314 15:33:33.277445 187649 finetune.py:68] layer 29_o @ epoch 3 new loss 0.0005473202909342945 old loss 0.0005516420351341367 BETTER
I0314 15:33:41.575274 186346 finetune.py:68] layer 28_o @ epoch 4 new loss 0.0004238667315803468 old loss 0.0004263246082700789 BETTER
W0314 15:33:43.048676 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_o proxy err 0.028875108808279037 tr(WHW.T) 24.59929656982422
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it]  9%|▉         | 3/32 [00:04<00:45,  1.56s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.52s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it] 22%|██▏       | 7/32 [00:10<00:36,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it]I0314 15:34:00.048517 188981 finetune.py:68] layer 30_o @ epoch 2 new loss 0.0009879602584987879 old loss 0.000994401634670794 BETTER
 34%|███▍      | 11/32 [00:16<00:30,  1.47s/it] 38%|███▊      | 12/32 [00:17<00:29,  1.47s/it] 41%|████      | 13/32 [00:19<00:27,  1.47s/it]I0314 15:34:04.235605 190381 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0015257474733516574 old loss 0.0015673128655180335 BETTER
 44%|████▍     | 14/32 [00:20<00:26,  1.46s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it]I0314 15:34:08.781301 187649 finetune.py:68] layer 29_o @ epoch 4 new loss 0.0005439998349174857 old loss 0.0005473202909342945 BETTER
 53%|█████▎    | 17/32 [00:25<00:21,  1.47s/it]W0314 15:34:10.232035 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it]29_o proxy err 0.018154200166463852 tr(WHW.T) 36.74747085571289
  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.50s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.48s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it]I0314 15:34:35.635234 188981 finetune.py:68] layer 30_o @ epoch 3 new loss 0.000983681995421648 old loss 0.0009879602584987879 BETTER
 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.50s/it]I0314 15:34:39.300022 190381 finetune.py:68] layer 31_o @ epoch 2 new loss 0.0015018811682239175 old loss 0.0015257474733516574 BETTER
I0314 15:34:39.696752 186346 finetune.py:45] layer 28_up initial loss 0.0010631337063387036
W0314 15:34:39.696969 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:28<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0314 15:35:07.788504 187649 finetune.py:45] layer 29_up initial loss 0.0014567135367542505
W0314 15:35:07.788960 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:35:11.631418 188981 finetune.py:68] layer 30_o @ epoch 4 new loss 0.0009795301593840122 old loss 0.000983681995421648 BETTER
W0314 15:35:13.303847 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 15:35:14.291277 190381 finetune.py:68] layer 31_o @ epoch 3 new loss 0.0014876957284286618 old loss 0.0015018811682239175 BETTER
30_o proxy err 0.016338393092155457 tr(WHW.T) 84.25099182128906
  0%|          | 0/32 [00:00<?, ?it/s]I0314 15:35:15.454305 186346 finetune.py:68] layer 28_up @ epoch 0 new loss 0.001047169091179967 old loss 0.0010631337063387036 BETTER
  3%|▎         | 1/32 [00:02<01:02,  2.01s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it]I0314 15:35:42.209361 187649 finetune.py:68] layer 29_up @ epoch 0 new loss 0.0014308388344943523 old loss 0.0014567135367542505 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it]I0314 15:35:49.286501 190381 finetune.py:76] layer 31_o @ epoch 4 new loss 0.0014903431292623281 old loss 0.0014876957284286618 WORSE
 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it]W0314 15:35:50.436067 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it]31_o proxy err 0.01087328139692545 tr(WHW.T) 182.37371826171875
  0%|          | 0/32 [00:00<?, ?it/s]I0314 15:35:52.068451 186346 finetune.py:68] layer 28_up @ epoch 1 new loss 0.0010382020846009254 old loss 0.001047169091179967 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it]  3%|▎         | 1/32 [00:02<01:02,  2.02s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it]  9%|▉         | 3/32 [00:05<00:46,  1.61s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it]I0314 15:36:11.070142 188981 finetune.py:45] layer 30_up initial loss 0.0028409233782440424
W0314 15:36:11.070402 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it]I0314 15:36:16.973092 187649 finetune.py:68] layer 29_up @ epoch 1 new loss 0.0014161220751702785 old loss 0.0014308388344943523 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it]I0314 15:36:28.813872 186346 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0010310484794899821 old loss 0.0010382020846009254 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
I0314 15:36:45.113366 188981 finetune.py:68] layer 30_up @ epoch 0 new loss 0.002773580839857459 old loss 0.0028409233782440424 BETTER
I0314 15:36:48.630348 190381 finetune.py:45] layer 31_up initial loss 0.009584621526300907
W0314 15:36:48.630777 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:36:51.810722 187649 finetune.py:68] layer 29_up @ epoch 2 new loss 0.0014038289664313197 old loss 0.0014161220751702785 BETTER
I0314 15:37:05.567418 186346 finetune.py:68] layer 28_up @ epoch 3 new loss 0.0010249646147713065 old loss 0.0010310484794899821 BETTER
I0314 15:37:19.562374 188981 finetune.py:68] layer 30_up @ epoch 1 new loss 0.0027321225497871637 old loss 0.002773580839857459 BETTER
I0314 15:37:21.299223 190381 finetune.py:68] layer 31_up @ epoch 0 new loss 0.00913695152848959 old loss 0.009584621526300907 BETTER
I0314 15:37:26.577193 187649 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0013940500793978572 old loss 0.0014038289664313197 BETTER
I0314 15:37:42.051776 186346 finetune.py:68] layer 28_up @ epoch 4 new loss 0.0010194857604801655 old loss 0.0010249646147713065 BETTER
W0314 15:37:43.458011 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_up proxy err 0.037233538925647736 tr(WHW.T) 2477.644775390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.87s/it]  6%|▋         | 2/32 [00:03<00:48,  1.63s/it]  9%|▉         | 3/32 [00:04<00:44,  1.55s/it] 12%|█▎        | 4/32 [00:06<00:42,  1.51s/it] 16%|█▌        | 5/32 [00:07<00:40,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:38,  1.48s/it]I0314 15:37:54.277095 188981 finetune.py:68] layer 30_up @ epoch 2 new loss 0.002701460849493742 old loss 0.0027321225497871637 BETTER
I0314 15:37:54.630480 190381 finetune.py:68] layer 31_up @ epoch 1 new loss 0.008864891715347767 old loss 0.00913695152848959 BETTER
 22%|██▏       | 7/32 [00:10<00:36,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:33,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.47s/it]I0314 15:38:01.200856 187649 finetune.py:68] layer 29_up @ epoch 4 new loss 0.0013852634001523256 old loss 0.0013940500793978572 BETTER
 34%|███▍      | 11/32 [00:16<00:30,  1.46s/it]W0314 15:38:02.499309 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 38%|███▊      | 12/32 [00:17<00:29,  1.47s/it]29_up proxy err 0.03045807033777237 tr(WHW.T) 3245.58349609375
  0%|          | 0/32 [00:00<?, ?it/s] 41%|████      | 13/32 [00:19<00:27,  1.47s/it] 44%|████▍     | 14/32 [00:20<00:26,  1.47s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it] 50%|█████     | 16/32 [00:23<00:23,  1.47s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 56%|█████▋    | 18/32 [00:26<00:20,  1.47s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.55s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 62%|██████▎   | 20/32 [00:29<00:17,  1.47s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 69%|██████▉   | 22/32 [00:32<00:14,  1.47s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 41%|████      | 13/32 [00:19<00:28,  1.50s/it] 84%|████████▍ | 27/32 [00:39<00:07,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.51s/it] 91%|█████████ | 29/32 [00:42<00:04,  1.47s/it]I0314 15:38:28.029613 190381 finetune.py:68] layer 31_up @ epoch 2 new loss 0.008650004863739014 old loss 0.008864891715347767 BETTER
 50%|█████     | 16/32 [00:24<00:24,  1.51s/it]I0314 15:38:28.898581 188981 finetune.py:68] layer 30_up @ epoch 3 new loss 0.00267519848421216 old loss 0.002701460849493742 BETTER
 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 97%|█████████▋| 31/32 [00:45<00:01,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]
 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it]I0314 15:38:40.231359 186346 finetune.py:45] layer 28_gate initial loss 0.0015439251437783241
W0314 15:38:40.231742 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0314 15:39:00.387850 187649 finetune.py:45] layer 29_gate initial loss 0.0020716835279017687
W0314 15:39:00.388235 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:39:01.579434 190381 finetune.py:68] layer 31_up @ epoch 3 new loss 0.008464929647743702 old loss 0.008650004863739014 BETTER
I0314 15:39:03.381889 188981 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0026519258972257376 old loss 0.00267519848421216 BETTER
W0314 15:39:04.716376 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_up proxy err 0.01875099167227745 tr(WHW.T) 5519.748046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it]I0314 15:39:14.714637 186346 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.001528130262158811 old loss 0.0015439251437783241 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it]I0314 15:39:33.042769 187649 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0020517604425549507 old loss 0.0020716835279017687 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it]I0314 15:39:34.846862 190381 finetune.py:68] layer 31_up @ epoch 4 new loss 0.008303111419081688 old loss 0.008464929647743702 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it]W0314 15:39:36.210544 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it]31_up proxy err 0.007463024463504553 tr(WHW.T) 12283.076171875
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it]  9%|▉         | 3/32 [00:04<00:46,  1.62s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it]I0314 15:39:50.105169 186346 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.0015195534797385335 old loss 0.001528130262158811 BETTER
 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.52s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it]I0314 15:40:02.622605 188981 finetune.py:45] layer 30_gate initial loss 0.0037096769083291292
W0314 15:40:02.622958 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it]I0314 15:40:06.267079 187649 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.002040369901806116 old loss 0.0020517604425549507 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]I0314 15:40:25.322687 186346 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.001513012102805078 old loss 0.0015195534797385335 BETTER
100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
I0314 15:40:34.382807 190381 finetune.py:45] layer 31_gate initial loss 0.010901425033807755
W0314 15:40:34.383288 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:40:35.133270 188981 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.003667655633762479 old loss 0.0037096769083291292 BETTER
I0314 15:40:39.419858 187649 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.002031921874731779 old loss 0.002040369901806116 BETTER
I0314 15:41:00.403813 186346 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.0015078011201694608 old loss 0.001513012102805078 BETTER
I0314 15:41:05.792697 190381 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.010668622329831123 old loss 0.010901425033807755 BETTER
I0314 15:41:08.127385 188981 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.003642326220870018 old loss 0.003667655633762479 BETTER
I0314 15:41:12.364666 187649 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0020246091298758984 old loss 0.002031921874731779 BETTER
I0314 15:41:35.397943 186346 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.0015031987568363547 old loss 0.0015078011201694608 BETTER
W0314 15:41:36.622507 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 15:41:37.977301 190381 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.01052882894873619 old loss 0.010668622329831123 BETTER
28_gate proxy err 0.01819802075624466 tr(WHW.T) 8674.1630859375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:40,  1.11it/s]I0314 15:41:40.974086 188981 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.0036222871858626604 old loss 0.003642326220870018 BETTER
  2%|▏         | 2/112 [00:01<01:04,  1.70it/s]  3%|▎         | 3/112 [00:01<00:52,  2.06it/s]  4%|▎         | 4/112 [00:01<00:47,  2.29it/s]  4%|▍         | 5/112 [00:02<00:44,  2.43it/s]  5%|▌         | 6/112 [00:02<00:41,  2.53it/s]  6%|▋         | 7/112 [00:03<00:40,  2.60it/s]  7%|▋         | 8/112 [00:03<00:39,  2.64it/s]  8%|▊         | 9/112 [00:03<00:38,  2.67it/s]  9%|▉         | 10/112 [00:04<00:37,  2.69it/s] 10%|▉         | 11/112 [00:04<00:37,  2.70it/s] 11%|█         | 12/112 [00:04<00:37,  2.70it/s]I0314 15:41:45.258333 187649 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.002018449129536748 old loss 0.0020246091298758984 BETTER
 12%|█▏        | 13/112 [00:05<00:36,  2.70it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.70it/s] 13%|█▎        | 15/112 [00:06<00:35,  2.72it/s]W0314 15:41:46.367826 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 14%|█▍        | 16/112 [00:06<00:35,  2.72it/s] 15%|█▌        | 17/112 [00:06<00:34,  2.73it/s] 16%|█▌        | 18/112 [00:07<00:34,  2.72it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.72it/s] 18%|█▊        | 20/112 [00:07<00:33,  2.72it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.72it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.72it/s] 21%|██        | 23/112 [00:08<00:32,  2.72it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.73it/s]29_gate proxy err 0.016520580276846886 tr(WHW.T) 9635.2978515625
  0%|          | 0/112 [00:00<?, ?it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.74it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.74it/s]  1%|          | 1/112 [00:00<01:36,  1.14it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.74it/s]  2%|▏         | 2/112 [00:01<01:04,  1.71it/s] 25%|██▌       | 28/112 [00:10<00:30,  2.71it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.72it/s]  4%|▎         | 4/112 [00:02<00:48,  2.22it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.73it/s]  4%|▍         | 5/112 [00:02<00:45,  2.35it/s] 28%|██▊       | 31/112 [00:11<00:29,  2.74it/s]  5%|▌         | 6/112 [00:02<00:43,  2.45it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.74it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s] 29%|██▉       | 33/112 [00:12<00:28,  2.74it/s]  7%|▋         | 8/112 [00:03<00:40,  2.54it/s] 30%|███       | 34/112 [00:12<00:28,  2.74it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.75it/s]  8%|▊         | 9/112 [00:03<00:40,  2.57it/s] 32%|███▏      | 36/112 [00:13<00:27,  2.75it/s]  9%|▉         | 10/112 [00:04<00:39,  2.58it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.74it/s] 10%|▉         | 11/112 [00:04<00:38,  2.60it/s] 34%|███▍      | 38/112 [00:14<00:26,  2.74it/s] 11%|█         | 12/112 [00:05<00:38,  2.61it/s] 35%|███▍      | 39/112 [00:14<00:26,  2.73it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.60it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.73it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.61it/s] 37%|███▋      | 41/112 [00:15<00:25,  2.74it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s] 38%|███▊      | 42/112 [00:15<00:25,  2.71it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.72it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.59it/s] 39%|███▉      | 44/112 [00:16<00:24,  2.72it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 40%|████      | 45/112 [00:17<00:24,  2.71it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.61it/s] 41%|████      | 46/112 [00:17<00:24,  2.71it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.61it/s] 42%|████▏     | 47/112 [00:17<00:24,  2.71it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.61it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.72it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.62it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.73it/s] 21%|██        | 23/112 [00:09<00:33,  2.62it/s] 45%|████▍     | 50/112 [00:18<00:22,  2.73it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.62it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.73it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.62it/s] 46%|████▋     | 52/112 [00:19<00:21,  2.73it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.62it/s] 47%|████▋     | 53/112 [00:19<00:21,  2.73it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.62it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.73it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.62it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.70it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.62it/s] 50%|█████     | 56/112 [00:21<00:20,  2.71it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.61it/s] 51%|█████     | 57/112 [00:21<00:20,  2.71it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.59it/s] 52%|█████▏    | 58/112 [00:21<00:19,  2.72it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.73it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.60it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.72it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.62it/s] 54%|█████▍    | 61/112 [00:22<00:18,  2.73it/s] 30%|███       | 34/112 [00:13<00:29,  2.62it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.73it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s] 56%|█████▋    | 63/112 [00:23<00:17,  2.73it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 57%|█████▋    | 64/112 [00:23<00:17,  2.73it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.74it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.63it/s] 59%|█████▉    | 66/112 [00:24<00:16,  2.74it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.64it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.73it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.63it/s] 61%|██████    | 68/112 [00:25<00:16,  2.73it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.63it/s] 62%|██████▏   | 69/112 [00:25<00:15,  2.71it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.63it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.72it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.62it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.73it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.62it/s] 64%|██████▍   | 72/112 [00:26<00:14,  2.72it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.73it/s] 41%|████      | 46/112 [00:18<00:25,  2.60it/s] 66%|██████▌   | 74/112 [00:27<00:13,  2.74it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.61it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.73it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.61it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.73it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.62it/s] 69%|██████▉   | 77/112 [00:28<00:12,  2.72it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.62it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.72it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.62it/s] 71%|███████   | 79/112 [00:29<00:12,  2.72it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.63it/s] 71%|███████▏  | 80/112 [00:29<00:11,  2.73it/s]I0314 15:42:10.108480 190381 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.010419712401926517 old loss 0.01052882894873619 BETTER
 47%|████▋     | 53/112 [00:20<00:22,  2.63it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.73it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.62it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.69it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.62it/s] 74%|███████▍  | 83/112 [00:30<00:10,  2.71it/s] 50%|█████     | 56/112 [00:21<00:21,  2.62it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.72it/s] 76%|███████▌  | 85/112 [00:31<00:09,  2.72it/s] 51%|█████     | 57/112 [00:22<00:21,  2.62it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.72it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.61it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.73it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.59it/s] 79%|███████▊  | 88/112 [00:32<00:08,  2.73it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.60it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.73it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.60it/s] 80%|████████  | 90/112 [00:33<00:08,  2.73it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.61it/s] 81%|████████▏ | 91/112 [00:33<00:07,  2.73it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.61it/s]I0314 15:42:14.070307 188981 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.0036050574854016304 old loss 0.0036222871858626604 BETTER
 82%|████████▏ | 92/112 [00:34<00:07,  2.72it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.62it/s] 83%|████████▎ | 93/112 [00:34<00:06,  2.72it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.63it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.73it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.62it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.73it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.62it/s] 86%|████████▌ | 96/112 [00:35<00:05,  2.70it/s] 61%|██████    | 68/112 [00:26<00:16,  2.62it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.71it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.62it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.72it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.62it/s] 88%|████████▊ | 99/112 [00:36<00:04,  2.72it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.61it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.71it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.58it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.72it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.60it/s] 91%|█████████ | 102/112 [00:37<00:03,  2.73it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.60it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.72it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s] 93%|█████████▎| 104/112 [00:38<00:02,  2.72it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.61it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.71it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.61it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.71it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.62it/s] 96%|█████████▌| 107/112 [00:39<00:01,  2.71it/s] 71%|███████   | 79/112 [00:30<00:12,  2.62it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.71it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.62it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.71it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.62it/s] 98%|█████████▊| 110/112 [00:40<00:00,  2.69it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.72it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.63it/s]100%|██████████| 112/112 [00:41<00:00,  2.71it/s]100%|██████████| 112/112 [00:41<00:00,  2.69it/s]
 74%|███████▍  | 83/112 [00:32<00:11,  2.62it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.61it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.57it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.58it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.60it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.60it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.61it/s] 80%|████████  | 90/112 [00:34<00:08,  2.61it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.62it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.62it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.63it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.63it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.63it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.62it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.63it/s]W0314 15:42:27.686000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.686000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.686000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.686000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.687000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.687000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.687000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.730000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.731000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.731000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.731000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.731000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 99/112 [00:38<00:04,  2.63it/s]W0314 15:42:27.910000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.910000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.910000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.910000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:27.910000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 89%|████████▉ | 100/112 [00:38<00:04,  2.59it/s]W0314 15:42:28.237000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.237000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.237000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.237000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.237000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.238000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.238000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.268000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.269000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.269000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.269000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.269000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.340000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.340000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.340000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.340000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:28.340000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 90%|█████████ | 101/112 [00:39<00:04,  2.60it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.62it/s]W0314 15:42:29.320000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.332000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.341000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.341000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 92%|█████████▏| 103/112 [00:39<00:03,  2.62it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.63it/s]W0314 15:42:29.803000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.803000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.803000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.804000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.804000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.804000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.804000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.834000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.834000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.834000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.834000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:29.834000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 105/112 [00:40<00:02,  2.64it/s]W0314 15:42:30.139000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.139000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.140000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.140000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.140000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.140000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.140000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.140000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.443000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.443000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.443000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.443000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.443000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:41<00:02,  2.64it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.64it/s]W0314 15:42:30.886000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:30.892000 139895565064000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 96%|█████████▋| 108/112 [00:41<00:01,  2.64it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.64it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.64it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.62it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]
I0314 15:42:38.099407 186346 finetune.py:45] layer 28_down initial loss 0.002190012950450182
W0314 15:42:38.099835 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0314 15:42:39.109000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.109000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.109000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.109000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.110000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.110000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.110000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.155000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.155000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.155000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.155000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.155000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.334000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.334000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.334000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.334000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.335000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.654000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.654000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.654000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.654000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.655000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.655000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.655000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.690000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.690000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.690000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.690000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.690000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.760000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.760000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.760000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.760000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:39.760000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:40.744000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:40.758000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:40.766000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:40.767000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.225000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.225000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.225000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.225000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.226000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.226000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.226000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.258000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.258000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.258000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.258000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.259000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.559000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.560000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.560000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.560000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.560000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.560000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.560000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.560000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.858000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.858000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.858000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.858000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:42:41.858000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:42:42.291000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:42:42.297000 139846403213120 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 15:42:42.364131 190381 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.010319767519831657 old loss 0.010419712401926517 BETTER
I0314 15:42:46.807719 188981 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.003589788218960166 old loss 0.0036050574854016304 BETTER
W0314 15:42:48.064723 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0314 15:42:49.355664 187649 finetune.py:45] layer 29_down initial loss 0.0029445169493556023
W0314 15:42:49.356088 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

30_gate proxy err 0.012927310541272163 tr(WHW.T) 13142.203125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:35,  1.17it/s]  2%|▏         | 2/112 [00:01<01:03,  1.73it/s]  3%|▎         | 3/112 [00:01<00:53,  2.04it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s]  4%|▍         | 5/112 [00:02<00:45,  2.35it/s]  5%|▌         | 6/112 [00:02<00:43,  2.43it/s]  6%|▋         | 7/112 [00:03<00:42,  2.48it/s]  7%|▋         | 8/112 [00:03<00:41,  2.51it/s]  8%|▊         | 9/112 [00:03<00:40,  2.52it/s]  9%|▉         | 10/112 [00:04<00:40,  2.55it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 11%|█         | 12/112 [00:05<00:38,  2.58it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.59it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.60it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.62it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.62it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.63it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.63it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.63it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.62it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.63it/s] 21%|██        | 23/112 [00:09<00:33,  2.62it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.62it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.59it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.61it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.62it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.62it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.62it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.62it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.63it/s] 30%|███       | 34/112 [00:13<00:29,  2.63it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.63it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.63it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.58it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.59it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.60it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.61it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.62it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.62it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.62it/s] 40%|████      | 45/112 [00:17<00:25,  2.62it/s] 41%|████      | 46/112 [00:18<00:25,  2.63it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.62it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.62it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.62it/s]I0314 15:43:10.568774 186346 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0021857600659132004 old loss 0.002190012950450182 BETTER
 45%|████▍     | 50/112 [00:19<00:23,  2.62it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.62it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.62it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.59it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.60it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.61it/s] 50%|█████     | 56/112 [00:21<00:21,  2.62it/s] 51%|█████     | 57/112 [00:22<00:20,  2.62it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.63it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.63it/s]I0314 15:43:14.519620 190381 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.010230079293251038 old loss 0.010319767519831657 BETTER
 54%|█████▎    | 60/112 [00:23<00:19,  2.63it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.63it/s] 55%|█████▌    | 62/112 [00:24<00:18,  2.63it/s]W0314 15:43:15.735249 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 63/112 [00:24<00:18,  2.63it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.62it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.62it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.58it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.59it/s] 61%|██████    | 68/112 [00:26<00:16,  2.60it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.60it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.61it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.61it/s]31_gate proxy err 0.0056966147385537624 tr(WHW.T) 25457.859375
  0%|          | 0/112 [00:00<?, ?it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.61it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.62it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.62it/s]I0314 15:43:20.267537 187649 finetune.py:68] layer 29_down @ epoch 0 new loss 0.0029390265699476004 old loss 0.0029445169493556023 BETTER
  2%|▏         | 2/112 [00:01<01:04,  1.71it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.63it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.62it/s]  4%|▎         | 4/112 [00:02<00:48,  2.22it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.62it/s]  4%|▍         | 5/112 [00:02<00:45,  2.34it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.61it/s]  5%|▌         | 6/112 [00:02<00:43,  2.43it/s] 71%|███████   | 79/112 [00:30<00:12,  2.62it/s]  6%|▋         | 7/112 [00:03<00:42,  2.49it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.62it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.60it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.61it/s]  9%|▉         | 10/112 [00:04<00:39,  2.57it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.62it/s] 10%|▉         | 11/112 [00:04<00:39,  2.57it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.63it/s] 11%|█         | 12/112 [00:05<00:38,  2.58it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.63it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.59it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.63it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.59it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.63it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.58it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.63it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.62it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.60it/s] 80%|████████  | 90/112 [00:34<00:08,  2.62it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.62it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.60it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.61it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.61it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.61it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.61it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.58it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.59it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.58it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.60it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.60it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.61it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.61it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.61it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.62it/s] 24%|██▍       | 27/112 [00:10<00:33,  2.57it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.63it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.59it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.63it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.61it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.64it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.62it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.65it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.63it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.65it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.64it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.63it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.63it/s] 30%|███       | 34/112 [00:13<00:29,  2.63it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.63it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.62it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.59it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.61it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.60it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.62it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.61it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.61it/s]100%|██████████| 112/112 [00:43<00:00,  2.61it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]
 36%|███▌      | 40/112 [00:15<00:27,  2.60it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.56it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.58it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s] 40%|████      | 45/112 [00:17<00:25,  2.61it/s] 41%|████      | 46/112 [00:18<00:25,  2.61it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.62it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.63it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.62it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.62it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.62it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.61it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.62it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.61it/s]W0314 15:43:40.500000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.501000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.501000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.501000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.501000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.501000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.501000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.543000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.543000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.543000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.543000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.543000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 49%|████▉     | 55/112 [00:21<00:21,  2.60it/s]W0314 15:43:40.720000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.720000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.720000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.721000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:43:40.721000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.042000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.042000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.042000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.043000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.043000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.043000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.043000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 56/112 [00:21<00:21,  2.57it/s]W0314 15:43:41.074000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.074000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.074000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.074000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.074000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.145000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.145000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.145000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.145000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:41.145000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 51%|█████     | 57/112 [00:22<00:21,  2.58it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.60it/s]W0314 15:43:42.116000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.122000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.129000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.129000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 59/112 [00:23<00:20,  2.60it/s]W0314 15:43:42.577000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.577000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.577000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.577000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.577000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.577000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.578000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
 54%|█████▎    | 60/112 [00:23<00:19,  2.61it/s]W0314 15:43:42.606000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.606000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.606000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.606000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.606000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.905000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.905000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.905000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.905000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.905000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.906000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.906000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:43:42.906000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
 54%|█████▍    | 61/112 [00:23<00:19,  2.61it/s]W0314 15:43:43.203000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:43:43.203000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:43:43.203000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:43:43.203000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:43:43.204000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 55%|█████▌    | 62/112 [00:24<00:19,  2.62it/s]W0314 15:43:43.635000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:43:43.641000 140501454653248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s]I0314 15:43:43.920700 186346 finetune.py:68] layer 28_down @ epoch 1 new loss 0.002184322802349925 old loss 0.0021857600659132004 BETTER
 57%|█████▋    | 64/112 [00:25<00:18,  2.61it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.62it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.62it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.62it/s] 61%|██████    | 68/112 [00:26<00:16,  2.61it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.60it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.60it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.61it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.62it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.62it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.62it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.62it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.63it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.62it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.62it/s]I0314 15:43:50.644444 188981 finetune.py:45] layer 30_down initial loss 0.004985158797353506
W0314 15:43:50.644822 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 73%|███████▎  | 82/112 [00:31<00:11,  2.61it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.58it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.59it/s]I0314 15:43:51.967217 187649 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0029373234137892723 old loss 0.0029390265699476004 BETTER
 76%|███████▌  | 85/112 [00:33<00:10,  2.61it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.61it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.61it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.63it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.63it/s] 80%|████████  | 90/112 [00:34<00:08,  2.64it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.64it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.63it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.62it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.62it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.62it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.61it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.60it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.57it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.59it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.59it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.61it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.62it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.62it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.63it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.63it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.63it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.63it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.63it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.63it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.62it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.61it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]
W0314 15:44:08.594000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.595000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.595000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.595000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.595000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.595000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.595000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.640000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.641000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.641000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.641000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.641000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.824000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.824000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.824000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.824000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:44:08.824000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.150000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.150000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.151000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.151000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.151000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.151000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.151000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.185000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.185000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.185000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.186000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.186000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.256000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.256000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.257000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.257000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:44:09.257000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.242000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.257000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.265000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.265000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.720000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.720000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.720000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.721000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.721000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.721000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.721000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.756000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.756000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.756000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.756000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:44:10.756000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.057000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.057000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.057000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.057000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.057000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.057000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.057000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.057000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.361000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.361000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.361000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.361000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.361000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.803000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:44:11.809000 140171223582528 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0314 15:44:16.911289 186346 finetune.py:68] layer 28_down @ epoch 2 new loss 0.002183713251724839 old loss 0.002184322802349925 BETTER
I0314 15:44:18.684469 190381 finetune.py:45] layer 31_down initial loss 0.013301140628755093
W0314 15:44:18.684840 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0314 15:44:21.336959 188981 finetune.py:68] layer 30_down @ epoch 0 new loss 0.004976104013621807 old loss 0.004985158797353506 BETTER
I0314 15:44:22.993609 187649 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0029364849906414747 old loss 0.0029373234137892723 BETTER
I0314 15:44:48.524945 190381 finetune.py:68] layer 31_down @ epoch 0 new loss 0.013276813551783562 old loss 0.013301140628755093 BETTER
I0314 15:44:49.815007 186346 finetune.py:68] layer 28_down @ epoch 3 new loss 0.00218323920853436 old loss 0.002183713251724839 BETTER
I0314 15:44:52.617194 188981 finetune.py:68] layer 30_down @ epoch 1 new loss 0.004973025526851416 old loss 0.004976104013621807 BETTER
I0314 15:44:54.189194 187649 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0029360419139266014 old loss 0.0029364849906414747 BETTER
I0314 15:45:18.932858 190381 finetune.py:68] layer 31_down @ epoch 1 new loss 0.013268006034195423 old loss 0.013276813551783562 BETTER
I0314 15:45:22.698055 186346 finetune.py:68] layer 28_down @ epoch 4 new loss 0.0021829120814800262 old loss 0.00218323920853436 BETTER
W0314 15:45:23.558949 186346 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

28_down proxy err 0.03861626610159874 tr(WHW.T) 82.57522583007812
I0314 15:45:23.997967 188981 finetune.py:68] layer 30_down @ epoch 2 new loss 0.004971652291715145 old loss 0.004973025526851416 BETTER
I0314 15:45:25.617270 187649 finetune.py:68] layer 29_down @ epoch 4 new loss 0.002935154130682349 old loss 0.0029360419139266014 BETTER
W0314 15:45:26.332442 187649 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

29_down proxy err 0.03230303153395653 tr(WHW.T) 133.3435516357422
I0314 15:45:49.602172 190381 finetune.py:68] layer 31_down @ epoch 2 new loss 0.013261495158076286 old loss 0.013268006034195423 BETTER
I0314 15:45:55.459977 188981 finetune.py:68] layer 30_down @ epoch 3 new loss 0.00497065857052803 old loss 0.004971652291715145 BETTER
I0314 15:46:20.410138 190381 finetune.py:68] layer 31_down @ epoch 3 new loss 0.013256651349365711 old loss 0.013261495158076286 BETTER
I0314 15:46:26.630675 188981 finetune.py:68] layer 30_down @ epoch 4 new loss 0.004969742149114609 old loss 0.00497065857052803 BETTER
W0314 15:46:27.297696 188981 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

30_down proxy err 0.017451582476496696 tr(WHW.T) 373.1302490234375
I0314 15:46:51.510923 190381 finetune.py:68] layer 31_down @ epoch 4 new loss 0.013252799399197102 old loss 0.013256651349365711 BETTER
W0314 15:46:52.198771 190381 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

31_down proxy err 0.005488150287419558 tr(WHW.T) 2787.776123046875
I0314 15:47:23.164315 211358 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 15:47:23.164457 211358 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 15:47:23.164500 211358 utils.py:162] NumExpr defaulting to 16 threads.
I0314 15:47:23.344439 211358 config.py:58] PyTorch version 2.4.0 available.
W0314 15:47:25.112790 211358 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0314 15:47:25.113559 211358 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:02,  2.52it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  2.57it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:01<00:01,  2.70it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:01,  2.83it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  2.97it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:02<00:00,  3.12it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  3.24it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  3.00it/s]
Some weights of the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:01,  3.37it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:01,  3.62it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:01,  3.72it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:00,  3.84it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:01<00:00,  3.93it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:01<00:00,  4.06it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  4.20it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:01<00:00,  3.97it/s]
W0314 15:47:32.834429 211358 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0314 15:47:32.835804 211358 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0314 15:47:32.851646 211358 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0314 15:47:32.864482 211358 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0314 15:47:32.877290 211358 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0314 15:47:32.894076 211358 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0314 15:47:32.929049 211358 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0314 15:47:32.962721 211358 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0314 15:47:32.986933 211358 hfize_llama.py:113] loaded layer 0
I0314 15:47:33.124902 211358 hfize_llama.py:113] loaded layer 1
I0314 15:47:33.259061 211358 hfize_llama.py:113] loaded layer 2
I0314 15:47:33.390807 211358 hfize_llama.py:113] loaded layer 3
I0314 15:47:33.506476 211358 hfize_llama.py:113] loaded layer 4
I0314 15:47:33.628896 211358 hfize_llama.py:113] loaded layer 5
I0314 15:47:33.738201 211358 hfize_llama.py:113] loaded layer 6
I0314 15:47:33.848445 211358 hfize_llama.py:113] loaded layer 7
I0314 15:47:33.973563 211358 hfize_llama.py:113] loaded layer 8
I0314 15:47:34.091963 211358 hfize_llama.py:113] loaded layer 9
I0314 15:47:34.194398 211358 hfize_llama.py:113] loaded layer 10
I0314 15:47:34.313170 211358 hfize_llama.py:113] loaded layer 11
I0314 15:47:34.437186 211358 hfize_llama.py:113] loaded layer 12
I0314 15:47:34.552742 211358 hfize_llama.py:113] loaded layer 13
I0314 15:47:34.677037 211358 hfize_llama.py:113] loaded layer 14
I0314 15:47:34.793853 211358 hfize_llama.py:113] loaded layer 15
I0314 15:47:34.907894 211358 hfize_llama.py:113] loaded layer 16
I0314 15:47:35.030791 211358 hfize_llama.py:113] loaded layer 17
I0314 15:47:35.153641 211358 hfize_llama.py:113] loaded layer 18
I0314 15:47:35.259894 211358 hfize_llama.py:113] loaded layer 19
I0314 15:47:35.407721 211358 hfize_llama.py:113] loaded layer 20
I0314 15:47:35.515891 211358 hfize_llama.py:113] loaded layer 21
I0314 15:47:35.637693 211358 hfize_llama.py:113] loaded layer 22
I0314 15:47:35.758587 211358 hfize_llama.py:113] loaded layer 23
I0314 15:47:35.875537 211358 hfize_llama.py:113] loaded layer 24
I0314 15:47:36.008679 211358 hfize_llama.py:113] loaded layer 25
I0314 15:47:36.117879 211358 hfize_llama.py:113] loaded layer 26
I0314 15:47:36.251357 211358 hfize_llama.py:113] loaded layer 27
I0314 15:47:36.378747 211358 hfize_llama.py:113] loaded layer 28
I0314 15:47:36.506238 211358 hfize_llama.py:113] loaded layer 29
I0314 15:47:36.633723 211358 hfize_llama.py:113] loaded layer 30
I0314 15:47:36.793016 211358 hfize_llama.py:113] loaded layer 31
I0314 15:47:36.793113 211358 hfize_llama.py:115] saving model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.74it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.34it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.03it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]
I0314 15:47:53.525915 211358 hfize_llama.py:122] successfully loaded hfized model
I0314 15:47:57.495620 212373 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 15:47:57.495771 212373 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 15:47:57.495812 212373 utils.py:162] NumExpr defaulting to 16 threads.
I0314 15:47:57.684004 212373 config.py:58] PyTorch version 2.4.0 available.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.04s/it]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.25s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:05,  1.26s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.27s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:06<00:02,  1.38s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:07<00:01,  1.26s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.10s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:08<00:00,  1.20s/it]
torch.float32
{'model.embed_tokens': 1, 'model.rotary_emb': 1, 'model.norm': 3, 'lm_head': 3, 'model.layers.0': 1, 'model.layers.1': 1, 'model.layers.2': 1, 'model.layers.3': 1, 'model.layers.4': 1, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 2, 'model.layers.11': 2, 'model.layers.12': 2, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3}
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.25it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s]
I0314 15:48:47.952985 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953185 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953285 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953360 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953446 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953509 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953569 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953640 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953700 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953757 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953814 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953874 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953933 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.953997 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954067 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954125 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954175 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954224 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954277 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954327 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954376 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954436 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954486 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954534 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954582 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954633 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954681 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954729 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954785 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954835 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954886 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954934 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.954987 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955035 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955083 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955142 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955192 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955239 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955289 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955343 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955394 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955443 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955501 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955556 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955606 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955657 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955711 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955763 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955813 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955918 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.955975 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956027 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956080 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956136 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956186 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956236 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956295 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956347 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956399 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956450 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956506 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956561 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956614 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956674 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956727 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956779 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956830 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956885 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956935 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.956986 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957046 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957097 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957147 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957198 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957251 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957300 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957350 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957406 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957457 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957507 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957557 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957612 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957661 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957711 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957769 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957820 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957869 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957918 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.957973 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958024 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958072 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958130 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958180 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958230 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958281 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958336 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958386 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958436 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958494 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958545 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958597 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958647 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958699 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958749 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958798 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958856 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958908 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.958959 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959011 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959066 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959116 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959166 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959224 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959276 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959327 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959376 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959429 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959480 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959529 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959587 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959639 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959689 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959739 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959793 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959858 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959913 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.959974 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960026 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960077 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960127 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960180 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960231 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960280 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960337 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960388 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960438 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960489 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960543 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960594 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960644 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960702 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960754 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960804 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960853 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960906 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.960956 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961006 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961064 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961116 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961167 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961217 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961282 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961333 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961382 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961442 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961493 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961544 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961594 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961649 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961699 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961749 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961807 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961857 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961907 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.961956 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962010 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962061 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962110 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962169 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962220 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962270 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962320 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962375 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962425 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962475 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962535 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962588 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962640 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962690 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962745 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962798 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962849 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962908 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.962960 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963011 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963062 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963116 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963166 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963216 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963281 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963333 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963384 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963434 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963489 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963538 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963587 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963648 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963699 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963752 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963804 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963868 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963922 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.963977 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964039 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964092 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964142 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964194 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964248 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964300 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964351 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964412 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964464 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964515 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964565 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964620 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964672 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964723 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964784 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964837 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964889 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964941 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.964996 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.965047 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:47.965097 212373 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0314 15:48:48.121157 212373 data_utils.py:336] using 512 training seqs, 128 validation seqs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0314 15:48:50.354201 213734 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0314 15:48:50.354324 213734 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0314 15:48:50.354384 213734 utils.py:162] NumExpr defaulting to 16 threads.
W0314 15:48:50.536000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.536000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.536000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.537000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.537000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.537000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.537000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
I0314 15:48:50.546226 213734 config.py:58] PyTorch version 2.4.0 available.
W0314 15:48:50.562000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.563000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.563000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.563000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.563000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.850000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.850000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.850000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.851000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:50.851000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.686000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.686000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.686000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.686000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.687000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.687000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.687000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.703000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.704000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.704000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.704000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.704000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.898000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.898000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.899000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.899000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:51.899000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]W0314 15:48:53.413000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.413000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.413000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.413000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.414000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.414000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.414000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.431000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.431000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.431000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.431000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:48:53.431000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:06,  1.13s/it]W0314 15:48:54.284000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:54.284000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:54.284000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:54.284000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:48:54.284000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.03s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.03s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.04s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.06s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.04s/it]W0314 15:48:59.114000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.115000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.115000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.115000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.115000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.115000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.115000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.144000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.144000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.144000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.144000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.144000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.12it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.02it/s]
W0314 15:48:59.308000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.308000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.308000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.308000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.308000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.523000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.523000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.523000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.523000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.523000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.523000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.524000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.543000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.543000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.543000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.543000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.544000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.607000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.607000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.607000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.607000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:48:59.607000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.321000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.624000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.625000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.625000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.625000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.625000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.625000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.625000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.647000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.647000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.647000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.647000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.647000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.894000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.894000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.894000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.894000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:00.894000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:01.234000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
W0314 15:49:06.320000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.320000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.321000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.321000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.321000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.321000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.321000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.583000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.583000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.583000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.583000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.583000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.761000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.761000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.761000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.761000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:06.761000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.078000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.078000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.078000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.078000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.079000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.079000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.079000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.112000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.112000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.112000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.112000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.112000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.181000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.181000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.182000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.182000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:07.182000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.151000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.158000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.165000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.165000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.597000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.597000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.597000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.597000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.597000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.597000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.598000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.627000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.627000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.627000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.627000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.627000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.908000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.908000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.908000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.909000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.909000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.909000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.909000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:08.909000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:09.185000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:09.185000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:09.185000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:09.186000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:09.186000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:09.585000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:09.590000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.595000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.595000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.595000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.595000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.595000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.595000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.595000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.636000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.636000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.636000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.636000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.636000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.784000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.784000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.784000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.784000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:15.784000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.054000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.054000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.054000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.054000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.055000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.055000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.055000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.087000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.087000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.087000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.087000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.087000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.154000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.154000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.154000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.154000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.154000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.982000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.988000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.994000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:16.994000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.384000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.384000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.384000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.384000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.384000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.384000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.384000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.414000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.414000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.414000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.415000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.415000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.668000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.668000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.668000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.668000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.668000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.668000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.668000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.668000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.940000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.940000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.940000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.940000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:17.940000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:18.331000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:18.337000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.645000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.645000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.645000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.646000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.646000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.646000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.646000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.687000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.687000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.687000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.687000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.687000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.835000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.835000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.836000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.836000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:25.836000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.106000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.106000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.106000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.106000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.107000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.107000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.107000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.138000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.138000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.138000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.138000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.138000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.203000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.203000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.203000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.204000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:26.204000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.018000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.023000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.030000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.030000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.427000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.427000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.427000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.428000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.428000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.428000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.428000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.458000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.458000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.458000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.459000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.459000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.714000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x6 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.714000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.714000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.714000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x3 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.714000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.714000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.714000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.714000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.989000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.989000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.989000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.989000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:27.989000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0314 15:49:28.378000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps0 is not in var_ranges, defaulting to unknown range.
W0314 15:49:28.382000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps5 is not in var_ranges, defaulting to unknown range.
W0314 15:49:31.827197 212373 logging.py:328] Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
W0314 16:00:09.570309 212373 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:391: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

I0314 16:00:11.150582 212373 finetune.py:394] initial loss 2.1967785358428955
W0314 16:00:14.339000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q0 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.339000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q4 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.339000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q5 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.339000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.340000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q6 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.340000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q1 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.340000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q3 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.380000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q6 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.381000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q4 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.381000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q1 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.381000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q5 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.381000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.526000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q6 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.526000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q4 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.526000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q1 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.527000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q5 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.527000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.797000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.797000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z4 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.797000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z0 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.797000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z6 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.798000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z5 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.798000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z1 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.798000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z3 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.829000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.829000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z6 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.829000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z5 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.829000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z1 is not in var_ranges, defaulting to unknown range.
W0314 16:00:14.829000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z4 is not in var_ranges, defaulting to unknown range.
W0314 16:00:15.142000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:15.142000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z6 is not in var_ranges, defaulting to unknown range.
W0314 16:00:15.142000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z5 is not in var_ranges, defaulting to unknown range.
W0314 16:00:15.142000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z1 is not in var_ranges, defaulting to unknown range.
W0314 16:00:15.142000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z4 is not in var_ranges, defaulting to unknown range.
W0314 16:00:15.947000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:15.952000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:15.959000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:15.959000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps0 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.353000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x6 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.353000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.353000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x4 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.353000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x3 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.353000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x1 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.353000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x5 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.353000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x0 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.383000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x1 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.383000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x5 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.383000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.383000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x0 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.383000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x4 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.636000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x6 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.637000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.637000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x4 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.637000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x3 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.637000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x1 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.637000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x5 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.637000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x0 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.637000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.908000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x1 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.908000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x5 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.908000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x2 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.908000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x0 is not in var_ranges, defaulting to unknown range.
W0314 16:00:16.908000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x4 is not in var_ranges, defaulting to unknown range.
W0314 16:00:17.297000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps0 is not in var_ranges, defaulting to unknown range.
W0314 16:00:17.302000 139942569531200 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps5 is not in var_ranges, defaulting to unknown range.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 128, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 110, in main
    finetune.finetune_susv_e2e(quant_model, start_dev, devset, orig_dtype,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune.py", line 402, in finetune_susv_e2e
    output = quant_model(
             ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/model/llama.py", line 1441, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/model/llama.py", line 1246, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/model/llama.py", line 985, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/model/llama.py", line 401, in forward
    down_proj = self.down_proj(
                ^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/linear/quantized_linear.py", line 86, in forward
    return self.no_ckpt_forward(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/linear/quantized_linear.py", line 138, in no_ckpt_forward
    result = self.codebook_class(input,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 441, in forward
    x = matmul_hadUt_cuda(x, had_left, K_left) / self.scale
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/matmul_had.py", line 119, in matmul_hadUt_cuda
    return matmul_hadU_cuda(X, hadK, K, transpose=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/matmul_had.py", line 114, in matmul_hadU_cuda
    input = hadK.to(input.device).to(input.dtype) @ input
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 1 has a total capacity of 47.51 GiB of which 230.75 MiB is free. Process 2578847 has 43.51 GiB memory in use. Process 2581093 has 3.76 GiB memory in use. Of the allocated memory 40.80 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.48it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.01it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.71it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 74, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 39, in main
    input_tok = gptq_data_utils.get_test_tokens(dataset,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/gptq_data_utils.py", line 209, in get_test_tokens
    return get_wikitext2(train_samples, seed, seqlen,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/gptq_data_utils.py", line 25, in get_wikitext2
    tokenizer = AutoTokenizer.from_pretrained('/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf', use_fast=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 844, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 676, in get_tokenizer_config
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: '/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.36it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.07it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.72it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.13s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.33it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 74, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 39, in main
    input_tok = gptq_data_utils.get_test_tokens(dataset,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/gptq_data_utils.py", line 209, in get_test_tokens
    return get_wikitext2(train_samples, seed, seqlen,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/gptq_data_utils.py", line 25, in get_wikitext2
    tokenizer = AutoTokenizer.from_pretrained('/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf', use_fast=False)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 844, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 676, in get_tokenizer_config
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: '/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
I0315 02:50:26.115779 252037 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 02:50:26.115947 252037 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 02:50:26.115992 252037 utils.py:162] NumExpr defaulting to 16 threads.
I0315 02:50:26.296522 252037 config.py:58] PyTorch version 2.4.0 available.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:05,  1.03it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.01s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.12s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.19s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.14s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.07s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.03it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.05s/it]
torch.float32
{'model.embed_tokens': 1, 'model.rotary_emb': 1, 'model.norm': 3, 'lm_head': 3, 'model.layers.0': 1, 'model.layers.1': 1, 'model.layers.2': 1, 'model.layers.3': 1, 'model.layers.4': 1, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 2, 'model.layers.11': 2, 'model.layers.12': 2, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3}
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.06it/s]
I0315 02:51:13.065757 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.065968 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066051 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066121 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066199 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066259 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066313 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066376 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066430 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066482 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066535 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066589 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066638 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066688 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066746 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066798 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066849 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066899 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.066951 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067000 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067049 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067108 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067157 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067206 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067254 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067304 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067352 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067400 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067456 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067507 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067556 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067605 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067657 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067705 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067752 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067812 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067888 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067937 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.067986 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068038 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068088 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068140 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068201 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068254 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068305 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068356 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068411 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068461 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068511 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068571 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068622 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068673 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068722 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068777 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068830 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068882 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068944 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.068996 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069048 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069098 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069154 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069206 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069258 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069318 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069370 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069421 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069471 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069525 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069574 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069624 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069683 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069734 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069785 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069835 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069888 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069937 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.069982 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070035 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070082 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070127 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070173 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070224 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070270 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070316 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070371 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070418 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070465 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070511 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070560 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070605 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070651 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070706 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070753 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070799 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070846 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070895 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070942 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.070989 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071043 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071090 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071137 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071183 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071233 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071279 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071325 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071378 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071426 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071472 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071518 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071567 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071613 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071657 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071711 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071758 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071805 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071863 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071914 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.071961 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072007 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072064 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072114 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072162 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072210 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072266 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072318 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072371 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072429 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072481 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072533 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072583 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072637 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072687 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072736 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072793 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072844 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072892 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072942 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.072995 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073047 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073096 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073154 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073203 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073253 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073303 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073357 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073407 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073455 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073513 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073563 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073614 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073662 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073730 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073780 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073827 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073884 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073934 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.073984 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074034 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074090 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074139 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074190 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074247 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074297 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074346 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074394 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074447 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074496 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074545 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074603 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074653 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074702 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074750 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074803 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074853 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074903 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.074959 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075010 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075060 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075109 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075161 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075212 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075261 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075319 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075369 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075418 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075468 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075521 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075571 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075619 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075675 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075725 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075774 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075845 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075904 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.075956 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076014 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076075 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076129 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076182 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076232 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076284 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076330 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076378 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076433 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076481 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076528 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076575 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076624 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076672 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076718 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076771 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076818 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076867 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076917 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.076970 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.077021 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.077070 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.077130 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.077183 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.077233 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.077282 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.077334 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.077389 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.077439 252037 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:51:13.228984 252037 data_utils.py:336] using 512 training seqs, 128 validation seqs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0315 02:51:15.467062 253388 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 02:51:15.467184 253388 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 02:51:15.467242 253388 utils.py:162] NumExpr defaulting to 16 threads.
W0315 02:51:15.574000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.575000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.575000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.575000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.575000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.575000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.575000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.602000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.602000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.602000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.603000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.603000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
I0315 02:51:15.661535 253388 config.py:58] PyTorch version 2.4.0 available.
W0315 02:51:15.891000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.891000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.891000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.891000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:15.892000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.741000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.741000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.741000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.741000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.741000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.741000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.741000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.759000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.759000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.759000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.759000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.759000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.956000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.956000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.957000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.957000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:16.957000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]W0315 02:51:18.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.497000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.497000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.497000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.498000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:18.498000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:07,  1.17s/it]W0315 02:51:19.347000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:19.347000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:19.347000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:19.347000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:19.347000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:05,  1.03s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.01s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:02,  1.02it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:04<00:01,  1.02it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:05<00:00,  1.05it/s]W0315 02:51:23.451000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.451000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.451000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.451000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.451000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.451000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.451000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.479000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.644000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.645000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.645000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.645000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.645000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.861000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.861000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.861000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.861000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.861000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.861000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.861000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.882000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.882000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.882000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.882000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.882000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.22it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.08it/s]
W0315 02:51:23.946000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.946000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.946000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.946000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:23.946000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.660000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.967000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.967000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.968000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.968000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.968000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.968000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.968000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.990000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.990000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.990000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.990000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:24.990000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:25.237000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:25.238000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:25.238000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:25.238000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:25.238000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:25.573000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
W0315 02:51:30.419000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.419000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.420000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.420000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.420000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.420000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.420000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.459000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.459000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.459000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.459000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.459000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.630000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.630000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.630000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.630000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.630000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.934000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.934000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.934000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.934000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.934000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.935000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.935000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.965000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.965000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.965000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.965000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:30.965000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:31.036000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:31.036000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:31.036000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:31.036000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:31.036000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.044000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.058000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.067000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.067000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.540000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.541000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.541000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.541000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.541000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.541000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.541000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.572000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.572000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.573000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.573000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.573000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.886000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:32.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:33.186000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:33.187000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:33.187000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:33.187000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:33.187000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:33.604000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:33.613000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.688000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.689000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.689000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.689000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.689000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.689000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.689000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.728000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.728000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.728000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.728000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.729000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:39.887000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.169000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.169000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.169000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.169000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.170000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.170000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.170000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.203000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.203000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.203000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.204000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.204000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.270000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.270000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.270000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.270000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:40.271000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.108000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.121000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.129000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.129000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.555000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.555000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.555000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.556000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.556000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.556000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.556000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.586000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.586000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.586000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.586000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.586000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.846000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.846000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.846000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.847000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.847000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.847000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.847000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:41.847000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:42.118000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:42.118000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:42.118000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:42.118000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:42.118000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:42.513000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:42.518000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.351000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.351000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.351000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.351000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.351000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.352000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.352000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.389000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.390000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.390000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.390000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.390000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.542000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.542000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.542000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.542000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.542000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.816000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.816000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.816000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.816000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.816000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.816000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.817000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.845000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.845000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.845000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.845000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.845000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.910000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.911000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.911000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.911000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:49.911000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:50.736000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:50.750000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:50.757000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:50.757000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.170000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.170000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.171000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.171000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.171000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.171000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.171000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.202000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.202000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.203000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.203000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.203000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.465000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.465000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.465000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.466000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.466000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.466000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.466000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.466000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.768000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.768000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.769000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.769000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:51:51.769000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:51:52.159000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:51:52.165000 140095128700736 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps5 is not in var_ranges, defaulting to unknown range.
W0315 02:51:55.109989 252037 logging.py:328] Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
I0315 02:55:45.413239 259004 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 02:55:45.413416 259004 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 02:55:45.413456 259004 utils.py:162] NumExpr defaulting to 16 threads.
I0315 02:55:45.604776 259004 config.py:58] PyTorch version 2.4.0 available.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:05,  1.05it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:01<00:04,  1.01it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:02<00:04,  1.00s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:03<00:03,  1.00s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.01s/it]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:05<00:00,  1.02it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:06<00:00,  1.08it/s]
torch.float32
{'model.embed_tokens': 1, 'model.rotary_emb': 1, 'model.norm': 3, 'lm_head': 3, 'model.layers.0': 1, 'model.layers.1': 1, 'model.layers.2': 1, 'model.layers.3': 1, 'model.layers.4': 1, 'model.layers.5': 1, 'model.layers.6': 1, 'model.layers.7': 1, 'model.layers.8': 1, 'model.layers.9': 1, 'model.layers.10': 2, 'model.layers.11': 2, 'model.layers.12': 2, 'model.layers.13': 2, 'model.layers.14': 2, 'model.layers.15': 2, 'model.layers.16': 2, 'model.layers.17': 2, 'model.layers.18': 2, 'model.layers.19': 2, 'model.layers.20': 2, 'model.layers.21': 2, 'model.layers.22': 3, 'model.layers.23': 3, 'model.layers.24': 3, 'model.layers.25': 3, 'model.layers.26': 3, 'model.layers.27': 3, 'model.layers.28': 3, 'model.layers.29': 3, 'model.layers.30': 3, 'model.layers.31': 3}
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.40s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.18it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.07it/s]
I0315 02:56:31.942449 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.942685 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.942783 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.942860 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.942946 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943010 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943066 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943139 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943202 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943263 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943321 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943384 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943443 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943504 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943573 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943633 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943691 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943749 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943807 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943895 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.943956 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944035 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944097 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944154 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944208 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944268 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944326 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944380 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944446 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944504 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944561 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944617 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944676 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944733 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944787 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944856 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944920 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.944979 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945034 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945097 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945153 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945208 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945274 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945330 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945389 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945442 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945502 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945558 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945612 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945674 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945728 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945782 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945836 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945898 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.945953 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946007 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946071 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946125 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946189 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946245 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946302 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946359 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946413 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946477 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946530 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946583 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946635 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946691 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946743 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946795 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946856 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946907 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.946958 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947009 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947064 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947116 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947167 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947228 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947280 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947331 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947382 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947435 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947486 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947534 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947589 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947637 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947684 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947730 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947782 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947839 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947888 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947945 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.947994 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948041 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948092 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948147 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948197 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948248 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948310 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948362 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948413 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948463 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948523 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948573 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948624 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948683 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948734 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948784 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948835 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948890 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948941 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.948991 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949053 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949104 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949155 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949204 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949258 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949309 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949359 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949419 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949471 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949522 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949572 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949627 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949680 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949732 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949792 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949843 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949893 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.949944 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950001 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950052 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950102 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950161 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950215 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950265 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950316 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950371 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950423 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950476 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950538 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950590 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950641 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950692 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950748 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950799 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950850 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950912 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.950963 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951014 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951064 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951129 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951182 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951232 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951292 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951341 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951391 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951440 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951495 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951546 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951595 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951653 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951704 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951754 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951803 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951870 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951922 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.951972 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952030 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952081 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952136 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952185 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952239 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952289 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952339 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952398 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952449 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952498 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952547 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952602 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952653 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952702 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952761 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952812 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952863 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952912 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.952970 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953022 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953071 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953130 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953181 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953232 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953281 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953335 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953385 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953436 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953495 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953547 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953598 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953649 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953705 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953755 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953805 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953863 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953914 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.953964 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954015 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954069 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954119 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954168 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954227 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954278 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954328 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954377 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954430 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954482 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954533 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954593 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954644 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954693 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954741 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954795 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954845 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:31.954896 259004 finetune_e2e_llama.py:102] overriding ft_prefetch_trellis
I0315 02:56:32.127575 259004 data_utils.py:336] using 512 training seqs, 128 validation seqs
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0315 02:56:34.390499 260501 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 02:56:34.390614 260501 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 02:56:34.390672 260501 utils.py:162] NumExpr defaulting to 16 threads.
I0315 02:56:34.578421 260501 config.py:58] PyTorch version 2.4.0 available.
W0315 02:56:34.602000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.602000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.602000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.602000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.602000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.602000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.602000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.629000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.629000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.629000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.629000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.629000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.929000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.929000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.929000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.930000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:34.930000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.783000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.783000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.783000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.783000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.783000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.783000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.783000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.801000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.802000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.802000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.802000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:35.802000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:36.014000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:36.014000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:36.014000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:36.014000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:36.014000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]W0315 02:56:37.576000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.576000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.576000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.576000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.576000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.576000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.576000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.594000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.594000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.594000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.594000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:37.594000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards:  14%|█▍        | 1/7 [00:01<00:08,  1.47s/it]W0315 02:56:38.464000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:38.464000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:38.465000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:38.465000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:38.465000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards:  29%|██▊       | 2/7 [00:02<00:06,  1.20s/it]Loading checkpoint shards:  43%|████▎     | 3/7 [00:03<00:04,  1.15s/it]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:04<00:03,  1.08s/it]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:05<00:02,  1.04s/it]W0315 02:56:42.612000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.612000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.612000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.612000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.613000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.613000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.613000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.643000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.643000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.644000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.644000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.644000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.814000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.814000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.814000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.814000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:42.814000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.033000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.034000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.034000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.034000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.034000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.034000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.034000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.056000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.056000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.056000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.056000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.056000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.120000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.120000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.121000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.121000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:43.121000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
Loading checkpoint shards:  86%|████████▌ | 6/7 [00:06<00:01,  1.12s/it]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.06it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:07<00:00,  1.06s/it]
W0315 02:56:43.828000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.143000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.144000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.144000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.144000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.144000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.144000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.144000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.165000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.165000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.165000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.165000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.165000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.415000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.415000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.415000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.415000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.415000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:44.757000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
W0315 02:56:49.613000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.613000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.614000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.614000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.614000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.614000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.614000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.654000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.654000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.654000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.655000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.655000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.821000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.821000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.821000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.821000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:49.821000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.124000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.124000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.125000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.125000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.125000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.125000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.125000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.157000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.157000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.157000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.158000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.158000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.227000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.227000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.227000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.227000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:50.227000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.154000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.166000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.174000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.174000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.679000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.680000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.680000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.680000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.680000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.680000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.680000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.712000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.713000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.713000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.713000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:51.713000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.064000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.064000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.064000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.064000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.064000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.065000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.065000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.065000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.366000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.366000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.366000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.366000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.366000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.804000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:52.809000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.941000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.941000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.941000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.941000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.942000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.942000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.942000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.983000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.983000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.983000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.983000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:58.983000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.139000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.139000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.140000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.140000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.140000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.422000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.422000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.422000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z3 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.422000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.422000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z0 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.423000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.423000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.454000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.454000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.454000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.455000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.455000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.525000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.525000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.525000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.525000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:56:59.525000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.349000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.362000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.370000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.370000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.763000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.763000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.763000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.764000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.764000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.764000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.764000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.796000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.796000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.796000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.796000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:00.797000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.062000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.062000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.062000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.062000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.062000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.062000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.062000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.062000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.345000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.345000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.345000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.345000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.345000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.741000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:01.746000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/3] ps5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.661000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.661000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.661000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.661000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q3 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.661000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.661000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.662000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.702000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.702000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.702000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.702000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.702000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.858000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.858000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.858000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.858000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q6 is not in var_ranges, defaulting to unknown range.
W0315 02:57:08.859000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] q5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.143000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.143000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.143000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z3 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.143000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.143000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.143000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.143000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.175000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.175000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.175000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.175000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.175000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.243000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z6 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.243000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.243000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.243000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:09.244000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] z1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.062000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.075000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.083000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.083000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.480000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.480000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.480000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.480000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.480000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.480000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.480000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.511000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.511000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.511000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.511000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.511000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.772000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.772000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x6 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.772000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.772000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.772000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.772000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.772000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x3 is not in var_ranges, defaulting to unknown range.
W0315 02:57:10.772000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:11.048000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x2 is not in var_ranges, defaulting to unknown range.
W0315 02:57:11.048000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:11.048000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:11.048000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x1 is not in var_ranges, defaulting to unknown range.
W0315 02:57:11.049000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] x4 is not in var_ranges, defaulting to unknown range.
W0315 02:57:11.449000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps0 is not in var_ranges, defaulting to unknown range.
W0315 02:57:11.454000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/4] ps5 is not in var_ranges, defaulting to unknown range.
W0315 02:57:14.430155 259004 logging.py:328] Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
W0315 03:07:51.629511 259004 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:391: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=True)

I0315 03:07:52.757520 259004 finetune.py:394] initial loss 2.1967785358428955
W0315 03:07:55.923000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.923000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.923000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.924000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.924000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.924000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.924000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.962000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.962000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.963000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.963000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:07:55.963000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.110000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.110000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.110000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.111000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.111000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.392000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.392000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.392000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.392000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.393000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.393000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.393000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.426000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.426000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.426000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.426000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.426000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.747000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.747000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.747000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.747000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:07:56.748000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.555000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.567000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.575000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.575000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.966000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.966000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.966000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.967000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.967000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.967000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.967000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.996000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.996000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.996000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.997000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:07:57.997000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.256000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.256000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.257000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.257000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.257000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.257000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.257000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.257000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.544000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.544000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.544000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.544000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.544000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.951000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:07:58.956000 139695482177344 torch/fx/experimental/symbolic_shapes.py:4449] [0/5] ps5 is not in var_ranges, defaulting to unknown range.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 128, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 110, in main
    finetune.finetune_susv_e2e(quant_model, start_dev, devset, orig_dtype,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune.py", line 402, in finetune_susv_e2e
    output = quant_model(
             ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/model/llama.py", line 1441, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/model/llama.py", line 1246, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/model/llama.py", line 985, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/model/llama.py", line 401, in forward
    down_proj = self.down_proj(
                ^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/linear/quantized_linear.py", line 86, in forward
    return self.no_ckpt_forward(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/linear/quantized_linear.py", line 138, in no_ckpt_forward
    result = self.codebook_class(input,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/codebook/bitshift.py", line 441, in forward
    x = matmul_hadUt_cuda(x, had_left, K_left) / self.scale
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/matmul_had.py", line 119, in matmul_hadUt_cuda
    return matmul_hadU_cuda(X, hadK, K, transpose=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/matmul_had.py", line 114, in matmul_hadU_cuda
    input = hadK.to(input.device).to(input.dtype) @ input
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 1 has a total capacity of 47.51 GiB of which 338.75 MiB is free. Process 2804843 has 43.40 GiB memory in use. Process 2807125 has 3.76 GiB memory in use. Of the allocated memory 40.80 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
I0315 03:10:13.064622 273589 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 03:10:13.064813 273589 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 03:10:13.064857 273589 utils.py:162] NumExpr defaulting to 16 threads.
I0315 03:10:13.246351 273589 config.py:58] PyTorch version 2.4.0 available.
