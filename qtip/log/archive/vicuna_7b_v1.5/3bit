[Stage: Quantize with Finetuning] K=3
I0623 23:44:19.525914 4970 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0623 23:44:19.526055 4970 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0623 23:44:19.526100 4970 utils.py:162] NumExpr defaulting to 16 threads.
I0623 23:44:19.707495 4970 config.py:58] PyTorch version 2.4.0 available.
W0623 23:44:25.864230 4970 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.36it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  7.77it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.05it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.21it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.28it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.37it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.19it/s]
I0623 23:44:27.144788 4970 quantize_finetune_llama.py:138] loaded model
I0623 23:45:22.581502 4970 quantize_finetune_llama.py:142] loaded dataset and devset
I0623 23:45:28.652783 4970 quantize_finetune_llama.py:162] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0623 23:46:30.259680 4970 quantize_finetune_llama.py:193] computed original embedding for layer 0 in 61.45309090614319s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0623 23:46:56.802973 4970 quantize_finetune_llama.py:162] layer 1 gpu 1
I0623 23:46:58.714338 5646 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0623 23:46:58.714442 5646 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0623 23:46:58.714505 5646 utils.py:162] NumExpr defaulting to 16 threads.
I0623 23:46:58.895021 5646 config.py:58] PyTorch version 2.4.0 available.
I0623 23:47:01.679695 5646 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0623 23:47:02.092822 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:02<00:16,  1.78it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.56it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.70it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.90it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.97it/s] 34%|███▍      | 11/32 [00:04<00:07,  3.00it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.05it/s] 41%|████      | 13/32 [00:05<00:06,  3.03it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.03it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.06it/s] 50%|█████     | 16/32 [00:06<00:05,  3.06it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.08it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.08it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.11it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.03it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.05it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.05it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.07it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.09it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.12it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.07it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.02it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.04it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.05it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.07it/s]100%|██████████| 32/32 [00:11<00:00,  3.08it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
W0623 23:47:17.724000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.725000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.725000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.725000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.725000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.725000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.725000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.752000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.753000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.753000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.753000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.753000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.769000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.769000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.769000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.769000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:47:17.769000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.088000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.088000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.088000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.089000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.089000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.970000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.971000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.971000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.971000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.971000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.971000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.971000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.988000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.988000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.988000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.988000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:47:18.989000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:47:19.224000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:47:19.224000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:47:19.225000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:47:19.225000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:47:19.225000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.394000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.394000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.394000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.394000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.394000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.394000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.395000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.412000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.412000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.412000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.412000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:47:20.412000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0623 23:47:21.310000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:47:21.310000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:47:21.310000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:47:21.310000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:47:21.310000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0623 23:47:27.148126 5646 finetune.py:45] layer 0_v initial loss 1.995418053013509e-08
W0623 23:47:27.148443 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:47:52.601405 4970 quantize_finetune_llama.py:193] computed original embedding for layer 1 in 55.650020360946655s
I0623 23:48:01.427017 5646 finetune.py:68] layer 0_v @ epoch 0 new loss 6.262748541985275e-09 old loss 1.995418053013509e-08 BETTER
I0623 23:48:01.877589 4970 quantize_finetune_llama.py:162] layer 2 gpu 2
I0623 23:48:03.791986 6152 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0623 23:48:03.792099 6152 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0623 23:48:03.792163 6152 utils.py:162] NumExpr defaulting to 16 threads.
I0623 23:48:03.988083 6152 config.py:58] PyTorch version 2.4.0 available.
I0623 23:48:06.385781 6152 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0623 23:48:06.711167 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:50,  1.63s/it]  6%|▋         | 2/32 [00:01<00:26,  1.13it/s]  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.17it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
W0623 23:48:23.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.405000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.405000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.405000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.405000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.405000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.423000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.423000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.423000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.423000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.423000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.754000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.755000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.755000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.755000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:48:23.755000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.670000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.671000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.671000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.671000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.671000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.671000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.671000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.690000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.691000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.691000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.691000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.691000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.940000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.940000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.940000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.941000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:48:24.941000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.164000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.164000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.164000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.164000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.164000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.164000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.164000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.183000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.183000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.183000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.183000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:48:26.183000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:48:27.111000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:48:27.111000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0623 23:48:27.111000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:48:27.111000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:48:27.111000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0623 23:48:32.883266 6152 finetune.py:45] layer 1_v initial loss 1.195689765154384e-05
W0623 23:48:32.883637 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:48:35.687908 5646 finetune.py:68] layer 0_v @ epoch 1 new loss 4.760779770407453e-09 old loss 6.262748541985275e-09 BETTER
I0623 23:49:04.430112 6152 finetune.py:68] layer 1_v @ epoch 0 new loss 7.873895242482831e-07 old loss 1.195689765154384e-05 BETTER
I0623 23:49:04.650904 4970 quantize_finetune_llama.py:193] computed original embedding for layer 2 in 62.577423095703125s
I0623 23:49:12.933726 5646 finetune.py:68] layer 0_v @ epoch 2 new loss 4.4242485230938655e-09 old loss 4.760779770407453e-09 BETTER
I0623 23:49:13.542687 4970 quantize_finetune_llama.py:162] layer 3 gpu 3
I0623 23:49:15.580714 6660 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0623 23:49:15.580838 6660 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0623 23:49:15.580901 6660 utils.py:162] NumExpr defaulting to 16 threads.
I0623 23:49:15.756419 6660 config.py:58] PyTorch version 2.4.0 available.
I0623 23:49:18.293426 6660 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0623 23:49:18.641667 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:55,  1.79s/it]  6%|▋         | 2/32 [00:02<00:28,  1.07it/s]  9%|▉         | 3/32 [00:02<00:19,  1.49it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.83it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.10it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.73it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 41%|████      | 13/32 [00:06<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 50%|█████     | 16/32 [00:07<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.94it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.94it/s]100%|██████████| 32/32 [00:12<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0623 23:49:35.918000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.918000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.918000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.918000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.918000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.918000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.919000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.945000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.945000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.946000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.946000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.946000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.963000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.963000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.963000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.963000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:49:35.963000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:49:36.304000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:49:36.305000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:49:36.305000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:49:36.305000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:49:36.305000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
I0623 23:49:37.020955 6152 finetune.py:76] layer 1_v @ epoch 1 new loss 1.5634210512871505e-06 old loss 7.873895242482831e-07 WORSE
W0623 23:49:37.242000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.242000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.242000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.242000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.242000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.242000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.242000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.261000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.261000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.261000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.261000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.261000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.515000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.515000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.515000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.515000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:49:37.515000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.737000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.737000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.737000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.738000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.738000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.738000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.738000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.756000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.756000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.756000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.756000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:49:38.756000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:49:39.701000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0623 23:49:39.701000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:49:39.701000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:49:39.701000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:49:39.701000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0623 23:49:46.360168 6660 finetune.py:45] layer 2_v initial loss 1.7347370885545388e-05
W0623 23:49:46.360598 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:49:47.851862 5646 finetune.py:68] layer 0_v @ epoch 3 new loss 4.265575004325228e-09 old loss 4.4242485230938655e-09 BETTER
I0623 23:50:10.108487 6152 finetune.py:76] layer 1_v @ epoch 2 new loss 1.1178193517480395e-06 old loss 7.873895242482831e-07 WORSE
I0623 23:50:17.206610 4970 quantize_finetune_llama.py:193] computed original embedding for layer 3 in 63.48557162284851s
I0623 23:50:20.033653 6660 finetune.py:68] layer 2_v @ epoch 0 new loss 7.436863597831689e-06 old loss 1.7347370885545388e-05 BETTER
I0623 23:50:25.343239 5646 finetune.py:68] layer 0_v @ epoch 4 new loss 4.151470722746353e-09 old loss 4.265575004325228e-09 BETTER
I0623 23:50:26.473626 4970 quantize_finetune_llama.py:162] layer 4 gpu 0
W0623 23:50:26.897413 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_v proxy err 0.0012265217956155539 tr(WHW.T) 4.000687122344971
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.60it/s]I0623 23:50:28.625682 7168 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0623 23:50:28.625826 7168 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0623 23:50:28.625889 7168 utils.py:162] NumExpr defaulting to 16 threads.
I0623 23:50:28.816410 7168 config.py:58] PyTorch version 2.4.0 available.
  6%|▋         | 2/32 [00:00<00:13,  2.18it/s]  9%|▉         | 3/32 [00:01<00:11,  2.49it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.64it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.78it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s]I0623 23:50:31.292589 7168 data_utils.py:336] using 256 training seqs, 128 validation seqs
 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.82it/s]W0623 23:50:31.688691 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 41%|████      | 13/32 [00:04<00:06,  2.89it/s]The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.86it/s] 50%|█████     | 16/32 [00:05<00:05,  2.89it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.93it/s]  3%|▎         | 1/32 [00:01<00:51,  1.66s/it] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s]  6%|▋         | 2/32 [00:02<00:26,  1.12it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.96it/s]  9%|▉         | 3/32 [00:02<00:18,  1.54it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.99it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.00it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.13it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.99it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.99it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.96it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.56it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.97it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.99it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.99it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.71it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.97it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.73it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.94it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.93it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]
 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 50%|█████     | 16/32 [00:07<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s]I0623 23:50:43.443477 6152 finetune.py:76] layer 1_v @ epoch 3 new loss 8.107156190817477e-07 old loss 7.873895242482831e-07 WORSE
 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.82it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0623 23:50:46.457791 5646 finetune.py:45] layer 0_q initial loss 4.4456891501454265e-09
W0623 23:50:46.458182 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0623 23:50:48.868000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.868000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.868000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.868000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.868000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.868000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.868000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.896000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.896000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.896000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.896000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.896000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.913000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.913000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.914000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.914000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:50:48.914000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:50:49.250000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0623 23:50:49.250000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0623 23:50:49.251000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0623 23:50:49.251000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0623 23:50:49.251000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.173000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.173000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.174000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.174000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.174000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.174000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.174000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.192000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.192000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.193000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.193000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.193000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.443000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.443000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.443000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.443000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0623 23:50:50.443000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.656000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.656000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.656000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.656000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.656000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.657000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.657000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.675000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.675000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.675000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.675000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:50:51.675000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0623 23:50:52.625000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0623 23:50:52.625000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0623 23:50:52.625000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0623 23:50:52.625000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0623 23:50:52.625000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0623 23:50:53.578420 6660 finetune.py:68] layer 2_v @ epoch 1 new loss 3.960919002565788e-06 old loss 7.436863597831689e-06 BETTER
I0623 23:50:58.971863 7168 finetune.py:45] layer 3_v initial loss 4.656636156141758e-05
W0623 23:50:58.972064 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:51:17.029710 6152 finetune.py:76] layer 1_v @ epoch 4 new loss 1.6238344642260927e-06 old loss 7.873895242482831e-07 WORSE
W0623 23:51:18.261957 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_v proxy err 0.003062827978283167 tr(WHW.T) 15.92224407196045
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:20,  1.52it/s]  6%|▋         | 2/32 [00:01<00:14,  2.05it/s]I0623 23:51:20.743085 5646 finetune.py:68] layer 0_q @ epoch 0 new loss 4.146826437789741e-09 old loss 4.4456891501454265e-09 BETTER
  9%|▉         | 3/32 [00:01<00:12,  2.32it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.46it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.56it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.62it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.69it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.75it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s]I0623 23:51:27.324734 6660 finetune.py:68] layer 2_v @ epoch 2 new loss 2.6367240479885368e-06 old loss 3.960919002565788e-06 BETTER
 66%|██████▌   | 21/32 [00:07<00:04,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.75it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s]I0623 23:51:31.010463 7168 finetune.py:68] layer 3_v @ epoch 0 new loss 1.3233152458269615e-05 old loss 4.656636156141758e-05 BETTER
 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
I0623 23:51:38.386532 6152 finetune.py:45] layer 1_q initial loss 2.3323555069509894e-06
W0623 23:51:38.386917 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:51:55.815253 5646 finetune.py:68] layer 0_q @ epoch 1 new loss 4.069435455278381e-09 old loss 4.146826437789741e-09 BETTER
I0623 23:52:01.331924 6660 finetune.py:68] layer 2_v @ epoch 3 new loss 2.1018654479121324e-06 old loss 2.6367240479885368e-06 BETTER
I0623 23:52:03.798667 7168 finetune.py:68] layer 3_v @ epoch 1 new loss 6.509009836008772e-06 old loss 1.3233152458269615e-05 BETTER
I0623 23:52:11.058053 6152 finetune.py:68] layer 1_q @ epoch 0 new loss 1.1457490245447843e-06 old loss 2.3323555069509894e-06 BETTER
I0623 23:52:30.659580 5646 finetune.py:68] layer 0_q @ epoch 2 new loss 3.994350183944562e-09 old loss 4.069435455278381e-09 BETTER
I0623 23:52:35.106170 6660 finetune.py:68] layer 2_v @ epoch 4 new loss 1.8693128822633298e-06 old loss 2.1018654479121324e-06 BETTER
I0623 23:52:36.208079 7168 finetune.py:68] layer 3_v @ epoch 2 new loss 4.8586816774331965e-06 old loss 6.509009836008772e-06 BETTER
W0623 23:52:36.622441 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_v proxy err 0.004884745460003614 tr(WHW.T) 135.96121215820312
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.64it/s]  6%|▋         | 2/32 [00:00<00:13,  2.24it/s]  9%|▉         | 3/32 [00:01<00:11,  2.48it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.61it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.72it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.82it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.87it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.88it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.90it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.92it/s] 41%|████      | 13/32 [00:04<00:06,  2.94it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.96it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.97it/s] 50%|█████     | 16/32 [00:05<00:05,  2.97it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.98it/s]I0623 23:52:44.348500 6152 finetune.py:68] layer 1_q @ epoch 1 new loss 7.998855835467111e-07 old loss 1.1457490245447843e-06 BETTER
 59%|█████▉    | 19/32 [00:06<00:04,  2.98it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.98it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.96it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.95it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.95it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.95it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.95it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.95it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]
I0623 23:52:56.446573 6660 finetune.py:45] layer 2_q initial loss 2.2008443920640275e-06
W0623 23:52:56.446980 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:53:06.293296 5646 finetune.py:68] layer 0_q @ epoch 3 new loss 3.929568226368474e-09 old loss 3.994350183944562e-09 BETTER
I0623 23:53:09.717857 7168 finetune.py:68] layer 3_v @ epoch 3 new loss 4.3088075472041965e-06 old loss 4.8586816774331965e-06 BETTER
I0623 23:53:18.091925 6152 finetune.py:76] layer 1_q @ epoch 2 new loss 8.797667874205217e-07 old loss 7.998855835467111e-07 WORSE
I0623 23:53:30.172348 6660 finetune.py:68] layer 2_q @ epoch 0 new loss 2.0248135115252808e-06 old loss 2.2008443920640275e-06 BETTER
I0623 23:53:41.814922 5646 finetune.py:68] layer 0_q @ epoch 4 new loss 3.881471588584873e-09 old loss 3.929568226368474e-09 BETTER
I0623 23:53:42.575670 7168 finetune.py:68] layer 3_v @ epoch 4 new loss 4.052997610415332e-06 old loss 4.3088075472041965e-06 BETTER
W0623 23:53:43.347128 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0623 23:53:44.174986 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_q proxy err 1.3185972420615144e-05 tr(WHW.T) 2662.06005859375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.69it/s]3_v proxy err 0.007330543827265501 tr(WHW.T) 288.3954162597656
  0%|          | 0/32 [00:00<?, ?it/s]  6%|▋         | 2/32 [00:00<00:13,  2.28it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s]  3%|▎         | 1/32 [00:00<00:20,  1.53it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.67it/s]  6%|▋         | 2/32 [00:01<00:14,  2.08it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s]  9%|▉         | 3/32 [00:01<00:12,  2.33it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.47it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.82it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.55it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.84it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.87it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.89it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 41%|████      | 13/32 [00:04<00:06,  2.90it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.90it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.66it/s] 50%|█████     | 16/32 [00:05<00:05,  2.91it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s]I0623 23:53:50.797001 6152 finetune.py:68] layer 1_q @ epoch 3 new loss 6.934382668077888e-07 old loss 7.998855835467111e-07 BETTER
 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.75it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.97it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.78it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.96it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.98it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.00it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.00it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.81it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.99it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.97it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.77it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.98it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.00it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.96it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]
 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
I0623 23:54:02.458066 5646 finetune.py:45] layer 0_k initial loss 4.0303516080086865e-09
W0623 23:54:02.458529 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:54:04.264554 7168 finetune.py:45] layer 3_q initial loss 5.687848442903487e-06
W0623 23:54:04.264898 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:54:04.321879 6660 finetune.py:68] layer 2_q @ epoch 1 new loss 1.9601554868131643e-06 old loss 2.0248135115252808e-06 BETTER
I0623 23:54:23.737544 6152 finetune.py:76] layer 1_q @ epoch 4 new loss 9.315662055087159e-07 old loss 6.934382668077888e-07 WORSE
W0623 23:54:24.810611 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_q proxy err 7.655804802197963e-05 tr(WHW.T) 4480.3662109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]  6%|▋         | 2/32 [00:00<00:13,  2.19it/s]  9%|▉         | 3/32 [00:01<00:11,  2.43it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.55it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.63it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.68it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.75it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.81it/s] 41%|████      | 13/32 [00:04<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s] 50%|█████     | 16/32 [00:05<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.73it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.73it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s]I0623 23:54:36.312952 7168 finetune.py:68] layer 3_q @ epoch 0 new loss 5.3181315706751775e-06 old loss 5.687848442903487e-06 BETTER
 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s]I0623 23:54:36.374085 5646 finetune.py:68] layer 0_k @ epoch 0 new loss 3.922251856636194e-09 old loss 4.0303516080086865e-09 BETTER
 91%|█████████ | 29/32 [00:10<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
I0623 23:54:38.144643 6660 finetune.py:68] layer 2_q @ epoch 2 new loss 1.921125431181281e-06 old loss 1.9601554868131643e-06 BETTER
I0623 23:54:44.643454 6152 finetune.py:45] layer 1_k initial loss 2.842313278961228e-06
W0623 23:54:44.643774 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:55:08.791875 7168 finetune.py:68] layer 3_q @ epoch 1 new loss 5.149862772668712e-06 old loss 5.3181315706751775e-06 BETTER
I0623 23:55:10.704084 5646 finetune.py:68] layer 0_k @ epoch 1 new loss 3.877948628883132e-09 old loss 3.922251856636194e-09 BETTER
I0623 23:55:12.102571 6660 finetune.py:68] layer 2_q @ epoch 3 new loss 1.8921906530522392e-06 old loss 1.921125431181281e-06 BETTER
I0623 23:55:17.448348 6152 finetune.py:68] layer 1_k @ epoch 0 new loss 9.073239084500528e-07 old loss 2.842313278961228e-06 BETTER
I0623 23:55:41.282051 7168 finetune.py:68] layer 3_q @ epoch 2 new loss 5.034662535763346e-06 old loss 5.149862772668712e-06 BETTER
I0623 23:55:45.535439 5646 finetune.py:68] layer 0_k @ epoch 2 new loss 3.841630569212384e-09 old loss 3.877948628883132e-09 BETTER
I0623 23:55:46.430467 6660 finetune.py:68] layer 2_q @ epoch 4 new loss 1.8683214193515596e-06 old loss 1.8921906530522392e-06 BETTER
W0623 23:55:47.854275 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_q proxy err 0.0003197755431756377 tr(WHW.T) 7194.947265625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.69it/s]  6%|▋         | 2/32 [00:00<00:13,  2.28it/s]  9%|▉         | 3/32 [00:01<00:11,  2.55it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.71it/s]I0623 23:55:50.650514 6152 finetune.py:68] layer 1_k @ epoch 1 new loss 8.292853408420342e-07 old loss 9.073239084500528e-07 BETTER
 16%|█▌        | 5/32 [00:01<00:09,  2.82it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.88it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.92it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.95it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.96it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.97it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s] 41%|████      | 13/32 [00:04<00:06,  2.96it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 50%|█████     | 16/32 [00:05<00:05,  2.95it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.97it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.98it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.98it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.99it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.00it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.98it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.99it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.98it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.98it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.99it/s]100%|██████████| 32/32 [00:10<00:00,  3.00it/s]100%|██████████| 32/32 [00:10<00:00,  2.91it/s]
I0623 23:56:06.930505 6660 finetune.py:45] layer 2_k initial loss 2.2771305339119863e-06
W0623 23:56:06.930787 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:56:14.203302 7168 finetune.py:68] layer 3_q @ epoch 3 new loss 4.947382421960356e-06 old loss 5.034662535763346e-06 BETTER
I0623 23:56:20.617106 5646 finetune.py:68] layer 0_k @ epoch 3 new loss 3.8189122975040846e-09 old loss 3.841630569212384e-09 BETTER
I0623 23:56:23.836036 6152 finetune.py:76] layer 1_k @ epoch 2 new loss 1.3541497310143313e-06 old loss 8.292853408420342e-07 WORSE
I0623 23:56:40.205794 6660 finetune.py:68] layer 2_k @ epoch 0 new loss 2.210660795753938e-06 old loss 2.2771305339119863e-06 BETTER
I0623 23:56:46.621114 7168 finetune.py:68] layer 3_q @ epoch 4 new loss 4.878979325440014e-06 old loss 4.947382421960356e-06 BETTER
W0623 23:56:48.169295 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_q proxy err 0.0010807232465595007 tr(WHW.T) 6735.1181640625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.67it/s]  6%|▋         | 2/32 [00:00<00:13,  2.20it/s]  9%|▉         | 3/32 [00:01<00:11,  2.43it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.56it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.62it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s] 41%|████      | 13/32 [00:04<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s]I0623 23:56:55.451478 5646 finetune.py:68] layer 0_k @ epoch 4 new loss 3.788990454722807e-09 old loss 3.8189122975040846e-09 BETTER
 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s]I0623 23:56:56.390587 6152 finetune.py:76] layer 1_k @ epoch 3 new loss 9.677359003035235e-07 old loss 8.292853408420342e-07 WORSE
 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s]W0623 23:56:56.945450 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:07<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s]0_k proxy err 6.423423656087834e-06 tr(WHW.T) 1663.4017333984375
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s]  3%|▎         | 1/32 [00:00<00:17,  1.73it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s]  6%|▋         | 2/32 [00:00<00:12,  2.31it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.83it/s]  9%|▉         | 3/32 [00:01<00:11,  2.59it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.74it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.84it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.89it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.87it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.93it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.86it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
 28%|██▊       | 9/32 [00:03<00:07,  2.94it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.94it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.97it/s] 41%|████      | 13/32 [00:04<00:06,  2.98it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.99it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.99it/s] 50%|█████     | 16/32 [00:05<00:05,  2.98it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.98it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.97it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.95it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.95it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.96it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.96it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.97it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.97it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.97it/s]I0623 23:57:07.739510 7168 finetune.py:45] layer 3_k initial loss 6.348290298774373e-06
W0623 23:57:07.739970 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 91%|█████████ | 29/32 [00:09<00:01,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.97it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.98it/s]100%|██████████| 32/32 [00:10<00:00,  2.97it/s]100%|██████████| 32/32 [00:10<00:00,  2.91it/s]
I0623 23:57:14.044850 6660 finetune.py:68] layer 2_k @ epoch 1 new loss 2.1862379071535543e-06 old loss 2.210660795753938e-06 BETTER
I0623 23:57:15.943839 5646 finetune.py:45] layer 0_o initial loss 9.674344880750141e-09
W0623 23:57:15.944200 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:57:29.161822 6152 finetune.py:76] layer 1_k @ epoch 4 new loss 1.0668600225471891e-06 old loss 8.292853408420342e-07 WORSE
W0623 23:57:30.160471 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_k proxy err 7.294867828022689e-05 tr(WHW.T) 4715.099609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.59it/s]  6%|▋         | 2/32 [00:00<00:14,  2.12it/s]  9%|▉         | 3/32 [00:01<00:12,  2.37it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.50it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.58it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.62it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.75it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s] 41%|████      | 13/32 [00:04<00:06,  2.76it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.73it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s]I0623 23:57:39.873375 7168 finetune.py:68] layer 3_k @ epoch 0 new loss 6.1291484598768875e-06 old loss 6.348290298774373e-06 BETTER
 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
I0623 23:57:47.986787 6660 finetune.py:68] layer 2_k @ epoch 2 new loss 2.167343836845248e-06 old loss 2.1862379071535543e-06 BETTER
I0623 23:57:49.632945 5646 finetune.py:68] layer 0_o @ epoch 0 new loss 9.262347333560683e-09 old loss 9.674344880750141e-09 BETTER
I0623 23:57:50.224438 6152 finetune.py:45] layer 1_o initial loss 1.6979759038804332e-06
W0623 23:57:50.225173 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:58:12.853981 7168 finetune.py:68] layer 3_k @ epoch 1 new loss 6.0628563005593605e-06 old loss 6.1291484598768875e-06 BETTER
I0623 23:58:21.378675 6660 finetune.py:68] layer 2_k @ epoch 3 new loss 2.1507098608708475e-06 old loss 2.167343836845248e-06 BETTER
I0623 23:58:22.040099 6152 finetune.py:68] layer 1_o @ epoch 0 new loss 1.044177793119161e-06 old loss 1.6979759038804332e-06 BETTER
I0623 23:58:23.844130 5646 finetune.py:68] layer 0_o @ epoch 1 new loss 9.100703302067359e-09 old loss 9.262347333560683e-09 BETTER
I0623 23:58:45.342033 7168 finetune.py:68] layer 3_k @ epoch 2 new loss 6.014365681039635e-06 old loss 6.0628563005593605e-06 BETTER
I0623 23:58:54.967417 6152 finetune.py:68] layer 1_o @ epoch 1 new loss 9.109081133829022e-07 old loss 1.044177793119161e-06 BETTER
I0623 23:58:55.133182 6660 finetune.py:68] layer 2_k @ epoch 4 new loss 2.1360815480875317e-06 old loss 2.1507098608708475e-06 BETTER
W0623 23:58:56.495890 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_k proxy err 0.00026948918821290135 tr(WHW.T) 9459.30078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:17,  1.73it/s]  6%|▋         | 2/32 [00:00<00:12,  2.33it/s]I0623 23:58:58.532548 5646 finetune.py:68] layer 0_o @ epoch 2 new loss 8.958351394028341e-09 old loss 9.100703302067359e-09 BETTER
  9%|▉         | 3/32 [00:01<00:11,  2.59it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.72it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.87it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.91it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.95it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.96it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.98it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.99it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.97it/s] 41%|████      | 13/32 [00:04<00:06,  2.98it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.98it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.99it/s] 50%|█████     | 16/32 [00:05<00:05,  2.98it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.98it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.00it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.02it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.03it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.04it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.04it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.03it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.02it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.02it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.03it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.02it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.00it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.02it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.00it/s]100%|██████████| 32/32 [00:10<00:00,  2.99it/s]100%|██████████| 32/32 [00:10<00:00,  2.94it/s]
I0623 23:59:16.005751 6660 finetune.py:45] layer 2_o initial loss 4.833021193917375e-06
W0623 23:59:16.006155 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0623 23:59:17.535391 7168 finetune.py:68] layer 3_k @ epoch 3 new loss 5.970868187432643e-06 old loss 6.014365681039635e-06 BETTER
I0623 23:59:28.420018 6152 finetune.py:76] layer 1_o @ epoch 2 new loss 1.1785549531850847e-06 old loss 9.109081133829022e-07 WORSE
I0623 23:59:33.440420 5646 finetune.py:68] layer 0_o @ epoch 3 new loss 8.843592524954147e-09 old loss 8.958351394028341e-09 BETTER
I0623 23:59:49.059103 6660 finetune.py:68] layer 2_o @ epoch 0 new loss 4.661966613639379e-06 old loss 4.833021193917375e-06 BETTER
I0623 23:59:50.050434 7168 finetune.py:68] layer 3_k @ epoch 4 new loss 5.936465186096029e-06 old loss 5.970868187432643e-06 BETTER
W0623 23:59:51.463327 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_k proxy err 0.000844539375975728 tr(WHW.T) 9335.904296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.67it/s]  6%|▋         | 2/32 [00:00<00:13,  2.23it/s]  9%|▉         | 3/32 [00:01<00:11,  2.48it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.60it/s] 16%|█▌        | 5/32 [00:01<00:10,  2.69it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.77it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.78it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.90it/s] 41%|████      | 13/32 [00:04<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.85it/s] 50%|█████     | 16/32 [00:05<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.83it/s]I0624 00:00:01.219310 6152 finetune.py:76] layer 1_o @ epoch 3 new loss 9.467042332289566e-07 old loss 9.109081133829022e-07 WORSE
 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
I0624 00:00:08.389393 5646 finetune.py:68] layer 0_o @ epoch 4 new loss 8.75564687419228e-09 old loss 8.843592524954147e-09 BETTER
W0624 00:00:10.001509 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:00:10.713831 7168 finetune.py:45] layer 3_o initial loss 1.2093036275473423e-05
W0624 00:00:10.714218 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

0_o proxy err 3.126413866993971e-05 tr(WHW.T) 1.0134251117706299
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:02<00:33,  1.11s/it]  9%|▉         | 3/32 [00:03<00:30,  1.05s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.03s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.02s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.01s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.01s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.01s/it]I0624 00:00:22.530247 6660 finetune.py:68] layer 2_o @ epoch 1 new loss 4.565506060316693e-06 old loss 4.661966613639379e-06 BETTER
 34%|███▍      | 11/32 [00:11<00:21,  1.01s/it] 38%|███▊      | 12/32 [00:12<00:19,  1.00it/s] 41%|████      | 13/32 [00:13<00:18,  1.01it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.01it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 50%|█████     | 16/32 [00:16<00:15,  1.02it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.02it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.02it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.02it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.02it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.02it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.02it/s]I0624 00:00:33.744354 6152 finetune.py:76] layer 1_o @ epoch 4 new loss 9.556703162161284e-07 old loss 9.109081133829022e-07 WORSE
 72%|███████▏  | 23/32 [00:23<00:08,  1.02it/s]W0624 00:00:34.700265 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:24<00:07,  1.02it/s]1_o proxy err 0.0007639802643097937 tr(WHW.T) 1.0817909240722656
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:25<00:06,  1.02it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.02it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.02it/s]  6%|▋         | 2/32 [00:02<00:35,  1.17s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.02it/s]  9%|▉         | 3/32 [00:03<00:32,  1.13s/it] 91%|█████████ | 29/32 [00:28<00:02,  1.02it/s] 12%|█▎        | 4/32 [00:04<00:30,  1.10s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.02it/s] 16%|█▌        | 5/32 [00:05<00:29,  1.09s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.02it/s]I0624 00:00:42.366940 7168 finetune.py:68] layer 3_o @ epoch 0 new loss 1.1470691788417753e-05 old loss 1.2093036275473423e-05 BETTER
 19%|█▉        | 6/32 [00:06<00:28,  1.08s/it]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]100%|██████████| 32/32 [00:31<00:00,  1.00it/s]
 22%|██▏       | 7/32 [00:07<00:26,  1.08s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.06s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.05s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.05s/it] 34%|███▍      | 11/32 [00:11<00:22,  1.05s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.05s/it]W0624 00:00:49.929000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.929000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.929000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.929000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.929000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.929000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.930000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
 41%|████      | 13/32 [00:13<00:19,  1.04s/it]W0624 00:00:49.958000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.958000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.958000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.959000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.959000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.973000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.973000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.973000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.973000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:00:49.974000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.132000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.132000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.132000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.132000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.132000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.360000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.360000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.360000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.360000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.361000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.361000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.361000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.381000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.381000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.381000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.381000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.382000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.445000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.445000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.445000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.445000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:00:50.445000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 14/32 [00:15<00:18,  1.04s/it]W0624 00:00:51.322000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.640000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.640000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.640000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.640000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.640000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.640000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.640000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.661000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.661000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.661000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.662000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.662000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.925000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.926000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.926000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.926000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:00:51.926000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 15/32 [00:16<00:17,  1.04s/it]W0624 00:00:52.195000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:17<00:16,  1.04s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.03s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.03s/it]I0624 00:00:55.963114 6660 finetune.py:68] layer 2_o @ epoch 2 new loss 4.497020654525841e-06 old loss 4.565506060316693e-06 BETTER
 59%|█████▉    | 19/32 [00:20<00:13,  1.03s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.04s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.04s/it]I0624 00:00:58.952726 5646 finetune.py:45] layer 0_up initial loss 8.01092525648528e-08
W0624 00:00:58.953207 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:23<00:10,  1.04s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.04s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.04s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.04s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.05s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.05s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.05s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.05s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]100%|██████████| 32/32 [00:33<00:00,  1.06s/it]
I0624 00:01:14.777358 7168 finetune.py:68] layer 3_o @ epoch 1 new loss 1.1203898793610279e-05 old loss 1.1470691788417753e-05 BETTER
W0624 00:01:16.486000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.486000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.486000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.487000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.487000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.487000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.487000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.517000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.517000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.517000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.517000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.517000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.533000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.533000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.533000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.533000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.533000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.686000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.686000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.686000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.686000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.686000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.918000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.918000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.919000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.919000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.919000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.919000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.919000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.941000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.941000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.942000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.942000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:01:16.942000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:01:17.006000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:01:17.006000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:01:17.006000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:01:17.006000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:01:17.006000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:01:17.873000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.186000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.186000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.186000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.186000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.186000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.186000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.186000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.208000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.208000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.208000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.208000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.208000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.479000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.480000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.480000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.480000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.480000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:01:18.746000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 00:01:25.186832 6152 finetune.py:45] layer 1_up initial loss 1.3636097719427198e-05
W0624 00:01:25.187162 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:01:29.511160 6660 finetune.py:68] layer 2_o @ epoch 3 new loss 4.447269020602107e-06 old loss 4.497020654525841e-06 BETTER
I0624 00:01:31.841903 5646 finetune.py:68] layer 0_up @ epoch 0 new loss 7.280053893055083e-08 old loss 8.01092525648528e-08 BETTER
I0624 00:01:47.333299 7168 finetune.py:68] layer 3_o @ epoch 2 new loss 1.1055820323235821e-05 old loss 1.1203898793610279e-05 BETTER
I0624 00:01:56.634533 6152 finetune.py:68] layer 1_up @ epoch 0 new loss 1.8196674318460282e-06 old loss 1.3636097719427198e-05 BETTER
I0624 00:02:03.212888 6660 finetune.py:68] layer 2_o @ epoch 4 new loss 4.407047526910901e-06 old loss 4.447269020602107e-06 BETTER
W0624 00:02:04.617776 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_o proxy err 0.005354640539735556 tr(WHW.T) 1.168103575706482
  0%|          | 0/32 [00:00<?, ?it/s]I0624 00:02:06.000802 5646 finetune.py:68] layer 0_up @ epoch 1 new loss 7.215950859063014e-08 old loss 7.280053893055083e-08 BETTER
  3%|▎         | 1/32 [00:01<00:39,  1.29s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.07s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.06s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.04s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.04s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 41%|████      | 13/32 [00:13<00:19,  1.04s/it]I0624 00:02:19.871268 7168 finetune.py:68] layer 3_o @ epoch 3 new loss 1.0959292012557853e-05 old loss 1.1055820323235821e-05 BETTER
 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.04s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.04s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.04s/it]I0624 00:02:28.975404 6152 finetune.py:68] layer 1_up @ epoch 1 new loss 1.8063291236103396e-06 old loss 1.8196674318460282e-06 BETTER
 72%|███████▏  | 23/32 [00:24<00:09,  1.03s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.03s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.04s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.04s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.04s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.06s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.06s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]
I0624 00:02:40.194149 5646 finetune.py:68] layer 0_up @ epoch 2 new loss 7.186589812135935e-08 old loss 7.215950859063014e-08 BETTER
W0624 00:02:46.010000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.010000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.010000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.010000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.010000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.011000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.011000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.039000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.039000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.039000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.039000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.039000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.054000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.054000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.054000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.054000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.054000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.211000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.211000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.211000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.211000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.211000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.442000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.442000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.442000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.442000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.443000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.443000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.443000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.464000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.465000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.465000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.465000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.465000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.526000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.526000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.526000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.527000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:02:46.527000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.391000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.700000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.700000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.700000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.701000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.701000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.701000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.701000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.973000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.973000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.973000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.973000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:02:47.973000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:02:48.234000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 00:02:52.219982 7168 finetune.py:68] layer 3_o @ epoch 4 new loss 1.0889948498515878e-05 old loss 1.0959292012557853e-05 BETTER
W0624 00:02:53.614549 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:02:54.862997 6660 finetune.py:45] layer 2_up initial loss 7.984177500475198e-06
W0624 00:02:54.863437 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

3_o proxy err 0.004841281101107597 tr(WHW.T) 3.1955478191375732
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.34s/it]  6%|▋         | 2/32 [00:02<00:35,  1.18s/it]  9%|▉         | 3/32 [00:03<00:32,  1.12s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.10s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.09s/it]I0624 00:03:01.209163 6152 finetune.py:76] layer 1_up @ epoch 2 new loss 1.8980050526806735e-06 old loss 1.8063291236103396e-06 WORSE
 19%|█▉        | 6/32 [00:06<00:28,  1.09s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.07s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.06s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.05s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.04s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it]I0624 00:03:14.166207 5646 finetune.py:68] layer 0_up @ epoch 3 new loss 7.166808302372374e-08 old loss 7.186589812135935e-08 BETTER
 59%|█████▉    | 19/32 [00:20<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.02s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.02s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.02s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.03s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.02s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.02s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.02s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.02s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.02s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.02s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.02s/it]I0624 00:03:26.827684 6660 finetune.py:68] layer 2_up @ epoch 0 new loss 7.926579201011918e-06 old loss 7.984177500475198e-06 BETTER
 97%|█████████▋| 31/32 [00:32<00:01,  1.02s/it]100%|██████████| 32/32 [00:33<00:00,  1.02s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
I0624 00:03:33.015524 6152 finetune.py:68] layer 1_up @ epoch 3 new loss 1.7925420934261638e-06 old loss 1.8063291236103396e-06 BETTER
W0624 00:03:34.986000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:03:34.987000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:03:34.987000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:03:34.987000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:03:34.987000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:03:34.987000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:03:34.987000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.018000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.018000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.018000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.018000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.018000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.034000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.035000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.035000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.035000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.035000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.189000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.189000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.189000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.189000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.189000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.417000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.417000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.417000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.417000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.417000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.417000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.417000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.438000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.438000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.438000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.439000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.439000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.505000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.505000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.505000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.506000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:03:35.506000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.378000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.685000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.686000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.686000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.686000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.686000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.686000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.686000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.707000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.707000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.707000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.708000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.708000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.962000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.963000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.963000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.963000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:03:36.963000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:03:37.237000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 00:03:43.578491 7168 finetune.py:45] layer 3_up initial loss 2.0227809727657586e-05
W0624 00:03:43.578868 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:03:48.046339 5646 finetune.py:68] layer 0_up @ epoch 4 new loss 7.155445302942098e-08 old loss 7.166808302372374e-08 BETTER
W0624 00:03:49.305736 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_up proxy err 0.003619072027504444 tr(WHW.T) 42.65648651123047
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it]  9%|▉         | 3/32 [00:03<00:31,  1.07s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it]I0624 00:03:59.375789 6660 finetune.py:68] layer 2_up @ epoch 1 new loss 7.897856448835228e-06 old loss 7.926579201011918e-06 BETTER
 28%|██▊       | 9/32 [00:09<00:22,  1.00it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.01it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.01it/s] 41%|████      | 13/32 [00:13<00:18,  1.02it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.02it/s]I0624 00:04:05.186668 6152 finetune.py:76] layer 1_up @ epoch 4 new loss 1.8400954786557122e-06 old loss 1.7925420934261638e-06 WORSE
 47%|████▋     | 15/32 [00:15<00:16,  1.02it/s]W0624 00:04:06.165099 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 50%|█████     | 16/32 [00:16<00:15,  1.02it/s]1_up proxy err 0.005524671636521816 tr(WHW.T) 108.75288391113281
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.02it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.02it/s]  3%|▎         | 1/32 [00:01<00:42,  1.37s/it] 59%|█████▉    | 19/32 [00:19<00:12,  1.02it/s]  6%|▋         | 2/32 [00:02<00:35,  1.18s/it] 62%|██████▎   | 20/32 [00:20<00:11,  1.02it/s]  9%|▉         | 3/32 [00:03<00:32,  1.13s/it] 66%|██████▌   | 21/32 [00:21<00:10,  1.02it/s] 12%|█▎        | 4/32 [00:04<00:31,  1.11s/it] 69%|██████▉   | 22/32 [00:22<00:09,  1.02it/s] 16%|█▌        | 5/32 [00:05<00:29,  1.10s/it] 72%|███████▏  | 23/32 [00:23<00:08,  1.02it/s]I0624 00:04:14.266390 7168 finetune.py:68] layer 3_up @ epoch 0 new loss 2.009806121350266e-05 old loss 2.0227809727657586e-05 BETTER
 19%|█▉        | 6/32 [00:06<00:28,  1.09s/it] 75%|███████▌  | 24/32 [00:24<00:07,  1.02it/s] 22%|██▏       | 7/32 [00:07<00:27,  1.08s/it] 78%|███████▊  | 25/32 [00:25<00:06,  1.02it/s] 25%|██▌       | 8/32 [00:08<00:25,  1.07s/it] 81%|████████▏ | 26/32 [00:25<00:05,  1.02it/s] 28%|██▊       | 9/32 [00:09<00:24,  1.07s/it] 84%|████████▍ | 27/32 [00:26<00:04,  1.02it/s] 31%|███▏      | 10/32 [00:10<00:23,  1.07s/it] 88%|████████▊ | 28/32 [00:27<00:03,  1.02it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.02it/s] 34%|███▍      | 11/32 [00:12<00:22,  1.07s/it] 94%|█████████▍| 30/32 [00:29<00:01,  1.02it/s] 38%|███▊      | 12/32 [00:13<00:21,  1.06s/it] 97%|█████████▋| 31/32 [00:30<00:00,  1.02it/s] 41%|████      | 13/32 [00:14<00:20,  1.07s/it]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]100%|██████████| 32/32 [00:31<00:00,  1.00it/s]
 44%|████▍     | 14/32 [00:15<00:19,  1.07s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.06s/it] 50%|█████     | 16/32 [00:17<00:17,  1.06s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.06s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.06s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.06s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.06s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.06s/it]I0624 00:04:30.232296 5646 finetune.py:45] layer 0_gate initial loss 1.4065194875456655e-07
W0624 00:04:30.232682 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:23<00:10,  1.06s/it]I0624 00:04:31.990920 6660 finetune.py:68] layer 2_up @ epoch 2 new loss 7.87476255936781e-06 old loss 7.897856448835228e-06 BETTER
 72%|███████▏  | 23/32 [00:24<00:09,  1.06s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.06s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.06s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.06s/it] 84%|████████▍ | 27/32 [00:29<00:05,  1.06s/it] 88%|████████▊ | 28/32 [00:30<00:04,  1.06s/it] 91%|█████████ | 29/32 [00:31<00:03,  1.06s/it] 94%|█████████▍| 30/32 [00:32<00:02,  1.06s/it] 97%|█████████▋| 31/32 [00:33<00:01,  1.06s/it]100%|██████████| 32/32 [00:34<00:00,  1.07s/it]100%|██████████| 32/32 [00:34<00:00,  1.07s/it]
I0624 00:04:45.722196 7168 finetune.py:68] layer 3_up @ epoch 1 new loss 2.0037114154547453e-05 old loss 2.009806121350266e-05 BETTER
I0624 00:04:49.471395 6152 finetune.py:45] layer 1_gate initial loss 1.1996082321275026e-05
W0624 00:04:49.471760 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:05:02.666084 5646 finetune.py:68] layer 0_gate @ epoch 0 new loss 1.2996717657642876e-07 old loss 1.4065194875456655e-07 BETTER
I0624 00:05:04.709676 6660 finetune.py:68] layer 2_up @ epoch 3 new loss 7.855981493776198e-06 old loss 7.87476255936781e-06 BETTER
I0624 00:05:17.640084 7168 finetune.py:68] layer 3_up @ epoch 2 new loss 1.9985591279692017e-05 old loss 2.0037114154547453e-05 BETTER
I0624 00:05:20.225939 6152 finetune.py:68] layer 1_gate @ epoch 0 new loss 2.5627139166317647e-06 old loss 1.1996082321275026e-05 BETTER
I0624 00:05:35.518402 5646 finetune.py:68] layer 0_gate @ epoch 1 new loss 1.2914259173157916e-07 old loss 1.2996717657642876e-07 BETTER
I0624 00:05:38.122630 6660 finetune.py:68] layer 2_up @ epoch 4 new loss 7.839142199372873e-06 old loss 7.855981493776198e-06 BETTER
W0624 00:05:39.395168 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_up proxy err 0.00828438252210617 tr(WHW.T) 195.6568145751953
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:02<00:35,  1.18s/it]  9%|▉         | 3/32 [00:03<00:32,  1.11s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.08s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.07s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.06s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it]I0624 00:05:49.164632 7168 finetune.py:68] layer 3_up @ epoch 3 new loss 1.993912155739963e-05 old loss 1.9985591279692017e-05 BETTER
 25%|██▌       | 8/32 [00:08<00:25,  1.05s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it]I0624 00:05:51.352415 6152 finetune.py:68] layer 1_gate @ epoch 1 new loss 2.380645582888974e-06 old loss 2.5627139166317647e-06 BETTER
 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.04s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.04s/it] 41%|████      | 13/32 [00:13<00:19,  1.04s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.04s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.05s/it] 50%|█████     | 16/32 [00:16<00:16,  1.04s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.04s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.04s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.04s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.04s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.04s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.04s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.05s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.06s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.06s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.06s/it]I0624 00:06:08.350867 5646 finetune.py:68] layer 0_gate @ epoch 2 new loss 1.288275086608337e-07 old loss 1.2914259173157916e-07 BETTER
 84%|████████▍ | 27/32 [00:28<00:05,  1.05s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.05s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.04s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]
I0624 00:06:20.584562 7168 finetune.py:68] layer 3_up @ epoch 4 new loss 1.989869815588463e-05 old loss 1.993912155739963e-05 BETTER
W0624 00:06:21.965927 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:06:22.406907 6152 finetune.py:76] layer 1_gate @ epoch 2 new loss 2.4592529825895326e-06 old loss 2.380645582888974e-06 WORSE
I0624 00:06:22.465748 6660 finetune.py:45] layer 2_gate initial loss 1.0800084965012502e-05
W0624 00:06:22.466132 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

3_up proxy err 0.010433141142129898 tr(WHW.T) 289.7748718261719
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.34s/it]  6%|▋         | 2/32 [00:02<00:34,  1.16s/it]  9%|▉         | 3/32 [00:03<00:32,  1.11s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.07s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.06s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.04s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.04s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.04s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.02s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it]I0624 00:06:41.124827 5646 finetune.py:68] layer 0_gate @ epoch 3 new loss 1.2868795806753042e-07 old loss 1.288275086608337e-07 BETTER
 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.03s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.03s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.03s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.04s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.04s/it]I0624 00:06:53.095000 6152 finetune.py:76] layer 1_gate @ epoch 3 new loss 2.4092557850963203e-06 old loss 2.380645582888974e-06 WORSE
I0624 00:06:53.188536 6660 finetune.py:68] layer 2_gate @ epoch 0 new loss 1.0747629858087748e-05 old loss 1.0800084965012502e-05 BETTER
 91%|█████████ | 29/32 [00:30<00:03,  1.04s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
I0624 00:07:03.801815 7168 finetune.py:45] layer 3_gate initial loss 2.769359343801625e-05
W0624 00:07:03.802170 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:07:13.566855 5646 finetune.py:68] layer 0_gate @ epoch 4 new loss 1.2853018915848224e-07 old loss 1.2868795806753042e-07 BETTER
W0624 00:07:14.587115 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_gate proxy err 0.002664976753294468 tr(WHW.T) 60.00325012207031
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:49,  1.72it/s]  2%|▏         | 2/86 [00:00<00:36,  2.30it/s]  3%|▎         | 3/86 [00:01<00:31,  2.60it/s]  5%|▍         | 4/86 [00:01<00:29,  2.77it/s]  6%|▌         | 5/86 [00:01<00:28,  2.87it/s]  7%|▋         | 6/86 [00:02<00:27,  2.91it/s]  8%|▊         | 7/86 [00:02<00:27,  2.92it/s]  9%|▉         | 8/86 [00:02<00:26,  2.94it/s] 10%|█         | 9/86 [00:03<00:26,  2.93it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.94it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.95it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.94it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.95it/s] 16%|█▋        | 14/86 [00:04<00:24,  2.96it/s] 17%|█▋        | 15/86 [00:05<00:23,  2.97it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.98it/s] 20%|█▉        | 17/86 [00:05<00:23,  2.98it/s]I0624 00:07:23.625991 6152 finetune.py:76] layer 1_gate @ epoch 4 new loss 2.520603175071301e-06 old loss 2.380645582888974e-06 WORSE
 21%|██        | 18/86 [00:06<00:22,  2.99it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.99it/s]W0624 00:07:24.398761 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 23%|██▎       | 20/86 [00:06<00:22,  3.00it/s]I0624 00:07:24.537822 6660 finetune.py:68] layer 2_gate @ epoch 1 new loss 1.0732367627497297e-05 old loss 1.0747629858087748e-05 BETTER
 24%|██▍       | 21/86 [00:07<00:21,  3.05it/s] 26%|██▌       | 22/86 [00:07<00:20,  3.06it/s] 27%|██▋       | 23/86 [00:07<00:20,  3.04it/s] 28%|██▊       | 24/86 [00:08<00:20,  3.02it/s] 29%|██▉       | 25/86 [00:08<00:20,  3.01it/s] 30%|███       | 26/86 [00:08<00:19,  3.03it/s] 31%|███▏      | 27/86 [00:09<00:19,  3.03it/s] 33%|███▎      | 28/86 [00:09<00:19,  3.04it/s]1_gate proxy err 0.0032457862980663776 tr(WHW.T) 209.63397216796875
  0%|          | 0/86 [00:00<?, ?it/s] 34%|███▎      | 29/86 [00:09<00:18,  3.04it/s] 35%|███▍      | 30/86 [00:10<00:18,  3.04it/s]  1%|          | 1/86 [00:00<00:52,  1.63it/s] 36%|███▌      | 31/86 [00:10<00:18,  3.04it/s]  2%|▏         | 2/86 [00:00<00:38,  2.17it/s] 37%|███▋      | 32/86 [00:10<00:17,  3.06it/s]  3%|▎         | 3/86 [00:01<00:34,  2.42it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.06it/s]  5%|▍         | 4/86 [00:01<00:32,  2.54it/s] 40%|███▉      | 34/86 [00:11<00:16,  3.08it/s]  6%|▌         | 5/86 [00:02<00:30,  2.63it/s] 41%|████      | 35/86 [00:11<00:16,  3.06it/s]  7%|▋         | 6/86 [00:02<00:29,  2.69it/s] 42%|████▏     | 36/86 [00:12<00:16,  3.05it/s]  8%|▊         | 7/86 [00:02<00:29,  2.71it/s] 43%|████▎     | 37/86 [00:12<00:16,  3.05it/s]  9%|▉         | 8/86 [00:03<00:28,  2.74it/s] 44%|████▍     | 38/86 [00:12<00:15,  3.05it/s] 45%|████▌     | 39/86 [00:13<00:15,  3.03it/s] 10%|█         | 9/86 [00:03<00:28,  2.74it/s] 47%|████▋     | 40/86 [00:13<00:15,  3.04it/s] 12%|█▏        | 10/86 [00:03<00:27,  2.76it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.02it/s] 13%|█▎        | 11/86 [00:04<00:27,  2.77it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.03it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.78it/s] 50%|█████     | 43/86 [00:14<00:14,  3.03it/s] 15%|█▌        | 13/86 [00:04<00:26,  2.76it/s] 51%|█████     | 44/86 [00:14<00:13,  3.05it/s] 16%|█▋        | 14/86 [00:05<00:26,  2.76it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.05it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.76it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.05it/s] 19%|█▊        | 16/86 [00:06<00:25,  2.75it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.05it/s] 20%|█▉        | 17/86 [00:06<00:25,  2.74it/s]I0624 00:07:33.678435 7168 finetune.py:68] layer 3_gate @ epoch 0 new loss 2.755823516054079e-05 old loss 2.769359343801625e-05 BETTER
 56%|█████▌    | 48/86 [00:16<00:12,  3.06it/s] 21%|██        | 18/86 [00:06<00:24,  2.79it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.06it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.04it/s] 22%|██▏       | 19/86 [00:07<00:24,  2.79it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.02it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.80it/s] 60%|██████    | 52/86 [00:17<00:11,  2.99it/s] 24%|██▍       | 21/86 [00:07<00:23,  2.80it/s] 62%|██████▏   | 53/86 [00:17<00:11,  2.97it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.80it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.00it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.80it/s] 64%|██████▍   | 55/86 [00:18<00:10,  2.99it/s] 28%|██▊       | 24/86 [00:08<00:22,  2.81it/s] 65%|██████▌   | 56/86 [00:18<00:10,  2.99it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.81it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.99it/s] 30%|███       | 26/86 [00:09<00:21,  2.81it/s] 67%|██████▋   | 58/86 [00:19<00:09,  2.99it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.82it/s] 69%|██████▊   | 59/86 [00:19<00:09,  3.00it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.80it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.01it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.82it/s] 71%|███████   | 61/86 [00:20<00:08,  3.02it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.81it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.03it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.79it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.02it/s] 37%|███▋      | 32/86 [00:11<00:19,  2.79it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.01it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.01it/s] 38%|███▊      | 33/86 [00:12<00:19,  2.78it/s] 77%|███████▋  | 66/86 [00:22<00:06,  3.02it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.78it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.00it/s] 41%|████      | 35/86 [00:12<00:18,  2.79it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.01it/s] 42%|████▏     | 36/86 [00:13<00:17,  2.79it/s] 80%|████████  | 69/86 [00:23<00:05,  3.00it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.80it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.00it/s] 44%|████▍     | 38/86 [00:13<00:17,  2.81it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.01it/s] 45%|████▌     | 39/86 [00:14<00:16,  2.81it/s] 84%|████████▎ | 72/86 [00:24<00:04,  3.02it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.81it/s] 85%|████████▍ | 73/86 [00:24<00:04,  3.03it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.81it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.04it/s] 49%|████▉     | 42/86 [00:15<00:15,  2.82it/s] 87%|████████▋ | 75/86 [00:25<00:03,  3.03it/s] 50%|█████     | 43/86 [00:15<00:15,  2.82it/s] 88%|████████▊ | 76/86 [00:25<00:03,  3.02it/s] 51%|█████     | 44/86 [00:15<00:14,  2.81it/s] 90%|████████▉ | 77/86 [00:25<00:02,  3.02it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.78it/s] 91%|█████████ | 78/86 [00:26<00:02,  3.03it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.03it/s] 53%|█████▎    | 46/86 [00:16<00:14,  2.78it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.02it/s] 55%|█████▍    | 47/86 [00:17<00:14,  2.78it/s] 94%|█████████▍| 81/86 [00:27<00:01,  2.99it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.79it/s] 95%|█████████▌| 82/86 [00:27<00:01,  2.99it/s] 57%|█████▋    | 49/86 [00:17<00:13,  2.81it/s] 97%|█████████▋| 83/86 [00:27<00:00,  3.01it/s] 58%|█████▊    | 50/86 [00:18<00:12,  2.81it/s] 98%|█████████▊| 84/86 [00:28<00:00,  3.03it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.81it/s] 99%|█████████▉| 85/86 [00:28<00:00,  3.02it/s] 60%|██████    | 52/86 [00:18<00:12,  2.81it/s]100%|██████████| 86/86 [00:28<00:00,  3.03it/s]100%|██████████| 86/86 [00:28<00:00,  2.99it/s]
 62%|██████▏   | 53/86 [00:19<00:11,  2.82it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.82it/s] 64%|██████▍   | 55/86 [00:19<00:11,  2.80it/s] 65%|██████▌   | 56/86 [00:20<00:10,  2.78it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.76it/s] 67%|██████▋   | 58/86 [00:21<00:10,  2.73it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.72it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.73it/s] 71%|███████   | 61/86 [00:22<00:09,  2.75it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.78it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.80it/s] 74%|███████▍  | 64/86 [00:23<00:07,  2.79it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.78it/s] 77%|███████▋  | 66/86 [00:23<00:07,  2.80it/s] 78%|███████▊  | 67/86 [00:24<00:06,  2.80it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.79it/s] 80%|████████  | 69/86 [00:24<00:06,  2.78it/s] 81%|████████▏ | 70/86 [00:25<00:05,  2.79it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.76it/s]W0624 00:07:53.244000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.244000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.244000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.244000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.244000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.244000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.245000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.292000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.292000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.292000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.293000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.293000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.309000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.309000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.309000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.309000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.309000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 84%|████████▎ | 72/86 [00:26<00:05,  2.75it/s]W0624 00:07:53.483000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.484000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.484000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.484000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.484000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 73/86 [00:26<00:04,  2.75it/s]W0624 00:07:53.818000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.818000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.818000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.818000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.818000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.818000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.818000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.851000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.851000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.851000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.851000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.851000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.932000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.932000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.933000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.933000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:07:53.933000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 74/86 [00:26<00:04,  2.73it/s] 87%|████████▋ | 75/86 [00:27<00:03,  2.76it/s] 88%|████████▊ | 76/86 [00:27<00:03,  2.77it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.79it/s]W0624 00:07:55.169000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.181000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.190000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.190000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 78/86 [00:28<00:02,  2.80it/s]W0624 00:07:55.646000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.646000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.646000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.646000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.646000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.646000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.646000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.680000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.680000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.680000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.680000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:07:55.680000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 92%|█████████▏| 79/86 [00:28<00:02,  2.79it/s]I0624 00:07:55.910883 6660 finetune.py:68] layer 2_gate @ epoch 2 new loss 1.0719953934312798e-05 old loss 1.0732367627497297e-05 BETTER
W0624 00:07:56.027000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.027000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.027000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.027000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.027000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.027000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.027000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.027000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 93%|█████████▎| 80/86 [00:28<00:02,  2.78it/s]W0624 00:07:56.312000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.312000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.312000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.312000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.313000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 81/86 [00:29<00:01,  2.78it/s]W0624 00:07:56.657000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:07:56.662000 139802822043456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 95%|█████████▌| 82/86 [00:29<00:01,  2.76it/s] 97%|█████████▋| 83/86 [00:30<00:01,  2.76it/s] 98%|█████████▊| 84/86 [00:30<00:00,  2.75it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.76it/s]100%|██████████| 86/86 [00:31<00:00,  2.75it/s]100%|██████████| 86/86 [00:31<00:00,  2.76it/s]
I0624 00:08:03.851253 5646 finetune.py:45] layer 0_down initial loss 4.506415223204385e-07
W0624 00:08:03.851717 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:08:04.243281 7168 finetune.py:68] layer 3_gate @ epoch 1 new loss 2.7516212867340073e-05 old loss 2.755823516054079e-05 BETTER
W0624 00:08:05.333000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.333000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.333000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.333000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.333000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.333000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.333000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.377000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.394000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.394000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.394000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.394000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.394000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.558000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.558000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.558000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.558000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.558000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.883000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.883000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.883000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.883000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.883000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.883000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.883000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.914000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.915000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.915000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.915000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.915000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.983000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.983000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.983000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.983000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:05.983000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.167000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.173000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.179000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.179000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.619000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.619000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.619000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.619000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.619000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.619000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.619000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.651000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.651000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.651000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.651000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.651000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.992000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.992000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.992000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.992000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.992000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.993000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.993000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:07.993000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:08:08.293000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:08:08.294000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:08:08.294000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:08:08.294000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:08:08.294000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:08:08.635000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:08:08.641000 140645072586560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 00:08:15.457281 6152 finetune.py:45] layer 1_down initial loss 1.2502634490374476e-05
W0624 00:08:15.457640 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:08:27.748435 6660 finetune.py:68] layer 2_gate @ epoch 3 new loss 1.0708605259424075e-05 old loss 1.0719953934312798e-05 BETTER
I0624 00:08:34.889909 7168 finetune.py:68] layer 3_gate @ epoch 2 new loss 2.7480193239171058e-05 old loss 2.7516212867340073e-05 BETTER
I0624 00:08:34.916217 5646 finetune.py:68] layer 0_down @ epoch 0 new loss 4.5004767912359966e-07 old loss 4.506415223204385e-07 BETTER
I0624 00:08:44.765438 6152 finetune.py:68] layer 1_down @ epoch 0 new loss 1.2478157259465661e-05 old loss 1.2502634490374476e-05 BETTER
I0624 00:08:59.502337 6660 finetune.py:68] layer 2_gate @ epoch 4 new loss 1.069766040018294e-05 old loss 1.0708605259424075e-05 BETTER
W0624 00:09:00.633514 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_gate proxy err 0.006300212815403938 tr(WHW.T) 296.4621276855469
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:52,  1.61it/s]  2%|▏         | 2/86 [00:00<00:39,  2.14it/s]  3%|▎         | 3/86 [00:01<00:34,  2.38it/s]I0624 00:09:05.502280 7168 finetune.py:68] layer 3_gate @ epoch 3 new loss 2.7446913009043783e-05 old loss 2.7480193239171058e-05 BETTER
  5%|▍         | 4/86 [00:01<00:32,  2.53it/s]  6%|▌         | 5/86 [00:02<00:30,  2.61it/s]I0624 00:09:06.162272 5646 finetune.py:68] layer 0_down @ epoch 1 new loss 4.499361807575042e-07 old loss 4.5004767912359966e-07 BETTER
  7%|▋         | 6/86 [00:02<00:29,  2.68it/s]  8%|▊         | 7/86 [00:02<00:28,  2.76it/s]  9%|▉         | 8/86 [00:03<00:28,  2.77it/s] 10%|█         | 9/86 [00:03<00:27,  2.78it/s] 12%|█▏        | 10/86 [00:03<00:27,  2.80it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.83it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.82it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.83it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.83it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.80it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.81it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.82it/s] 21%|██        | 18/86 [00:06<00:24,  2.83it/s] 22%|██▏       | 19/86 [00:07<00:23,  2.84it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.85it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.86it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.87it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.89it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.89it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.89it/s] 30%|███       | 26/86 [00:09<00:20,  2.88it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.88it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.87it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.85it/s]I0624 00:09:14.479038 6152 finetune.py:68] layer 1_down @ epoch 1 new loss 1.2383451576170046e-05 old loss 1.2478157259465661e-05 BETTER
 35%|███▍      | 30/86 [00:10<00:19,  2.86it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.87it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.87it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.87it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.87it/s] 41%|████      | 35/86 [00:12<00:17,  2.88it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.88it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.88it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.87it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.86it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.85it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.85it/s] 49%|████▉     | 42/86 [00:15<00:15,  2.82it/s] 50%|█████     | 43/86 [00:15<00:15,  2.84it/s] 51%|█████     | 44/86 [00:15<00:14,  2.85it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.86it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.87it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.87it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.85it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.85it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.86it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.86it/s] 60%|██████    | 52/86 [00:18<00:11,  2.86it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.86it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.85it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.84it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.81it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.82it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.83it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.83it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.85it/s] 71%|███████   | 61/86 [00:21<00:08,  2.86it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.87it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.86it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.87it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.86it/s] 77%|███████▋  | 66/86 [00:23<00:07,  2.85it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.85it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.84it/s] 80%|████████  | 69/86 [00:24<00:05,  2.85it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.84it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.87it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.87it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.86it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.86it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.86it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.88it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.89it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.90it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.89it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.87it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.85it/s] 95%|█████████▌| 82/86 [00:29<00:01,  2.83it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.80it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.80it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.81it/s]100%|██████████| 86/86 [00:30<00:00,  2.80it/s]100%|██████████| 86/86 [00:30<00:00,  2.82it/s]
I0624 00:09:35.947092 7168 finetune.py:68] layer 3_gate @ epoch 4 new loss 2.7415579097578302e-05 old loss 2.7446913009043783e-05 BETTER
W0624 00:09:37.046308 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:09:37.412009 5646 finetune.py:76] layer 0_down @ epoch 2 new loss 4.499398187363113e-07 old loss 4.499361807575042e-07 WORSE
3_gate proxy err 0.0074025364592671394 tr(WHW.T) 477.2920837402344
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:52,  1.62it/s]  2%|▏         | 2/86 [00:00<00:39,  2.15it/s]W0624 00:09:41.169000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.169000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.169000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.169000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.170000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.170000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.170000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.213000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.213000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.213000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.213000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.213000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 3/86 [00:01<00:34,  2.41it/s]W0624 00:09:41.229000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.229000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.229000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.229000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.230000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.402000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.402000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.402000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.402000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.402000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
  5%|▍         | 4/86 [00:01<00:32,  2.53it/s]W0624 00:09:41.722000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.723000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.757000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.757000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.758000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.758000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.758000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.834000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.834000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.835000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.835000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:41.835000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
  6%|▌         | 5/86 [00:02<00:30,  2.61it/s]  7%|▋         | 6/86 [00:02<00:30,  2.66it/s]  8%|▊         | 7/86 [00:02<00:29,  2.70it/s]  9%|▉         | 8/86 [00:03<00:28,  2.73it/s]W0624 00:09:43.046000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.057000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.065000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.065000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 10%|█         | 9/86 [00:03<00:27,  2.77it/s]W0624 00:09:43.513000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.513000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.513000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.513000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.513000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.514000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.514000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.544000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.544000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.544000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.544000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.545000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 12%|█▏        | 10/86 [00:03<00:27,  2.79it/s]W0624 00:09:43.891000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.892000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.892000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.892000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.892000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.892000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.892000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:43.892000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
 13%|█▎        | 11/86 [00:04<00:26,  2.80it/s]W0624 00:09:44.193000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:09:44.193000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:09:44.193000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:09:44.193000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:09:44.193000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
I0624 00:09:44.354550 6152 finetune.py:76] layer 1_down @ epoch 2 new loss 1.2414697266649455e-05 old loss 1.2383451576170046e-05 WORSE
 14%|█▍        | 12/86 [00:04<00:26,  2.79it/s]W0624 00:09:44.527000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:09:44.532000 140540295587648 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 15%|█▌        | 13/86 [00:04<00:26,  2.80it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.82it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.83it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.83it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.84it/s] 21%|██        | 18/86 [00:06<00:24,  2.82it/s] 22%|██▏       | 19/86 [00:07<00:23,  2.84it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.84it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.84it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.86it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.86it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.85it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.85it/s] 30%|███       | 26/86 [00:09<00:20,  2.86it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.86it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.87it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.88it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.89it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.89it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.90it/s]I0624 00:09:51.754636 6660 finetune.py:45] layer 2_down initial loss 1.8827495296136476e-05
W0624 00:09:51.755101 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 33/86 [00:11<00:18,  2.88it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.88it/s] 41%|████      | 35/86 [00:12<00:17,  2.89it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.90it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.91it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.89it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.89it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.90it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.90it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.90it/s] 50%|█████     | 43/86 [00:15<00:14,  2.91it/s] 51%|█████     | 44/86 [00:15<00:14,  2.91it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.90it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.90it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.89it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.88it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.89it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.88it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.89it/s] 60%|██████    | 52/86 [00:18<00:11,  2.89it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.88it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.88it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.89it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.89it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.88it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.88it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.89it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.87it/s] 71%|███████   | 61/86 [00:21<00:08,  2.85it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.86it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.85it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.84it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.85it/s] 77%|███████▋  | 66/86 [00:23<00:07,  2.85it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.86it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.87it/s] 80%|████████  | 69/86 [00:24<00:05,  2.88it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.88it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.89it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.89it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.89it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.89it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.88it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.86it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.86it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.86it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.85it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.85it/s]I0624 00:10:08.389607 5646 finetune.py:68] layer 0_down @ epoch 3 new loss 4.498700150179502e-07 old loss 4.499361807575042e-07 BETTER
 94%|█████████▍| 81/86 [00:28<00:01,  2.84it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.84it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.83it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.83it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.84it/s]100%|██████████| 86/86 [00:30<00:00,  2.82it/s]100%|██████████| 86/86 [00:30<00:00,  2.83it/s]
I0624 00:10:13.672034 6152 finetune.py:68] layer 1_down @ epoch 3 new loss 1.2363765563350171e-05 old loss 1.2383451576170046e-05 BETTER
W0624 00:10:17.155000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.156000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.156000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.156000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.156000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.156000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.156000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.198000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.198000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.199000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.199000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.199000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.215000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.215000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.215000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.215000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.215000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.387000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.387000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.387000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.387000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.387000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.712000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.712000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.712000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.712000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.713000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.713000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.713000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.744000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.744000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.744000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.745000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.745000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.814000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.814000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.814000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.814000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:17.814000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.015000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.027000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.036000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.036000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.481000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.481000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.481000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.481000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.481000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.481000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.481000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.515000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.515000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.515000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.515000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.515000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.865000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.866000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.866000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.866000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.866000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.866000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.866000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:19.866000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:20.161000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:10:20.161000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:10:20.161000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:10:20.162000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:10:20.162000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:10:20.503000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:10:20.508000 140055051474752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 00:10:20.912526 6660 finetune.py:68] layer 2_down @ epoch 0 new loss 1.8820788682205603e-05 old loss 1.8827495296136476e-05 BETTER
I0624 00:10:27.322196 7168 finetune.py:45] layer 3_down initial loss 4.4531116145662963e-05
W0624 00:10:27.322489 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:10:39.581518 5646 finetune.py:68] layer 0_down @ epoch 4 new loss 4.4983875113757676e-07 old loss 4.498700150179502e-07 BETTER
W0624 00:10:40.359169 5646 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

0_down proxy err 0.0018726205453276634 tr(WHW.T) 0.7173300981521606
I0624 00:10:43.419090 6152 finetune.py:76] layer 1_down @ epoch 4 new loss 1.241490554093616e-05 old loss 1.2363765563350171e-05 WORSE
W0624 00:10:44.359721 6152 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

1_down proxy err 6.239060166990384e-05 tr(WHW.T) 13950.9609375
I0624 00:10:51.175367 6660 finetune.py:68] layer 2_down @ epoch 1 new loss 1.882007927633822e-05 old loss 1.8820788682205603e-05 BETTER
I0624 00:10:55.944542 7168 finetune.py:68] layer 3_down @ epoch 0 new loss 4.451698623597622e-05 old loss 4.4531116145662963e-05 BETTER
I0624 00:11:21.498547 6660 finetune.py:68] layer 2_down @ epoch 2 new loss 1.8818460375769064e-05 old loss 1.882007927633822e-05 BETTER
I0624 00:11:25.338044 7168 finetune.py:68] layer 3_down @ epoch 1 new loss 4.451450513442978e-05 old loss 4.451698623597622e-05 BETTER
I0624 00:11:50.320727 4970 quantize_finetune_llama.py:193] computed original embedding for layer 4 in 61.52258276939392s
I0624 00:11:50.682875 4970 quantize_finetune_llama.py:162] layer 5 gpu 1
I0624 00:11:51.580327 6660 finetune.py:68] layer 2_down @ epoch 3 new loss 1.8817778254742734e-05 old loss 1.8818460375769064e-05 BETTER
I0624 00:11:52.754820 7866 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 00:11:52.754962 7866 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 00:11:52.755023 7866 utils.py:162] NumExpr defaulting to 16 threads.
I0624 00:11:52.976789 7866 config.py:58] PyTorch version 2.4.0 available.
I0624 00:11:54.780087 7168 finetune.py:68] layer 3_down @ epoch 2 new loss 4.451064523891546e-05 old loss 4.451450513442978e-05 BETTER
I0624 00:11:55.450028 7866 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 00:11:55.906432 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:02<00:16,  1.79it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.58it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.71it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.92it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.98it/s] 41%|████      | 13/32 [00:05<00:06,  2.99it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.98it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.00it/s] 50%|█████     | 16/32 [00:06<00:05,  3.01it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.00it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.01it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.01it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.01it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.02it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.02it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.03it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.04it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.03it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.02it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.02it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.03it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.02it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.03it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.03it/s]100%|██████████| 32/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
W0624 00:12:11.925000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.925000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.925000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.925000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.926000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.926000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.926000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.954000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.954000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.954000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.954000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.954000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.972000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.973000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.973000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.973000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:12:11.973000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:12:12.300000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:12:12.300000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:12:12.300000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:12:12.300000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:12:12.300000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.212000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.212000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.213000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.213000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.213000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.213000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.213000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.231000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.231000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.232000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.232000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.232000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.481000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.481000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.481000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.481000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:12:13.481000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.673000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.674000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.674000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.674000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.674000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.674000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.674000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.693000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.693000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.693000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.693000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:12:14.693000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:12:15.620000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:12:15.620000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:12:15.620000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:12:15.620000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:12:15.620000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 00:12:21.631764 6660 finetune.py:68] layer 2_down @ epoch 4 new loss 1.8816004740074277e-05 old loss 1.8817778254742734e-05 BETTER
I0624 00:12:22.166345 7866 finetune.py:45] layer 4_v initial loss 5.832615352119319e-05
W0624 00:12:22.166613 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0624 00:12:22.297581 6660 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

2_down proxy err 0.011058459989726543 tr(WHW.T) 2.9677963256835938
I0624 00:12:24.330299 7168 finetune.py:68] layer 3_down @ epoch 3 new loss 4.450667984201573e-05 old loss 4.451064523891546e-05 BETTER
I0624 00:12:53.853436 7168 finetune.py:68] layer 3_down @ epoch 4 new loss 4.4504784455057234e-05 old loss 4.450667984201573e-05 BETTER
W0624 00:12:54.602986 7168 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

3_down proxy err 0.011461521498858929 tr(WHW.T) 6.165841579437256
I0624 00:12:56.279111 7866 finetune.py:68] layer 4_v @ epoch 0 new loss 1.541112214908935e-05 old loss 5.832615352119319e-05 BETTER
I0624 00:13:25.911142 4970 quantize_finetune_llama.py:193] computed original embedding for layer 5 in 59.10054421424866s
I0624 00:13:26.312145 4970 quantize_finetune_llama.py:162] layer 6 gpu 2
I0624 00:13:28.476634 8370 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 00:13:28.476744 8370 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 00:13:28.476802 8370 utils.py:162] NumExpr defaulting to 16 threads.
I0624 00:13:28.652271 8370 config.py:58] PyTorch version 2.4.0 available.
I0624 00:13:31.019667 8370 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 00:13:31.424156 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:13:31.457368 7866 finetune.py:68] layer 4_v @ epoch 1 new loss 8.934418474382255e-06 old loss 1.541112214908935e-05 BETTER
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.61s/it]  6%|▋         | 2/32 [00:01<00:25,  1.18it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.56it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.92it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.98it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.03it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.07it/s] 41%|████      | 13/32 [00:05<00:06,  3.09it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.12it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.10it/s] 50%|█████     | 16/32 [00:06<00:05,  3.12it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.15it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.14it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.13it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.14it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.15it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.13it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.11it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.13it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.13it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.14it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.15it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.15it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.16it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.10it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.12it/s]100%|██████████| 32/32 [00:11<00:00,  3.13it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
W0624 00:13:47.117000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.117000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.118000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.118000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.118000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.118000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.118000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.143000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.143000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.143000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.143000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.143000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.160000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.160000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.160000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.160000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.160000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.475000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.475000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.475000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.475000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:13:47.475000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.352000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.352000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.352000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.352000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.352000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.352000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.353000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.370000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.370000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.370000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.370000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.370000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.608000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.608000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.608000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.608000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:13:48.608000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.741000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.741000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.741000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.742000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.742000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.742000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.742000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.759000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.759000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.759000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.760000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:13:49.760000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:13:50.660000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:13:50.660000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:13:50.660000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:13:50.660000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:13:50.661000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 00:13:56.264569 8370 finetune.py:45] layer 5_v initial loss 7.489472045563161e-05
W0624 00:13:56.264803 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:14:06.882499 7866 finetune.py:68] layer 4_v @ epoch 2 new loss 7.424264822475379e-06 old loss 8.934418474382255e-06 BETTER
I0624 00:14:25.432614 4970 quantize_finetune_llama.py:193] computed original embedding for layer 6 in 58.7318332195282s
I0624 00:14:25.797998 4970 quantize_finetune_llama.py:162] layer 7 gpu 3
I0624 00:14:27.905494 8876 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 00:14:27.905619 8876 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 00:14:27.905709 8876 utils.py:162] NumExpr defaulting to 16 threads.
I0624 00:14:28.094010 8370 finetune.py:68] layer 5_v @ epoch 0 new loss 2.200997550971806e-05 old loss 7.489472045563161e-05 BETTER
I0624 00:14:28.098024 8876 config.py:58] PyTorch version 2.4.0 available.
I0624 00:14:30.494272 8876 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 00:14:30.899946 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:50,  1.64s/it]  6%|▋         | 2/32 [00:01<00:26,  1.14it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.91it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.18it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.83it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.88it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.88it/s]I0624 00:14:42.374858 7866 finetune.py:68] layer 4_v @ epoch 3 new loss 6.8187923716322985e-06 old loss 7.424264822475379e-06 BETTER
 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
W0624 00:14:47.602000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.603000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.603000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.603000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.603000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.603000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.603000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.630000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.630000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.630000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.631000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.631000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.648000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.648000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.648000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.648000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.648000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.987000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.987000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.987000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.987000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:14:47.987000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.900000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.900000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.900000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.900000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.900000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.900000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.900000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.919000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.919000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.919000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.919000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:14:48.920000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:14:49.158000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:14:49.158000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:14:49.158000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:14:49.158000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:14:49.158000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.355000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.356000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.356000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.356000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.356000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.356000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.356000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.373000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.373000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.373000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.373000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:14:50.373000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:14:51.303000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:14:51.303000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:14:51.303000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:14:51.303000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:14:51.303000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 00:14:57.726160 8876 finetune.py:45] layer 6_v initial loss 8.392794552491978e-05
W0624 00:14:57.726352 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:15:00.475231 8370 finetune.py:68] layer 5_v @ epoch 1 new loss 1.5732846804894507e-05 old loss 2.200997550971806e-05 BETTER
I0624 00:15:17.514874 7866 finetune.py:68] layer 4_v @ epoch 4 new loss 6.4782198023749515e-06 old loss 6.8187923716322985e-06 BETTER
W0624 00:15:19.014687 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_v proxy err 0.007799881976097822 tr(WHW.T) 282.5361328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.69it/s]  6%|▋         | 2/32 [00:00<00:12,  2.32it/s]  9%|▉         | 3/32 [00:01<00:10,  2.64it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.73it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.85it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.94it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.99it/s] 25%|██▌       | 8/32 [00:02<00:07,  3.01it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.04it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.03it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.98it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.02it/s] 41%|████      | 13/32 [00:04<00:06,  3.07it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.10it/s]I0624 00:15:25.032479 4970 quantize_finetune_llama.py:193] computed original embedding for layer 7 in 58.8063178062439s
 47%|████▋     | 15/32 [00:05<00:05,  3.09it/s]I0624 00:15:25.412675 4970 quantize_finetune_llama.py:162] layer 8 gpu 0
 50%|█████     | 16/32 [00:05<00:05,  3.05it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.03it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.00it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.03it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.06it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.06it/s]I0624 00:15:27.491990 9386 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 00:15:27.492143 9386 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 00:15:27.492229 9386 utils.py:162] NumExpr defaulting to 16 threads.
I0624 00:15:27.677414 9386 config.py:58] PyTorch version 2.4.0 available.
 72%|███████▏  | 23/32 [00:07<00:02,  3.07it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.08it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.07it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.05it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.01it/s]I0624 00:15:29.435900 8876 finetune.py:68] layer 6_v @ epoch 0 new loss 2.319702434760984e-05 old loss 8.392794552491978e-05 BETTER
 88%|████████▊ | 28/32 [00:09<00:01,  3.00it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.01it/s]I0624 00:15:30.089993 9386 data_utils.py:336] using 256 training seqs, 128 validation seqs
 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s]W0624 00:15:30.423896 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 97%|█████████▋| 31/32 [00:10<00:00,  3.01it/s]100%|██████████| 32/32 [00:10<00:00,  3.05it/s]100%|██████████| 32/32 [00:10<00:00,  2.97it/s]
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:47,  1.54s/it]I0624 00:15:33.304420 8370 finetune.py:68] layer 5_v @ epoch 2 new loss 1.408555453963345e-05 old loss 1.5732846804894507e-05 BETTER
  6%|▋         | 2/32 [00:01<00:25,  1.19it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.88it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.89it/s]I0624 00:15:37.986652 7866 finetune.py:45] layer 4_q initial loss 9.200586646329612e-06
W0624 00:15:37.987301 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 16/32 [00:06<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.89it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.88it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
W0624 00:15:46.991000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:15:46.991000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:15:46.991000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:15:46.991000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:15:46.991000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:15:46.991000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:15:46.991000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.017000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.017000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.017000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.017000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.017000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.033000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.033000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.033000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.033000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.033000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.363000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.363000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.363000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.364000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:15:47.364000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.269000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.269000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.270000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.270000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.270000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.270000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.270000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.287000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.287000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.287000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.288000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.288000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.529000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.529000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.529000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.529000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:15:48.529000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.721000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.721000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.721000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.721000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.721000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.721000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.722000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.740000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.740000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.740000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.740000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:15:49.741000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:15:50.670000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:15:50.671000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:15:50.671000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:15:50.671000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:15:50.671000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 00:15:56.089962 9386 finetune.py:45] layer 7_v initial loss 8.747355605009943e-05
W0624 00:15:56.090152 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:16:01.792402 8876 finetune.py:68] layer 6_v @ epoch 1 new loss 1.7939917597686872e-05 old loss 2.319702434760984e-05 BETTER
I0624 00:16:06.514230 8370 finetune.py:68] layer 5_v @ epoch 3 new loss 1.3319173376658e-05 old loss 1.408555453963345e-05 BETTER
I0624 00:16:12.282511 7866 finetune.py:68] layer 4_q @ epoch 0 new loss 8.721161975699943e-06 old loss 9.200586646329612e-06 BETTER
I0624 00:16:27.460556 9386 finetune.py:68] layer 7_v @ epoch 0 new loss 2.8137670597061515e-05 old loss 8.747355605009943e-05 BETTER
I0624 00:16:34.642501 8876 finetune.py:68] layer 6_v @ epoch 2 new loss 1.6304125892929733e-05 old loss 1.7939917597686872e-05 BETTER
I0624 00:16:39.545203 8370 finetune.py:68] layer 5_v @ epoch 4 new loss 1.2850803614128381e-05 old loss 1.3319173376658e-05 BETTER
W0624 00:16:41.006241 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_v proxy err 0.008894604630768299 tr(WHW.T) 307.33966064453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.61it/s]  6%|▋         | 2/32 [00:00<00:13,  2.17it/s]  9%|▉         | 3/32 [00:01<00:11,  2.46it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.76it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.82it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.89it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.90it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.92it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.92it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 41%|████      | 13/32 [00:04<00:06,  2.90it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s]I0624 00:16:47.409058 7866 finetune.py:68] layer 4_q @ epoch 1 new loss 8.463784979539923e-06 old loss 8.721161975699943e-06 BETTER
 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 50%|█████     | 16/32 [00:05<00:05,  2.92it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.93it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.93it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.91it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.91it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.90it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
I0624 00:16:59.800655 8370 finetune.py:45] layer 5_q initial loss 1.6870431863935664e-05
W0624 00:16:59.801070 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:16:59.931445 9386 finetune.py:68] layer 7_v @ epoch 1 new loss 2.3721713660052046e-05 old loss 2.8137670597061515e-05 BETTER
I0624 00:17:07.874113 8876 finetune.py:68] layer 6_v @ epoch 3 new loss 1.5450943465111777e-05 old loss 1.6304125892929733e-05 BETTER
I0624 00:17:22.846185 7866 finetune.py:68] layer 4_q @ epoch 2 new loss 8.28305201139301e-06 old loss 8.463784979539923e-06 BETTER
I0624 00:17:32.150245 8370 finetune.py:68] layer 5_q @ epoch 0 new loss 1.615549081179779e-05 old loss 1.6870431863935664e-05 BETTER
I0624 00:17:32.253391 9386 finetune.py:68] layer 7_v @ epoch 2 new loss 2.217961082351394e-05 old loss 2.3721713660052046e-05 BETTER
I0624 00:17:41.185536 8876 finetune.py:68] layer 6_v @ epoch 4 new loss 1.4897242181177717e-05 old loss 1.5450943465111777e-05 BETTER
W0624 00:17:42.594156 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_v proxy err 0.008518880233168602 tr(WHW.T) 457.15081787109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.59it/s]  6%|▋         | 2/32 [00:00<00:14,  2.14it/s]  9%|▉         | 3/32 [00:01<00:12,  2.41it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.57it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.70it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.78it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 41%|████      | 13/32 [00:04<00:06,  2.84it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 50%|█████     | 16/32 [00:05<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
I0624 00:17:58.080862 7866 finetune.py:68] layer 4_q @ epoch 3 new loss 8.146995241986588e-06 old loss 8.28305201139301e-06 BETTER
I0624 00:18:01.617166 8876 finetune.py:45] layer 6_q initial loss 2.312761716893874e-05
W0624 00:18:01.617520 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:18:04.286063 9386 finetune.py:68] layer 7_v @ epoch 3 new loss 2.1321051463019103e-05 old loss 2.217961082351394e-05 BETTER
I0624 00:18:05.438284 8370 finetune.py:68] layer 5_q @ epoch 1 new loss 1.5767580407555215e-05 old loss 1.615549081179779e-05 BETTER
I0624 00:18:33.347516 7866 finetune.py:68] layer 4_q @ epoch 4 new loss 8.032488949538674e-06 old loss 8.146995241986588e-06 BETTER
I0624 00:18:33.826663 8876 finetune.py:68] layer 6_q @ epoch 0 new loss 2.2067451936891302e-05 old loss 2.312761716893874e-05 BETTER
W0624 00:18:34.770274 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_q proxy err 0.0011283415369689465 tr(WHW.T) 6570.5009765625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:17,  1.78it/s]  6%|▋         | 2/32 [00:00<00:12,  2.38it/s]I0624 00:18:36.728606 9386 finetune.py:68] layer 7_v @ epoch 4 new loss 2.075928932754323e-05 old loss 2.1321051463019103e-05 BETTER
  9%|▉         | 3/32 [00:01<00:10,  2.64it/s] 12%|█▎        | 4/32 [00:01<00:09,  2.81it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.92it/s] 19%|█▉        | 6/32 [00:02<00:08,  3.00it/s] 22%|██▏       | 7/32 [00:02<00:08,  3.04it/s]W0624 00:18:38.374886 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:18:38.509124 8370 finetune.py:68] layer 5_q @ epoch 2 new loss 1.5485940821236e-05 old loss 1.5767580407555215e-05 BETTER
 25%|██▌       | 8/32 [00:02<00:07,  3.06it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.09it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.09it/s]7_v proxy err 0.008775374852120876 tr(WHW.T) 502.64825439453125
  0%|          | 0/32 [00:00<?, ?it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.11it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.12it/s]  3%|▎         | 1/32 [00:00<00:20,  1.55it/s] 41%|████      | 13/32 [00:04<00:06,  3.13it/s]  6%|▋         | 2/32 [00:01<00:14,  2.10it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.13it/s]  9%|▉         | 3/32 [00:01<00:12,  2.38it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.12it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.54it/s] 50%|█████     | 16/32 [00:05<00:05,  3.11it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.62it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.10it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.10it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.68it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.12it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.73it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.09it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 66%|██████▌   | 21/32 [00:06<00:03,  3.11it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.79it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.13it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.82it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.10it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 75%|███████▌  | 24/32 [00:07<00:02,  3.10it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.85it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.12it/s] 41%|████      | 13/32 [00:04<00:06,  2.87it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.11it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 84%|████████▍ | 27/32 [00:08<00:01,  3.11it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.88it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.11it/s] 50%|█████     | 16/32 [00:05<00:05,  2.87it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.12it/s] 94%|█████████▍| 30/32 [00:09<00:00,  3.12it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.87it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.11it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s]100%|██████████| 32/32 [00:10<00:00,  3.12it/s]100%|██████████| 32/32 [00:10<00:00,  3.04it/s]
 59%|█████▉    | 19/32 [00:06<00:04,  2.86it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.86it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.89it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.91it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.90it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
I0624 00:18:53.562870 7866 finetune.py:45] layer 4_k initial loss 1.0521598596824333e-05
W0624 00:18:53.563254 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:18:57.362973 9386 finetune.py:45] layer 7_q initial loss 3.289818414486945e-05
W0624 00:18:57.363341 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:19:06.638011 8876 finetune.py:68] layer 6_q @ epoch 1 new loss 2.1523565010284074e-05 old loss 2.2067451936891302e-05 BETTER
I0624 00:19:11.158906 8370 finetune.py:68] layer 5_q @ epoch 3 new loss 1.5258293387887534e-05 old loss 1.5485940821236e-05 BETTER
I0624 00:19:27.522748 7866 finetune.py:68] layer 4_k @ epoch 0 new loss 1.015883572108578e-05 old loss 1.0521598596824333e-05 BETTER
I0624 00:19:28.986223 9386 finetune.py:68] layer 7_q @ epoch 0 new loss 3.132187339360826e-05 old loss 3.289818414486945e-05 BETTER
I0624 00:19:39.742465 8876 finetune.py:68] layer 6_q @ epoch 2 new loss 2.111782850988675e-05 old loss 2.1523565010284074e-05 BETTER
I0624 00:19:44.033581 8370 finetune.py:68] layer 5_q @ epoch 4 new loss 1.50740097524249e-05 old loss 1.5258293387887534e-05 BETTER
W0624 00:19:46.105045 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_q proxy err 0.001371303224004805 tr(WHW.T) 6421.22900390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.63it/s]  6%|▋         | 2/32 [00:00<00:13,  2.16it/s]  9%|▉         | 3/32 [00:01<00:11,  2.42it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.57it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.73it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.80it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 41%|████      | 13/32 [00:04<00:06,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.83it/s] 50%|█████     | 16/32 [00:05<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.85it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.80it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
I0624 00:20:01.177718 9386 finetune.py:68] layer 7_q @ epoch 1 new loss 3.059474329347722e-05 old loss 3.132187339360826e-05 BETTER
I0624 00:20:02.073843 7866 finetune.py:68] layer 4_k @ epoch 1 new loss 1.0037959327746648e-05 old loss 1.015883572108578e-05 BETTER
I0624 00:20:06.117182 8370 finetune.py:45] layer 5_k initial loss 1.8106256902683526e-05
W0624 00:20:06.118104 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:20:12.756680 8876 finetune.py:68] layer 6_q @ epoch 3 new loss 2.0794966985704377e-05 old loss 2.111782850988675e-05 BETTER
I0624 00:20:33.367083 9386 finetune.py:68] layer 7_q @ epoch 2 new loss 3.006507540703751e-05 old loss 3.059474329347722e-05 BETTER
I0624 00:20:37.024977 7866 finetune.py:68] layer 4_k @ epoch 2 new loss 9.948426850314718e-06 old loss 1.0037959327746648e-05 BETTER
I0624 00:20:39.180973 8370 finetune.py:68] layer 5_k @ epoch 0 new loss 1.7609841961530037e-05 old loss 1.8106256902683526e-05 BETTER
I0624 00:20:45.528160 8876 finetune.py:68] layer 6_q @ epoch 4 new loss 2.0525198124232702e-05 old loss 2.0794966985704377e-05 BETTER
W0624 00:20:46.888200 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_q proxy err 0.0017889674054458737 tr(WHW.T) 7274.650390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.72it/s]  6%|▋         | 2/32 [00:00<00:13,  2.27it/s]  9%|▉         | 3/32 [00:01<00:11,  2.49it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.72it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.78it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.84it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.88it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 41%|████      | 13/32 [00:04<00:06,  2.90it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.91it/s] 50%|█████     | 16/32 [00:05<00:05,  2.92it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.91it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.92it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.92it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.92it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.93it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
I0624 00:21:05.540310 8876 finetune.py:45] layer 6_k initial loss 2.6747104129754007e-05
W0624 00:21:05.540693 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:21:05.556012 9386 finetune.py:68] layer 7_q @ epoch 3 new loss 2.9630116841872223e-05 old loss 3.006507540703751e-05 BETTER
I0624 00:21:12.034581 7866 finetune.py:68] layer 4_k @ epoch 3 new loss 9.871885595202912e-06 old loss 9.948426850314718e-06 BETTER
I0624 00:21:12.439152 8370 finetune.py:68] layer 5_k @ epoch 1 new loss 1.743097891448997e-05 old loss 1.7609841961530037e-05 BETTER
I0624 00:21:37.493953 8876 finetune.py:68] layer 6_k @ epoch 0 new loss 2.588488678156864e-05 old loss 2.6747104129754007e-05 BETTER
I0624 00:21:37.626124 9386 finetune.py:68] layer 7_q @ epoch 4 new loss 2.9270864615682513e-05 old loss 2.9630116841872223e-05 BETTER
W0624 00:21:39.182046 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_q proxy err 0.001938407775014639 tr(WHW.T) 7393.67626953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:17,  1.74it/s]  6%|▋         | 2/32 [00:00<00:13,  2.27it/s]  9%|▉         | 3/32 [00:01<00:11,  2.52it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.65it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.74it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.77it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.83it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 41%|████      | 13/32 [00:04<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s]I0624 00:21:45.507919 8370 finetune.py:68] layer 5_k @ epoch 2 new loss 1.7292370102950372e-05 old loss 1.743097891448997e-05 BETTER
 47%|████▋     | 15/32 [00:05<00:05,  2.88it/s] 50%|█████     | 16/32 [00:05<00:05,  2.89it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.89it/s]I0624 00:21:46.758686 7866 finetune.py:68] layer 4_k @ epoch 4 new loss 9.810211849980988e-06 old loss 9.871885595202912e-06 BETTER
 59%|█████▉    | 19/32 [00:06<00:04,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.87it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.89it/s]W0624 00:21:48.139672 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:08<00:03,  2.88it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.88it/s]4_k proxy err 0.0007877878379076719 tr(WHW.T) 9821.72265625
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.88it/s]  3%|▎         | 1/32 [00:00<00:17,  1.74it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s]  6%|▋         | 2/32 [00:00<00:13,  2.30it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.90it/s]  9%|▉         | 3/32 [00:01<00:11,  2.56it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.72it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.88it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.81it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.88it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
 22%|██▏       | 7/32 [00:02<00:08,  2.93it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.97it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.99it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.99it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.99it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.01it/s] 41%|████      | 13/32 [00:04<00:06,  3.01it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.01it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.01it/s] 50%|█████     | 16/32 [00:05<00:05,  3.00it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.97it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.99it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.01it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.01it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.00it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.01it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.03it/s]I0624 00:21:57.958299 9386 finetune.py:45] layer 7_k initial loss 3.836164250969887e-05
W0624 00:21:57.958666 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:08<00:01,  3.04it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.02it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.00it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.99it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.99it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.99it/s]100%|██████████| 32/32 [00:10<00:00,  2.97it/s]100%|██████████| 32/32 [00:10<00:00,  2.93it/s]
I0624 00:22:07.223712 7866 finetune.py:45] layer 4_o initial loss 2.039466744463425e-05
W0624 00:22:07.224182 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:22:10.020587 8876 finetune.py:68] layer 6_k @ epoch 1 new loss 2.562373992986977e-05 old loss 2.588488678156864e-05 BETTER
I0624 00:22:18.966689 8370 finetune.py:68] layer 5_k @ epoch 3 new loss 1.7170399587485008e-05 old loss 1.7292370102950372e-05 BETTER
I0624 00:22:29.430451 9386 finetune.py:68] layer 7_k @ epoch 0 new loss 3.7244066334096715e-05 old loss 3.836164250969887e-05 BETTER
I0624 00:22:40.851714 7866 finetune.py:68] layer 4_o @ epoch 0 new loss 1.927920493471902e-05 old loss 2.039466744463425e-05 BETTER
I0624 00:22:42.111076 8876 finetune.py:68] layer 6_k @ epoch 2 new loss 2.542067159083672e-05 old loss 2.562373992986977e-05 BETTER
I0624 00:22:52.528494 8370 finetune.py:68] layer 5_k @ epoch 4 new loss 1.7071692127501592e-05 old loss 1.7170399587485008e-05 BETTER
W0624 00:22:54.038027 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_k proxy err 0.0009423083974979818 tr(WHW.T) 10140.7763671875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]  6%|▋         | 2/32 [00:00<00:13,  2.18it/s]  9%|▉         | 3/32 [00:01<00:11,  2.46it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.60it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.67it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.72it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s] 41%|████      | 13/32 [00:04<00:06,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 50%|█████     | 16/32 [00:05<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.86it/s]I0624 00:23:01.441904 9386 finetune.py:68] layer 7_k @ epoch 1 new loss 3.688201104523614e-05 old loss 3.7244066334096715e-05 BETTER
 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.85it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]
I0624 00:23:14.077589 8370 finetune.py:45] layer 5_o initial loss 3.2212905352935195e-05
W0624 00:23:14.078054 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:23:14.845203 8876 finetune.py:68] layer 6_k @ epoch 3 new loss 2.525174022594001e-05 old loss 2.542067159083672e-05 BETTER
I0624 00:23:15.829096 7866 finetune.py:68] layer 4_o @ epoch 1 new loss 1.9004752175533213e-05 old loss 1.927920493471902e-05 BETTER
I0624 00:23:33.791064 9386 finetune.py:68] layer 7_k @ epoch 2 new loss 3.661358277895488e-05 old loss 3.688201104523614e-05 BETTER
I0624 00:23:46.645828 8370 finetune.py:68] layer 5_o @ epoch 0 new loss 3.062128962483257e-05 old loss 3.2212905352935195e-05 BETTER
I0624 00:23:47.332673 8876 finetune.py:68] layer 6_k @ epoch 4 new loss 2.5111850845860317e-05 old loss 2.525174022594001e-05 BETTER
W0624 00:23:48.697212 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_k proxy err 0.001376815722323954 tr(WHW.T) 9821.9287109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.71it/s]  6%|▋         | 2/32 [00:00<00:13,  2.26it/s]I0624 00:23:50.885816 7866 finetune.py:68] layer 4_o @ epoch 2 new loss 1.8854687368730083e-05 old loss 1.9004752175533213e-05 BETTER
  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.67it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.74it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.81it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.84it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.87it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.89it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.89it/s] 41%|████      | 13/32 [00:04<00:06,  2.89it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s] 50%|█████     | 16/32 [00:05<00:05,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.89it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.89it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.87it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.88it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.89it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.88it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.88it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.92it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.91it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.92it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]
I0624 00:24:05.820523 9386 finetune.py:68] layer 7_k @ epoch 3 new loss 3.638653288362548e-05 old loss 3.661358277895488e-05 BETTER
I0624 00:24:07.586944 8876 finetune.py:45] layer 6_o initial loss 4.7525860281893983e-05
W0624 00:24:07.587283 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:24:19.488462 8370 finetune.py:68] layer 5_o @ epoch 1 new loss 3.0284845706773922e-05 old loss 3.062128962483257e-05 BETTER
I0624 00:24:25.425143 7866 finetune.py:68] layer 4_o @ epoch 3 new loss 1.874811277957633e-05 old loss 1.8854687368730083e-05 BETTER
I0624 00:24:37.871254 9386 finetune.py:68] layer 7_k @ epoch 4 new loss 3.617425318225287e-05 old loss 3.638653288362548e-05 BETTER
W0624 00:24:39.313621 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:24:39.498637 8876 finetune.py:68] layer 6_o @ epoch 0 new loss 4.553677354124375e-05 old loss 4.7525860281893983e-05 BETTER
7_k proxy err 0.0014955276856198907 tr(WHW.T) 9674.88671875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.68it/s]  6%|▋         | 2/32 [00:00<00:13,  2.23it/s]  9%|▉         | 3/32 [00:01<00:11,  2.51it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.67it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.76it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.83it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.87it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.89it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.88it/s] 41%|████      | 13/32 [00:04<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.89it/s] 50%|█████     | 16/32 [00:05<00:05,  2.92it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.93it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.96it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.92it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.93it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.96it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.93it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.93it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]
I0624 00:24:52.280927 8370 finetune.py:68] layer 5_o @ epoch 2 new loss 3.0074819733272307e-05 old loss 3.0284845706773922e-05 BETTER
I0624 00:24:57.986484 9386 finetune.py:45] layer 7_o initial loss 6.613553705392405e-05
W0624 00:24:57.986793 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:25:00.074445 7866 finetune.py:68] layer 4_o @ epoch 4 new loss 1.866382262960542e-05 old loss 1.874811277957633e-05 BETTER
W0624 00:25:01.418844 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_o proxy err 0.006563653703778982 tr(WHW.T) 4.753800392150879
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it]  6%|▋         | 2/32 [00:02<00:32,  1.10s/it]  9%|▉         | 3/32 [00:03<00:30,  1.05s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.03s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.00s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.00s/it] 22%|██▏       | 7/32 [00:07<00:24,  1.00it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.00it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.01it/s]I0624 00:25:11.849123 8876 finetune.py:68] layer 6_o @ epoch 1 new loss 4.512564919423312e-05 old loss 4.553677354124375e-05 BETTER
 31%|███▏      | 10/32 [00:10<00:21,  1.01it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.01it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.01it/s] 41%|████      | 13/32 [00:13<00:18,  1.01it/s] 44%|████▍     | 14/32 [00:14<00:17,  1.01it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 50%|█████     | 16/32 [00:16<00:15,  1.01it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.01it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.01it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.01it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.01it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.01it/s]I0624 00:25:24.880319 8370 finetune.py:68] layer 5_o @ epoch 3 new loss 2.9907374482718296e-05 old loss 3.0074819733272307e-05 BETTER
 72%|███████▏  | 23/32 [00:22<00:08,  1.02it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.02it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.02it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.02it/s]I0624 00:25:29.436471 9386 finetune.py:68] layer 7_o @ epoch 0 new loss 6.380543345585465e-05 old loss 6.613553705392405e-05 BETTER
 84%|████████▍ | 27/32 [00:26<00:04,  1.02it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.03it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.03it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.03it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.03it/s]100%|██████████| 32/32 [00:31<00:00,  1.03it/s]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
W0624 00:25:41.139000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.140000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.140000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.140000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.140000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.140000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.140000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.169000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.169000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.169000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.169000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.169000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.185000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.185000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.185000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.185000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.186000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.348000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.348000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.348000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.348000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.348000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.575000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.575000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.575000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.575000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.575000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.575000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.575000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.595000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.595000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.595000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.595000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.595000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.664000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.664000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.664000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.664000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:25:41.664000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.535000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.846000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.846000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.846000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.846000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.846000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.846000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.846000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.868000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.868000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.868000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.868000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:25:42.868000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:25:43.131000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:25:43.132000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:25:43.132000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:25:43.132000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:25:43.132000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:25:43.397000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 00:25:44.078764 8876 finetune.py:68] layer 6_o @ epoch 2 new loss 4.483746306505054e-05 old loss 4.512564919423312e-05 BETTER
I0624 00:25:50.033349 7866 finetune.py:45] layer 4_up initial loss 3.635776010924019e-05
W0624 00:25:50.033773 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:25:57.891190 8370 finetune.py:68] layer 5_o @ epoch 4 new loss 2.9769662432954647e-05 old loss 2.9907374482718296e-05 BETTER
W0624 00:25:59.284658 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_o proxy err 0.00691594323143363 tr(WHW.T) 7.398568153381348
  0%|          | 0/32 [00:00<?, ?it/s]I0624 00:26:01.429319 9386 finetune.py:68] layer 7_o @ epoch 1 new loss 6.320150714600459e-05 old loss 6.380543345585465e-05 BETTER
  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]  6%|▋         | 2/32 [00:02<00:34,  1.15s/it]  9%|▉         | 3/32 [00:03<00:32,  1.11s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.09s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.08s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.07s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.06s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.06s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.06s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.06s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.05s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.05s/it] 41%|████      | 13/32 [00:13<00:19,  1.05s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.05s/it]I0624 00:26:16.391850 8876 finetune.py:68] layer 6_o @ epoch 3 new loss 4.460656055016443e-05 old loss 4.483746306505054e-05 BETTER
 47%|████▋     | 15/32 [00:15<00:17,  1.05s/it] 50%|█████     | 16/32 [00:17<00:16,  1.04s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.05s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.06s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.06s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.06s/it]I0624 00:26:22.815932 7866 finetune.py:68] layer 4_up @ epoch 0 new loss 3.6093475500820205e-05 old loss 3.635776010924019e-05 BETTER
 66%|██████▌   | 21/32 [00:22<00:11,  1.06s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.06s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.06s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.05s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.05s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.05s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.06s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.06s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.05s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.05s/it]I0624 00:26:33.658499 9386 finetune.py:68] layer 7_o @ epoch 2 new loss 6.277566717471927e-05 old loss 6.320150714600459e-05 BETTER
100%|██████████| 32/32 [00:33<00:00,  1.06s/it]100%|██████████| 32/32 [00:33<00:00,  1.06s/it]
W0624 00:26:41.526000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.526000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.526000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.526000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.527000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.527000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.527000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.553000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.553000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.553000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.554000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.554000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.568000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.568000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.568000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.569000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.569000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.729000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.729000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.729000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.730000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.730000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.959000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.959000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.959000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.960000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.960000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.960000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.960000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.980000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.980000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.980000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.980000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:26:41.981000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:26:42.043000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:26:42.043000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:26:42.043000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:26:42.043000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:26:42.043000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:26:42.935000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.259000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.259000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.259000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.259000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.259000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.259000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.259000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.281000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.281000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.281000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.281000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.281000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.551000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.552000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.552000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.552000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.552000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:26:43.812000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 00:26:49.060312 8876 finetune.py:68] layer 6_o @ epoch 4 new loss 4.440327757038176e-05 old loss 4.460656055016443e-05 BETTER
W0624 00:26:50.405406 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:26:50.701285 8370 finetune.py:45] layer 5_up initial loss 5.741582572227344e-05
W0624 00:26:50.701702 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

6_o proxy err 0.007557644974440336 tr(WHW.T) 10.478287696838379
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.29s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.07s/it]I0624 00:26:56.138402 7866 finetune.py:68] layer 4_up @ epoch 1 new loss 3.5968798329122365e-05 old loss 3.6093475500820205e-05 BETTER
 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.04s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.05s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.05s/it] 41%|████      | 13/32 [00:13<00:19,  1.05s/it]I0624 00:27:05.606627 9386 finetune.py:68] layer 7_o @ epoch 3 new loss 6.242149538593367e-05 old loss 6.277566717471927e-05 BETTER
 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.01s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.01s/it]I0624 00:27:21.742857 8370 finetune.py:68] layer 5_up @ epoch 0 new loss 5.7010038290172815e-05 old loss 5.741582572227344e-05 BETTER
 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.03s/it]
I0624 00:27:29.621793 7866 finetune.py:68] layer 4_up @ epoch 2 new loss 3.585933882277459e-05 old loss 3.5968798329122365e-05 BETTER
W0624 00:27:31.029000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.029000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.029000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.029000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.030000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.030000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.030000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.059000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.060000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.060000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.060000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.060000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.076000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.076000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.076000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.076000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.076000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.235000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.236000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.236000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.236000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.236000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.458000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.458000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.458000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.458000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.459000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.459000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.459000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.481000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.481000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.481000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.481000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.481000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.548000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.548000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.548000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.548000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:27:31.548000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.413000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.719000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.719000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.719000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.720000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.720000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.720000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.720000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.742000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.742000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.742000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.742000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.742000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.999000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:27:32.999000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:27:33.000000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:27:33.000000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:27:33.000000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:27:33.265000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 00:27:37.518031 9386 finetune.py:68] layer 7_o @ epoch 4 new loss 6.211177969817072e-05 old loss 6.242149538593367e-05 BETTER
W0624 00:27:38.924417 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:27:39.465364 8876 finetune.py:45] layer 6_up initial loss 8.673266711411998e-05
W0624 00:27:39.465739 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

7_o proxy err 0.008392338640987873 tr(WHW.T) 13.241655349731445
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.06s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.04s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.04s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 41%|████      | 13/32 [00:13<00:19,  1.02s/it]I0624 00:27:53.885396 8370 finetune.py:68] layer 5_up @ epoch 1 new loss 5.67774914088659e-05 old loss 5.7010038290172815e-05 BETTER
 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.03s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.03s/it]I0624 00:28:03.230151 7866 finetune.py:68] layer 4_up @ epoch 3 new loss 3.576481685740873e-05 old loss 3.585933882277459e-05 BETTER
 72%|███████▏  | 23/32 [00:23<00:09,  1.03s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.03s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.04s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.05s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.05s/it]I0624 00:28:10.098566 8876 finetune.py:68] layer 6_up @ epoch 0 new loss 8.602489106124267e-05 old loss 8.673266711411998e-05 BETTER
 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.04s/it]100%|██████████| 32/32 [00:33<00:00,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
W0624 00:28:19.899000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.899000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.899000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.899000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.899000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.900000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.900000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.928000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.928000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.928000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.928000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.928000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.943000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.943000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.943000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.943000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:28:19.943000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.101000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.101000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.102000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.102000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.102000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.332000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.332000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.333000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.333000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.333000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.333000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.333000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.353000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.354000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.354000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.354000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.354000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.418000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.418000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.418000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.419000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:28:20.419000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.294000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.605000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.605000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.605000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.605000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.605000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.606000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.606000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.626000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.626000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.626000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.626000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.626000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.888000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.889000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.889000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.889000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:28:21.889000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:28:22.147000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 00:28:25.670634 8370 finetune.py:68] layer 5_up @ epoch 2 new loss 5.657080328091979e-05 old loss 5.67774914088659e-05 BETTER
I0624 00:28:28.242685 9386 finetune.py:45] layer 7_up initial loss 0.00012013194645987824
W0624 00:28:28.243093 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:28:36.412607 7866 finetune.py:68] layer 4_up @ epoch 4 new loss 3.567102976376191e-05 old loss 3.576481685740873e-05 BETTER
W0624 00:28:37.647231 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_up proxy err 0.010307671502232552 tr(WHW.T) 408.0725402832031
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:02<00:33,  1.11s/it]I0624 00:28:41.418447 8876 finetune.py:68] layer 6_up @ epoch 1 new loss 8.559703564969823e-05 old loss 8.602489106124267e-05 BETTER
  9%|▉         | 3/32 [00:03<00:30,  1.05s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.02s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.01s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.00it/s] 22%|██▏       | 7/32 [00:07<00:24,  1.01it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.02it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.02it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.02it/s] 34%|███▍      | 11/32 [00:11<00:20,  1.02it/s] 38%|███▊      | 12/32 [00:12<00:19,  1.02it/s] 41%|████      | 13/32 [00:13<00:18,  1.02it/s] 44%|████▍     | 14/32 [00:13<00:17,  1.03it/s] 47%|████▋     | 15/32 [00:14<00:16,  1.02it/s] 50%|█████     | 16/32 [00:15<00:15,  1.01it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.01it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.01it/s]I0624 00:28:57.785830 8370 finetune.py:68] layer 5_up @ epoch 3 new loss 5.637534923152998e-05 old loss 5.657080328091979e-05 BETTER
 59%|█████▉    | 19/32 [00:18<00:12,  1.01it/s]I0624 00:28:58.257675 9386 finetune.py:68] layer 7_up @ epoch 0 new loss 0.00011899331730091944 old loss 0.00012013194645987824 BETTER
 62%|██████▎   | 20/32 [00:19<00:11,  1.02it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.02it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.02it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.02it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.01it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.01it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.01it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.01it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.01it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.01it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.01it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.02it/s]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]
I0624 00:29:13.080408 8876 finetune.py:68] layer 6_up @ epoch 2 new loss 8.52265366120264e-05 old loss 8.559703564969823e-05 BETTER
I0624 00:29:18.726281 7866 finetune.py:45] layer 4_gate initial loss 4.976259151590057e-05
W0624 00:29:18.726689 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:29:29.108875 9386 finetune.py:68] layer 7_up @ epoch 1 new loss 0.00011829635332105681 old loss 0.00011899331730091944 BETTER
I0624 00:29:30.287105 8370 finetune.py:68] layer 5_up @ epoch 4 new loss 5.619819421553984e-05 old loss 5.637534923152998e-05 BETTER
W0624 00:29:31.697596 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_up proxy err 0.01025333721190691 tr(WHW.T) 517.9923095703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.34s/it]  6%|▋         | 2/32 [00:02<00:35,  1.18s/it]  9%|▉         | 3/32 [00:03<00:32,  1.13s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.10s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.08s/it] 19%|█▉        | 6/32 [00:06<00:28,  1.08s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.07s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.07s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.06s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.06s/it]I0624 00:29:44.768538 8876 finetune.py:68] layer 6_up @ epoch 3 new loss 8.488517778459936e-05 old loss 8.52265366120264e-05 BETTER
 34%|███▍      | 11/32 [00:11<00:22,  1.06s/it] 38%|███▊      | 12/32 [00:12<00:21,  1.06s/it] 41%|████      | 13/32 [00:14<00:20,  1.06s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.06s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.06s/it] 50%|█████     | 16/32 [00:17<00:17,  1.07s/it]I0624 00:29:51.148972 7866 finetune.py:68] layer 4_gate @ epoch 0 new loss 4.947951310896315e-05 old loss 4.976259151590057e-05 BETTER
 53%|█████▎    | 17/32 [00:18<00:15,  1.06s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.06s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.06s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.06s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.06s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.05s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.05s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.05s/it]I0624 00:29:59.898771 9386 finetune.py:68] layer 7_up @ epoch 2 new loss 0.00011769211414502934 old loss 0.00011829635332105681 BETTER
 78%|███████▊  | 25/32 [00:26<00:07,  1.06s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.05s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.05s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.05s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.05s/it] 97%|█████████▋| 31/32 [00:33<00:01,  1.05s/it]100%|██████████| 32/32 [00:34<00:00,  1.04s/it]100%|██████████| 32/32 [00:34<00:00,  1.06s/it]
I0624 00:30:15.032588 8370 finetune.py:45] layer 5_gate initial loss 7.747240306343883e-05
W0624 00:30:15.032958 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:30:16.259852 8876 finetune.py:68] layer 6_up @ epoch 4 new loss 8.457988587906584e-05 old loss 8.488517778459936e-05 BETTER
W0624 00:30:17.503284 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_up proxy err 0.010503864847123623 tr(WHW.T) 628.2874145507812
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it]  6%|▋         | 2/32 [00:02<00:34,  1.16s/it]  9%|▉         | 3/32 [00:03<00:32,  1.11s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.08s/it]I0624 00:30:23.730655 7866 finetune.py:68] layer 4_gate @ epoch 1 new loss 4.938335041515529e-05 old loss 4.947951310896315e-05 BETTER
 16%|█▌        | 5/32 [00:05<00:28,  1.06s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.06s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.06s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.06s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.06s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.07s/it] 34%|███▍      | 11/32 [00:11<00:22,  1.07s/it]I0624 00:30:30.690405 9386 finetune.py:68] layer 7_up @ epoch 3 new loss 0.00011715365690179169 old loss 0.00011769211414502934 BETTER
 38%|███▊      | 12/32 [00:12<00:21,  1.06s/it] 41%|████      | 13/32 [00:13<00:20,  1.06s/it] 44%|████▍     | 14/32 [00:15<00:18,  1.05s/it] 47%|████▋     | 15/32 [00:16<00:17,  1.05s/it] 50%|█████     | 16/32 [00:17<00:16,  1.05s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.04s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.04s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.05s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.04s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.04s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.04s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.04s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.04s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.04s/it]I0624 00:30:45.475146 8370 finetune.py:68] layer 5_gate @ epoch 0 new loss 7.705070311203599e-05 old loss 7.747240306343883e-05 BETTER
 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.04s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.04s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.04s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.04s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]100%|██████████| 32/32 [00:33<00:00,  1.06s/it]
I0624 00:30:56.335705 7866 finetune.py:68] layer 4_gate @ epoch 2 new loss 4.930123031954281e-05 old loss 4.938335041515529e-05 BETTER
I0624 00:30:59.668272 8876 finetune.py:45] layer 6_gate initial loss 0.00011582543083932251
W0624 00:30:59.668851 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:31:01.339900 9386 finetune.py:68] layer 7_up @ epoch 4 new loss 0.0001166582151199691 old loss 0.00011715365690179169 BETTER
W0624 00:31:02.553496 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_up proxy err 0.010362912900745869 tr(WHW.T) 745.9009399414062
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it]  6%|▋         | 2/32 [00:02<00:34,  1.15s/it]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.07s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.04s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.04s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it]I0624 00:31:16.279695 8370 finetune.py:68] layer 5_gate @ epoch 1 new loss 7.687968172831461e-05 old loss 7.705070311203599e-05 BETTER
 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.04s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.05s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.05s/it]I0624 00:31:28.899690 7866 finetune.py:68] layer 4_gate @ epoch 3 new loss 4.922093285131268e-05 old loss 4.930123031954281e-05 BETTER
 75%|███████▌  | 24/32 [00:25<00:08,  1.05s/it]I0624 00:31:29.539060 8876 finetune.py:68] layer 6_gate @ epoch 0 new loss 0.00011512642231537029 old loss 0.00011582543083932251 BETTER
 78%|███████▊  | 25/32 [00:26<00:07,  1.04s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.04s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.03s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.03s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.02s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
I0624 00:31:44.119325 9386 finetune.py:45] layer 7_gate initial loss 0.0001590102765476331
W0624 00:31:44.119636 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:31:47.375208 8370 finetune.py:68] layer 5_gate @ epoch 2 new loss 7.67219316912815e-05 old loss 7.687968172831461e-05 BETTER
I0624 00:31:59.939911 8876 finetune.py:68] layer 6_gate @ epoch 1 new loss 0.00011482759146019816 old loss 0.00011512642231537029 BETTER
I0624 00:32:01.593874 7866 finetune.py:68] layer 4_gate @ epoch 4 new loss 4.9148202379001305e-05 old loss 4.922093285131268e-05 BETTER
W0624 00:32:02.627711 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_gate proxy err 0.0064096408896148205 tr(WHW.T) 810.9996337890625
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:50,  1.68it/s]  2%|▏         | 2/86 [00:00<00:37,  2.26it/s]  3%|▎         | 3/86 [00:01<00:32,  2.56it/s]  5%|▍         | 4/86 [00:01<00:30,  2.73it/s]  6%|▌         | 5/86 [00:01<00:28,  2.83it/s]  7%|▋         | 6/86 [00:02<00:27,  2.89it/s]  8%|▊         | 7/86 [00:02<00:26,  2.94it/s]  9%|▉         | 8/86 [00:02<00:26,  2.97it/s] 10%|█         | 9/86 [00:03<00:25,  2.98it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.99it/s] 13%|█▎        | 11/86 [00:03<00:24,  3.00it/s] 14%|█▍        | 12/86 [00:04<00:24,  3.00it/s] 15%|█▌        | 13/86 [00:04<00:24,  3.01it/s] 16%|█▋        | 14/86 [00:04<00:23,  3.02it/s] 17%|█▋        | 15/86 [00:05<00:23,  2.99it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.99it/s] 20%|█▉        | 17/86 [00:05<00:23,  3.00it/s] 21%|██        | 18/86 [00:06<00:22,  3.00it/s] 22%|██▏       | 19/86 [00:06<00:22,  3.01it/s] 23%|██▎       | 20/86 [00:06<00:21,  3.02it/s] 24%|██▍       | 21/86 [00:07<00:21,  3.03it/s] 26%|██▌       | 22/86 [00:07<00:21,  3.03it/s] 27%|██▋       | 23/86 [00:07<00:20,  3.03it/s]I0624 00:32:13.748473 9386 finetune.py:68] layer 7_gate @ epoch 0 new loss 0.0001579386298544705 old loss 0.0001590102765476331 BETTER
 28%|██▊       | 24/86 [00:08<00:20,  3.03it/s] 29%|██▉       | 25/86 [00:08<00:20,  3.01it/s] 30%|███       | 26/86 [00:08<00:20,  3.00it/s] 31%|███▏      | 27/86 [00:09<00:19,  2.99it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.97it/s] 34%|███▎      | 29/86 [00:09<00:19,  2.98it/s] 35%|███▍      | 30/86 [00:10<00:18,  2.97it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.97it/s] 37%|███▋      | 32/86 [00:10<00:18,  2.98it/s] 38%|███▊      | 33/86 [00:11<00:17,  2.98it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.99it/s] 41%|████      | 35/86 [00:11<00:17,  2.99it/s] 42%|████▏     | 36/86 [00:12<00:16,  2.99it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.99it/s] 44%|████▍     | 38/86 [00:12<00:16,  2.98it/s]I0624 00:32:18.595069 8370 finetune.py:68] layer 5_gate @ epoch 3 new loss 7.657647802261636e-05 old loss 7.67219316912815e-05 BETTER
 45%|████▌     | 39/86 [00:13<00:15,  2.99it/s] 47%|████▋     | 40/86 [00:13<00:15,  3.01it/s] 48%|████▊     | 41/86 [00:13<00:15,  2.98it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.98it/s] 50%|█████     | 43/86 [00:14<00:14,  2.98it/s] 51%|█████     | 44/86 [00:14<00:14,  2.99it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.00it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.01it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.01it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.01it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.02it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.03it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.04it/s] 60%|██████    | 52/86 [00:17<00:11,  3.02it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.01it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.01it/s] 64%|██████▍   | 55/86 [00:18<00:10,  2.99it/s] 65%|██████▌   | 56/86 [00:18<00:10,  3.00it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.00it/s] 67%|██████▋   | 58/86 [00:19<00:09,  3.00it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.00it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.02it/s] 71%|███████   | 61/86 [00:20<00:08,  3.02it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.02it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.03it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.03it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.02it/s] 77%|███████▋  | 66/86 [00:22<00:06,  3.04it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.03it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.02it/s] 80%|████████  | 69/86 [00:23<00:05,  3.00it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.00it/s] 83%|████████▎ | 71/86 [00:23<00:05,  2.99it/s] 84%|████████▎ | 72/86 [00:24<00:04,  3.01it/s] 85%|████████▍ | 73/86 [00:24<00:04,  3.01it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.02it/s] 87%|████████▋ | 75/86 [00:25<00:03,  3.02it/s]I0624 00:32:30.749672 8876 finetune.py:68] layer 6_gate @ epoch 2 new loss 0.00011455180356279016 old loss 0.00011482759146019816 BETTER
 88%|████████▊ | 76/86 [00:25<00:03,  3.02it/s] 90%|████████▉ | 77/86 [00:25<00:02,  3.03it/s] 91%|█████████ | 78/86 [00:26<00:02,  3.04it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.03it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.03it/s] 94%|█████████▍| 81/86 [00:27<00:01,  3.01it/s] 95%|█████████▌| 82/86 [00:27<00:01,  3.00it/s] 97%|█████████▋| 83/86 [00:27<00:01,  2.98it/s] 98%|█████████▊| 84/86 [00:28<00:00,  2.98it/s] 99%|█████████▉| 85/86 [00:28<00:00,  2.99it/s]100%|██████████| 86/86 [00:28<00:00,  3.00it/s]100%|██████████| 86/86 [00:28<00:00,  2.98it/s]
W0624 00:32:41.368000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.368000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.368000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.369000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.369000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.369000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.369000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.412000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.412000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.412000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.412000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.412000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.428000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.428000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.428000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.428000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.428000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.596000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.596000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.596000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.596000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.596000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.924000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.924000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.925000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.925000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.925000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.925000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.925000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.956000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.956000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.956000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.956000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:41.956000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:42.031000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:32:42.031000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:42.031000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:32:42.031000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:42.032000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.260000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.275000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.284000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.284000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.751000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.752000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.752000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.752000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.752000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.752000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.752000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.785000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.785000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.785000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.786000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:32:43.786000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
I0624 00:32:43.914226 9386 finetune.py:68] layer 7_gate @ epoch 1 new loss 0.0001574546768097207 old loss 0.0001579386298544705 BETTER
W0624 00:32:44.137000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.138000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.138000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.138000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.138000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.138000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.138000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.138000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.436000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.436000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.436000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.436000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.437000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.777000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:32:44.783000 140647068788544 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 00:32:49.692945 8370 finetune.py:68] layer 5_gate @ epoch 4 new loss 7.644034485565498e-05 old loss 7.657647802261636e-05 BETTER
W0624 00:32:50.959326 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:32:51.911251 7866 finetune.py:45] layer 4_down initial loss 8.066240843618289e-05
W0624 00:32:51.911654 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

5_gate proxy err 0.006174095440655947 tr(WHW.T) 1075.52001953125
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:51,  1.65it/s]  2%|▏         | 2/86 [00:00<00:38,  2.19it/s]  3%|▎         | 3/86 [00:01<00:34,  2.42it/s]  5%|▍         | 4/86 [00:01<00:32,  2.55it/s]  6%|▌         | 5/86 [00:02<00:30,  2.62it/s]  7%|▋         | 6/86 [00:02<00:29,  2.68it/s]  8%|▊         | 7/86 [00:02<00:29,  2.72it/s]  9%|▉         | 8/86 [00:03<00:28,  2.77it/s] 10%|█         | 9/86 [00:03<00:27,  2.79it/s] 12%|█▏        | 10/86 [00:03<00:27,  2.81it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.82it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.82it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.82it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.82it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.81it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.81it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.81it/s] 21%|██        | 18/86 [00:06<00:24,  2.79it/s] 22%|██▏       | 19/86 [00:07<00:24,  2.78it/s]I0624 00:33:01.256841 8876 finetune.py:68] layer 6_gate @ epoch 3 new loss 0.00011430442827986553 old loss 0.00011455180356279016 BETTER
 23%|██▎       | 20/86 [00:07<00:23,  2.78it/s] 24%|██▍       | 21/86 [00:07<00:23,  2.81it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.82it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.82it/s] 28%|██▊       | 24/86 [00:08<00:22,  2.81it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.81it/s] 30%|███       | 26/86 [00:09<00:21,  2.79it/s] 31%|███▏      | 27/86 [00:09<00:21,  2.79it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.80it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.80it/s] 35%|███▍      | 30/86 [00:10<00:20,  2.78it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.79it/s] 37%|███▋      | 32/86 [00:11<00:19,  2.77it/s] 38%|███▊      | 33/86 [00:12<00:19,  2.79it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.80it/s] 41%|████      | 35/86 [00:12<00:18,  2.80it/s] 42%|████▏     | 36/86 [00:13<00:17,  2.80it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.81it/s] 44%|████▍     | 38/86 [00:13<00:17,  2.82it/s] 45%|████▌     | 39/86 [00:14<00:16,  2.80it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.80it/s] 48%|████▊     | 41/86 [00:14<00:16,  2.80it/s] 49%|████▉     | 42/86 [00:15<00:15,  2.79it/s] 50%|█████     | 43/86 [00:15<00:15,  2.79it/s] 51%|█████     | 44/86 [00:15<00:14,  2.82it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.78it/s] 53%|█████▎    | 46/86 [00:16<00:14,  2.79it/s] 55%|█████▍    | 47/86 [00:17<00:13,  2.81it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.81it/s] 57%|█████▋    | 49/86 [00:17<00:13,  2.82it/s] 58%|█████▊    | 50/86 [00:18<00:12,  2.83it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.82it/s] 60%|██████    | 52/86 [00:18<00:12,  2.82it/s] 62%|██████▏   | 53/86 [00:19<00:11,  2.83it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.81it/s]I0624 00:33:13.853383 9386 finetune.py:68] layer 7_gate @ epoch 2 new loss 0.00015702872769907117 old loss 0.0001574546768097207 BETTER
 64%|██████▍   | 55/86 [00:19<00:11,  2.82it/s] 65%|██████▌   | 56/86 [00:20<00:10,  2.80it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.79it/s] 67%|██████▋   | 58/86 [00:20<00:10,  2.79it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.77it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.77it/s] 71%|███████   | 61/86 [00:22<00:09,  2.78it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.79it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.79it/s] 74%|███████▍  | 64/86 [00:23<00:07,  2.81it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.80it/s] 77%|███████▋  | 66/86 [00:23<00:07,  2.79it/s] 78%|███████▊  | 67/86 [00:24<00:06,  2.78it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.77it/s] 80%|████████  | 69/86 [00:24<00:06,  2.76it/s] 81%|████████▏ | 70/86 [00:25<00:05,  2.74it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.72it/s] 84%|████████▎ | 72/86 [00:25<00:05,  2.73it/s] 85%|████████▍ | 73/86 [00:26<00:04,  2.74it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.74it/s] 87%|████████▋ | 75/86 [00:27<00:03,  2.76it/s] 88%|████████▊ | 76/86 [00:27<00:03,  2.77it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.78it/s] 91%|█████████ | 78/86 [00:28<00:02,  2.78it/s]I0624 00:33:22.436202 7866 finetune.py:68] layer 4_down @ epoch 0 new loss 8.063426503213122e-05 old loss 8.066240843618289e-05 BETTER
 92%|█████████▏| 79/86 [00:28<00:02,  2.82it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.81it/s] 94%|█████████▍| 81/86 [00:29<00:01,  2.80it/s] 95%|█████████▌| 82/86 [00:29<00:01,  2.79it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.80it/s] 98%|█████████▊| 84/86 [00:30<00:00,  2.80it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.77it/s]100%|██████████| 86/86 [00:31<00:00,  2.78it/s]100%|██████████| 86/86 [00:31<00:00,  2.77it/s]
I0624 00:33:31.806960 8876 finetune.py:68] layer 6_gate @ epoch 4 new loss 0.00011406803241698071 old loss 0.00011430442827986553 BETTER
W0624 00:33:32.213000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.213000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.213000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.213000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.213000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.214000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.214000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.256000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.256000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.256000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.256000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.256000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.272000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.272000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.272000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.272000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.272000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.446000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.446000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.446000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.447000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.447000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.773000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.773000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.773000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.773000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.773000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.774000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.774000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.808000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.808000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.808000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.808000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.808000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.820948 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0624 00:33:32.878000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.878000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.878000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.878000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:32.878000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.093000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.104000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.112000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.112000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.571000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.572000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.572000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.572000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.572000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.572000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.572000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.602000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.602000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.603000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.603000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.603000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.960000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.961000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.961000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.961000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.961000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.961000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.961000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:34.961000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:35.263000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:33:35.263000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:33:35.263000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:33:35.263000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:33:35.263000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
6_gate proxy err 0.005828837398439646 tr(WHW.T) 1474.7581787109375
  0%|          | 0/86 [00:00<?, ?it/s]W0624 00:33:35.598000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:33:35.604000 140299974399808 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
  1%|          | 1/86 [00:00<00:51,  1.64it/s]  2%|▏         | 2/86 [00:00<00:38,  2.19it/s]  3%|▎         | 3/86 [00:01<00:33,  2.46it/s]  5%|▍         | 4/86 [00:01<00:31,  2.61it/s]  6%|▌         | 5/86 [00:02<00:30,  2.69it/s]  7%|▋         | 6/86 [00:02<00:29,  2.75it/s]  8%|▊         | 7/86 [00:02<00:28,  2.79it/s]  9%|▉         | 8/86 [00:03<00:27,  2.83it/s] 10%|█         | 9/86 [00:03<00:27,  2.85it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.86it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.85it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.84it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.83it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.82it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.82it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.81it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.80it/s] 21%|██        | 18/86 [00:06<00:24,  2.77it/s] 22%|██▏       | 19/86 [00:06<00:24,  2.78it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.79it/s]I0624 00:33:42.702157 8370 finetune.py:45] layer 5_down initial loss 0.00012263821554370224
W0624 00:33:42.702621 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 24%|██▍       | 21/86 [00:07<00:23,  2.79it/s] 26%|██▌       | 22/86 [00:08<00:22,  2.79it/s]I0624 00:33:43.641474 9386 finetune.py:68] layer 7_gate @ epoch 3 new loss 0.00015663857629988343 old loss 0.00015702872769907117 BETTER
 27%|██▋       | 23/86 [00:08<00:22,  2.81it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.86it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.85it/s] 30%|███       | 26/86 [00:09<00:20,  2.86it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.88it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.89it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.88it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.87it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.87it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.85it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.86it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.86it/s] 41%|████      | 35/86 [00:12<00:17,  2.87it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.87it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.88it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.89it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.90it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.90it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.89it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.89it/s] 50%|█████     | 43/86 [00:15<00:14,  2.88it/s] 51%|█████     | 44/86 [00:15<00:14,  2.87it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.85it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.86it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.88it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.87it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.88it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.88it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.90it/s] 60%|██████    | 52/86 [00:18<00:11,  2.90it/s]I0624 00:33:53.816698 7866 finetune.py:68] layer 4_down @ epoch 1 new loss 8.06302487035282e-05 old loss 8.063426503213122e-05 BETTER
 62%|██████▏   | 53/86 [00:18<00:11,  2.89it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.88it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.88it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.87it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.85it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.85it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.86it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.86it/s] 71%|███████   | 61/86 [00:21<00:08,  2.87it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.88it/s] 73%|███████▎  | 63/86 [00:22<00:07,  2.88it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.88it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.88it/s] 77%|███████▋  | 66/86 [00:23<00:06,  2.86it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.86it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.86it/s] 80%|████████  | 69/86 [00:24<00:05,  2.86it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.87it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.84it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.84it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.86it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.86it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.84it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.84it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.85it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.87it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.89it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.89it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.89it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.89it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.87it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.84it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.84it/s]100%|██████████| 86/86 [00:30<00:00,  2.85it/s]100%|██████████| 86/86 [00:30<00:00,  2.84it/s]
I0624 00:34:11.709775 8370 finetune.py:68] layer 5_down @ epoch 0 new loss 0.00012260099174454808 old loss 0.00012263821554370224 BETTER
W0624 00:34:12.451000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.452000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.452000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.452000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.452000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.452000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.452000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.495000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.495000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.495000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.495000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.495000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.511000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.511000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.511000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.512000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.512000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.686000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.686000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.686000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.686000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:12.686000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.018000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.018000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.018000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.018000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.018000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.019000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.019000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.054000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.054000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.055000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.055000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.055000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.127000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.127000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.127000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.128000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:13.128000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
I0624 00:34:13.431950 9386 finetune.py:68] layer 7_gate @ epoch 4 new loss 0.00015627073298674077 old loss 0.00015663857629988343 BETTER
W0624 00:34:14.305000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.317000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.325000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.325000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.416286 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0624 00:34:14.764000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.764000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.764000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.764000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.764000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.764000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.764000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.795000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.795000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.795000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.796000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:14.796000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.150000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.150000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.150000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.150000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.150000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.150000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.150000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.151000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.441000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.442000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.442000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.442000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.442000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.778000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:15.783000 140710093158208 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
7_gate proxy err 0.0057058511301875114 tr(WHW.T) 1749.750244140625
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:49,  1.71it/s]  2%|▏         | 2/86 [00:00<00:37,  2.22it/s]  3%|▎         | 3/86 [00:01<00:33,  2.46it/s]  5%|▍         | 4/86 [00:01<00:31,  2.59it/s]  6%|▌         | 5/86 [00:02<00:30,  2.67it/s]  7%|▋         | 6/86 [00:02<00:29,  2.72it/s]  8%|▊         | 7/86 [00:02<00:28,  2.76it/s]  9%|▉         | 8/86 [00:03<00:28,  2.78it/s] 10%|█         | 9/86 [00:03<00:27,  2.80it/s] 12%|█▏        | 10/86 [00:03<00:27,  2.80it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.80it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.81it/s] 15%|█▌        | 13/86 [00:04<00:26,  2.81it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.79it/s]I0624 00:34:22.306400 8876 finetune.py:45] layer 6_down initial loss 0.00018048929632641375
W0624 00:34:22.306795 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 17%|█▋        | 15/86 [00:05<00:25,  2.80it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.82it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.84it/s] 21%|██        | 18/86 [00:06<00:23,  2.86it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.87it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.88it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.89it/s] 26%|██▌       | 22/86 [00:07<00:22,  2.90it/s]I0624 00:34:25.003751 7866 finetune.py:68] layer 4_down @ epoch 2 new loss 8.062470442382619e-05 old loss 8.06302487035282e-05 BETTER
 27%|██▋       | 23/86 [00:08<00:21,  2.89it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.89it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.90it/s] 30%|███       | 26/86 [00:09<00:20,  2.90it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.87it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.88it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.89it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.91it/s] 36%|███▌      | 31/86 [00:11<00:18,  2.92it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.92it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.92it/s] 40%|███▉      | 34/86 [00:12<00:17,  2.91it/s] 41%|████      | 35/86 [00:12<00:17,  2.91it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.91it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.91it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.91it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.92it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.89it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.88it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.89it/s] 50%|█████     | 43/86 [00:15<00:14,  2.90it/s] 51%|█████     | 44/86 [00:15<00:14,  2.92it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.93it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.92it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.92it/s] 56%|█████▌    | 48/86 [00:16<00:13,  2.91it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.91it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.91it/s] 59%|█████▉    | 51/86 [00:17<00:12,  2.91it/s] 60%|██████    | 52/86 [00:18<00:11,  2.90it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.86it/s] 63%|██████▎   | 54/86 [00:18<00:11,  2.88it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.90it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.91it/s] 66%|██████▋   | 57/86 [00:20<00:09,  2.92it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.92it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.93it/s] 70%|██████▉   | 60/86 [00:21<00:08,  2.93it/s] 71%|███████   | 61/86 [00:21<00:08,  2.92it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.93it/s] 73%|███████▎  | 63/86 [00:22<00:07,  2.91it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.90it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.89it/s] 77%|███████▋  | 66/86 [00:23<00:06,  2.87it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.88it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.89it/s] 80%|████████  | 69/86 [00:24<00:05,  2.89it/s]I0624 00:34:41.164261 8370 finetune.py:68] layer 5_down @ epoch 1 new loss 0.00012259281356818974 old loss 0.00012260099174454808 BETTER
 81%|████████▏ | 70/86 [00:24<00:05,  2.90it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.90it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.91it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.91it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.92it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.92it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.90it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.89it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.89it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.88it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.89it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.88it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.89it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.89it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.89it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.86it/s]100%|██████████| 86/86 [00:30<00:00,  2.85it/s]100%|██████████| 86/86 [00:30<00:00,  2.86it/s]
I0624 00:34:50.793628 8876 finetune.py:68] layer 6_down @ epoch 0 new loss 0.00018041756993625313 old loss 0.00018048929632641375 BETTER
W0624 00:34:53.741000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.741000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.742000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.742000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.742000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.742000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.742000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.782000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.782000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.782000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.782000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.782000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.798000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.798000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.798000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.798000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.798000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.973000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.973000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.973000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.973000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:53.974000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.299000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.299000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.299000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.299000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.299000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.299000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.299000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.334000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.334000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.334000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.334000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.334000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.408000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.409000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.409000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.409000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:54.409000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:55.605000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:55.618000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:55.626000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:55.626000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.082000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.082000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.082000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.082000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.082000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.082000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.083000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.113000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.114000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.114000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.114000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.114000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
I0624 00:34:56.215371 7866 finetune.py:68] layer 4_down @ epoch 3 new loss 8.062369306571782e-05 old loss 8.062470442382619e-05 BETTER
W0624 00:34:56.465000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.465000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.465000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.466000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.466000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.466000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.466000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.466000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.769000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.769000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.770000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.770000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:34:56.770000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:34:57.123000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:34:57.128000 140123583870784 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 00:35:03.665753 9386 finetune.py:45] layer 7_down initial loss 0.00024495425168424845
W0624 00:35:03.666173 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:35:10.731667 8370 finetune.py:68] layer 5_down @ epoch 2 new loss 0.0001225862215505913 old loss 0.00012259281356818974 BETTER
I0624 00:35:20.043282 8876 finetune.py:68] layer 6_down @ epoch 1 new loss 0.0001804018538678065 old loss 0.00018041756993625313 BETTER
I0624 00:35:27.717317 7866 finetune.py:68] layer 4_down @ epoch 4 new loss 8.061729749897495e-05 old loss 8.062369306571782e-05 BETTER
W0624 00:35:28.420342 7866 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

4_down proxy err 0.010947093367576599 tr(WHW.T) 12.042593002319336
I0624 00:35:31.689302 9386 finetune.py:68] layer 7_down @ epoch 0 new loss 0.0002448612649459392 old loss 0.00024495425168424845 BETTER
I0624 00:35:40.640994 8370 finetune.py:68] layer 5_down @ epoch 3 new loss 0.00012258374772500247 old loss 0.0001225862215505913 BETTER
I0624 00:35:49.381148 8876 finetune.py:68] layer 6_down @ epoch 2 new loss 0.0001803968771127984 old loss 0.0001804018538678065 BETTER
I0624 00:36:00.336741 9386 finetune.py:68] layer 7_down @ epoch 1 new loss 0.0002448484592605382 old loss 0.0002448612649459392 BETTER
I0624 00:36:10.613450 8370 finetune.py:68] layer 5_down @ epoch 4 new loss 0.00012257337220944464 old loss 0.00012258374772500247 BETTER
W0624 00:36:11.337750 8370 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

5_down proxy err 0.012334304861724377 tr(WHW.T) 15.877784729003906
I0624 00:36:18.700257 8876 finetune.py:68] layer 6_down @ epoch 3 new loss 0.00018039345741271973 old loss 0.0001803968771127984 BETTER
I0624 00:36:28.935219 9386 finetune.py:68] layer 7_down @ epoch 2 new loss 0.00024483504239469767 old loss 0.0002448484592605382 BETTER
I0624 00:36:47.792787 8876 finetune.py:68] layer 6_down @ epoch 4 new loss 0.00018037772679235786 old loss 0.00018039345741271973 BETTER
W0624 00:36:48.488095 8876 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

6_down proxy err 0.012323820032179356 tr(WHW.T) 22.96258544921875
I0624 00:36:58.031240 9386 finetune.py:68] layer 7_down @ epoch 3 new loss 0.00024482564185746014 old loss 0.00024483504239469767 BETTER
I0624 00:37:17.713621 4970 quantize_finetune_llama.py:193] computed original embedding for layer 8 in 62.508666038513184s
I0624 00:37:18.145708 4970 quantize_finetune_llama.py:162] layer 9 gpu 1
I0624 00:37:20.304796 10082 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 00:37:20.304958 10082 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 00:37:20.305022 10082 utils.py:162] NumExpr defaulting to 16 threads.
I0624 00:37:20.498409 10082 config.py:58] PyTorch version 2.4.0 available.
I0624 00:37:22.923618 10082 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 00:37:23.292424 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:01<00:15,  1.82it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.67it/s]I0624 00:37:27.590837 9386 finetune.py:68] layer 7_down @ epoch 4 new loss 0.0002448143786750734 old loss 0.00024482564185746014 BETTER
 22%|██▏       | 7/32 [00:03<00:08,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.95it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.04it/s]W0624 00:37:28.440283 9386 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 31%|███▏      | 10/32 [00:04<00:07,  3.06it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.12it/s]7_down proxy err 0.01246328093111515 tr(WHW.T) 30.452617645263672
 38%|███▊      | 12/32 [00:04<00:06,  3.16it/s] 41%|████      | 13/32 [00:05<00:05,  3.18it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.21it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.23it/s] 50%|█████     | 16/32 [00:06<00:04,  3.24it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.20it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.22it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.24it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.24it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.25it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.25it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.21it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.16it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.15it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.15it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.18it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.21it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.21it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.23it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.24it/s]100%|██████████| 32/32 [00:11<00:00,  3.21it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]
W0624 00:37:38.529000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.530000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.530000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.530000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.530000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.530000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.530000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.556000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.556000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.556000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.556000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.556000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.572000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.572000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.572000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.572000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.572000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.885000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.885000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.885000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.885000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:37:38.885000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.740000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.740000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.741000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.741000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.741000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.741000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.741000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.758000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.758000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.758000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.758000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.758000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:37:39.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.131000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.132000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.132000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.132000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.132000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.132000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.132000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.149000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.149000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.149000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.150000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:37:41.150000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:37:42.020000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:37:42.020000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:37:42.020000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:37:42.020000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:37:42.020000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 00:37:48.132501 10082 finetune.py:45] layer 8_v initial loss 9.66483712545596e-05
W0624 00:37:48.134178 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:38:17.762162 4970 quantize_finetune_llama.py:193] computed original embedding for layer 9 in 59.154468059539795s
I0624 00:38:18.149948 4970 quantize_finetune_llama.py:162] layer 10 gpu 2
I0624 00:38:20.167548 10586 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 00:38:20.167711 10586 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 00:38:20.167792 10586 utils.py:162] NumExpr defaulting to 16 threads.
I0624 00:38:20.388153 10586 config.py:58] PyTorch version 2.4.0 available.
I0624 00:38:21.683068 10082 finetune.py:68] layer 8_v @ epoch 0 new loss 3.7875957787036896e-05 old loss 9.66483712545596e-05 BETTER
I0624 00:38:22.958792 10586 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 00:38:23.369891 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:52,  1.68s/it]  6%|▋         | 2/32 [00:02<00:26,  1.12it/s]  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.75it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.80it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.80it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0624 00:38:40.916000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.916000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.916000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.916000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.917000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.917000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.917000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.943000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.943000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.943000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.943000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.943000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.960000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.960000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.960000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.960000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:38:40.960000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:38:41.277000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:38:41.277000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:38:41.278000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:38:41.278000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:38:41.278000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.176000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.176000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.176000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.176000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.176000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.177000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.177000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.194000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.194000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.194000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.194000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.194000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.436000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.437000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.437000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.437000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:38:42.437000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.601000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.602000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.602000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.602000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.602000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.602000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.602000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.620000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.620000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.620000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.620000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:38:43.620000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:38:44.540000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:38:44.540000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:38:44.540000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:38:44.540000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:38:44.540000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 00:38:51.896010 10586 finetune.py:45] layer 9_v initial loss 0.0001079495923477225
W0624 00:38:51.896242 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:38:56.070730 10082 finetune.py:68] layer 8_v @ epoch 1 new loss 3.3456639357609674e-05 old loss 3.7875957787036896e-05 BETTER
I0624 00:39:19.160006 4970 quantize_finetune_llama.py:193] computed original embedding for layer 10 in 60.40869736671448s
I0624 00:39:19.607695 4970 quantize_finetune_llama.py:162] layer 11 gpu 3
I0624 00:39:21.547228 11092 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 00:39:21.547365 11092 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 00:39:21.547428 11092 utils.py:162] NumExpr defaulting to 16 threads.
I0624 00:39:21.861474 11092 config.py:58] PyTorch version 2.4.0 available.
I0624 00:39:24.531540 11092 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 00:39:24.921623 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:39:25.116317 10586 finetune.py:68] layer 9_v @ epoch 0 new loss 4.887884279014543e-05 old loss 0.0001079495923477225 BETTER
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:54,  1.74s/it]  6%|▋         | 2/32 [00:02<00:27,  1.08it/s]  9%|▉         | 3/32 [00:02<00:19,  1.51it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.85it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.11it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.57it/s]I0624 00:39:30.362184 10082 finetune.py:68] layer 8_v @ epoch 2 new loss 3.173973527736962e-05 old loss 3.3456639357609674e-05 BETTER
 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.84it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.88it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.90it/s] 50%|█████     | 16/32 [00:06<00:05,  2.92it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.88it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.87it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
W0624 00:39:41.840000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.840000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.840000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.840000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.840000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.840000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.840000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.866000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.866000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.866000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.866000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.866000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.882000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.882000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.882000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.883000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:39:41.883000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:39:42.211000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:39:42.211000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:39:42.211000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:39:42.211000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:39:42.211000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.111000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.111000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.111000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.111000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.111000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.111000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.111000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.129000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.129000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.130000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.130000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.130000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.382000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.382000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.382000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.382000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:39:43.382000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.604000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.604000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.604000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.604000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.604000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.605000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.605000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.623000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.623000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.623000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.623000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:39:44.623000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:39:45.560000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:39:45.561000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:39:45.561000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:39:45.561000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:39:45.561000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 00:39:51.541977 11092 finetune.py:45] layer 10_v initial loss 0.00011333884322084486
W0624 00:39:51.542155 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:39:59.073224 10586 finetune.py:68] layer 9_v @ epoch 1 new loss 4.435018854564987e-05 old loss 4.887884279014543e-05 BETTER
I0624 00:40:04.835838 10082 finetune.py:68] layer 8_v @ epoch 3 new loss 3.077752626268193e-05 old loss 3.173973527736962e-05 BETTER
I0624 00:40:16.868218 4970 quantize_finetune_llama.py:193] computed original embedding for layer 11 in 56.621779441833496s
I0624 00:40:17.264197 4970 quantize_finetune_llama.py:162] layer 12 gpu 0
I0624 00:40:19.217638 11598 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 00:40:19.217745 11598 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 00:40:19.217806 11598 utils.py:162] NumExpr defaulting to 16 threads.
I0624 00:40:19.396462 11598 config.py:58] PyTorch version 2.4.0 available.
I0624 00:40:21.760688 11598 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 00:40:22.123710 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:40:23.008440 11092 finetune.py:68] layer 10_v @ epoch 0 new loss 6.492834654636681e-05 old loss 0.00011333884322084486 BETTER
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:53,  1.73s/it]  6%|▋         | 2/32 [00:02<00:27,  1.09it/s]  9%|▉         | 3/32 [00:02<00:19,  1.53it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.84it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.83it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.89it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.91it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.97it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.97it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.98it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.99it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.99it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.01it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.01it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.98it/s]I0624 00:40:33.138792 10586 finetune.py:68] layer 9_v @ epoch 2 new loss 4.243627699906938e-05 old loss 4.435018854564987e-05 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.97it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.95it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.96it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.96it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:12<00:00,  2.98it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
W0624 00:40:38.445000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.446000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.446000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.446000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.446000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.446000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.446000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.474000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.474000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.474000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.474000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.474000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.491000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.491000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.491000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.492000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.492000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.819000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.819000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.819000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.819000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:40:38.819000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
I0624 00:40:39.461793 10082 finetune.py:68] layer 8_v @ epoch 4 new loss 3.008744533872232e-05 old loss 3.077752626268193e-05 BETTER
W0624 00:40:39.763000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.764000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.764000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.764000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.764000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.764000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.764000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.783000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.783000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.783000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.783000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:40:39.783000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:40:40.025000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:40:40.026000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:40:40.026000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:40:40.026000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:40:40.026000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:40:40.956226 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0624 00:40:41.239000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.239000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.239000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.239000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.239000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.239000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.239000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.258000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.258000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.258000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.258000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:40:41.258000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
8_v proxy err 0.008236249908804893 tr(WHW.T) 554.015380859375
  0%|          | 0/32 [00:00<?, ?it/s]W0624 00:40:42.178000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:40:42.178000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:40:42.178000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:40:42.178000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:40:42.178000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
  6%|▋         | 2/32 [00:00<00:13,  2.30it/s]  9%|▉         | 3/32 [00:01<00:11,  2.61it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.79it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.93it/s] 19%|█▉        | 6/32 [00:02<00:08,  3.00it/s] 22%|██▏       | 7/32 [00:02<00:08,  3.04it/s] 25%|██▌       | 8/32 [00:02<00:07,  3.07it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.10it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.11it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.10it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.09it/s] 41%|████      | 13/32 [00:04<00:06,  3.10it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.10it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.10it/s] 50%|█████     | 16/32 [00:05<00:05,  3.10it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.10it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.14it/s]I0624 00:40:47.983535 11598 finetune.py:45] layer 11_v initial loss 0.0001460045896237716
W0624 00:40:47.983707 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 19/32 [00:06<00:04,  3.14it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.14it/s] 66%|██████▌   | 21/32 [00:06<00:03,  3.14it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.16it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.15it/s] 75%|███████▌  | 24/32 [00:07<00:02,  3.14it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.13it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.15it/s] 84%|████████▍ | 27/32 [00:08<00:01,  3.12it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.11it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.07it/s] 94%|█████████▍| 30/32 [00:09<00:00,  3.06it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.06it/s]100%|██████████| 32/32 [00:10<00:00,  3.06it/s]100%|██████████| 32/32 [00:10<00:00,  3.03it/s]
I0624 00:40:55.263061 11092 finetune.py:68] layer 10_v @ epoch 1 new loss 6.0390233556972817e-05 old loss 6.492834654636681e-05 BETTER
I0624 00:40:59.205153 10082 finetune.py:45] layer 8_q initial loss 4.585370697895996e-05
W0624 00:40:59.205510 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:41:06.615074 10586 finetune.py:68] layer 9_v @ epoch 3 new loss 4.12646695622243e-05 old loss 4.243627699906938e-05 BETTER
I0624 00:41:19.556483 11598 finetune.py:68] layer 11_v @ epoch 0 new loss 7.151455065468326e-05 old loss 0.0001460045896237716 BETTER
I0624 00:41:28.169597 11092 finetune.py:68] layer 10_v @ epoch 2 new loss 5.799283098895103e-05 old loss 6.0390233556972817e-05 BETTER
I0624 00:41:33.548436 10082 finetune.py:68] layer 8_q @ epoch 0 new loss 4.3942753109149635e-05 old loss 4.585370697895996e-05 BETTER
I0624 00:41:40.488153 10586 finetune.py:68] layer 9_v @ epoch 4 new loss 4.047756374347955e-05 old loss 4.12646695622243e-05 BETTER
W0624 00:41:42.240437 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_v proxy err 0.008578560315072536 tr(WHW.T) 592.6890258789062
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:20,  1.55it/s]  6%|▋         | 2/32 [00:01<00:14,  2.05it/s]  9%|▉         | 3/32 [00:01<00:12,  2.30it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.44it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.53it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.60it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s]I0624 00:41:51.989304 11598 finetune.py:68] layer 11_v @ epoch 1 new loss 6.501469761133194e-05 old loss 7.151455065468326e-05 BETTER
 72%|███████▏  | 23/32 [00:08<00:03,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
I0624 00:42:01.253356 11092 finetune.py:68] layer 10_v @ epoch 3 new loss 5.642090764013119e-05 old loss 5.799283098895103e-05 BETTER
I0624 00:42:02.340596 10586 finetune.py:45] layer 9_q initial loss 6.0196984122740105e-05
W0624 00:42:02.341183 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:42:08.442209 10082 finetune.py:68] layer 8_q @ epoch 1 new loss 4.300404543755576e-05 old loss 4.3942753109149635e-05 BETTER
I0624 00:42:24.607558 11598 finetune.py:68] layer 11_v @ epoch 2 new loss 6.192571891006082e-05 old loss 6.501469761133194e-05 BETTER
I0624 00:42:34.263319 11092 finetune.py:68] layer 10_v @ epoch 4 new loss 5.52843303012196e-05 old loss 5.642090764013119e-05 BETTER
I0624 00:42:35.262427 10586 finetune.py:68] layer 9_q @ epoch 0 new loss 5.798986603622325e-05 old loss 6.0196984122740105e-05 BETTER
W0624 00:42:35.764659 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_v proxy err 0.008422265760600567 tr(WHW.T) 611.7044677734375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.61it/s]  6%|▋         | 2/32 [00:00<00:13,  2.21it/s]  9%|▉         | 3/32 [00:01<00:11,  2.48it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.74it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.77it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 41%|████      | 13/32 [00:04<00:06,  2.84it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.86it/s] 50%|█████     | 16/32 [00:05<00:05,  2.86it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s]I0624 00:42:43.344969 10082 finetune.py:68] layer 8_q @ epoch 2 new loss 4.230042759445496e-05 old loss 4.300404543755576e-05 BETTER
 59%|█████▉    | 19/32 [00:06<00:04,  2.91it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.93it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.96it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.96it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.97it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.97it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.96it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.97it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
I0624 00:42:54.557270 11092 finetune.py:45] layer 10_q initial loss 7.880234625190496e-05
W0624 00:42:54.557694 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:42:57.230684 11598 finetune.py:68] layer 11_v @ epoch 3 new loss 6.008455602568574e-05 old loss 6.192571891006082e-05 BETTER
I0624 00:43:08.485333 10586 finetune.py:68] layer 9_q @ epoch 1 new loss 5.683850031346083e-05 old loss 5.798986603622325e-05 BETTER
I0624 00:43:18.007624 10082 finetune.py:68] layer 8_q @ epoch 3 new loss 4.173228080617264e-05 old loss 4.230042759445496e-05 BETTER
I0624 00:43:26.568875 11092 finetune.py:68] layer 10_q @ epoch 0 new loss 7.598403317388147e-05 old loss 7.880234625190496e-05 BETTER
I0624 00:43:29.999433 11598 finetune.py:68] layer 11_v @ epoch 4 new loss 5.871194298379123e-05 old loss 6.008455602568574e-05 BETTER
W0624 00:43:31.650974 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_v proxy err 0.008972583338618279 tr(WHW.T) 768.251708984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:20,  1.54it/s]  6%|▋         | 2/32 [00:01<00:14,  2.10it/s]  9%|▉         | 3/32 [00:01<00:12,  2.39it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.53it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.63it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.68it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.75it/s] 41%|████      | 13/32 [00:04<00:06,  2.74it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.72it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.70it/s]I0624 00:43:41.762027 10586 finetune.py:68] layer 9_q @ epoch 2 new loss 5.597582276095636e-05 old loss 5.683850031346083e-05 BETTER
 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.75it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.79it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.79it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
I0624 00:43:51.396832 11598 finetune.py:45] layer 11_q initial loss 8.352679287781939e-05
W0624 00:43:51.397155 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:43:52.882638 10082 finetune.py:68] layer 8_q @ epoch 4 new loss 4.122559403185733e-05 old loss 4.173228080617264e-05 BETTER
W0624 00:43:54.361462 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_q proxy err 0.0020568575710058212 tr(WHW.T) 6955.78955078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.67it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s]  9%|▉         | 3/32 [00:01<00:11,  2.54it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.71it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.81it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.87it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.92it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.95it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.97it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.98it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.99it/s]I0624 00:43:59.459193 11092 finetune.py:68] layer 10_q @ epoch 1 new loss 7.450763223459944e-05 old loss 7.598403317388147e-05 BETTER
 38%|███▊      | 12/32 [00:04<00:06,  3.04it/s] 41%|████      | 13/32 [00:04<00:06,  3.10it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.05it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.07it/s] 50%|█████     | 16/32 [00:05<00:05,  3.08it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.08it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.10it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.12it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.12it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.11it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.12it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.14it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.13it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.11it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.13it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.12it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.09it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.10it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.11it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.12it/s]100%|██████████| 32/32 [00:10<00:00,  3.11it/s]100%|██████████| 32/32 [00:10<00:00,  3.00it/s]
I0624 00:44:12.961969 10082 finetune.py:45] layer 8_k initial loss 5.2741375839104876e-05
W0624 00:44:12.962402 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:44:15.467839 10586 finetune.py:68] layer 9_q @ epoch 3 new loss 5.521465573110618e-05 old loss 5.597582276095636e-05 BETTER
I0624 00:44:23.621120 11598 finetune.py:68] layer 11_q @ epoch 0 new loss 8.044960850384086e-05 old loss 8.352679287781939e-05 BETTER
I0624 00:44:32.649759 11092 finetune.py:68] layer 10_q @ epoch 2 new loss 7.33711858629249e-05 old loss 7.450763223459944e-05 BETTER
I0624 00:44:47.263296 10082 finetune.py:68] layer 8_k @ epoch 0 new loss 5.1382608944550157e-05 old loss 5.2741375839104876e-05 BETTER
I0624 00:44:49.101918 10586 finetune.py:68] layer 9_q @ epoch 4 new loss 5.4602041927864775e-05 old loss 5.521465573110618e-05 BETTER
W0624 00:44:50.659530 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_q proxy err 0.002322982996702194 tr(WHW.T) 6604.3642578125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.56it/s]  6%|▋         | 2/32 [00:01<00:14,  2.07it/s]  9%|▉         | 3/32 [00:01<00:12,  2.31it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.44it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.52it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.64it/s]I0624 00:44:56.828033 11598 finetune.py:68] layer 11_q @ epoch 1 new loss 7.896323222666979e-05 old loss 8.044960850384086e-05 BETTER
 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.72it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.72it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.73it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.75it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.71it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
I0624 00:45:05.681018 11092 finetune.py:68] layer 10_q @ epoch 3 new loss 7.2379109042231e-05 old loss 7.33711858629249e-05 BETTER
I0624 00:45:10.860997 10586 finetune.py:45] layer 9_k initial loss 7.04419580870308e-05
W0624 00:45:10.861346 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:45:21.863535 10082 finetune.py:68] layer 8_k @ epoch 1 new loss 5.095005326438695e-05 old loss 5.1382608944550157e-05 BETTER
I0624 00:45:30.133923 11598 finetune.py:68] layer 11_q @ epoch 2 new loss 7.777690188959241e-05 old loss 7.896323222666979e-05 BETTER
I0624 00:45:38.548101 11092 finetune.py:68] layer 10_q @ epoch 4 new loss 7.153137266868725e-05 old loss 7.2379109042231e-05 BETTER
W0624 00:45:40.167899 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_q proxy err 0.0023750278633087873 tr(WHW.T) 6659.2216796875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.68it/s]  6%|▋         | 2/32 [00:00<00:13,  2.26it/s]  9%|▉         | 3/32 [00:01<00:11,  2.52it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.65it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s]I0624 00:45:43.555344 10586 finetune.py:68] layer 9_k @ epoch 0 new loss 6.868752825539559e-05 old loss 7.04419580870308e-05 BETTER
 22%|██▏       | 7/32 [00:02<00:08,  2.85it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.86it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.89it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 41%|████      | 13/32 [00:04<00:06,  2.91it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.93it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 50%|█████     | 16/32 [00:05<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.96it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.92it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.93it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.96it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.94it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.96it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]
I0624 00:45:56.428684 10082 finetune.py:68] layer 8_k @ epoch 2 new loss 5.061080082668923e-05 old loss 5.095005326438695e-05 BETTER
I0624 00:45:59.180952 11092 finetune.py:45] layer 10_k initial loss 9.066374332178384e-05
W0624 00:45:59.181288 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:46:03.058904 11598 finetune.py:68] layer 11_q @ epoch 3 new loss 7.685755554120988e-05 old loss 7.777690188959241e-05 BETTER
I0624 00:46:16.861967 10586 finetune.py:68] layer 9_k @ epoch 1 new loss 6.808806938352063e-05 old loss 6.868752825539559e-05 BETTER
I0624 00:46:30.814135 11092 finetune.py:68] layer 10_k @ epoch 0 new loss 8.84606852196157e-05 old loss 9.066374332178384e-05 BETTER
I0624 00:46:31.015580 10082 finetune.py:68] layer 8_k @ epoch 3 new loss 5.032326589571312e-05 old loss 5.061080082668923e-05 BETTER
I0624 00:46:36.244358 11598 finetune.py:68] layer 11_q @ epoch 4 new loss 7.601291144965217e-05 old loss 7.685755554120988e-05 BETTER
W0624 00:46:37.700772 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_q proxy err 0.0025634784251451492 tr(WHW.T) 6864.81494140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.64it/s]  6%|▋         | 2/32 [00:00<00:13,  2.17it/s]  9%|▉         | 3/32 [00:01<00:11,  2.45it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.59it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.71it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.74it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.79it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 41%|████      | 13/32 [00:04<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 50%|█████     | 16/32 [00:05<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.73it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s]I0624 00:46:49.844897 10586 finetune.py:68] layer 9_k @ epoch 2 new loss 6.760854012100026e-05 old loss 6.808806938352063e-05 BETTER
 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
I0624 00:46:57.167558 11598 finetune.py:45] layer 11_k initial loss 9.472037345403805e-05
W0624 00:46:57.168109 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:47:03.364413 11092 finetune.py:68] layer 10_k @ epoch 1 new loss 8.767117105890065e-05 old loss 8.84606852196157e-05 BETTER
I0624 00:47:05.447319 10082 finetune.py:68] layer 8_k @ epoch 4 new loss 5.0070728320861235e-05 old loss 5.032326589571312e-05 BETTER
W0624 00:47:06.888610 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_k proxy err 0.0014475577045232058 tr(WHW.T) 9993.60546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.69it/s]  6%|▋         | 2/32 [00:00<00:12,  2.32it/s]  9%|▉         | 3/32 [00:01<00:11,  2.60it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.77it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.87it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.94it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.96it/s] 25%|██▌       | 8/32 [00:02<00:08,  3.00it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.02it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.04it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.04it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.04it/s] 41%|████      | 13/32 [00:04<00:06,  3.06it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.08it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.09it/s] 50%|█████     | 16/32 [00:05<00:05,  3.08it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.10it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.11it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.09it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.08it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.10it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.06it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.04it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.05it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.06it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.07it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.07it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.06it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.08it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.10it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.08it/s]100%|██████████| 32/32 [00:10<00:00,  3.08it/s]100%|██████████| 32/32 [00:10<00:00,  3.00it/s]
I0624 00:47:23.367100 10586 finetune.py:68] layer 9_k @ epoch 3 new loss 6.721559475408867e-05 old loss 6.760854012100026e-05 BETTER
I0624 00:47:25.530191 10082 finetune.py:45] layer 8_o initial loss 9.163826325675473e-05
W0624 00:47:25.530561 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:47:29.232441 11598 finetune.py:68] layer 11_k @ epoch 0 new loss 9.261524974135682e-05 old loss 9.472037345403805e-05 BETTER
I0624 00:47:36.074318 11092 finetune.py:68] layer 10_k @ epoch 2 new loss 8.70385265443474e-05 old loss 8.767117105890065e-05 BETTER
I0624 00:47:56.716266 10586 finetune.py:68] layer 9_k @ epoch 4 new loss 6.686941196676344e-05 old loss 6.721559475408867e-05 BETTER
W0624 00:47:58.261453 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_k proxy err 0.00158040935639292 tr(WHW.T) 10201.720703125
  0%|          | 0/32 [00:00<?, ?it/s]I0624 00:47:59.431473 10082 finetune.py:68] layer 8_o @ epoch 0 new loss 8.897530642570928e-05 old loss 9.163826325675473e-05 BETTER
  3%|▎         | 1/32 [00:00<00:19,  1.60it/s]  6%|▋         | 2/32 [00:00<00:14,  2.12it/s]  9%|▉         | 3/32 [00:01<00:12,  2.37it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.50it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.57it/s]I0624 00:48:01.736473 11598 finetune.py:68] layer 11_k @ epoch 1 new loss 9.191066055791453e-05 old loss 9.261524974135682e-05 BETTER
 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.70it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.73it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.75it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.77it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.76it/s]I0624 00:48:09.203840 11092 finetune.py:68] layer 10_k @ epoch 3 new loss 8.646179776405916e-05 old loss 8.70385265443474e-05 BETTER
 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
I0624 00:48:18.195690 10586 finetune.py:45] layer 9_o initial loss 0.0001259996060980484
W0624 00:48:18.195976 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:48:33.955443 10082 finetune.py:68] layer 8_o @ epoch 1 new loss 8.817078196443617e-05 old loss 8.897530642570928e-05 BETTER
I0624 00:48:34.448338 11598 finetune.py:68] layer 11_k @ epoch 2 new loss 9.130864782491699e-05 old loss 9.191066055791453e-05 BETTER
I0624 00:48:41.571712 11092 finetune.py:68] layer 10_k @ epoch 4 new loss 8.599375723861158e-05 old loss 8.646179776405916e-05 BETTER
W0624 00:48:43.078011 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_k proxy err 0.0016321510775014758 tr(WHW.T) 10306.509765625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.68it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.76it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.84it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.90it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.92it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.93it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.95it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.97it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.94it/s] 41%|████      | 13/32 [00:04<00:06,  2.95it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.96it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.98it/s] 50%|█████     | 16/32 [00:05<00:05,  2.98it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.98it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.99it/s]I0624 00:48:50.568042 10586 finetune.py:68] layer 9_o @ epoch 0 new loss 0.00012246155529282987 old loss 0.0001259996060980484 BETTER
 59%|█████▉    | 19/32 [00:06<00:04,  3.00it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.01it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.00it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.01it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.03it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.02it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.01it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.00it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.02it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.98it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.00it/s]100%|██████████| 32/32 [00:10<00:00,  2.98it/s]100%|██████████| 32/32 [00:10<00:00,  2.91it/s]
I0624 00:49:01.647509 11092 finetune.py:45] layer 10_o initial loss 0.00016343370953109115
W0624 00:49:01.647946 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:49:07.215595 11598 finetune.py:68] layer 11_k @ epoch 3 new loss 9.09263762878254e-05 old loss 9.130864782491699e-05 BETTER
I0624 00:49:08.149398 10082 finetune.py:68] layer 8_o @ epoch 2 new loss 8.757461182540283e-05 old loss 8.817078196443617e-05 BETTER
I0624 00:49:23.653374 10586 finetune.py:68] layer 9_o @ epoch 1 new loss 0.0001213203213410452 old loss 0.00012246155529282987 BETTER
I0624 00:49:33.157870 11092 finetune.py:68] layer 10_o @ epoch 0 new loss 0.00015990040265023708 old loss 0.00016343370953109115 BETTER
I0624 00:49:39.796940 11598 finetune.py:68] layer 11_k @ epoch 4 new loss 9.040370787261054e-05 old loss 9.09263762878254e-05 BETTER
W0624 00:49:41.357359 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_k proxy err 0.0017530940240249038 tr(WHW.T) 9986.591796875
  0%|          | 0/32 [00:00<?, ?it/s]I0624 00:49:42.518023 10082 finetune.py:68] layer 8_o @ epoch 3 new loss 8.707707456778735e-05 old loss 8.757461182540283e-05 BETTER
  3%|▎         | 1/32 [00:00<00:18,  1.64it/s]  6%|▋         | 2/32 [00:00<00:13,  2.18it/s]  9%|▉         | 3/32 [00:01<00:12,  2.40it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.52it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.61it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.70it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 41%|████      | 13/32 [00:04<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.79it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.77it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
I0624 00:49:56.859453 10586 finetune.py:68] layer 9_o @ epoch 2 new loss 0.00012041980517096817 old loss 0.0001213203213410452 BETTER
I0624 00:50:01.003327 11598 finetune.py:45] layer 11_o initial loss 0.0001767631620168686
W0624 00:50:01.003709 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:50:05.200152 11092 finetune.py:68] layer 10_o @ epoch 1 new loss 0.00015842021093703806 old loss 0.00015990040265023708 BETTER
I0624 00:50:16.946251 10082 finetune.py:68] layer 8_o @ epoch 4 new loss 8.663574408274144e-05 old loss 8.707707456778735e-05 BETTER
W0624 00:50:18.301911 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_o proxy err 0.009229235351085663 tr(WHW.T) 17.81512451171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it]  6%|▋         | 2/32 [00:02<00:32,  1.08s/it]  9%|▉         | 3/32 [00:03<00:29,  1.03s/it] 12%|█▎        | 4/32 [00:04<00:27,  1.00it/s] 16%|█▌        | 5/32 [00:05<00:26,  1.02it/s] 19%|█▉        | 6/32 [00:06<00:25,  1.03it/s] 22%|██▏       | 7/32 [00:06<00:24,  1.03it/s] 25%|██▌       | 8/32 [00:07<00:23,  1.04it/s] 28%|██▊       | 9/32 [00:08<00:22,  1.04it/s] 31%|███▏      | 10/32 [00:09<00:21,  1.04it/s] 34%|███▍      | 11/32 [00:10<00:20,  1.04it/s]I0624 00:50:30.464082 10586 finetune.py:68] layer 9_o @ epoch 3 new loss 0.00011967260070377961 old loss 0.00012041980517096817 BETTER
 38%|███▊      | 12/32 [00:11<00:19,  1.04it/s] 41%|████      | 13/32 [00:12<00:18,  1.04it/s] 44%|████▍     | 14/32 [00:13<00:17,  1.04it/s]I0624 00:50:33.384455 11598 finetune.py:68] layer 11_o @ epoch 0 new loss 0.0001724609173834324 old loss 0.0001767631620168686 BETTER
 47%|████▋     | 15/32 [00:14<00:16,  1.04it/s] 50%|█████     | 16/32 [00:15<00:15,  1.03it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.02it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.02it/s]I0624 00:50:37.873998 11092 finetune.py:68] layer 10_o @ epoch 2 new loss 0.0001572354231029749 old loss 0.00015842021093703806 BETTER
 59%|█████▉    | 19/32 [00:18<00:12,  1.01it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.01it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.02it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.02it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.03it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.03it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.03it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.04it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.04it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.04it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.04it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.04it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.04it/s]100%|██████████| 32/32 [00:31<00:00,  1.04it/s]100%|██████████| 32/32 [00:31<00:00,  1.03it/s]
W0624 00:50:57.281000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.281000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.281000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.281000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.282000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.282000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.282000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.312000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.312000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.312000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.312000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.312000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.327000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.327000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.327000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.327000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.327000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.485000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.485000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.485000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.485000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.485000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.717000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.717000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.718000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.718000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.718000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.718000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.718000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.738000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.738000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.738000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.739000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.739000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.800000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.801000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.801000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.801000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:50:57.801000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:50:58.669000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:50:58.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:50:58.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:50:58.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:50:58.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:50:58.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:50:58.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:50:58.989000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.010000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.010000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.010000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.010000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.010000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.269000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.269000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.269000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.269000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.269000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:50:59.525000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 00:51:03.509564 10586 finetune.py:68] layer 9_o @ epoch 4 new loss 0.00011902824917342514 old loss 0.00011967260070377961 BETTER
W0624 00:51:05.090215 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:51:05.925293 11598 finetune.py:68] layer 11_o @ epoch 1 new loss 0.0001709532953100279 old loss 0.0001724609173834324 BETTER
I0624 00:51:05.932776 10082 finetune.py:45] layer 8_up initial loss 0.00015786149015184492
W0624 00:51:05.933172 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

9_o proxy err 0.009890126064419746 tr(WHW.T) 24.078948974609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:02<00:35,  1.20s/it]  9%|▉         | 3/32 [00:03<00:33,  1.15s/it]I0624 00:51:09.930179 11092 finetune.py:68] layer 10_o @ epoch 3 new loss 0.00015624333173036575 old loss 0.0001572354231029749 BETTER
 12%|█▎        | 4/32 [00:04<00:31,  1.12s/it] 16%|█▌        | 5/32 [00:05<00:30,  1.11s/it] 19%|█▉        | 6/32 [00:06<00:28,  1.11s/it] 22%|██▏       | 7/32 [00:07<00:27,  1.10s/it] 25%|██▌       | 8/32 [00:08<00:26,  1.10s/it] 28%|██▊       | 9/32 [00:10<00:25,  1.10s/it] 31%|███▏      | 10/32 [00:11<00:24,  1.10s/it] 34%|███▍      | 11/32 [00:12<00:23,  1.10s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.09s/it] 41%|████      | 13/32 [00:14<00:20,  1.09s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.09s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.09s/it] 50%|█████     | 16/32 [00:17<00:17,  1.08s/it] 53%|█████▎    | 17/32 [00:18<00:16,  1.08s/it] 56%|█████▋    | 18/32 [00:19<00:15,  1.08s/it] 59%|█████▉    | 19/32 [00:20<00:14,  1.08s/it] 62%|██████▎   | 20/32 [00:22<00:12,  1.08s/it] 66%|██████▌   | 21/32 [00:23<00:11,  1.08s/it] 69%|██████▉   | 22/32 [00:24<00:10,  1.08s/it] 72%|███████▏  | 23/32 [00:25<00:09,  1.09s/it] 75%|███████▌  | 24/32 [00:26<00:08,  1.09s/it] 78%|███████▊  | 25/32 [00:27<00:07,  1.08s/it] 81%|████████▏ | 26/32 [00:28<00:06,  1.09s/it] 84%|████████▍ | 27/32 [00:29<00:05,  1.09s/it] 88%|████████▊ | 28/32 [00:30<00:04,  1.10s/it]I0624 00:51:37.927245 10082 finetune.py:68] layer 8_up @ epoch 0 new loss 0.00015634516603313386 old loss 0.00015786149015184492 BETTER
 91%|█████████ | 29/32 [00:31<00:03,  1.10s/it]I0624 00:51:38.461164 11598 finetune.py:68] layer 11_o @ epoch 2 new loss 0.00016976006736513227 old loss 0.0001709532953100279 BETTER
 94%|█████████▍| 30/32 [00:32<00:02,  1.10s/it] 97%|█████████▋| 31/32 [00:34<00:01,  1.10s/it]100%|██████████| 32/32 [00:35<00:00,  1.10s/it]100%|██████████| 32/32 [00:35<00:00,  1.10s/it]
I0624 00:51:42.200275 11092 finetune.py:68] layer 10_o @ epoch 4 new loss 0.00015535704733338207 old loss 0.00015624333173036575 BETTER
W0624 00:51:43.520665 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_o proxy err 0.010062657296657562 tr(WHW.T) 32.652992248535156
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it]  6%|▋         | 2/32 [00:02<00:33,  1.12s/it]  9%|▉         | 3/32 [00:03<00:30,  1.07s/it]W0624 00:51:48.283000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.283000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.283000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.283000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.283000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.283000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.283000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.313000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.313000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.313000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.313000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.313000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.328000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.329000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.329000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.329000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.329000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.487000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.487000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.487000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.487000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.487000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.716000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.716000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.716000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.716000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.716000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.716000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.716000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.737000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.737000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.737000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.737000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.738000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.801000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.802000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.802000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.802000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:51:48.802000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it]W0624 00:51:49.690000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it]W0624 00:51:50.008000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.008000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.008000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.008000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.008000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.008000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.008000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.029000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.029000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.029000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.029000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.029000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.295000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.295000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.295000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.295000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.295000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:51:50.562000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.00s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.00s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.00s/it]I0624 00:51:57.098976 10586 finetune.py:45] layer 9_up initial loss 0.00020394963212311268
W0624 00:51:57.099389 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:13<00:19,  1.01s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.00s/it] 47%|████▋     | 15/32 [00:15<00:16,  1.00it/s] 50%|█████     | 16/32 [00:16<00:16,  1.00s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.00s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.00s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.00s/it] 62%|██████▎   | 20/32 [00:20<00:11,  1.00it/s] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.03s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.04s/it]I0624 00:52:10.785003 10082 finetune.py:68] layer 8_up @ epoch 1 new loss 0.00015540757158305496 old loss 0.00015634516603313386 BETTER
I0624 00:52:10.879055 11598 finetune.py:68] layer 11_o @ epoch 3 new loss 0.00016877916641533375 old loss 0.00016976006736513227 BETTER
 81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.02s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.01s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.00s/it]100%|██████████| 32/32 [00:32<00:00,  1.00s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
W0624 00:52:23.665000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.665000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.665000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.665000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.665000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.665000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.666000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.693000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.693000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.693000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.693000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.693000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.710000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.710000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.710000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.710000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.710000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.869000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.869000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.870000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.870000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:52:23.870000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.093000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.093000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.093000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.094000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.094000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.094000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.094000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.115000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.115000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.115000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.115000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.115000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.177000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.177000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.177000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.178000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:52:24.178000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.044000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.355000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.355000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.356000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.356000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.356000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.356000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.356000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.377000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.377000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.377000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.377000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.378000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.638000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.638000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.639000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.639000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.639000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:52:25.892000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 00:52:28.735119 10586 finetune.py:68] layer 9_up @ epoch 0 new loss 0.00020196819968987256 old loss 0.00020394963212311268 BETTER
I0624 00:52:32.024084 11092 finetune.py:45] layer 10_up initial loss 0.00025229956372641027
W0624 00:52:32.024529 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:52:43.223113 11598 finetune.py:68] layer 11_o @ epoch 4 new loss 0.00016792853421065956 old loss 0.00016877916641533375 BETTER
I0624 00:52:43.443343 10082 finetune.py:68] layer 8_up @ epoch 2 new loss 0.00015459724818356335 old loss 0.00015540757158305496 BETTER
W0624 00:52:44.533494 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_o proxy err 0.010826047509908676 tr(WHW.T) 32.37555694580078
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it]  6%|▋         | 2/32 [00:02<00:34,  1.15s/it]  9%|▉         | 3/32 [00:03<00:31,  1.10s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.08s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.07s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.06s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.06s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.05s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.05s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.05s/it] 34%|███▍      | 11/32 [00:11<00:22,  1.06s/it] 38%|███▊      | 12/32 [00:12<00:21,  1.06s/it] 41%|████      | 13/32 [00:13<00:20,  1.06s/it]I0624 00:53:00.619450 10586 finetune.py:68] layer 9_up @ epoch 1 new loss 0.00020071218023076653 old loss 0.00020196819968987256 BETTER
 44%|████▍     | 14/32 [00:14<00:19,  1.06s/it] 47%|████▋     | 15/32 [00:16<00:17,  1.05s/it]I0624 00:53:02.566980 11092 finetune.py:68] layer 10_up @ epoch 0 new loss 0.00024981872411444783 old loss 0.00025229956372641027 BETTER
 50%|█████     | 16/32 [00:17<00:16,  1.05s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.05s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.05s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.05s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.05s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.04s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.04s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.04s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.05s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.04s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.05s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.05s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.04s/it]I0624 00:53:16.432919 10082 finetune.py:68] layer 8_up @ epoch 3 new loss 0.00015385249571409076 old loss 0.00015459724818356335 BETTER
 94%|█████████▍| 30/32 [00:31<00:02,  1.04s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.04s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]100%|██████████| 32/32 [00:33<00:00,  1.06s/it]
W0624 00:53:26.185000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.185000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.185000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.185000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.186000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.186000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.186000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.214000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.214000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.214000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.214000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.215000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.229000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.229000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.229000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.229000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.230000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.386000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.386000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.386000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.386000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.387000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.619000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.619000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.619000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.619000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.619000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.619000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.619000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.640000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.640000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.640000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.641000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.641000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.705000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.705000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.705000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.705000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:53:26.705000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.594000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.921000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.921000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.921000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.922000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.922000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.922000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.922000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.941000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.942000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.942000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.942000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:53:27.942000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:53:28.212000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:53:28.213000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:53:28.213000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:53:28.213000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:53:28.213000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:53:28.497000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 00:53:32.799302 10586 finetune.py:68] layer 9_up @ epoch 2 new loss 0.0001996279606828466 old loss 0.00020071218023076653 BETTER
I0624 00:53:34.002213 11092 finetune.py:68] layer 10_up @ epoch 1 new loss 0.00024823492276482284 old loss 0.00024981872411444783 BETTER
I0624 00:53:34.921629 11598 finetune.py:45] layer 11_up initial loss 0.00028028912493027747
W0624 00:53:34.921997 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:53:49.771507 10082 finetune.py:68] layer 8_up @ epoch 4 new loss 0.00015319247904699296 old loss 0.00015385249571409076 BETTER
W0624 00:53:51.093741 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_up proxy err 0.009759712032973766 tr(WHW.T) 878.5654296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:38,  1.25s/it]  6%|▋         | 2/32 [00:02<00:32,  1.08s/it]  9%|▉         | 3/32 [00:03<00:29,  1.03s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.01s/it] 16%|█▌        | 5/32 [00:05<00:26,  1.01it/s] 19%|█▉        | 6/32 [00:06<00:25,  1.01it/s] 22%|██▏       | 7/32 [00:07<00:24,  1.02it/s] 25%|██▌       | 8/32 [00:08<00:23,  1.03it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.02it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.01it/s] 34%|███▍      | 11/32 [00:10<00:20,  1.02it/s] 38%|███▊      | 12/32 [00:11<00:19,  1.02it/s]I0624 00:54:04.918338 10586 finetune.py:68] layer 9_up @ epoch 3 new loss 0.00019864109344780445 old loss 0.0001996279606828466 BETTER
 41%|████      | 13/32 [00:12<00:18,  1.02it/s]I0624 00:54:05.400013 11092 finetune.py:68] layer 10_up @ epoch 2 new loss 0.00024685959215275943 old loss 0.00024823492276482284 BETTER
I0624 00:54:05.610143 11598 finetune.py:68] layer 11_up @ epoch 0 new loss 0.0002775986213237047 old loss 0.00028028912493027747 BETTER
 44%|████▍     | 14/32 [00:13<00:17,  1.02it/s] 47%|████▋     | 15/32 [00:14<00:16,  1.03it/s] 50%|█████     | 16/32 [00:15<00:15,  1.03it/s] 53%|█████▎    | 17/32 [00:16<00:14,  1.04it/s] 56%|█████▋    | 18/32 [00:17<00:13,  1.04it/s] 59%|█████▉    | 19/32 [00:18<00:12,  1.03it/s] 62%|██████▎   | 20/32 [00:19<00:11,  1.04it/s] 66%|██████▌   | 21/32 [00:20<00:10,  1.03it/s] 69%|██████▉   | 22/32 [00:21<00:09,  1.03it/s] 72%|███████▏  | 23/32 [00:22<00:08,  1.03it/s] 75%|███████▌  | 24/32 [00:23<00:07,  1.03it/s] 78%|███████▊  | 25/32 [00:24<00:06,  1.03it/s] 81%|████████▏ | 26/32 [00:25<00:05,  1.04it/s] 84%|████████▍ | 27/32 [00:26<00:04,  1.04it/s] 88%|████████▊ | 28/32 [00:27<00:03,  1.04it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.03it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.04it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.04it/s]100%|██████████| 32/32 [00:31<00:00,  1.04it/s]100%|██████████| 32/32 [00:31<00:00,  1.02it/s]
I0624 00:54:31.113700 10082 finetune.py:45] layer 8_gate initial loss 0.00020730774849653244
W0624 00:54:31.114090 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:54:36.768014 11092 finetune.py:68] layer 10_up @ epoch 3 new loss 0.0002456307702232152 old loss 0.00024685959215275943 BETTER
I0624 00:54:36.851670 10586 finetune.py:68] layer 9_up @ epoch 4 new loss 0.00019774885731749237 old loss 0.00019864109344780445 BETTER
I0624 00:54:37.121867 11598 finetune.py:68] layer 11_up @ epoch 1 new loss 0.00027591080288402736 old loss 0.0002775986213237047 BETTER
W0624 00:54:38.177307 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_up proxy err 0.009451955556869507 tr(WHW.T) 991.7684326171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.36s/it]  6%|▋         | 2/32 [00:02<00:35,  1.20s/it]  9%|▉         | 3/32 [00:03<00:33,  1.15s/it] 12%|█▎        | 4/32 [00:04<00:31,  1.13s/it] 16%|█▌        | 5/32 [00:05<00:30,  1.12s/it] 19%|█▉        | 6/32 [00:06<00:28,  1.11s/it] 22%|██▏       | 7/32 [00:07<00:27,  1.10s/it] 25%|██▌       | 8/32 [00:09<00:26,  1.10s/it] 28%|██▊       | 9/32 [00:10<00:25,  1.10s/it] 31%|███▏      | 10/32 [00:11<00:24,  1.10s/it] 34%|███▍      | 11/32 [00:12<00:23,  1.10s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.10s/it] 41%|████      | 13/32 [00:14<00:20,  1.10s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.11s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.10s/it] 50%|█████     | 16/32 [00:17<00:17,  1.10s/it] 53%|█████▎    | 17/32 [00:18<00:16,  1.09s/it] 56%|█████▋    | 18/32 [00:19<00:15,  1.09s/it] 59%|█████▉    | 19/32 [00:21<00:14,  1.09s/it] 62%|██████▎   | 20/32 [00:22<00:13,  1.08s/it]I0624 00:55:02.540402 10082 finetune.py:68] layer 8_gate @ epoch 0 new loss 0.00020604429300874472 old loss 0.00020730774849653244 BETTER
 66%|██████▌   | 21/32 [00:23<00:11,  1.08s/it] 69%|██████▉   | 22/32 [00:24<00:10,  1.08s/it] 72%|███████▏  | 23/32 [00:25<00:09,  1.09s/it] 75%|███████▌  | 24/32 [00:26<00:08,  1.09s/it] 78%|███████▊  | 25/32 [00:27<00:07,  1.10s/it]I0624 00:55:08.203865 11092 finetune.py:68] layer 10_up @ epoch 4 new loss 0.0002445246500428766 old loss 0.0002456307702232152 BETTER
 81%|████████▏ | 26/32 [00:28<00:06,  1.09s/it]I0624 00:55:08.535224 11598 finetune.py:68] layer 11_up @ epoch 2 new loss 0.0002744311932474375 old loss 0.00027591080288402736 BETTER
W0624 00:55:09.428710 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:29<00:05,  1.10s/it] 88%|████████▊ | 28/32 [00:30<00:04,  1.10s/it]10_up proxy err 0.009160387329757214 tr(WHW.T) 1090.142333984375
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:31<00:03,  1.09s/it]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it] 94%|█████████▍| 30/32 [00:33<00:02,  1.09s/it]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it] 97%|█████████▋| 31/32 [00:34<00:01,  1.09s/it]  9%|▉         | 3/32 [00:03<00:31,  1.08s/it]100%|██████████| 32/32 [00:35<00:00,  1.09s/it]100%|██████████| 32/32 [00:35<00:00,  1.10s/it]
 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.01s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.01s/it]I0624 00:55:22.383233 10586 finetune.py:45] layer 9_gate initial loss 0.00026279897429049015
W0624 00:55:22.383780 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 41%|████      | 13/32 [00:13<00:19,  1.01s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.01s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.01s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.01s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.01s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.04s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.04s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.05s/it]I0624 00:55:34.431460 10082 finetune.py:68] layer 8_gate @ epoch 1 new loss 0.00020540955301839858 old loss 0.00020604429300874472 BETTER
 75%|███████▌  | 24/32 [00:24<00:08,  1.04s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.03s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.03s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.02s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it]I0624 00:55:40.063607 11598 finetune.py:68] layer 11_up @ epoch 3 new loss 0.0002731321728788316 old loss 0.0002744311932474375 BETTER
 91%|█████████ | 29/32 [00:29<00:03,  1.02s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.00s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
I0624 00:55:50.587841 11092 finetune.py:45] layer 10_gate initial loss 0.000320744322380051
W0624 00:55:50.588286 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:55:52.756585 10586 finetune.py:68] layer 9_gate @ epoch 0 new loss 0.0002612944517750293 old loss 0.00026279897429049015 BETTER
I0624 00:56:06.282687 10082 finetune.py:68] layer 8_gate @ epoch 2 new loss 0.0002048417809419334 old loss 0.00020540955301839858 BETTER
I0624 00:56:11.544512 11598 finetune.py:68] layer 11_up @ epoch 4 new loss 0.00027193824644200504 old loss 0.0002731321728788316 BETTER
W0624 00:56:12.811352 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_up proxy err 0.00978227611631155 tr(WHW.T) 1140.204833984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.39s/it]  6%|▋         | 2/32 [00:02<00:35,  1.20s/it]  9%|▉         | 3/32 [00:03<00:32,  1.13s/it] 12%|█▎        | 4/32 [00:04<00:31,  1.11s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.11s/it]I0624 00:56:20.505402 11092 finetune.py:68] layer 10_gate @ epoch 0 new loss 0.00031889777164906263 old loss 0.000320744322380051 BETTER
 19%|█▉        | 6/32 [00:06<00:28,  1.10s/it] 22%|██▏       | 7/32 [00:07<00:27,  1.09s/it] 25%|██▌       | 8/32 [00:08<00:26,  1.09s/it]I0624 00:56:23.989058 10586 finetune.py:68] layer 9_gate @ epoch 1 new loss 0.0002604676701594144 old loss 0.0002612944517750293 BETTER
 28%|██▊       | 9/32 [00:10<00:25,  1.09s/it] 31%|███▏      | 10/32 [00:11<00:23,  1.08s/it] 34%|███▍      | 11/32 [00:12<00:22,  1.07s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.07s/it] 41%|████      | 13/32 [00:14<00:20,  1.07s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.07s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.07s/it] 50%|█████     | 16/32 [00:17<00:17,  1.07s/it] 53%|█████▎    | 17/32 [00:18<00:16,  1.07s/it] 56%|█████▋    | 18/32 [00:19<00:15,  1.07s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.07s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.07s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.07s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.07s/it]I0624 00:56:38.737483 10082 finetune.py:68] layer 8_gate @ epoch 3 new loss 0.00020433403551578522 old loss 0.0002048417809419334 BETTER
 72%|███████▏  | 23/32 [00:24<00:09,  1.07s/it] 75%|███████▌  | 24/32 [00:26<00:08,  1.06s/it] 78%|███████▊  | 25/32 [00:27<00:07,  1.07s/it] 81%|████████▏ | 26/32 [00:28<00:06,  1.07s/it] 84%|████████▍ | 27/32 [00:29<00:05,  1.07s/it] 88%|████████▊ | 28/32 [00:30<00:04,  1.07s/it] 91%|█████████ | 29/32 [00:31<00:03,  1.07s/it] 94%|█████████▍| 30/32 [00:32<00:02,  1.07s/it] 97%|█████████▋| 31/32 [00:33<00:01,  1.06s/it]100%|██████████| 32/32 [00:34<00:00,  1.06s/it]100%|██████████| 32/32 [00:34<00:00,  1.08s/it]
I0624 00:56:51.405001 11092 finetune.py:68] layer 10_gate @ epoch 1 new loss 0.0003178792539983988 old loss 0.00031889777164906263 BETTER
I0624 00:56:55.280076 10586 finetune.py:68] layer 9_gate @ epoch 2 new loss 0.00025971789727918804 old loss 0.0002604676701594144 BETTER
I0624 00:56:56.167545 11598 finetune.py:45] layer 11_gate initial loss 0.00035941891837865114
W0624 00:56:56.167984 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:57:11.163644 10082 finetune.py:68] layer 8_gate @ epoch 4 new loss 0.0002038491511484608 old loss 0.00020433403551578522 BETTER
W0624 00:57:12.182479 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_gate proxy err 0.005663668736815453 tr(WHW.T) 1848.013427734375
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:49,  1.71it/s]  2%|▏         | 2/86 [00:00<00:36,  2.32it/s]  3%|▎         | 3/86 [00:01<00:31,  2.64it/s]  5%|▍         | 4/86 [00:01<00:29,  2.82it/s]  6%|▌         | 5/86 [00:01<00:27,  2.90it/s]  7%|▋         | 6/86 [00:02<00:26,  2.97it/s]  8%|▊         | 7/86 [00:02<00:26,  3.02it/s]  9%|▉         | 8/86 [00:02<00:25,  3.02it/s] 10%|█         | 9/86 [00:03<00:25,  3.02it/s] 12%|█▏        | 10/86 [00:03<00:25,  3.02it/s] 13%|█▎        | 11/86 [00:03<00:24,  3.00it/s] 14%|█▍        | 12/86 [00:04<00:24,  3.00it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.98it/s] 16%|█▋        | 14/86 [00:04<00:23,  3.00it/s] 17%|█▋        | 15/86 [00:05<00:23,  3.00it/s] 19%|█▊        | 16/86 [00:05<00:23,  3.01it/s] 20%|█▉        | 17/86 [00:05<00:22,  3.01it/s] 21%|██        | 18/86 [00:06<00:22,  3.02it/s] 22%|██▏       | 19/86 [00:06<00:22,  3.03it/s] 23%|██▎       | 20/86 [00:06<00:21,  3.03it/s]I0624 00:57:21.982144 11092 finetune.py:68] layer 10_gate @ epoch 2 new loss 0.00031697109807282686 old loss 0.0003178792539983988 BETTER
 24%|██▍       | 21/86 [00:07<00:21,  3.05it/s] 26%|██▌       | 22/86 [00:07<00:20,  3.11it/s] 27%|██▋       | 23/86 [00:07<00:20,  3.09it/s] 28%|██▊       | 24/86 [00:08<00:20,  3.09it/s] 29%|██▉       | 25/86 [00:08<00:19,  3.10it/s] 30%|███       | 26/86 [00:08<00:19,  3.08it/s] 31%|███▏      | 27/86 [00:09<00:19,  3.08it/s] 33%|███▎      | 28/86 [00:09<00:18,  3.07it/s] 34%|███▎      | 29/86 [00:09<00:18,  3.09it/s] 35%|███▍      | 30/86 [00:10<00:18,  3.09it/s] 36%|███▌      | 31/86 [00:10<00:17,  3.11it/s] 37%|███▋      | 32/86 [00:10<00:17,  3.10it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.10it/s] 40%|███▉      | 34/86 [00:11<00:16,  3.11it/s]I0624 00:57:26.398419 10586 finetune.py:68] layer 9_gate @ epoch 3 new loss 0.00025906439987011254 old loss 0.00025971789727918804 BETTER
 41%|████      | 35/86 [00:11<00:16,  3.12it/s]I0624 00:57:26.679254 11598 finetune.py:68] layer 11_gate @ epoch 0 new loss 0.00035737972939386964 old loss 0.00035941891837865114 BETTER
 42%|████▏     | 36/86 [00:11<00:16,  3.12it/s] 43%|████▎     | 37/86 [00:12<00:15,  3.13it/s] 44%|████▍     | 38/86 [00:12<00:15,  3.11it/s] 45%|████▌     | 39/86 [00:12<00:15,  3.11it/s] 47%|████▋     | 40/86 [00:13<00:14,  3.11it/s] 48%|████▊     | 41/86 [00:13<00:14,  3.11it/s] 49%|████▉     | 42/86 [00:13<00:14,  3.09it/s] 50%|█████     | 43/86 [00:14<00:13,  3.08it/s] 51%|█████     | 44/86 [00:14<00:13,  3.08it/s] 52%|█████▏    | 45/86 [00:14<00:13,  3.09it/s] 53%|█████▎    | 46/86 [00:15<00:12,  3.11it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.12it/s] 56%|█████▌    | 48/86 [00:15<00:12,  3.12it/s] 57%|█████▋    | 49/86 [00:16<00:11,  3.11it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.10it/s] 59%|█████▉    | 51/86 [00:16<00:11,  3.11it/s] 60%|██████    | 52/86 [00:17<00:10,  3.12it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.12it/s] 63%|██████▎   | 54/86 [00:17<00:10,  3.10it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.09it/s] 65%|██████▌   | 56/86 [00:18<00:09,  3.08it/s] 66%|██████▋   | 57/86 [00:18<00:09,  3.10it/s] 67%|██████▋   | 58/86 [00:19<00:09,  3.10it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.10it/s] 70%|██████▉   | 60/86 [00:19<00:08,  3.10it/s] 71%|███████   | 61/86 [00:20<00:08,  3.09it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.11it/s] 73%|███████▎  | 63/86 [00:20<00:07,  3.12it/s] 74%|███████▍  | 64/86 [00:20<00:07,  3.12it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.11it/s] 77%|███████▋  | 66/86 [00:21<00:06,  3.10it/s] 78%|███████▊  | 67/86 [00:21<00:06,  3.11it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.10it/s] 80%|████████  | 69/86 [00:22<00:05,  3.11it/s] 81%|████████▏ | 70/86 [00:22<00:05,  3.08it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.08it/s] 84%|████████▎ | 72/86 [00:23<00:04,  3.07it/s] 85%|████████▍ | 73/86 [00:23<00:04,  3.09it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.10it/s] 87%|████████▋ | 75/86 [00:24<00:03,  3.11it/s] 88%|████████▊ | 76/86 [00:24<00:03,  3.10it/s] 90%|████████▉ | 77/86 [00:25<00:02,  3.10it/s] 91%|█████████ | 78/86 [00:25<00:02,  3.10it/s] 92%|█████████▏| 79/86 [00:25<00:02,  3.12it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.12it/s] 94%|█████████▍| 81/86 [00:26<00:01,  3.11it/s] 95%|█████████▌| 82/86 [00:26<00:01,  3.10it/s] 97%|█████████▋| 83/86 [00:27<00:00,  3.10it/s] 98%|█████████▊| 84/86 [00:27<00:00,  3.11it/s] 99%|█████████▉| 85/86 [00:27<00:00,  3.09it/s]100%|██████████| 86/86 [00:28<00:00,  3.10it/s]100%|██████████| 86/86 [00:28<00:00,  3.06it/s]
W0624 00:57:49.715000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.716000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.716000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.716000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.716000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.716000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.716000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.758000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.758000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.758000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.758000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.758000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.774000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.775000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.775000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.775000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.775000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.949000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.949000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.949000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.949000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:57:49.949000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.280000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.280000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.280000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.280000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.280000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.280000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.280000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.315000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.315000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.315000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.315000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.315000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.388000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.388000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.388000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.388000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:57:50.388000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:51.605000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:51.619000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:51.627000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:51.628000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.088000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.088000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.088000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.088000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.088000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.089000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.089000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.121000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.121000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.121000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.121000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.122000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
I0624 00:57:52.470750 11092 finetune.py:68] layer 10_gate @ epoch 3 new loss 0.0003161522909067571 old loss 0.00031697109807282686 BETTER
W0624 00:57:52.481000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.481000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.481000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.481000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.481000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.482000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.482000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.482000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.764000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.764000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.764000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.764000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:57:52.765000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:57:53.110000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:57:53.115000 139652917921600 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 00:57:57.222042 11598 finetune.py:68] layer 11_gate @ epoch 1 new loss 0.0003562761703506112 old loss 0.00035737972939386964 BETTER
I0624 00:57:57.592288 10586 finetune.py:68] layer 9_gate @ epoch 4 new loss 0.00025841951719485223 old loss 0.00025906439987011254 BETTER
W0624 00:57:58.748997 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:57:59.939488 10082 finetune.py:45] layer 8_down initial loss 0.0003152353165205568
W0624 00:57:59.939907 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

9_gate proxy err 0.005496287252753973 tr(WHW.T) 2013.740478515625
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:53,  1.58it/s]  2%|▏         | 2/86 [00:00<00:39,  2.11it/s]  3%|▎         | 3/86 [00:01<00:35,  2.37it/s]  5%|▍         | 4/86 [00:01<00:32,  2.50it/s]  6%|▌         | 5/86 [00:02<00:31,  2.57it/s]  7%|▋         | 6/86 [00:02<00:30,  2.59it/s]  8%|▊         | 7/86 [00:02<00:30,  2.62it/s]  9%|▉         | 8/86 [00:03<00:29,  2.66it/s] 10%|█         | 9/86 [00:03<00:29,  2.65it/s] 12%|█▏        | 10/86 [00:03<00:28,  2.66it/s] 13%|█▎        | 11/86 [00:04<00:27,  2.68it/s] 14%|█▍        | 12/86 [00:04<00:27,  2.67it/s] 15%|█▌        | 13/86 [00:05<00:27,  2.69it/s] 16%|█▋        | 14/86 [00:05<00:26,  2.70it/s] 17%|█▋        | 15/86 [00:05<00:26,  2.72it/s] 19%|█▊        | 16/86 [00:06<00:25,  2.73it/s] 20%|█▉        | 17/86 [00:06<00:25,  2.72it/s] 21%|██        | 18/86 [00:06<00:25,  2.72it/s] 22%|██▏       | 19/86 [00:07<00:24,  2.72it/s] 23%|██▎       | 20/86 [00:07<00:24,  2.71it/s] 24%|██▍       | 21/86 [00:08<00:23,  2.72it/s] 26%|██▌       | 22/86 [00:08<00:23,  2.72it/s] 27%|██▋       | 23/86 [00:08<00:23,  2.71it/s] 28%|██▊       | 24/86 [00:09<00:23,  2.68it/s] 29%|██▉       | 25/86 [00:09<00:22,  2.68it/s] 30%|███       | 26/86 [00:09<00:22,  2.68it/s] 31%|███▏      | 27/86 [00:10<00:21,  2.70it/s] 33%|███▎      | 28/86 [00:10<00:21,  2.70it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.72it/s] 35%|███▍      | 30/86 [00:11<00:20,  2.75it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.76it/s] 37%|███▋      | 32/86 [00:12<00:19,  2.76it/s] 38%|███▊      | 33/86 [00:12<00:19,  2.76it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.76it/s] 41%|████      | 35/86 [00:13<00:18,  2.76it/s] 42%|████▏     | 36/86 [00:13<00:18,  2.76it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.76it/s] 44%|████▍     | 38/86 [00:14<00:17,  2.76it/s] 45%|████▌     | 39/86 [00:14<00:17,  2.71it/s] 47%|████▋     | 40/86 [00:14<00:17,  2.69it/s] 48%|████▊     | 41/86 [00:15<00:16,  2.72it/s] 49%|████▉     | 42/86 [00:15<00:16,  2.73it/s] 50%|█████     | 43/86 [00:16<00:15,  2.74it/s] 51%|█████     | 44/86 [00:16<00:15,  2.76it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.76it/s] 53%|█████▎    | 46/86 [00:17<00:14,  2.77it/s] 55%|█████▍    | 47/86 [00:17<00:14,  2.78it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.79it/s] 57%|█████▋    | 49/86 [00:18<00:13,  2.79it/s] 58%|█████▊    | 50/86 [00:18<00:12,  2.79it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.77it/s] 60%|██████    | 52/86 [00:19<00:12,  2.75it/s] 62%|██████▏   | 53/86 [00:19<00:11,  2.76it/s] 63%|██████▎   | 54/86 [00:20<00:11,  2.76it/s] 64%|██████▍   | 55/86 [00:20<00:11,  2.77it/s] 65%|██████▌   | 56/86 [00:20<00:10,  2.77it/s] 66%|██████▋   | 57/86 [00:21<00:10,  2.79it/s]I0624 00:58:22.942076 11092 finetune.py:68] layer 10_gate @ epoch 4 new loss 0.0003153703873977065 old loss 0.0003161522909067571 BETTER
 67%|██████▋   | 58/86 [00:21<00:10,  2.77it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.76it/s] 70%|██████▉   | 60/86 [00:22<00:09,  2.74it/s]W0624 00:58:24.060206 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 71%|███████   | 61/86 [00:22<00:09,  2.73it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.73it/s] 73%|███████▎  | 63/86 [00:23<00:08,  2.73it/s] 74%|███████▍  | 64/86 [00:23<00:08,  2.73it/s] 76%|███████▌  | 65/86 [00:24<00:07,  2.72it/s] 77%|███████▋  | 66/86 [00:24<00:07,  2.70it/s] 78%|███████▊  | 67/86 [00:24<00:07,  2.70it/s] 79%|███████▉  | 68/86 [00:25<00:06,  2.69it/s]10_gate proxy err 0.005428730510175228 tr(WHW.T) 2113.48779296875
  0%|          | 0/86 [00:00<?, ?it/s] 80%|████████  | 69/86 [00:25<00:06,  2.70it/s]I0624 00:58:27.520897 11598 finetune.py:68] layer 11_gate @ epoch 2 new loss 0.00035530226887203753 old loss 0.0003562761703506112 BETTER
 81%|████████▏ | 70/86 [00:25<00:05,  2.71it/s]  1%|          | 1/86 [00:00<00:52,  1.61it/s] 83%|████████▎ | 71/86 [00:26<00:05,  2.77it/s]  2%|▏         | 2/86 [00:00<00:38,  2.16it/s] 84%|████████▎ | 72/86 [00:26<00:05,  2.75it/s]  3%|▎         | 3/86 [00:01<00:34,  2.42it/s] 85%|████████▍ | 73/86 [00:26<00:04,  2.75it/s]  5%|▍         | 4/86 [00:01<00:31,  2.57it/s] 86%|████████▌ | 74/86 [00:27<00:04,  2.76it/s]  6%|▌         | 5/86 [00:02<00:30,  2.66it/s] 87%|████████▋ | 75/86 [00:27<00:03,  2.77it/s]  7%|▋         | 6/86 [00:02<00:29,  2.71it/s] 88%|████████▊ | 76/86 [00:28<00:03,  2.78it/s]I0624 00:58:29.776440 10082 finetune.py:68] layer 8_down @ epoch 0 new loss 0.00031513121211901307 old loss 0.0003152353165205568 BETTER
  8%|▊         | 7/86 [00:02<00:28,  2.76it/s] 90%|████████▉ | 77/86 [00:28<00:03,  2.78it/s]  9%|▉         | 8/86 [00:03<00:27,  2.86it/s] 91%|█████████ | 78/86 [00:28<00:02,  2.77it/s] 10%|█         | 9/86 [00:03<00:26,  2.86it/s] 92%|█████████▏| 79/86 [00:29<00:02,  2.77it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.87it/s] 93%|█████████▎| 80/86 [00:29<00:02,  2.75it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.88it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.90it/s] 94%|█████████▍| 81/86 [00:29<00:01,  2.73it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.90it/s] 95%|█████████▌| 82/86 [00:30<00:01,  2.72it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.91it/s] 97%|█████████▋| 83/86 [00:30<00:01,  2.73it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.92it/s] 98%|█████████▊| 84/86 [00:30<00:00,  2.72it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.92it/s] 99%|█████████▉| 85/86 [00:31<00:00,  2.73it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.93it/s]100%|██████████| 86/86 [00:31<00:00,  2.74it/s]100%|██████████| 86/86 [00:31<00:00,  2.71it/s]
 21%|██        | 18/86 [00:06<00:23,  2.94it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.94it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.93it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.92it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.92it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.91it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.91it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.91it/s] 30%|███       | 26/86 [00:09<00:20,  2.89it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.92it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.93it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.93it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.92it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.92it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.92it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.92it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.93it/s] 41%|████      | 35/86 [00:12<00:17,  2.94it/s] 42%|████▏     | 36/86 [00:12<00:16,  2.94it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.94it/s]W0624 00:58:40.184000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.185000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.185000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.185000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.185000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.185000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.185000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.230000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.230000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.230000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.230000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.230000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.247000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.247000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.247000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.247000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.247000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 38/86 [00:13<00:16,  2.94it/s]W0624 00:58:40.420000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.421000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.421000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.421000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.421000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 45%|████▌     | 39/86 [00:13<00:16,  2.91it/s]W0624 00:58:40.749000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.749000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.749000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.749000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.749000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.749000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.749000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.785000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.785000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.785000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.785000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.785000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.855000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.855000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.855000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.856000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:58:40.856000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 40/86 [00:14<00:15,  2.91it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.92it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.93it/s]W0624 00:58:42.088000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 43/86 [00:15<00:14,  2.95it/s]W0624 00:58:42.102000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.110000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.111000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 51%|█████     | 44/86 [00:15<00:14,  2.95it/s]W0624 00:58:42.574000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.574000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.574000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.575000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.575000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.575000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.575000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.607000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.607000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.607000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.607000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.607000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 52%|█████▏    | 45/86 [00:15<00:13,  2.95it/s]W0624 00:58:42.959000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.959000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.959000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.960000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.960000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.960000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.960000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:58:42.960000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 46/86 [00:16<00:13,  2.94it/s]W0624 00:58:43.262000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:58:43.262000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:58:43.262000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:58:43.262000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:58:43.262000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 55%|█████▍    | 47/86 [00:16<00:13,  2.94it/s]W0624 00:58:43.610000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:58:43.615000 139902861645632 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 56%|█████▌    | 48/86 [00:16<00:12,  2.95it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.94it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.94it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.95it/s] 60%|██████    | 52/86 [00:18<00:11,  2.95it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.92it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.93it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.93it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.95it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.96it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.96it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.96it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.96it/s] 71%|███████   | 61/86 [00:21<00:08,  2.95it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.95it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.92it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.92it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.92it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.90it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.91it/s]I0624 00:58:50.532547 10586 finetune.py:45] layer 9_down initial loss 0.0003919879673048854
W0624 00:58:50.532952 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 79%|███████▉  | 68/86 [00:23<00:06,  2.91it/s] 80%|████████  | 69/86 [00:23<00:05,  2.92it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.92it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.94it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.95it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.97it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.96it/s] 87%|████████▋ | 75/86 [00:25<00:03,  2.95it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.93it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.93it/s] 91%|█████████ | 78/86 [00:26<00:02,  2.93it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.91it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.92it/s] 94%|█████████▍| 81/86 [00:27<00:01,  2.93it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.93it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.91it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.89it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.87it/s]100%|██████████| 86/86 [00:29<00:00,  2.86it/s]100%|██████████| 86/86 [00:29<00:00,  2.89it/s]
I0624 00:58:57.974497 11598 finetune.py:68] layer 11_gate @ epoch 3 new loss 0.0003544077044352889 old loss 0.00035530226887203753 BETTER
I0624 00:59:00.474265 10082 finetune.py:68] layer 8_down @ epoch 1 new loss 0.00031510816188529134 old loss 0.00031513121211901307 BETTER
W0624 00:59:03.654000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.654000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.654000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.654000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.655000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.655000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.655000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.696000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.696000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.696000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.697000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.697000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.713000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.713000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.713000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.713000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.713000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.891000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.891000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.891000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.891000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 00:59:03.892000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.214000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.214000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.214000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.215000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.215000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.215000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.215000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.250000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.251000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.251000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.251000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.251000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.324000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.324000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.324000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.324000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:04.324000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.523000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.535000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.543000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.543000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.984000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.984000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.984000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.985000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.985000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.985000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:05.985000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.013000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.013000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.014000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.014000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.014000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.362000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.362000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.362000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.363000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.363000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.363000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.363000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.363000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.659000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.659000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.659000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.659000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 00:59:06.659000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 00:59:07.000000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 00:59:07.005000 140198705325888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 00:59:13.656567 11092 finetune.py:45] layer 10_down initial loss 0.00046924565685912967
W0624 00:59:13.656950 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 00:59:20.021919 10586 finetune.py:68] layer 9_down @ epoch 0 new loss 0.00039185344940051436 old loss 0.0003919879673048854 BETTER
I0624 00:59:28.697080 11598 finetune.py:68] layer 11_gate @ epoch 4 new loss 0.0003535828145686537 old loss 0.0003544077044352889 BETTER
W0624 00:59:29.797729 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 00:59:31.605283 10082 finetune.py:68] layer 8_down @ epoch 2 new loss 0.00031509011751040816 old loss 0.00031510816188529134 BETTER
11_gate proxy err 0.005714244209229946 tr(WHW.T) 2189.198486328125
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:50,  1.67it/s]  2%|▏         | 2/86 [00:00<00:38,  2.20it/s]  3%|▎         | 3/86 [00:01<00:33,  2.45it/s]  5%|▍         | 4/86 [00:01<00:31,  2.59it/s]  6%|▌         | 5/86 [00:02<00:30,  2.66it/s]  7%|▋         | 6/86 [00:02<00:29,  2.71it/s]  8%|▊         | 7/86 [00:02<00:28,  2.76it/s]  9%|▉         | 8/86 [00:03<00:27,  2.81it/s] 10%|█         | 9/86 [00:03<00:27,  2.82it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.84it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.86it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.87it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.88it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.86it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.86it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.86it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.86it/s] 21%|██        | 18/86 [00:06<00:23,  2.86it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.86it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.84it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.86it/s] 26%|██▌       | 22/86 [00:07<00:22,  2.87it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.87it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.88it/s] 29%|██▉       | 25/86 [00:08<00:21,  2.88it/s] 30%|███       | 26/86 [00:09<00:20,  2.87it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.86it/s]I0624 00:59:42.439229 11092 finetune.py:68] layer 10_down @ epoch 0 new loss 0.0004690722853410989 old loss 0.00046924565685912967 BETTER
 33%|███▎      | 28/86 [00:10<00:20,  2.86it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.86it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.86it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.85it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.84it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.82it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.84it/s] 41%|████      | 35/86 [00:12<00:17,  2.84it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.82it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.82it/s] 44%|████▍     | 38/86 [00:13<00:17,  2.81it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.81it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.82it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.82it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.82it/s] 50%|█████     | 43/86 [00:15<00:15,  2.80it/s] 51%|█████     | 44/86 [00:15<00:15,  2.80it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.79it/s] 53%|█████▎    | 46/86 [00:16<00:14,  2.77it/s] 55%|█████▍    | 47/86 [00:16<00:14,  2.78it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.78it/s] 57%|█████▋    | 49/86 [00:17<00:13,  2.79it/s]I0624 00:59:50.276536 10586 finetune.py:68] layer 9_down @ epoch 1 new loss 0.0003918195143342018 old loss 0.00039185344940051436 BETTER
 58%|█████▊    | 50/86 [00:17<00:12,  2.84it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.84it/s] 60%|██████    | 52/86 [00:18<00:11,  2.85it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.86it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.86it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.85it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.84it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.85it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.82it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.83it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.84it/s] 71%|███████   | 61/86 [00:21<00:08,  2.85it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.86it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.87it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.88it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.88it/s] 77%|███████▋  | 66/86 [00:23<00:06,  2.88it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.88it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.87it/s] 80%|████████  | 69/86 [00:24<00:05,  2.86it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.86it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.84it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.83it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.84it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.84it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.86it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.87it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.87it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.88it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.90it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.90it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.90it/s] 95%|█████████▌| 82/86 [00:29<00:01,  2.90it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.89it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.86it/s]I0624 01:00:02.698041 10082 finetune.py:68] layer 8_down @ epoch 3 new loss 0.0003150828124489635 old loss 0.00031509011751040816 BETTER
 99%|█████████▉| 85/86 [00:30<00:00,  2.85it/s]100%|██████████| 86/86 [00:30<00:00,  2.85it/s]100%|██████████| 86/86 [00:30<00:00,  2.83it/s]
W0624 01:00:09.940000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.941000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.941000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.941000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.941000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.941000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.941000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.985000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.985000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.985000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.985000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:09.985000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.001000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.001000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.001000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.001000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.001000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.172000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.172000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.172000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.172000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.172000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.511000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.511000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.511000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.511000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.511000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.512000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.512000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.544000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.545000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.545000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.545000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.545000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.615000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.615000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.615000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.615000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:10.615000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
I0624 01:00:11.792221 11092 finetune.py:68] layer 10_down @ epoch 1 new loss 0.0004690520581789315 old loss 0.0004690722853410989 BETTER
W0624 01:00:11.848000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:11.862000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:11.871000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:11.871000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.335000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.335000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.335000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.335000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.335000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.335000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.336000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.368000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.368000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.368000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.368000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.368000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.726000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.726000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.726000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.726000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.726000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.726000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.726000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:00:12.726000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:13.029000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:00:13.030000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:00:13.030000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:00:13.030000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:00:13.030000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:00:13.371000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:00:13.377000 139967972292416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 01:00:20.133095 11598 finetune.py:45] layer 11_down initial loss 0.0005258906749077141
W0624 01:00:20.133482 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:00:20.290593 10586 finetune.py:68] layer 9_down @ epoch 2 new loss 0.00039179218583740294 old loss 0.0003918195143342018 BETTER
I0624 01:00:33.742161 10082 finetune.py:68] layer 8_down @ epoch 4 new loss 0.00031506508821621537 old loss 0.0003150828124489635 BETTER
W0624 01:00:34.446747 10082 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

8_down proxy err 0.012683428823947906 tr(WHW.T) 37.545013427734375
I0624 01:00:41.425073 11092 finetune.py:68] layer 10_down @ epoch 2 new loss 0.00046903284965083003 old loss 0.0004690520581789315 BETTER
I0624 01:00:49.171788 11598 finetune.py:68] layer 11_down @ epoch 0 new loss 0.0005256901495158672 old loss 0.0005258906749077141 BETTER
I0624 01:00:50.493701 10586 finetune.py:68] layer 9_down @ epoch 3 new loss 0.00039177676080726087 old loss 0.00039179218583740294 BETTER
I0624 01:01:10.774378 11092 finetune.py:68] layer 10_down @ epoch 3 new loss 0.000469009333755821 old loss 0.00046903284965083003 BETTER
I0624 01:01:19.076365 11598 finetune.py:68] layer 11_down @ epoch 1 new loss 0.0005256857839412987 old loss 0.0005256901495158672 BETTER
I0624 01:01:20.695764 10586 finetune.py:68] layer 9_down @ epoch 4 new loss 0.00039176843711175025 old loss 0.00039177676080726087 BETTER
W0624 01:01:21.421906 10586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

9_down proxy err 0.012884690426290035 tr(WHW.T) 44.03498077392578
I0624 01:01:40.050368 11092 finetune.py:68] layer 10_down @ epoch 4 new loss 0.00046898878645151854 old loss 0.000469009333755821 BETTER
W0624 01:01:40.738312 11092 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

10_down proxy err 0.012517528608441353 tr(WHW.T) 51.777793884277344
I0624 01:01:49.019351 11598 finetune.py:68] layer 11_down @ epoch 2 new loss 0.0005256312433630228 old loss 0.0005256857839412987 BETTER
I0624 01:02:19.331041 11598 finetune.py:68] layer 11_down @ epoch 3 new loss 0.0005256025469861925 old loss 0.0005256312433630228 BETTER
I0624 01:02:26.193683 4970 quantize_finetune_llama.py:193] computed original embedding for layer 12 in 61.38030695915222s
I0624 01:02:26.616810 4970 quantize_finetune_llama.py:162] layer 13 gpu 1
I0624 01:02:28.549976 12296 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:02:28.550084 12296 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:02:28.550149 12296 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:02:28.733218 12296 config.py:58] PyTorch version 2.4.0 available.
I0624 01:02:31.065213 12296 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:02:31.435113 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.35s/it]  6%|▋         | 2/32 [00:01<00:22,  1.33it/s]  9%|▉         | 3/32 [00:02<00:16,  1.77it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.12it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.56it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s] 41%|████      | 13/32 [00:05<00:06,  2.91it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.94it/s] 50%|█████     | 16/32 [00:06<00:05,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.94it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.95it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.94it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.94it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.93it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
W0624 01:02:47.683000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.683000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.683000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.683000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.684000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.684000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.684000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.711000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.711000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.711000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.711000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.711000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.728000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.729000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.729000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.729000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:02:47.729000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.068000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.068000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.068000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.068000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.068000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.998000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.998000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.998000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.998000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.998000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.999000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:02:48.999000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:02:49.017000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:02:49.017000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:02:49.017000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:02:49.018000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:02:49.018000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:02:49.268000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:02:49.268000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:02:49.268000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:02:49.268000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:02:49.268000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
I0624 01:02:49.610672 11598 finetune.py:76] layer 11_down @ epoch 4 new loss 0.0005256240838207304 old loss 0.0005256025469861925 WORSE
W0624 01:02:50.294770 11598 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0624 01:02:50.491000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.492000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.492000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.492000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.492000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.492000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.492000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.511000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.511000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.511000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.511000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:02:50.511000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
11_down proxy err 0.013226725161075592 tr(WHW.T) 55.157859802246094
W0624 01:02:51.457000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:02:51.458000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:02:51.458000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:02:51.458000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:02:51.458000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 01:02:57.754650 12296 finetune.py:45] layer 12_v initial loss 0.00014576088869944215
W0624 01:02:57.755069 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:03:25.775578 4970 quantize_finetune_llama.py:193] computed original embedding for layer 13 in 58.73506450653076s
I0624 01:03:26.187731 4970 quantize_finetune_llama.py:162] layer 14 gpu 2
I0624 01:03:28.247689 12800 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:03:28.247858 12800 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:03:28.247929 12800 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:03:28.449890 12800 config.py:58] PyTorch version 2.4.0 available.
I0624 01:03:31.003997 12800 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:03:31.500539 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 01:03:31.614987 12296 finetune.py:68] layer 12_v @ epoch 0 new loss 7.422307680826634e-05 old loss 0.00014576088869944215 BETTER
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:52,  1.68s/it]  6%|▋         | 2/32 [00:02<00:27,  1.10it/s]  9%|▉         | 3/32 [00:02<00:19,  1.51it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.83it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.08it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.68it/s] 41%|████      | 13/32 [00:06<00:06,  2.73it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s] 50%|█████     | 16/32 [00:07<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.75it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.75it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.74it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.75it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.77it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.75it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.75it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
W0624 01:03:48.928000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.928000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.928000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.928000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.928000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.928000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.928000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.956000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.956000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.956000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.956000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.956000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.975000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.975000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.975000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.975000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:03:48.975000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:03:49.324000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:03:49.324000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:03:49.324000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:03:49.324000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:03:49.324000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.266000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.266000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.266000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.266000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.266000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.266000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.267000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.285000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.285000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.285000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.285000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.286000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.541000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.541000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.541000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.541000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:03:50.541000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.800000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.801000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.801000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.801000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.801000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.801000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.801000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.819000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.820000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.820000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.820000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:03:51.820000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:03:52.776000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:03:52.777000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:03:52.777000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:03:52.777000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:03:52.777000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 01:03:58.781577 12800 finetune.py:45] layer 13_v initial loss 0.00017069248133338988
W0624 01:03:58.781872 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:04:06.686075 12296 finetune.py:68] layer 12_v @ epoch 1 new loss 6.813789514126256e-05 old loss 7.422307680826634e-05 BETTER
I0624 01:04:25.667795 4970 quantize_finetune_llama.py:193] computed original embedding for layer 14 in 59.04574704170227s
I0624 01:04:26.058375 4970 quantize_finetune_llama.py:162] layer 15 gpu 3
I0624 01:04:28.347208 13306 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:04:28.347329 13306 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:04:28.347392 13306 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:04:28.537638 13306 config.py:58] PyTorch version 2.4.0 available.
I0624 01:04:30.770437 12800 finetune.py:68] layer 13_v @ epoch 0 new loss 8.685576904099435e-05 old loss 0.00017069248133338988 BETTER
I0624 01:04:30.967429 13306 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:04:31.389446 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:51,  1.66s/it]  6%|▋         | 2/32 [00:02<00:26,  1.13it/s]  9%|▉         | 3/32 [00:02<00:18,  1.56it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.75it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.90it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.91it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.93it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.95it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.90it/s]I0624 01:04:41.748018 12296 finetune.py:68] layer 12_v @ epoch 2 new loss 6.534954445669428e-05 old loss 6.813789514126256e-05 BETTER
 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.93it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.90it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
W0624 01:04:48.130000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.130000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.130000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.130000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.131000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.131000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.131000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.158000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.158000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.158000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.158000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.158000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.175000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.176000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.176000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.176000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.176000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.512000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.512000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.512000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.512000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:04:48.512000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.436000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.436000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.437000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.437000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.437000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.437000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.437000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.455000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.455000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.456000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.456000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.456000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.705000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.706000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.706000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.706000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:04:49.706000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.905000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.905000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.905000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.905000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.905000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.905000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.905000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.924000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.924000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.924000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.924000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:04:50.924000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:04:51.864000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:04:51.864000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:04:51.864000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:04:51.864000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:04:51.864000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 01:04:58.167895 13306 finetune.py:45] layer 14_v initial loss 0.00017558776016812772
W0624 01:04:58.168288 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:05:03.496044 12800 finetune.py:68] layer 13_v @ epoch 1 new loss 7.992134487722069e-05 old loss 8.685576904099435e-05 BETTER
I0624 01:05:16.644923 12296 finetune.py:68] layer 12_v @ epoch 3 new loss 6.359916005749255e-05 old loss 6.534954445669428e-05 BETTER
I0624 01:05:24.857122 4970 quantize_finetune_llama.py:193] computed original embedding for layer 15 in 58.33691215515137s
I0624 01:05:25.245882 4970 quantize_finetune_llama.py:162] layer 16 gpu 0
I0624 01:05:27.356739 13812 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:05:27.356904 13812 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:05:27.356968 13812 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:05:27.630104 13812 config.py:58] PyTorch version 2.4.0 available.
I0624 01:05:29.811268 13306 finetune.py:68] layer 14_v @ epoch 0 new loss 0.00010319696593796834 old loss 0.00017558776016812772 BETTER
I0624 01:05:30.337780 13812 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:05:30.762242 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:54,  1.76s/it]  6%|▋         | 2/32 [00:02<00:27,  1.08it/s]  9%|▉         | 3/32 [00:02<00:19,  1.50it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.86it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.13it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.36it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.61it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s]I0624 01:05:36.862517 12800 finetune.py:68] layer 13_v @ epoch 2 new loss 7.663309952476993e-05 old loss 7.992134487722069e-05 BETTER
 34%|███▍      | 11/32 [00:05<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.86it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.85it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.89it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
W0624 01:05:47.893000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.894000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.894000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.894000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.894000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.894000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.894000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.919000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.919000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.919000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.919000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.919000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.935000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.936000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.936000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.936000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:05:47.936000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:05:48.265000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:05:48.265000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:05:48.265000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:05:48.265000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:05:48.265000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.177000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.177000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.177000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.177000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.177000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.177000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.178000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.195000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.195000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.195000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.195000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.195000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.446000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.447000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.447000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.447000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:05:49.447000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.639000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.639000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.639000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.639000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.639000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.639000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.640000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.658000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.658000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.658000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.658000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:05:50.659000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:05:51.607000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:05:51.607000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:05:51.608000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:05:51.608000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:05:51.608000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
I0624 01:05:51.872176 12296 finetune.py:68] layer 12_v @ epoch 4 new loss 6.231266161194071e-05 old loss 6.359916005749255e-05 BETTER
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0624 01:05:53.448606 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_v proxy err 0.009385103359818459 tr(WHW.T) 736.8345947265625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.56it/s]  6%|▋         | 2/32 [00:00<00:14,  2.14it/s]  9%|▉         | 3/32 [00:01<00:11,  2.43it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.59it/s] 16%|█▌        | 5/32 [00:02<00:09,  2.70it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.76it/s]I0624 01:05:57.060933 13812 finetune.py:45] layer 15_v initial loss 0.00020973649225197732
W0624 01:05:57.061227 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:02<00:08,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.85it/s] 41%|████      | 13/32 [00:04<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.87it/s] 50%|█████     | 16/32 [00:05<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.87it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.87it/s]I0624 01:06:02.209932 13306 finetune.py:68] layer 14_v @ epoch 1 new loss 9.587701060809195e-05 old loss 0.00010319696593796834 BETTER
 69%|██████▉   | 22/32 [00:07<00:03,  2.86it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.89it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.90it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.90it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.91it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]
I0624 01:06:09.786135 12800 finetune.py:68] layer 13_v @ epoch 3 new loss 7.471432763850316e-05 old loss 7.663309952476993e-05 BETTER
I0624 01:06:13.182104 12296 finetune.py:45] layer 12_q initial loss 9.153019345831126e-05
W0624 01:06:13.182497 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:06:28.068092 13812 finetune.py:68] layer 15_v @ epoch 0 new loss 0.00011327931861160323 old loss 0.00020973649225197732 BETTER
I0624 01:06:34.617346 13306 finetune.py:68] layer 14_v @ epoch 2 new loss 9.22387553146109e-05 old loss 9.587701060809195e-05 BETTER
I0624 01:06:42.904465 12800 finetune.py:68] layer 13_v @ epoch 4 new loss 7.31986147002317e-05 old loss 7.471432763850316e-05 BETTER
W0624 01:06:44.483844 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_v proxy err 0.010509590618312359 tr(WHW.T) 740.3930053710938
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:20,  1.54it/s]  6%|▋         | 2/32 [00:01<00:14,  2.09it/s]  9%|▉         | 3/32 [00:01<00:12,  2.36it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.51it/s]I0624 01:06:47.459755 12296 finetune.py:68] layer 12_q @ epoch 0 new loss 8.830094884615391e-05 old loss 9.153019345831126e-05 BETTER
 16%|█▌        | 5/32 [00:02<00:10,  2.64it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.73it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.74it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.77it/s] 41%|████      | 13/32 [00:04<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.81it/s] 50%|█████     | 16/32 [00:05<00:05,  2.83it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.79it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.79it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
I0624 01:06:59.927735 13812 finetune.py:68] layer 15_v @ epoch 1 new loss 0.00010521080548642203 old loss 0.00011327931861160323 BETTER
I0624 01:07:03.929536 12800 finetune.py:45] layer 13_q initial loss 0.00010297039989382029
W0624 01:07:03.929889 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:07:07.297396 13306 finetune.py:68] layer 14_v @ epoch 3 new loss 8.974288357421756e-05 old loss 9.22387553146109e-05 BETTER
I0624 01:07:22.056455 12296 finetune.py:68] layer 12_q @ epoch 1 new loss 8.659667219035327e-05 old loss 8.830094884615391e-05 BETTER
I0624 01:07:31.831740 13812 finetune.py:68] layer 15_v @ epoch 2 new loss 0.0001011756612570025 old loss 0.00010521080548642203 BETTER
I0624 01:07:36.092849 12800 finetune.py:68] layer 13_q @ epoch 0 new loss 9.971550753107294e-05 old loss 0.00010297039989382029 BETTER
I0624 01:07:40.140059 13306 finetune.py:68] layer 14_v @ epoch 4 new loss 8.79219442140311e-05 old loss 8.974288357421756e-05 BETTER
W0624 01:07:41.703137 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_v proxy err 0.010501362383365631 tr(WHW.T) 739.513916015625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.60it/s]  6%|▋         | 2/32 [00:00<00:13,  2.17it/s]  9%|▉         | 3/32 [00:01<00:11,  2.46it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.82it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.85it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 41%|████      | 13/32 [00:04<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.90it/s] 50%|█████     | 16/32 [00:05<00:05,  2.91it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.91it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.90it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.93it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.90it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.89it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.90it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.90it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.90it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]
I0624 01:07:56.754865 12296 finetune.py:68] layer 12_q @ epoch 2 new loss 8.530618652002886e-05 old loss 8.659667219035327e-05 BETTER
I0624 01:08:00.753223 13306 finetune.py:45] layer 14_q initial loss 0.00012323027476668358
W0624 01:08:00.753566 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:08:03.746059 13812 finetune.py:68] layer 15_v @ epoch 3 new loss 9.843866428127512e-05 old loss 0.0001011756612570025 BETTER
I0624 01:08:09.406119 12800 finetune.py:68] layer 13_q @ epoch 1 new loss 9.771563054528087e-05 old loss 9.971550753107294e-05 BETTER
I0624 01:08:32.255041 12296 finetune.py:68] layer 12_q @ epoch 3 new loss 8.428209548583254e-05 old loss 8.530618652002886e-05 BETTER
I0624 01:08:32.946599 13306 finetune.py:68] layer 14_q @ epoch 0 new loss 0.00011889301094925031 old loss 0.00012323027476668358 BETTER
I0624 01:08:36.004330 13812 finetune.py:68] layer 15_v @ epoch 4 new loss 9.641257929615676e-05 old loss 9.843866428127512e-05 BETTER
W0624 01:08:37.473970 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_v proxy err 0.010450744070112705 tr(WHW.T) 784.601806640625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.55it/s]  6%|▋         | 2/32 [00:00<00:13,  2.14it/s]  9%|▉         | 3/32 [00:01<00:11,  2.42it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.56it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.73it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.83it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.86it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.90it/s]I0624 01:08:43.112867 12800 finetune.py:68] layer 13_q @ epoch 2 new loss 9.62467456702143e-05 old loss 9.771563054528087e-05 BETTER
 41%|████      | 13/32 [00:04<00:06,  2.90it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.90it/s] 50%|█████     | 16/32 [00:05<00:05,  2.91it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.91it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.89it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.91it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.92it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.91it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.92it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.90it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.91it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]
I0624 01:08:56.296312 13812 finetune.py:45] layer 15_q initial loss 0.00013516101171262562
W0624 01:08:56.296728 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:09:06.174761 13306 finetune.py:68] layer 14_q @ epoch 1 new loss 0.00011657065624604002 old loss 0.00011889301094925031 BETTER
I0624 01:09:07.887558 12296 finetune.py:68] layer 12_q @ epoch 4 new loss 8.342755609191954e-05 old loss 8.428209548583254e-05 BETTER
W0624 01:09:09.437844 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_q proxy err 0.0027861623093485832 tr(WHW.T) 6755.609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.67it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.65it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.72it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.76it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.78it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 41%|████      | 13/32 [00:04<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.88it/s] 50%|█████     | 16/32 [00:05<00:05,  2.90it/s]I0624 01:09:16.486010 12800 finetune.py:68] layer 13_q @ epoch 3 new loss 9.500078158453107e-05 old loss 9.62467456702143e-05 BETTER
 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.95it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.94it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.96it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.97it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.98it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.99it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.97it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.00it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.00it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]
I0624 01:09:27.867998 13812 finetune.py:68] layer 15_q @ epoch 0 new loss 0.00013021186168771237 old loss 0.00013516101171262562 BETTER
I0624 01:09:28.649451 12296 finetune.py:45] layer 12_k initial loss 0.00010560918599367142
W0624 01:09:28.649836 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:09:39.327072 13306 finetune.py:68] layer 14_q @ epoch 2 new loss 0.00011471510515548289 old loss 0.00011657065624604002 BETTER
I0624 01:09:49.605433 12800 finetune.py:68] layer 13_q @ epoch 4 new loss 9.394072549184784e-05 old loss 9.500078158453107e-05 BETTER
W0624 01:09:51.027145 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_q proxy err 0.002911507850512862 tr(WHW.T) 6652.5361328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]  6%|▋         | 2/32 [00:00<00:13,  2.21it/s]  9%|▉         | 3/32 [00:01<00:11,  2.45it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.59it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.70it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.76it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 41%|████      | 13/32 [00:04<00:06,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.82it/s] 50%|█████     | 16/32 [00:05<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.76it/s]I0624 01:10:00.037862 13812 finetune.py:68] layer 15_q @ epoch 1 new loss 0.00012743232946377248 old loss 0.00013021186168771237 BETTER
 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.77it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.75it/s]I0624 01:10:02.866385 12296 finetune.py:68] layer 12_k @ epoch 0 new loss 0.00010327524068998173 old loss 0.00010560918599367142 BETTER
 94%|█████████▍| 30/32 [00:10<00:00,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
I0624 01:10:10.472521 12800 finetune.py:45] layer 13_k initial loss 0.00011707386147463694
W0624 01:10:10.472894 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:10:11.839548 13306 finetune.py:68] layer 14_q @ epoch 3 new loss 0.0001131429016822949 old loss 0.00011471510515548289 BETTER
I0624 01:10:32.087742 13812 finetune.py:68] layer 15_q @ epoch 2 new loss 0.0001251998037332669 old loss 0.00012743232946377248 BETTER
I0624 01:10:37.762881 12296 finetune.py:68] layer 12_k @ epoch 1 new loss 0.00010238694812869653 old loss 0.00010327524068998173 BETTER
I0624 01:10:42.624214 12800 finetune.py:68] layer 13_k @ epoch 0 new loss 0.00011452358739916235 old loss 0.00011707386147463694 BETTER
I0624 01:10:44.224262 13306 finetune.py:68] layer 14_q @ epoch 4 new loss 0.0001118620130000636 old loss 0.0001131429016822949 BETTER
W0624 01:10:45.658680 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_q proxy err 0.002888293005526066 tr(WHW.T) 6908.2705078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.70it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s]  9%|▉         | 3/32 [00:01<00:11,  2.50it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.66it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.81it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.84it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.86it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.90it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 41%|████      | 13/32 [00:04<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.89it/s] 50%|█████     | 16/32 [00:05<00:05,  2.90it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.91it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.91it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.90it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.89it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.89it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.88it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
I0624 01:11:04.477886 13306 finetune.py:45] layer 14_k initial loss 0.00013954420865047723
W0624 01:11:04.478152 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:11:04.478744 13812 finetune.py:68] layer 15_q @ epoch 3 new loss 0.0001233532966580242 old loss 0.0001251998037332669 BETTER
I0624 01:11:12.878679 12296 finetune.py:68] layer 12_k @ epoch 2 new loss 0.00010174827912123874 old loss 0.00010238694812869653 BETTER
I0624 01:11:15.636035 12800 finetune.py:68] layer 13_k @ epoch 1 new loss 0.00011361182987457141 old loss 0.00011452358739916235 BETTER
I0624 01:11:36.190582 13306 finetune.py:68] layer 14_k @ epoch 0 new loss 0.0001366722717648372 old loss 0.00013954420865047723 BETTER
I0624 01:11:36.589063 13812 finetune.py:68] layer 15_q @ epoch 4 new loss 0.00012181418424006552 old loss 0.0001233532966580242 BETTER
W0624 01:11:37.958048 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_q proxy err 0.0027381679974496365 tr(WHW.T) 6934.982421875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.60it/s]  6%|▋         | 2/32 [00:00<00:13,  2.16it/s]  9%|▉         | 3/32 [00:01<00:11,  2.44it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.58it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.66it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.72it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.79it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.84it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.89it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 41%|████      | 13/32 [00:04<00:06,  2.91it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.90it/s] 50%|█████     | 16/32 [00:05<00:05,  2.87it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.91it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.90it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.90it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.91it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.92it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s]I0624 01:11:48.441921 12296 finetune.py:68] layer 12_k @ epoch 3 new loss 0.00010118444333784282 old loss 0.00010174827912123874 BETTER
 84%|████████▍ | 27/32 [00:09<00:01,  2.92it/s]I0624 01:11:48.805628 12800 finetune.py:68] layer 13_k @ epoch 2 new loss 0.00011287076631560922 old loss 0.00011361182987457141 BETTER
 88%|████████▊ | 28/32 [00:09<00:01,  2.94it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.94it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]
I0624 01:11:56.641222 13812 finetune.py:45] layer 15_k initial loss 0.00015326695574913174
W0624 01:11:56.641771 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:12:09.038889 13306 finetune.py:68] layer 14_k @ epoch 1 new loss 0.00013557316560763866 old loss 0.0001366722717648372 BETTER
I0624 01:12:21.715214 12800 finetune.py:68] layer 13_k @ epoch 3 new loss 0.0001122578396461904 old loss 0.00011287076631560922 BETTER
I0624 01:12:23.367464 12296 finetune.py:68] layer 12_k @ epoch 4 new loss 0.00010069440031656995 old loss 0.00010118444333784282 BETTER
W0624 01:12:24.767644 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_k proxy err 0.0019529436249285936 tr(WHW.T) 10264.55078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:17,  1.76it/s]  6%|▋         | 2/32 [00:00<00:12,  2.31it/s]  9%|▉         | 3/32 [00:01<00:11,  2.57it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.73it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.85it/s]I0624 01:12:28.072641 13812 finetune.py:68] layer 15_k @ epoch 0 new loss 0.0001495557080488652 old loss 0.00015326695574913174 BETTER
 22%|██▏       | 7/32 [00:02<00:08,  2.89it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.90it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.92it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.94it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.95it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.99it/s] 41%|████      | 13/32 [00:04<00:06,  2.99it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.99it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.99it/s] 50%|█████     | 16/32 [00:05<00:05,  2.99it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.98it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.96it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.93it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.93it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.94it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.93it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.93it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.93it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]
I0624 01:12:41.888472 13306 finetune.py:68] layer 14_k @ epoch 2 new loss 0.00013469578698277473 old loss 0.00013557316560763866 BETTER
I0624 01:12:43.888255 12296 finetune.py:45] layer 12_o initial loss 0.0001930473663378507
W0624 01:12:43.888694 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:12:54.980339 12800 finetune.py:68] layer 13_k @ epoch 4 new loss 0.00011167461343575269 old loss 0.0001122578396461904 BETTER
W0624 01:12:56.460589 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_k proxy err 0.0020475187338888645 tr(WHW.T) 9807.25390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.68it/s]  6%|▋         | 2/32 [00:00<00:13,  2.21it/s]  9%|▉         | 3/32 [00:01<00:11,  2.45it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.59it/s] 16%|█▌        | 5/32 [00:01<00:10,  2.69it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.76it/s]I0624 01:13:00.162806 13812 finetune.py:68] layer 15_k @ epoch 1 new loss 0.0001482128573115915 old loss 0.0001495557080488652 BETTER
 22%|██▏       | 7/32 [00:02<00:08,  2.78it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.80it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 41%|████      | 13/32 [00:04<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.83it/s] 50%|█████     | 16/32 [00:05<00:05,  2.83it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.83it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
I0624 01:13:14.802676 13306 finetune.py:68] layer 14_k @ epoch 3 new loss 0.0001340503804385662 old loss 0.00013469578698277473 BETTER
I0624 01:13:15.937557 12800 finetune.py:45] layer 13_o initial loss 0.00021335086785256863
W0624 01:13:15.937895 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:13:18.137700 12296 finetune.py:68] layer 12_o @ epoch 0 new loss 0.0001888642000267282 old loss 0.0001930473663378507 BETTER
I0624 01:13:32.152490 13812 finetune.py:68] layer 15_k @ epoch 2 new loss 0.00014710522373206913 old loss 0.0001482128573115915 BETTER
I0624 01:13:47.350491 13306 finetune.py:68] layer 14_k @ epoch 4 new loss 0.00013329410285223275 old loss 0.0001340503804385662 BETTER
I0624 01:13:48.247680 12800 finetune.py:68] layer 13_o @ epoch 0 new loss 0.00020816572941839695 old loss 0.00021335086785256863 BETTER
W0624 01:13:48.740280 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_k proxy err 0.0019337472040206194 tr(WHW.T) 10748.44140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.72it/s]  6%|▋         | 2/32 [00:00<00:13,  2.28it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.69it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.80it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.85it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.89it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.92it/s]I0624 01:13:52.871604 12296 finetune.py:68] layer 12_o @ epoch 1 new loss 0.00018717559578362852 old loss 0.0001888642000267282 BETTER
 28%|██▊       | 9/32 [00:03<00:07,  2.95it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.97it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.97it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.98it/s] 41%|████      | 13/32 [00:04<00:06,  2.99it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.99it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.98it/s] 50%|█████     | 16/32 [00:05<00:05,  2.95it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.94it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.97it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.96it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.98it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.99it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.99it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.99it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.00it/s] 91%|█████████ | 29/32 [00:09<00:01,  2.98it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.95it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]
I0624 01:14:04.183067 13812 finetune.py:68] layer 15_k @ epoch 3 new loss 0.00014619133435189724 old loss 0.00014710522373206913 BETTER
I0624 01:14:08.094478 13306 finetune.py:45] layer 14_o initial loss 0.00025860860478132963
W0624 01:14:08.094886 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:14:21.271557 12800 finetune.py:68] layer 13_o @ epoch 1 new loss 0.00020621370640583336 old loss 0.00020816572941839695 BETTER
I0624 01:14:27.839991 12296 finetune.py:68] layer 12_o @ epoch 2 new loss 0.00018586969235911965 old loss 0.00018717559578362852 BETTER
I0624 01:14:36.436427 13812 finetune.py:68] layer 15_k @ epoch 4 new loss 0.0001453989534638822 old loss 0.00014619133435189724 BETTER
W0624 01:14:37.822201 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_k proxy err 0.0019649132154881954 tr(WHW.T) 10361.5234375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]  6%|▋         | 2/32 [00:00<00:13,  2.21it/s]I0624 01:14:40.098709 13306 finetune.py:68] layer 14_o @ epoch 0 new loss 0.00025333015946671367 old loss 0.00025860860478132963 BETTER
  9%|▉         | 3/32 [00:01<00:11,  2.46it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.67it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.77it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.82it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.90it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.92it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.92it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 41%|████      | 13/32 [00:04<00:06,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 50%|█████     | 16/32 [00:05<00:05,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.89it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.91it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.91it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.90it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.89it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.90it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]
I0624 01:14:54.824717 12800 finetune.py:68] layer 13_o @ epoch 2 new loss 0.00020470115123316646 old loss 0.00020621370640583336 BETTER
I0624 01:14:56.548483 13812 finetune.py:45] layer 15_o initial loss 0.00028213439509272575
W0624 01:14:56.548893 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:15:03.245068 12296 finetune.py:68] layer 12_o @ epoch 3 new loss 0.00018476771947462112 old loss 0.00018586969235911965 BETTER
I0624 01:15:12.429919 13306 finetune.py:68] layer 14_o @ epoch 1 new loss 0.000251065066549927 old loss 0.00025333015946671367 BETTER
I0624 01:15:27.708784 12800 finetune.py:68] layer 13_o @ epoch 3 new loss 0.00020343087089713663 old loss 0.00020470115123316646 BETTER
I0624 01:15:27.959149 13812 finetune.py:68] layer 15_o @ epoch 0 new loss 0.000275913072982803 old loss 0.00028213439509272575 BETTER
I0624 01:15:38.319688 12296 finetune.py:68] layer 12_o @ epoch 4 new loss 0.00018380320398136973 old loss 0.00018476771947462112 BETTER
W0624 01:15:39.766876 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_o proxy err 0.011435598134994507 tr(WHW.T) 33.65260314941406
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.27s/it]  6%|▋         | 2/32 [00:02<00:33,  1.12s/it]  9%|▉         | 3/32 [00:03<00:30,  1.07s/it]I0624 01:15:45.069813 13306 finetune.py:68] layer 14_o @ epoch 2 new loss 0.00024928117636591196 old loss 0.000251065066549927 BETTER
 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it]I0624 01:15:59.949243 13812 finetune.py:68] layer 15_o @ epoch 1 new loss 0.00027320411754772067 old loss 0.000275913072982803 BETTER
 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it]I0624 01:16:00.785075 12800 finetune.py:68] layer 13_o @ epoch 4 new loss 0.00020235507690813392 old loss 0.00020343087089713663 BETTER
 62%|██████▎   | 20/32 [00:20<00:12,  1.01s/it]W0624 01:16:02.155248 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it]13_o proxy err 0.01067265123128891 tr(WHW.T) 40.389564514160156
  0%|          | 0/32 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.00it/s]  3%|▎         | 1/32 [00:01<00:40,  1.29s/it] 72%|███████▏  | 23/32 [00:23<00:08,  1.01it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.01it/s]  6%|▋         | 2/32 [00:02<00:34,  1.13s/it] 78%|███████▊  | 25/32 [00:25<00:06,  1.02it/s]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 81%|████████▏ | 26/32 [00:26<00:05,  1.02it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.06s/it] 84%|████████▍ | 27/32 [00:27<00:04,  1.03it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it] 88%|████████▊ | 28/32 [00:28<00:03,  1.02it/s] 19%|█▉        | 6/32 [00:06<00:26,  1.04s/it] 91%|█████████ | 29/32 [00:29<00:02,  1.02it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 94%|█████████▍| 30/32 [00:30<00:01,  1.02it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 97%|█████████▋| 31/32 [00:31<00:00,  1.03it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.02it/s]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]
 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.04s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.04s/it] 41%|████      | 13/32 [00:13<00:19,  1.03s/it]I0624 01:16:17.399963 13306 finetune.py:68] layer 14_o @ epoch 3 new loss 0.0002477892558090389 old loss 0.00024928117636591196 BETTER
 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it]W0624 01:16:19.781000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.781000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.781000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.781000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.781000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.781000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.781000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.809000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.810000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.810000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.810000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.810000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.824000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.824000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.825000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.825000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.825000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.976000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.976000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.976000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.976000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:19.976000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 16/32 [00:16<00:16,  1.03s/it]W0624 01:16:20.203000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.203000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.203000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.203000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.203000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.203000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.203000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.222000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.222000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.222000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.222000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.222000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.285000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.285000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.285000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.285000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:20.285000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it]W0624 01:16:21.123000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.420000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.420000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.420000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.420000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.420000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.420000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.421000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.441000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.441000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.441000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.441000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.441000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.691000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.691000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.691000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.691000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.691000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:21.940000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.05s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.06s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.06s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.07s/it]I0624 01:16:28.500592 12296 finetune.py:45] layer 12_up initial loss 0.00031169390422292054
W0624 01:16:28.500960 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 78%|███████▊  | 25/32 [00:26<00:07,  1.06s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.05s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.05s/it]I0624 01:16:31.918674 13812 finetune.py:68] layer 15_o @ epoch 2 new loss 0.00027109048096463084 old loss 0.00027320411754772067 BETTER
 88%|████████▊ | 28/32 [00:29<00:04,  1.05s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.05s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.05s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.04s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]100%|██████████| 32/32 [00:33<00:00,  1.05s/it]
W0624 01:16:43.490000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.490000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.490000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.490000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.490000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.490000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.490000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.518000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.518000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.518000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.518000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.518000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.533000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.534000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.534000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.534000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.534000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.693000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.693000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.693000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.693000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.693000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.925000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.925000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.925000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.925000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.925000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.925000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.925000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.946000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.946000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.947000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.947000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:43.947000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:44.010000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:44.010000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:44.010000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:44.010000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:44.010000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:44.894000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.209000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.209000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.209000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.209000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.209000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.209000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.210000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.232000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.232000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.232000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.232000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.232000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.495000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.495000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.495000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.495000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.495000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:16:45.762000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 01:16:49.515061 13306 finetune.py:68] layer 14_o @ epoch 4 new loss 0.0002464421559125185 old loss 0.0002477892558090389 BETTER
W0624 01:16:50.870380 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_o proxy err 0.011642097495496273 tr(WHW.T) 44.134986877441406
  0%|          | 0/32 [00:00<?, ?it/s]I0624 01:16:52.208505 12800 finetune.py:45] layer 13_up initial loss 0.00035265719634480774
W0624 01:16:52.208931 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:02<00:33,  1.12s/it]  9%|▉         | 3/32 [00:03<00:30,  1.07s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it]I0624 01:17:01.399172 12296 finetune.py:68] layer 12_up @ epoch 0 new loss 0.000308617134578526 old loss 0.00031169390422292054 BETTER
 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.04s/it]I0624 01:17:03.909378 13812 finetune.py:68] layer 15_o @ epoch 3 new loss 0.00026934524066746235 old loss 0.00027109048096463084 BETTER
 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.02s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.01s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.01s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.01s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.01s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it]I0624 01:17:23.840247 12800 finetune.py:68] layer 13_up @ epoch 0 new loss 0.00034886691719293594 old loss 0.00035265719634480774 BETTER
 97%|█████████▋| 31/32 [00:31<00:01,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.00s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
W0624 01:17:31.499000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.499000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.499000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.499000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.499000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.499000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.499000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.530000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.530000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.530000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.531000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.531000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.546000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.546000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.546000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.546000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.546000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.706000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.706000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.706000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.707000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.707000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.946000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.947000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.947000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.947000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.947000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.947000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.947000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.969000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.970000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.970000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.970000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:17:31.970000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:17:32.037000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:17:32.037000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:17:32.037000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:17:32.037000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:17:32.037000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:17:32.944000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.263000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.264000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.264000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.264000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.264000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.264000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.264000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.287000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.287000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.287000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.287000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.287000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.552000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.552000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.552000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.552000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.552000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:17:33.822000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 01:17:35.425194 12296 finetune.py:68] layer 12_up @ epoch 1 new loss 0.00030667916871607304 old loss 0.000308617134578526 BETTER
I0624 01:17:36.021298 13812 finetune.py:68] layer 15_o @ epoch 4 new loss 0.0002678324526641518 old loss 0.00026934524066746235 BETTER
W0624 01:17:37.421179 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_o proxy err 0.010739111341536045 tr(WHW.T) 52.296504974365234
  0%|          | 0/32 [00:00<?, ?it/s]I0624 01:17:40.082283 13306 finetune.py:45] layer 14_up initial loss 0.0004175863286945969
W0624 01:17:40.082603 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:02<00:34,  1.15s/it]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.06s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.04s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.02s/it] 41%|████      | 13/32 [00:13<00:19,  1.02s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.02s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it]I0624 01:17:56.176664 12800 finetune.py:68] layer 13_up @ epoch 1 new loss 0.0003463811590336263 old loss 0.00034886691719293594 BETTER
 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.02s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.03s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.03s/it]I0624 01:18:09.233394 12296 finetune.py:68] layer 12_up @ epoch 2 new loss 0.0003050066588912159 old loss 0.00030667916871607304 BETTER
 94%|█████████▍| 30/32 [00:30<00:02,  1.04s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.04s/it]I0624 01:18:10.946879 13306 finetune.py:68] layer 14_up @ epoch 0 new loss 0.0004132830654270947 old loss 0.0004175863286945969 BETTER
100%|██████████| 32/32 [00:33<00:00,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.03s/it]
W0624 01:18:18.308000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.309000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.309000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.309000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.309000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.309000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.309000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.339000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.339000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.339000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.339000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.339000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.355000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.355000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.355000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.355000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.355000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.511000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.511000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.511000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.511000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.512000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.745000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.745000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.745000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.745000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.745000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.745000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.745000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.766000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.766000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.766000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.766000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.766000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.829000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.829000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.829000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.829000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:18:18.829000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:18:19.705000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.020000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.020000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.020000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.020000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.020000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.020000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.021000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.042000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.042000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.042000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.042000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.042000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.301000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.302000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.302000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.302000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.302000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:18:20.560000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 01:18:26.644878 13812 finetune.py:45] layer 15_up initial loss 0.00047222914872691035
W0624 01:18:26.645230 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:18:28.294899 12800 finetune.py:68] layer 13_up @ epoch 2 new loss 0.0003442727611400187 old loss 0.0003463811590336263 BETTER
I0624 01:18:42.288947 13306 finetune.py:68] layer 14_up @ epoch 1 new loss 0.00041063170647248626 old loss 0.0004132830654270947 BETTER
I0624 01:18:43.223348 12296 finetune.py:68] layer 12_up @ epoch 3 new loss 0.00030350033193826675 old loss 0.0003050066588912159 BETTER
I0624 01:18:56.718674 13812 finetune.py:68] layer 15_up @ epoch 0 new loss 0.0004667794855777174 old loss 0.00047222914872691035 BETTER
I0624 01:19:00.359813 12800 finetune.py:68] layer 13_up @ epoch 3 new loss 0.0003424315946176648 old loss 0.0003442727611400187 BETTER
I0624 01:19:13.626904 13306 finetune.py:68] layer 14_up @ epoch 2 new loss 0.00040834941319189966 old loss 0.00041063170647248626 BETTER
I0624 01:19:17.112670 12296 finetune.py:68] layer 12_up @ epoch 4 new loss 0.0003021624870598316 old loss 0.00030350033193826675 BETTER
W0624 01:19:18.394590 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_up proxy err 0.009961806237697601 tr(WHW.T) 1222.8060302734375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it]  9%|▉         | 3/32 [00:03<00:30,  1.06s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.02s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.01s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.00s/it]I0624 01:19:27.270699 13812 finetune.py:68] layer 15_up @ epoch 1 new loss 0.00046341377310454845 old loss 0.0004667794855777174 BETTER
 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it]I0624 01:19:32.512835 12800 finetune.py:68] layer 13_up @ epoch 4 new loss 0.00034074668656103313 old loss 0.0003424315946176648 BETTER
 41%|████      | 13/32 [00:13<00:19,  1.02s/it]W0624 01:19:33.684812 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it]13_up proxy err 0.009782676585018635 tr(WHW.T) 1336.540283203125
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.01s/it] 50%|█████     | 16/32 [00:16<00:15,  1.00it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it] 53%|█████▎    | 17/32 [00:17<00:14,  1.01it/s]  6%|▋         | 2/32 [00:02<00:34,  1.15s/it] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 59%|█████▉    | 19/32 [00:19<00:12,  1.01it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.07s/it] 62%|██████▎   | 20/32 [00:20<00:11,  1.02it/s] 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it] 66%|██████▌   | 21/32 [00:21<00:10,  1.02it/s] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it] 69%|██████▉   | 22/32 [00:22<00:09,  1.02it/s] 22%|██▏       | 7/32 [00:07<00:25,  1.04s/it] 72%|███████▏  | 23/32 [00:23<00:08,  1.02it/s] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 75%|███████▌  | 24/32 [00:24<00:07,  1.03it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it] 78%|███████▊  | 25/32 [00:25<00:06,  1.03it/s]I0624 01:19:45.180449 13306 finetune.py:68] layer 14_up @ epoch 3 new loss 0.0004063751548528671 old loss 0.00040834941319189966 BETTER
 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 81%|████████▏ | 26/32 [00:26<00:05,  1.03it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 84%|████████▍ | 27/32 [00:27<00:04,  1.02it/s] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 88%|████████▊ | 28/32 [00:28<00:03,  1.02it/s] 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 91%|█████████ | 29/32 [00:29<00:02,  1.02it/s] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 94%|█████████▍| 30/32 [00:30<00:01,  1.02it/s] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 97%|█████████▋| 31/32 [00:31<00:00,  1.02it/s] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.02it/s]100%|██████████| 32/32 [00:32<00:00,  1.00s/it]
 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.04s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.05s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.06s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.07s/it]I0624 01:19:57.862074 13812 finetune.py:68] layer 15_up @ epoch 2 new loss 0.00046060021850280464 old loss 0.00046341377310454845 BETTER
 69%|██████▉   | 22/32 [00:23<00:10,  1.07s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.08s/it]I0624 01:19:59.266664 12296 finetune.py:45] layer 12_gate initial loss 0.0004034369485452771
W0624 01:19:59.267043 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 75%|███████▌  | 24/32 [00:25<00:08,  1.08s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.07s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.06s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.07s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.06s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.06s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.06s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.06s/it]100%|██████████| 32/32 [00:33<00:00,  1.06s/it]100%|██████████| 32/32 [00:33<00:00,  1.06s/it]
I0624 01:20:16.117988 12800 finetune.py:45] layer 13_gate initial loss 0.00045949697960168123
W0624 01:20:16.118412 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:20:16.655306 13306 finetune.py:68] layer 14_up @ epoch 4 new loss 0.0004045763926114887 old loss 0.0004063751548528671 BETTER
W0624 01:20:17.822517 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_up proxy err 0.010099764913320541 tr(WHW.T) 1436.1839599609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]  6%|▋         | 2/32 [00:02<00:33,  1.12s/it]  9%|▉         | 3/32 [00:03<00:30,  1.07s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it]I0624 01:20:28.716987 13812 finetune.py:68] layer 15_up @ epoch 3 new loss 0.00045810011215507984 old loss 0.00046060021850280464 BETTER
 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it]I0624 01:20:31.536386 12296 finetune.py:68] layer 12_gate @ epoch 0 new loss 0.00040109799010679126 old loss 0.0004034369485452771 BETTER
 38%|███▊      | 12/32 [00:12<00:20,  1.02s/it] 41%|████      | 13/32 [00:13<00:19,  1.01s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.01s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.01s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.01s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.01s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.01s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.01s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it]I0624 01:20:47.384715 12800 finetune.py:68] layer 13_gate @ epoch 0 new loss 0.0004565696290228516 old loss 0.00045949697960168123 BETTER
 88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.01s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
I0624 01:20:59.292489 13306 finetune.py:45] layer 14_gate initial loss 0.0005463875131681561
W0624 01:20:59.293159 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:20:59.975814 13812 finetune.py:68] layer 15_up @ epoch 4 new loss 0.0004558736691251397 old loss 0.00045810011215507984 BETTER
W0624 01:21:01.209154 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_up proxy err 0.009877540171146393 tr(WHW.T) 1588.4970703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it]I0624 01:21:04.680015 12296 finetune.py:68] layer 12_gate @ epoch 1 new loss 0.0003998112224508077 old loss 0.00040109799010679126 BETTER
  6%|▋         | 2/32 [00:02<00:34,  1.16s/it]  9%|▉         | 3/32 [00:03<00:31,  1.10s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.07s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.06s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.05s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.04s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.04s/it] 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it]I0624 01:21:19.055011 12800 finetune.py:68] layer 13_gate @ epoch 1 new loss 0.000454979162896052 old loss 0.0004565696290228516 BETTER
 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.03s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.03s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.04s/it]I0624 01:21:29.596152 13306 finetune.py:68] layer 14_gate @ epoch 0 new loss 0.0005429048906080425 old loss 0.0005463875131681561 BETTER
 84%|████████▍ | 27/32 [00:28<00:05,  1.03s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.03s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.02s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.03s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.03s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
I0624 01:21:37.671677 12296 finetune.py:68] layer 12_gate @ epoch 2 new loss 0.0003986764349974692 old loss 0.0003998112224508077 BETTER
I0624 01:21:42.611884 13812 finetune.py:45] layer 15_gate initial loss 0.0006276719504967332
W0624 01:21:42.612233 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:21:50.247127 12800 finetune.py:68] layer 13_gate @ epoch 2 new loss 0.0004535956250037998 old loss 0.000454979162896052 BETTER
I0624 01:22:00.173641 13306 finetune.py:68] layer 14_gate @ epoch 1 new loss 0.0005411308375187218 old loss 0.0005429048906080425 BETTER
I0624 01:22:10.202532 12296 finetune.py:68] layer 12_gate @ epoch 3 new loss 0.0003976470325142145 old loss 0.0003986764349974692 BETTER
I0624 01:22:11.962459 13812 finetune.py:68] layer 15_gate @ epoch 0 new loss 0.0006232809973880649 old loss 0.0006276719504967332 BETTER
I0624 01:22:21.218490 12800 finetune.py:68] layer 13_gate @ epoch 3 new loss 0.00045235143625177443 old loss 0.0004535956250037998 BETTER
I0624 01:22:30.516527 13306 finetune.py:68] layer 14_gate @ epoch 2 new loss 0.0005395899643190205 old loss 0.0005411308375187218 BETTER
I0624 01:22:41.709712 13812 finetune.py:68] layer 15_gate @ epoch 1 new loss 0.0006210788851603866 old loss 0.0006232809973880649 BETTER
I0624 01:22:42.634726 12296 finetune.py:68] layer 12_gate @ epoch 4 new loss 0.00039669941179454327 old loss 0.0003976470325142145 BETTER
W0624 01:22:43.705261 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_gate proxy err 0.006056600250303745 tr(WHW.T) 2187.003173828125
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:50,  1.69it/s]  2%|▏         | 2/86 [00:00<00:37,  2.23it/s]  3%|▎         | 3/86 [00:01<00:33,  2.49it/s]  5%|▍         | 4/86 [00:01<00:31,  2.64it/s]  6%|▌         | 5/86 [00:01<00:29,  2.71it/s]  7%|▋         | 6/86 [00:02<00:28,  2.76it/s]  8%|▊         | 7/86 [00:02<00:28,  2.80it/s]  9%|▉         | 8/86 [00:03<00:27,  2.82it/s] 10%|█         | 9/86 [00:03<00:27,  2.85it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.87it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.88it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.89it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.91it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.91it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.90it/s]I0624 01:22:52.089757 12800 finetune.py:68] layer 13_gate @ epoch 4 new loss 0.0004511966253630817 old loss 0.00045235143625177443 BETTER
 19%|█▊        | 16/86 [00:05<00:24,  2.91it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.90it/s] 21%|██        | 18/86 [00:06<00:23,  2.90it/s]W0624 01:22:53.163653 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 22%|██▏       | 19/86 [00:06<00:22,  2.94it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.95it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.93it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.95it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.97it/s] 28%|██▊       | 24/86 [00:08<00:20,  2.96it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.96it/s] 30%|███       | 26/86 [00:09<00:20,  2.98it/s]13_gate proxy err 0.0059446170926094055 tr(WHW.T) 2328.02001953125
  0%|          | 0/86 [00:00<?, ?it/s] 31%|███▏      | 27/86 [00:09<00:19,  3.02it/s] 33%|███▎      | 28/86 [00:09<00:19,  3.01it/s]  1%|          | 1/86 [00:00<00:50,  1.68it/s] 34%|███▎      | 29/86 [00:10<00:18,  3.03it/s]  2%|▏         | 2/86 [00:00<00:37,  2.25it/s] 35%|███▍      | 30/86 [00:10<00:18,  3.05it/s]  3%|▎         | 3/86 [00:01<00:32,  2.54it/s] 36%|███▌      | 31/86 [00:10<00:18,  3.04it/s]  5%|▍         | 4/86 [00:01<00:30,  2.68it/s] 37%|███▋      | 32/86 [00:11<00:17,  3.04it/s]  6%|▌         | 5/86 [00:01<00:29,  2.77it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.06it/s]  7%|▋         | 6/86 [00:02<00:28,  2.83it/s] 40%|███▉      | 34/86 [00:11<00:16,  3.06it/s]  8%|▊         | 7/86 [00:02<00:27,  2.83it/s] 41%|████      | 35/86 [00:12<00:16,  3.05it/s]  9%|▉         | 8/86 [00:02<00:27,  2.87it/s] 42%|████▏     | 36/86 [00:12<00:16,  3.05it/s] 10%|█         | 9/86 [00:03<00:26,  2.91it/s] 43%|████▎     | 37/86 [00:12<00:15,  3.08it/s] 12%|█▏        | 10/86 [00:03<00:25,  2.92it/s] 44%|████▍     | 38/86 [00:13<00:15,  3.09it/s] 13%|█▎        | 11/86 [00:03<00:25,  2.94it/s] 45%|████▌     | 39/86 [00:13<00:15,  3.09it/s] 14%|█▍        | 12/86 [00:04<00:24,  2.96it/s] 47%|████▋     | 40/86 [00:13<00:14,  3.10it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.96it/s] 48%|████▊     | 41/86 [00:14<00:14,  3.06it/s]I0624 01:23:00.890779 13306 finetune.py:68] layer 14_gate @ epoch 3 new loss 0.0005382344825193286 old loss 0.0005395899643190205 BETTER
 16%|█▋        | 14/86 [00:04<00:24,  2.96it/s] 49%|████▉     | 42/86 [00:14<00:14,  3.07it/s] 50%|█████     | 43/86 [00:14<00:13,  3.08it/s] 17%|█▋        | 15/86 [00:05<00:23,  2.96it/s] 51%|█████     | 44/86 [00:15<00:13,  3.08it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.96it/s] 52%|█████▏    | 45/86 [00:15<00:13,  3.05it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.94it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.07it/s] 21%|██        | 18/86 [00:06<00:23,  2.96it/s] 55%|█████▍    | 47/86 [00:15<00:12,  3.06it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.94it/s] 56%|█████▌    | 48/86 [00:16<00:12,  3.04it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.93it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.07it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.95it/s] 58%|█████▊    | 50/86 [00:16<00:11,  3.06it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.93it/s] 59%|█████▉    | 51/86 [00:17<00:11,  3.05it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.90it/s] 60%|██████    | 52/86 [00:17<00:11,  3.06it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.93it/s] 62%|██████▏   | 53/86 [00:17<00:10,  3.06it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.93it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.07it/s] 30%|███       | 26/86 [00:09<00:20,  2.94it/s] 64%|██████▍   | 55/86 [00:18<00:10,  3.08it/s] 31%|███▏      | 27/86 [00:09<00:19,  2.96it/s] 65%|██████▌   | 56/86 [00:18<00:09,  3.08it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.95it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.07it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.95it/s] 67%|██████▋   | 58/86 [00:19<00:09,  3.07it/s] 35%|███▍      | 30/86 [00:10<00:18,  2.96it/s] 69%|██████▊   | 59/86 [00:19<00:08,  3.04it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.90it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.04it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.91it/s] 71%|███████   | 61/86 [00:20<00:08,  3.06it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.92it/s] 72%|███████▏  | 62/86 [00:20<00:07,  3.07it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.92it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.07it/s] 41%|████      | 35/86 [00:12<00:17,  2.92it/s] 74%|███████▍  | 64/86 [00:21<00:07,  3.09it/s] 76%|███████▌  | 65/86 [00:21<00:06,  3.08it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.92it/s] 77%|███████▋  | 66/86 [00:22<00:06,  3.05it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.89it/s] 78%|███████▊  | 67/86 [00:22<00:06,  3.05it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.90it/s] 79%|███████▉  | 68/86 [00:22<00:05,  3.08it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.91it/s] 80%|████████  | 69/86 [00:23<00:05,  3.07it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.92it/s] 81%|████████▏ | 70/86 [00:23<00:05,  3.06it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.92it/s] 83%|████████▎ | 71/86 [00:23<00:04,  3.06it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.93it/s] 84%|████████▎ | 72/86 [00:24<00:04,  3.05it/s] 50%|█████     | 43/86 [00:14<00:14,  2.94it/s] 85%|████████▍ | 73/86 [00:24<00:04,  3.06it/s] 51%|█████     | 44/86 [00:15<00:14,  2.90it/s] 86%|████████▌ | 74/86 [00:24<00:03,  3.07it/s]I0624 01:23:11.480302 13812 finetune.py:68] layer 15_gate @ epoch 2 new loss 0.0006191771244630218 old loss 0.0006210788851603866 BETTER
 52%|█████▏    | 45/86 [00:15<00:14,  2.90it/s] 87%|████████▋ | 75/86 [00:25<00:03,  3.02it/s] 53%|█████▎    | 46/86 [00:15<00:13,  2.87it/s] 88%|████████▊ | 76/86 [00:25<00:03,  3.02it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.89it/s] 90%|████████▉ | 77/86 [00:25<00:03,  2.99it/s] 56%|█████▌    | 48/86 [00:16<00:13,  2.88it/s] 91%|█████████ | 78/86 [00:26<00:02,  3.00it/s] 57%|█████▋    | 49/86 [00:16<00:12,  2.90it/s] 92%|█████████▏| 79/86 [00:26<00:02,  3.01it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.91it/s] 93%|█████████▎| 80/86 [00:26<00:01,  3.02it/s] 59%|█████▉    | 51/86 [00:17<00:12,  2.92it/s] 94%|█████████▍| 81/86 [00:27<00:01,  3.03it/s] 60%|██████    | 52/86 [00:18<00:11,  2.89it/s] 95%|█████████▌| 82/86 [00:27<00:01,  2.99it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.86it/s] 97%|█████████▋| 83/86 [00:27<00:01,  2.96it/s] 63%|██████▎   | 54/86 [00:18<00:11,  2.85it/s] 98%|█████████▊| 84/86 [00:28<00:00,  2.97it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.87it/s] 99%|█████████▉| 85/86 [00:28<00:00,  3.00it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.87it/s]100%|██████████| 86/86 [00:28<00:00,  2.99it/s]100%|██████████| 86/86 [00:28<00:00,  2.98it/s]
 66%|██████▋   | 57/86 [00:19<00:10,  2.90it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.86it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.85it/s] 70%|██████▉   | 60/86 [00:20<00:09,  2.88it/s] 71%|███████   | 61/86 [00:21<00:08,  2.88it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.90it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.93it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.95it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.94it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.93it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.93it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.94it/s] 80%|████████  | 69/86 [00:23<00:05,  2.93it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.95it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.96it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.97it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.98it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.98it/s] 87%|████████▋ | 75/86 [00:25<00:03,  2.99it/s]W0624 01:23:21.852000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.853000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.853000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.853000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.853000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.853000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.853000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.891000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.891000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.891000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.891000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.891000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.906000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.906000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.906000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.906000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:21.906000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.069000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.069000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.069000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.069000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.069000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 76/86 [00:26<00:03,  2.99it/s]W0624 01:23:22.377000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.377000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.377000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.377000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.377000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.377000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.377000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.406000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.406000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.407000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.407000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.407000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 90%|████████▉ | 77/86 [00:26<00:03,  2.98it/s]W0624 01:23:22.479000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.480000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.480000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.480000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:22.480000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 91%|█████████ | 78/86 [00:26<00:02,  2.94it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.92it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.92it/s]W0624 01:23:23.662000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:23.668000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:23.673000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:23.674000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 81/86 [00:27<00:01,  2.91it/s]W0624 01:23:24.110000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.110000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.110000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.110000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.110000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.110000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.111000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.139000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.139000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.139000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.139000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.139000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 95%|█████████▌| 82/86 [00:28<00:01,  2.93it/s]W0624 01:23:24.489000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.489000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.490000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.490000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.490000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.490000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.490000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.490000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 83/86 [00:28<00:01,  2.91it/s]W0624 01:23:24.792000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.792000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.792000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.792000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:24.793000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 98%|█████████▊| 84/86 [00:28<00:00,  2.90it/s]W0624 01:23:25.123000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:25.129000 140329588209472 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 85/86 [00:29<00:00,  2.92it/s]100%|██████████| 86/86 [00:29<00:00,  2.91it/s]100%|██████████| 86/86 [00:29<00:00,  2.90it/s]
I0624 01:23:31.296452 13306 finetune.py:68] layer 14_gate @ epoch 4 new loss 0.0005369496066123247 old loss 0.0005382344825193286 BETTER
I0624 01:23:32.057760 12296 finetune.py:45] layer 12_down initial loss 0.0005935787921771407
W0624 01:23:32.058140 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0624 01:23:32.334000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.334000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.334000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.334000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.335000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.335000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.335000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.358168 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0624 01:23:32.376000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.377000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.377000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.377000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.377000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.393000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.393000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.393000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.393000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.393000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.572000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.572000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.572000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.572000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.572000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.906000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.906000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.906000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.906000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.906000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.906000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.906000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.940000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.940000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.940000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.940000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:32.940000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:33.011000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:33.011000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:33.011000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:33.011000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:33.012000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.222000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.228000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.234000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.234000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.678000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.678000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.678000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.678000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.679000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.679000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.679000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.712000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.712000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.712000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.712000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:34.712000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.053000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.054000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.054000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.054000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.054000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.054000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.054000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.054000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
14_gate proxy err 0.006328345742076635 tr(WHW.T) 2410.61962890625
  0%|          | 0/86 [00:00<?, ?it/s]W0624 01:23:35.347000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.347000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.347000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.347000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.347000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.686000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:23:35.691000 140537232590656 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
  1%|          | 1/86 [00:00<00:50,  1.68it/s]  2%|▏         | 2/86 [00:00<00:37,  2.26it/s]  3%|▎         | 3/86 [00:01<00:32,  2.54it/s]  5%|▍         | 4/86 [00:01<00:30,  2.67it/s]  6%|▌         | 5/86 [00:01<00:29,  2.74it/s]  7%|▋         | 6/86 [00:02<00:28,  2.77it/s]  8%|▊         | 7/86 [00:02<00:28,  2.79it/s]  9%|▉         | 8/86 [00:03<00:27,  2.81it/s] 10%|█         | 9/86 [00:03<00:27,  2.83it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.83it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.83it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.82it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.83it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.85it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.86it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.87it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.88it/s]I0624 01:23:41.557787 13812 finetune.py:68] layer 15_gate @ epoch 3 new loss 0.0006175434100441635 old loss 0.0006191771244630218 BETTER
 21%|██        | 18/86 [00:06<00:23,  2.92it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.96it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.95it/s]I0624 01:23:42.474081 12800 finetune.py:45] layer 13_down initial loss 0.000685873965267092
W0624 01:23:42.474456 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 24%|██▍       | 21/86 [00:07<00:22,  2.95it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.94it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.95it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.94it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.96it/s] 30%|███       | 26/86 [00:09<00:20,  2.97it/s] 31%|███▏      | 27/86 [00:09<00:19,  2.97it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.97it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.97it/s] 35%|███▍      | 30/86 [00:10<00:18,  2.98it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.99it/s] 37%|███▋      | 32/86 [00:11<00:18,  3.00it/s] 38%|███▊      | 33/86 [00:11<00:17,  3.00it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.99it/s] 41%|████      | 35/86 [00:12<00:17,  2.98it/s] 42%|████▏     | 36/86 [00:12<00:16,  2.97it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.97it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.96it/s] 45%|████▌     | 39/86 [00:13<00:15,  2.96it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.97it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.98it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.98it/s] 50%|█████     | 43/86 [00:14<00:14,  2.98it/s] 51%|█████     | 44/86 [00:15<00:14,  2.98it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.99it/s] 53%|█████▎    | 46/86 [00:15<00:13,  3.00it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.99it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.99it/s] 57%|█████▋    | 49/86 [00:16<00:12,  3.00it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.99it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.97it/s] 60%|██████    | 52/86 [00:17<00:11,  2.95it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.95it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.96it/s] 64%|██████▍   | 55/86 [00:18<00:10,  2.97it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.98it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.98it/s] 67%|██████▋   | 58/86 [00:19<00:09,  2.98it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.98it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.99it/s] 71%|███████   | 61/86 [00:20<00:08,  2.99it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.99it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.99it/s] 74%|███████▍  | 64/86 [00:21<00:07,  2.97it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.96it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.95it/s] 78%|███████▊  | 67/86 [00:22<00:06,  2.95it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.96it/s] 80%|████████  | 69/86 [00:23<00:05,  2.97it/s] 81%|████████▏ | 70/86 [00:23<00:05,  2.98it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.99it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.99it/s] 85%|████████▍ | 73/86 [00:24<00:04,  2.98it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.98it/s] 87%|████████▋ | 75/86 [00:25<00:03,  2.99it/s] 88%|████████▊ | 76/86 [00:25<00:03,  2.99it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.99it/s] 91%|█████████ | 78/86 [00:26<00:02,  3.00it/s] 92%|█████████▏| 79/86 [00:26<00:02,  2.97it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.98it/s] 94%|█████████▍| 81/86 [00:27<00:01,  2.97it/s]I0624 01:24:02.924415 12296 finetune.py:68] layer 12_down @ epoch 0 new loss 0.0005932885105721653 old loss 0.0005935787921771407 BETTER
 95%|█████████▌| 82/86 [00:27<00:01,  2.96it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.95it/s] 98%|█████████▊| 84/86 [00:28<00:00,  2.95it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.96it/s]100%|██████████| 86/86 [00:29<00:00,  2.97it/s]100%|██████████| 86/86 [00:29<00:00,  2.93it/s]
I0624 01:24:11.498892 13812 finetune.py:68] layer 15_gate @ epoch 4 new loss 0.0006159868207760155 old loss 0.0006175434100441635 BETTER
W0624 01:24:11.549000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.549000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.549000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.549000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.549000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.549000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.550000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.592000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.593000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.593000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.593000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.593000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.608000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.608000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.608000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.608000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.608000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.771000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.771000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.771000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.771000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:11.771000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.085000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.085000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.086000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.086000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.086000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.086000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.086000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
I0624 01:24:12.100322 12800 finetune.py:68] layer 13_down @ epoch 0 new loss 0.0006855866522528231 old loss 0.000685873965267092 BETTER
W0624 01:24:12.118000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.118000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.118000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.118000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.118000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.186000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.186000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.186000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.186000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.186000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:12.485615 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0624 01:24:13.380000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.386000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.392000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.392000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.837000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.837000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.837000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.837000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.837000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.837000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.837000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.868000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.868000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.868000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.868000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:13.868000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.219000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.220000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.220000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.220000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.220000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.220000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.220000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.220000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.523000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.523000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.523000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.523000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.523000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.856000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:14.861000 140433176610624 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
15_gate proxy err 0.0063696312718093395 tr(WHW.T) 2590.056884765625
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:50,  1.69it/s]  2%|▏         | 2/86 [00:00<00:36,  2.27it/s]  3%|▎         | 3/86 [00:01<00:32,  2.56it/s]  5%|▍         | 4/86 [00:01<00:30,  2.72it/s]  6%|▌         | 5/86 [00:01<00:29,  2.77it/s]  7%|▋         | 6/86 [00:02<00:28,  2.80it/s]  8%|▊         | 7/86 [00:02<00:28,  2.82it/s]  9%|▉         | 8/86 [00:02<00:27,  2.82it/s] 10%|█         | 9/86 [00:03<00:27,  2.82it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.82it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.82it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.82it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.81it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.80it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.79it/s] 19%|█▊        | 16/86 [00:05<00:25,  2.79it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.79it/s]I0624 01:24:21.456790 13306 finetune.py:45] layer 14_down initial loss 0.0008108944166451693
W0624 01:24:21.457201 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 21%|██        | 18/86 [00:06<00:24,  2.80it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.80it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.81it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.84it/s] 26%|██▌       | 22/86 [00:07<00:22,  2.86it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.87it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.87it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.86it/s] 30%|███       | 26/86 [00:09<00:20,  2.86it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.86it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.85it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.85it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.85it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.85it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.85it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.86it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.85it/s] 41%|████      | 35/86 [00:12<00:17,  2.86it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.86it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.86it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.85it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.84it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.85it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.84it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.84it/s] 50%|█████     | 43/86 [00:15<00:15,  2.84it/s] 51%|█████     | 44/86 [00:15<00:14,  2.85it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.85it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.86it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.86it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.86it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.86it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.86it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.86it/s] 60%|██████    | 52/86 [00:18<00:11,  2.86it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.85it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.83it/s]I0624 01:24:34.165980 12296 finetune.py:68] layer 12_down @ epoch 1 new loss 0.0005932612693868577 old loss 0.0005932885105721653 BETTER
 64%|██████▍   | 55/86 [00:19<00:10,  2.84it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.85it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.86it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.85it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.85it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.85it/s] 71%|███████   | 61/86 [00:21<00:08,  2.85it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.86it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.86it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.87it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.87it/s] 77%|███████▋  | 66/86 [00:23<00:06,  2.87it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.86it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.84it/s] 80%|████████  | 69/86 [00:24<00:05,  2.84it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.84it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.83it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.84it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.86it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.87it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.87it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.87it/s]I0624 01:24:42.132822 12800 finetune.py:68] layer 13_down @ epoch 1 new loss 0.000685549748595804 old loss 0.0006855866522528231 BETTER
 90%|████████▉ | 77/86 [00:27<00:03,  2.87it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.87it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.85it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.85it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.84it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.84it/s] 97%|█████████▋| 83/86 [00:29<00:01,  2.84it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.85it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.85it/s]100%|██████████| 86/86 [00:30<00:00,  2.86it/s]100%|██████████| 86/86 [00:30<00:00,  2.83it/s]
I0624 01:24:49.904697 13306 finetune.py:68] layer 14_down @ epoch 0 new loss 0.0008105591987259686 old loss 0.0008108944166451693 BETTER
W0624 01:24:52.033000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.034000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.034000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.034000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.034000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.034000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.034000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.077000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.077000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.077000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.077000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.077000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.094000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.094000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.094000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.094000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.094000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.257000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.257000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.257000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.257000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.257000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.582000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.582000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.583000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.583000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.583000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.583000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.583000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.616000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.616000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.616000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.616000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.616000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.690000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.690000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.690000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.690000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:52.690000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:53.906000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:53.920000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:53.928000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:53.928000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.378000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.379000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.379000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.379000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.379000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.379000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.379000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.411000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.411000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.411000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.411000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.412000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.760000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.760000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.760000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.761000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.761000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.761000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.761000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:54.761000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:24:55.059000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:24:55.059000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:24:55.059000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:24:55.060000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:55.060000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:24:55.396000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:24:55.401000 139897133320000 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 01:25:01.796010 13812 finetune.py:45] layer 15_down initial loss 0.0009473657119087875
W0624 01:25:01.796278 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:25:05.492424 12296 finetune.py:68] layer 12_down @ epoch 2 new loss 0.0005932154599577188 old loss 0.0005932612693868577 BETTER
I0624 01:25:11.753532 12800 finetune.py:68] layer 13_down @ epoch 2 new loss 0.000685520120896399 old loss 0.000685549748595804 BETTER
I0624 01:25:18.854975 13306 finetune.py:68] layer 14_down @ epoch 1 new loss 0.0008105230517685413 old loss 0.0008105591987259686 BETTER
I0624 01:25:29.648341 13812 finetune.py:68] layer 15_down @ epoch 0 new loss 0.0009469773503951728 old loss 0.0009473657119087875 BETTER
I0624 01:25:36.620770 12296 finetune.py:68] layer 12_down @ epoch 3 new loss 0.0005932016647420824 old loss 0.0005932154599577188 BETTER
I0624 01:25:41.399052 12800 finetune.py:68] layer 13_down @ epoch 3 new loss 0.0006854904349893332 old loss 0.000685520120896399 BETTER
I0624 01:25:47.704240 13306 finetune.py:68] layer 14_down @ epoch 2 new loss 0.0008104684529826045 old loss 0.0008105230517685413 BETTER
I0624 01:25:57.927028 13812 finetune.py:68] layer 15_down @ epoch 1 new loss 0.0009469295619055629 old loss 0.0009469773503951728 BETTER
I0624 01:26:07.889661 12296 finetune.py:68] layer 12_down @ epoch 4 new loss 0.0005931837949901819 old loss 0.0005932016647420824 BETTER
W0624 01:26:08.667587 12296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

12_down proxy err 0.013358264230191708 tr(WHW.T) 62.28217697143555
I0624 01:26:11.205133 12800 finetune.py:68] layer 13_down @ epoch 4 new loss 0.000685456907376647 old loss 0.0006854904349893332 BETTER
W0624 01:26:11.977069 12800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

13_down proxy err 0.013332989066839218 tr(WHW.T) 74.21698760986328
I0624 01:26:17.416256 13306 finetune.py:68] layer 14_down @ epoch 3 new loss 0.0008104435401037335 old loss 0.0008104684529826045 BETTER
I0624 01:26:26.669500 13812 finetune.py:68] layer 15_down @ epoch 2 new loss 0.000946865591686219 old loss 0.0009469295619055629 BETTER
I0624 01:26:47.252647 13306 finetune.py:68] layer 14_down @ epoch 4 new loss 0.0008104090811684728 old loss 0.0008104435401037335 BETTER
W0624 01:26:48.414287 13306 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

14_down proxy err 0.013760743662714958 tr(WHW.T) 84.1088638305664
I0624 01:26:55.666961 13812 finetune.py:68] layer 15_down @ epoch 3 new loss 0.000946844753343612 old loss 0.000946865591686219 BETTER
I0624 01:27:18.682724 4970 quantize_finetune_llama.py:193] computed original embedding for layer 16 in 62.89467787742615s
I0624 01:27:19.145353 4970 quantize_finetune_llama.py:162] layer 17 gpu 1
I0624 01:27:21.102902 14510 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:27:21.103004 14510 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:27:21.103061 14510 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:27:21.290624 14510 config.py:58] PyTorch version 2.4.0 available.
I0624 01:27:24.051769 14510 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:27:24.483266 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 01:27:24.612465 13812 finetune.py:68] layer 15_down @ epoch 4 new loss 0.0009468120988458395 old loss 0.000946844753343612 BETTER
W0624 01:27:25.293710 13812 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

15_down proxy err 0.013539508916437626 tr(WHW.T) 103.57707214355469
The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.38s/it]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]  9%|▉         | 3/32 [00:02<00:16,  1.76it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.09it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.48it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.91it/s] 41%|████      | 13/32 [00:05<00:06,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.91it/s] 50%|█████     | 16/32 [00:06<00:05,  2.89it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.94it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.96it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.95it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.93it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.97it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.96it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.95it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.99it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  2.99it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
W0624 01:27:40.745000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.745000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.745000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.745000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.745000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.745000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.745000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.770000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.770000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.770000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.771000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.771000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.786000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.786000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.786000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.786000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:27:40.787000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.098000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.098000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.098000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.098000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.099000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.981000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.981000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.981000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.981000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.981000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.981000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.981000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.998000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.998000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.998000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.998000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:27:41.998000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:27:42.233000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:27:42.233000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:27:42.233000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:27:42.233000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:27:42.233000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.366000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.366000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.366000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.366000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.366000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.366000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.366000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.384000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.384000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.384000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.384000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:27:43.384000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:27:44.289000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:27:44.289000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:27:44.289000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:27:44.289000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:27:44.289000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 01:27:50.293958 14510 finetune.py:45] layer 16_v initial loss 0.0003005620383191854
W0624 01:27:50.294397 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:28:19.335278 4970 quantize_finetune_llama.py:193] computed original embedding for layer 17 in 59.75051975250244s
I0624 01:28:19.758859 4970 quantize_finetune_llama.py:162] layer 18 gpu 2
I0624 01:28:21.914479 15014 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:28:21.914699 15014 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:28:21.914761 15014 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:28:22.174009 15014 config.py:58] PyTorch version 2.4.0 available.
I0624 01:28:24.190525 14510 finetune.py:68] layer 16_v @ epoch 0 new loss 0.00014308796380646527 old loss 0.0003005620383191854 BETTER
I0624 01:28:24.599172 15014 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:28:25.011019 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.60s/it]  6%|▋         | 2/32 [00:01<00:25,  1.18it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.02it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 28%|██▊       | 9/32 [00:04<00:07,  2.89it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.97it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.03it/s] 38%|███▊      | 12/32 [00:05<00:06,  3.07it/s] 41%|████      | 13/32 [00:05<00:06,  3.11it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.13it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.11it/s] 50%|█████     | 16/32 [00:06<00:05,  3.08it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.09it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.11it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.13it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.14it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.16it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.11it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.10it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.12it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.13it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.15it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.15it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.13it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.15it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.02it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
W0624 01:28:40.907000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.907000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.907000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.907000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.907000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.908000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.908000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.933000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.933000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.933000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.934000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.934000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.950000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.950000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.950000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.950000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:28:40.950000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:28:41.265000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:28:41.265000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:28:41.265000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:28:41.265000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:28:41.265000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.156000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.156000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.156000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.156000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.156000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.156000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.157000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.174000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.174000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.174000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.174000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.174000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.409000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.409000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.409000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.409000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:28:42.409000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.558000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.559000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.559000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.559000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.559000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.559000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.559000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.577000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.577000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.577000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.577000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:28:43.577000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:28:44.474000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:28:44.475000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:28:44.475000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:28:44.475000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:28:44.475000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 01:28:51.060897 15014 finetune.py:45] layer 17_v initial loss 0.000301254476653412
W0624 01:28:51.061332 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:28:59.126234 14510 finetune.py:68] layer 16_v @ epoch 1 new loss 0.00013119308277964592 old loss 0.00014308796380646527 BETTER
I0624 01:29:19.169976 4970 quantize_finetune_llama.py:193] computed original embedding for layer 18 in 58.686527967453s
I0624 01:29:19.537453 4970 quantize_finetune_llama.py:162] layer 19 gpu 3
I0624 01:29:21.672769 15520 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:29:21.672932 15520 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:29:21.673008 15520 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:29:21.939118 15520 config.py:58] PyTorch version 2.4.0 available.
I0624 01:29:23.289410 15014 finetune.py:68] layer 17_v @ epoch 0 new loss 0.00012848338519688696 old loss 0.000301254476653412 BETTER
I0624 01:29:24.551663 15520 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:29:24.968638 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:51,  1.66s/it]  6%|▋         | 2/32 [00:02<00:26,  1.12it/s]  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.14it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.66it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.70it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.75it/s] 50%|█████     | 16/32 [00:07<00:05,  2.77it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s]I0624 01:29:34.357224 14510 finetune.py:68] layer 16_v @ epoch 2 new loss 0.00012563655036501586 old loss 0.00013119308277964592 BETTER
 59%|█████▉    | 19/32 [00:08<00:04,  2.80it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.79it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.78it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
W0624 01:29:42.642000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.642000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.642000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.643000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.643000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.643000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.643000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.670000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.670000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.670000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.670000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.670000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.687000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.687000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.687000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.687000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:29:42.688000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.013000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.013000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.013000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.013000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.013000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.920000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.920000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.920000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.920000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.920000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.920000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.920000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.937000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.937000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.937000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.938000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:29:43.938000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:29:44.179000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:29:44.179000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:29:44.179000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:29:44.179000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:29:44.179000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.358000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.358000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.358000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.359000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.359000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.359000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.359000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.376000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.376000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.376000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.377000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:29:45.377000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:29:46.306000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:29:46.306000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:29:46.306000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:29:46.306000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:29:46.306000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 01:29:53.489977 15520 finetune.py:45] layer 18_v initial loss 0.0004410644469317049
W0624 01:29:53.490437 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:29:56.328418 15014 finetune.py:68] layer 17_v @ epoch 1 new loss 0.00011559289850993082 old loss 0.00012848338519688696 BETTER
I0624 01:30:09.802979 14510 finetune.py:68] layer 16_v @ epoch 3 new loss 0.00012201813660794869 old loss 0.00012563655036501586 BETTER
I0624 01:30:21.035327 4970 quantize_finetune_llama.py:193] computed original embedding for layer 19 in 61.026851177215576s
I0624 01:30:21.428302 4970 quantize_finetune_llama.py:162] layer 20 gpu 0
I0624 01:30:23.583873 16026 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:30:23.584014 16026 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:30:23.584084 16026 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:30:23.764419 16026 config.py:58] PyTorch version 2.4.0 available.
I0624 01:30:25.478140 15520 finetune.py:68] layer 18_v @ epoch 0 new loss 0.00015839020488783717 old loss 0.0004410644469317049 BETTER
I0624 01:30:26.296367 16026 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:30:26.757887 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]I0624 01:30:29.096948 15014 finetune.py:68] layer 17_v @ epoch 2 new loss 0.00010992893658112735 old loss 0.00011559289850993082 BETTER
  3%|▎         | 1/32 [00:01<00:55,  1.81s/it]  6%|▋         | 2/32 [00:02<00:28,  1.06it/s]  9%|▉         | 3/32 [00:02<00:19,  1.50it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.18it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.69it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.78it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.84it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.88it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.93it/s] 41%|████      | 13/32 [00:05<00:06,  2.96it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.97it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.97it/s] 50%|█████     | 16/32 [00:06<00:05,  2.99it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.01it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.01it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.01it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.01it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.01it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.02it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.01it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.01it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.02it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.01it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.00it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.00it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.01it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.02it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:12<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
W0624 01:30:43.496000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.496000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.496000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.497000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.497000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.497000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.497000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.525000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.525000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.525000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.525000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.525000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.542000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.543000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.543000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.543000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.543000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.869000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.869000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.869000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.869000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:30:43.870000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.770000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.770000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.771000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.771000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.771000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.771000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.771000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.789000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.789000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.789000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.789000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:30:44.790000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
I0624 01:30:44.902312 14510 finetune.py:68] layer 16_v @ epoch 4 new loss 0.0001194156429846771 old loss 0.00012201813660794869 BETTER
W0624 01:30:45.030000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:30:45.030000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:30:45.030000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:30:45.030000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:30:45.030000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.212000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.212000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.212000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.212000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.212000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.212000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.212000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.229000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.230000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.230000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.230000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.230000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:30:46.609248 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0624 01:30:47.152000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:30:47.152000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:30:47.153000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:30:47.153000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:30:47.153000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
16_v proxy err 0.011873142793774605 tr(WHW.T) 789.170166015625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.59it/s]  6%|▋         | 2/32 [00:00<00:14,  2.12it/s]  9%|▉         | 3/32 [00:01<00:12,  2.39it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.59it/s] 16%|█▌        | 5/32 [00:02<00:09,  2.71it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.77it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.78it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 41%|████      | 13/32 [00:04<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.88it/s]I0624 01:30:53.291865 16026 finetune.py:45] layer 19_v initial loss 0.0004562920657917857
W0624 01:30:53.292067 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 47%|████▋     | 15/32 [00:05<00:06,  2.83it/s] 50%|█████     | 16/32 [00:05<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.75it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.73it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s]I0624 01:30:58.226407 15520 finetune.py:68] layer 18_v @ epoch 1 new loss 0.00013958678755443543 old loss 0.00015839020488783717 BETTER
 91%|█████████ | 29/32 [00:10<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
I0624 01:31:02.066966 15014 finetune.py:68] layer 17_v @ epoch 3 new loss 0.00010641555854817852 old loss 0.00010992893658112735 BETTER
I0624 01:31:06.683942 14510 finetune.py:45] layer 16_q initial loss 0.000162476600962691
W0624 01:31:06.684314 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:31:24.816293 16026 finetune.py:68] layer 19_v @ epoch 0 new loss 0.00016434775898233056 old loss 0.0004562920657917857 BETTER
I0624 01:31:31.395161 15520 finetune.py:68] layer 18_v @ epoch 2 new loss 0.0001322509051533416 old loss 0.00013958678755443543 BETTER
I0624 01:31:34.971175 15014 finetune.py:68] layer 17_v @ epoch 4 new loss 0.00010384606866864488 old loss 0.00010641555854817852 BETTER
W0624 01:31:36.443289 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_v proxy err 0.011826325207948685 tr(WHW.T) 847.1815795898438
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.57it/s]  6%|▋         | 2/32 [00:00<00:13,  2.16it/s]  9%|▉         | 3/32 [00:01<00:11,  2.46it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.62it/s] 16%|█▌        | 5/32 [00:02<00:09,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.82it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.84it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.87it/s]I0624 01:31:41.006389 14510 finetune.py:68] layer 16_q @ epoch 0 new loss 0.0001564056146889925 old loss 0.000162476600962691 BETTER
 31%|███▏      | 10/32 [00:03<00:07,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.88it/s] 41%|████      | 13/32 [00:04<00:06,  2.89it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s] 50%|█████     | 16/32 [00:05<00:05,  2.92it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.91it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.91it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.92it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.89it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.89it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.90it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.92it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.91it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.91it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
I0624 01:31:55.270693 15014 finetune.py:45] layer 17_q initial loss 0.00014524205471388996
W0624 01:31:55.271041 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:31:56.890889 16026 finetune.py:68] layer 19_v @ epoch 1 new loss 0.00014261087926570326 old loss 0.00016434775898233056 BETTER
I0624 01:32:04.555631 15520 finetune.py:68] layer 18_v @ epoch 3 new loss 0.00012798105308320373 old loss 0.0001322509051533416 BETTER
I0624 01:32:16.226091 14510 finetune.py:68] layer 16_q @ epoch 1 new loss 0.00015306433488149196 old loss 0.0001564056146889925 BETTER
I0624 01:32:27.101694 15014 finetune.py:68] layer 17_q @ epoch 0 new loss 0.00013857793237548321 old loss 0.00014524205471388996 BETTER
I0624 01:32:29.437051 16026 finetune.py:68] layer 19_v @ epoch 2 new loss 0.00013465265510603786 old loss 0.00014261087926570326 BETTER
I0624 01:32:38.363763 15520 finetune.py:68] layer 18_v @ epoch 4 new loss 0.00012508318468462676 old loss 0.00012798105308320373 BETTER
W0624 01:32:39.912897 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_v proxy err 0.01259175967425108 tr(WHW.T) 1004.873046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:20,  1.55it/s]  6%|▋         | 2/32 [00:01<00:14,  2.10it/s]  9%|▉         | 3/32 [00:01<00:12,  2.37it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.52it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.62it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.70it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.74it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 41%|████      | 13/32 [00:04<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 50%|█████     | 16/32 [00:05<00:05,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.74it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s]I0624 01:32:52.155015 14510 finetune.py:68] layer 16_q @ epoch 2 new loss 0.00015048502245917916 old loss 0.00015306433488149196 BETTER
 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
I0624 01:32:59.559947 15520 finetune.py:45] layer 18_q initial loss 0.00018104810442309827
W0624 01:32:59.560320 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:33:00.293795 15014 finetune.py:68] layer 17_q @ epoch 1 new loss 0.0001351897808490321 old loss 0.00013857793237548321 BETTER
I0624 01:33:02.388197 16026 finetune.py:68] layer 19_v @ epoch 3 new loss 0.00013017233868595213 old loss 0.00013465265510603786 BETTER
I0624 01:33:27.950961 14510 finetune.py:68] layer 16_q @ epoch 3 new loss 0.00014841090887784958 old loss 0.00015048502245917916 BETTER
I0624 01:33:32.044238 15520 finetune.py:68] layer 18_q @ epoch 0 new loss 0.00017250738164875656 old loss 0.00018104810442309827 BETTER
I0624 01:33:33.804245 15014 finetune.py:68] layer 17_q @ epoch 2 new loss 0.0001326244237134233 old loss 0.0001351897808490321 BETTER
I0624 01:33:34.716067 16026 finetune.py:68] layer 19_v @ epoch 4 new loss 0.00012705007975455374 old loss 0.00013017233868595213 BETTER
W0624 01:33:36.273267 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_v proxy err 0.012561033479869366 tr(WHW.T) 1024.4833984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.61it/s]  6%|▋         | 2/32 [00:00<00:13,  2.21it/s]  9%|▉         | 3/32 [00:01<00:11,  2.51it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.66it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.76it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.83it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.88it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.91it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.91it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.92it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.94it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s] 41%|████      | 13/32 [00:04<00:06,  2.94it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 50%|█████     | 16/32 [00:05<00:05,  2.95it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.95it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.96it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.95it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.92it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.94it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.95it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.95it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.95it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]
I0624 01:33:54.997362 16026 finetune.py:45] layer 19_q initial loss 0.00017611193470656872
W0624 01:33:54.998004 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:34:03.450228 14510 finetune.py:68] layer 16_q @ epoch 4 new loss 0.0001466142275603488 old loss 0.00014841090887784958 BETTER
W0624 01:34:05.077803 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 01:34:05.571993 15520 finetune.py:68] layer 18_q @ epoch 1 new loss 0.0001682869769865647 old loss 0.00017250738164875656 BETTER
16_q proxy err 0.002821877598762512 tr(WHW.T) 6989.55029296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.70it/s]I0624 01:34:06.891651 15014 finetune.py:68] layer 17_q @ epoch 3 new loss 0.00013046945969108492 old loss 0.0001326244237134233 BETTER
  6%|▋         | 2/32 [00:00<00:13,  2.26it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.69it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.77it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.82it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.86it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.90it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.89it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.88it/s] 41%|████      | 13/32 [00:04<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.91it/s] 50%|█████     | 16/32 [00:05<00:05,  2.90it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.91it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.87it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.85it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.88it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.88it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
I0624 01:34:24.485031 14510 finetune.py:45] layer 16_k initial loss 0.00017915194621309638
W0624 01:34:24.485380 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:34:26.583469 16026 finetune.py:68] layer 19_q @ epoch 0 new loss 0.00016795433475635946 old loss 0.00017611193470656872 BETTER
I0624 01:34:38.914135 15520 finetune.py:68] layer 18_q @ epoch 2 new loss 0.00016501797654200345 old loss 0.0001682869769865647 BETTER
I0624 01:34:39.361919 15014 finetune.py:68] layer 17_q @ epoch 4 new loss 0.0001286898914258927 old loss 0.00013046945969108492 BETTER
W0624 01:34:40.800492 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_q proxy err 0.0030405884608626366 tr(WHW.T) 6869.9462890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.67it/s]  6%|▋         | 2/32 [00:00<00:13,  2.24it/s]  9%|▉         | 3/32 [00:01<00:11,  2.52it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.69it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.80it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.86it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.89it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.92it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.94it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.96it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.95it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.94it/s] 41%|████      | 13/32 [00:04<00:06,  2.91it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.91it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s] 50%|█████     | 16/32 [00:05<00:05,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.93it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.94it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.94it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.93it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.92it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.91it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.92it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.94it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]
I0624 01:34:58.638967 16026 finetune.py:68] layer 19_q @ epoch 1 new loss 0.00016420608153566718 old loss 0.00016795433475635946 BETTER
I0624 01:34:58.891271 14510 finetune.py:68] layer 16_k @ epoch 0 new loss 0.00017563406436238438 old loss 0.00017915194621309638 BETTER
I0624 01:34:59.472549 15014 finetune.py:45] layer 17_k initial loss 0.00016024033538997173
W0624 01:34:59.472831 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:35:12.179441 15520 finetune.py:68] layer 18_q @ epoch 3 new loss 0.00016248092288151383 old loss 0.00016501797654200345 BETTER
I0624 01:35:30.830398 16026 finetune.py:68] layer 19_q @ epoch 2 new loss 0.00016131915617734194 old loss 0.00016420608153566718 BETTER
I0624 01:35:31.209155 15014 finetune.py:68] layer 17_k @ epoch 0 new loss 0.00015645756502635777 old loss 0.00016024033538997173 BETTER
I0624 01:35:33.750150 14510 finetune.py:68] layer 16_k @ epoch 1 new loss 0.00017418712377548218 old loss 0.00017563406436238438 BETTER
I0624 01:35:45.790851 15520 finetune.py:68] layer 18_q @ epoch 4 new loss 0.00016028231766540557 old loss 0.00016248092288151383 BETTER
W0624 01:35:47.377471 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_q proxy err 0.003197193145751953 tr(WHW.T) 7192.1396484375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.63it/s]  6%|▋         | 2/32 [00:00<00:13,  2.16it/s]  9%|▉         | 3/32 [00:01<00:12,  2.39it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.54it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.64it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.70it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.78it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.78it/s] 41%|████      | 13/32 [00:04<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.76it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 50%|█████     | 16/32 [00:05<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.78it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.79it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.76it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.77it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.79it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
I0624 01:36:03.404515 16026 finetune.py:68] layer 19_q @ epoch 3 new loss 0.00015908856585156173 old loss 0.00016131915617734194 BETTER
I0624 01:36:04.034775 15014 finetune.py:68] layer 17_k @ epoch 1 new loss 0.00015507801435887814 old loss 0.00015645756502635777 BETTER
I0624 01:36:06.981681 15520 finetune.py:45] layer 18_k initial loss 0.00020422505622263998
W0624 01:36:06.981933 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:36:09.177658 14510 finetune.py:68] layer 16_k @ epoch 2 new loss 0.00017303395725321025 old loss 0.00017418712377548218 BETTER
I0624 01:36:36.256453 16026 finetune.py:68] layer 19_q @ epoch 4 new loss 0.00015718104259576648 old loss 0.00015908856585156173 BETTER
I0624 01:36:36.924110 15014 finetune.py:68] layer 17_k @ epoch 2 new loss 0.00015396997332572937 old loss 0.00015507801435887814 BETTER
W0624 01:36:37.673088 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_q proxy err 0.0033885554876178503 tr(WHW.T) 6614.32275390625
  0%|          | 0/32 [00:00<?, ?it/s]I0624 01:36:39.243561 15520 finetune.py:68] layer 18_k @ epoch 0 new loss 0.00019842282927129418 old loss 0.00020422505622263998 BETTER
  3%|▎         | 1/32 [00:00<00:17,  1.73it/s]  6%|▋         | 2/32 [00:00<00:13,  2.29it/s]  9%|▉         | 3/32 [00:01<00:11,  2.55it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.73it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.84it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.90it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.92it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.95it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.98it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.00it/s] 34%|███▍      | 11/32 [00:03<00:07,  3.00it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.99it/s] 41%|████      | 13/32 [00:04<00:06,  2.98it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.97it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.98it/s] 50%|█████     | 16/32 [00:05<00:05,  2.98it/s]I0624 01:36:44.290735 14510 finetune.py:68] layer 16_k @ epoch 3 new loss 0.00017206341726705432 old loss 0.00017303395725321025 BETTER
 53%|█████▎    | 17/32 [00:05<00:05,  2.98it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.00it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.00it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.01it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.00it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.00it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.00it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.01it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.00it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.96it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.97it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.99it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.01it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.00it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.00it/s]100%|██████████| 32/32 [00:10<00:00,  3.00it/s]100%|██████████| 32/32 [00:10<00:00,  2.93it/s]
I0624 01:36:56.136465 16026 finetune.py:45] layer 19_k initial loss 0.00019474109285511076
W0624 01:36:56.136742 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:37:09.075483 15014 finetune.py:68] layer 17_k @ epoch 3 new loss 0.00015305752458516508 old loss 0.00015396997332572937 BETTER
I0624 01:37:12.429158 15520 finetune.py:68] layer 18_k @ epoch 1 new loss 0.00019666331354528666 old loss 0.00019842282927129418 BETTER
I0624 01:37:19.373388 14510 finetune.py:68] layer 16_k @ epoch 4 new loss 0.00017124871374107897 old loss 0.00017206341726705432 BETTER
W0624 01:37:20.907000 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_k proxy err 0.0018704594112932682 tr(WHW.T) 11134.3994140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.69it/s]  6%|▋         | 2/32 [00:00<00:13,  2.24it/s]  9%|▉         | 3/32 [00:01<00:11,  2.50it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.83it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.87it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.90it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.92it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.93it/s] 41%|████      | 13/32 [00:04<00:06,  2.91it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.93it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s]I0624 01:37:27.639953 16026 finetune.py:68] layer 19_k @ epoch 0 new loss 0.00019060491467826068 old loss 0.00019474109285511076 BETTER
 50%|█████     | 16/32 [00:05<00:05,  2.91it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.92it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.92it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.92it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.90it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.85it/s]
I0624 01:37:40.328191 14510 finetune.py:45] layer 16_o initial loss 0.00034622897510416806
W0624 01:37:40.328543 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:37:41.563662 15014 finetune.py:68] layer 17_k @ epoch 4 new loss 0.00015223142690956593 old loss 0.00015305752458516508 BETTER
W0624 01:37:42.954936 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_k proxy err 0.002147854072973132 tr(WHW.T) 10170.638671875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.63it/s]  6%|▋         | 2/32 [00:00<00:13,  2.20it/s]I0624 01:37:45.220674 15520 finetune.py:68] layer 18_k @ epoch 2 new loss 0.0001952879101736471 old loss 0.00019666331354528666 BETTER
  9%|▉         | 3/32 [00:01<00:11,  2.49it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.64it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.84it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.87it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.88it/s] 41%|████      | 13/32 [00:04<00:06,  2.90it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 50%|█████     | 16/32 [00:05<00:05,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.91it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.92it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.94it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.94it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.86it/s]
I0624 01:37:59.901507 16026 finetune.py:68] layer 19_k @ epoch 1 new loss 0.0001889772538561374 old loss 0.00019060491467826068 BETTER
I0624 01:38:01.859613 15014 finetune.py:45] layer 17_o initial loss 0.00029690912924706936
W0624 01:38:01.860035 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:38:14.252616 14510 finetune.py:68] layer 16_o @ epoch 0 new loss 0.00033746674307622015 old loss 0.00034622897510416806 BETTER
I0624 01:38:18.064588 15520 finetune.py:68] layer 18_k @ epoch 3 new loss 0.0001942562812473625 old loss 0.0001952879101736471 BETTER
I0624 01:38:32.147611 16026 finetune.py:68] layer 19_k @ epoch 2 new loss 0.00018782285042107105 old loss 0.0001889772538561374 BETTER
I0624 01:38:33.517943 15014 finetune.py:68] layer 17_o @ epoch 0 new loss 0.00028908433159813285 old loss 0.00029690912924706936 BETTER
I0624 01:38:49.127804 14510 finetune.py:68] layer 16_o @ epoch 1 new loss 0.000334092415869236 old loss 0.00033746674307622015 BETTER
I0624 01:38:51.315353 15520 finetune.py:68] layer 18_k @ epoch 4 new loss 0.0001933221792569384 old loss 0.0001942562812473625 BETTER
W0624 01:38:52.816719 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_k proxy err 0.0024159078020602465 tr(WHW.T) 9951.529296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]  6%|▋         | 2/32 [00:00<00:13,  2.18it/s]  9%|▉         | 3/32 [00:01<00:12,  2.41it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.55it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.62it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 41%|████      | 13/32 [00:04<00:06,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.81it/s] 50%|█████     | 16/32 [00:05<00:05,  2.81it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s]I0624 01:39:04.318630 16026 finetune.py:68] layer 19_k @ epoch 3 new loss 0.0001868609106168151 old loss 0.00018782285042107105 BETTER
 91%|█████████ | 29/32 [00:10<00:01,  2.78it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.78it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
I0624 01:39:05.792937 15014 finetune.py:68] layer 17_o @ epoch 1 new loss 0.00028641277458518744 old loss 0.00028908433159813285 BETTER
I0624 01:39:12.399348 15520 finetune.py:45] layer 18_o initial loss 0.00036743050441145897
W0624 01:39:12.400015 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:39:23.873810 14510 finetune.py:68] layer 16_o @ epoch 2 new loss 0.00033141416497528553 old loss 0.000334092415869236 BETTER
I0624 01:39:36.313529 16026 finetune.py:68] layer 19_k @ epoch 4 new loss 0.00018603209173306823 old loss 0.0001868609106168151 BETTER
W0624 01:39:37.702189 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 01:39:38.052715 15014 finetune.py:68] layer 17_o @ epoch 2 new loss 0.00028437230503186584 old loss 0.00028641277458518744 BETTER
19_k proxy err 0.0023240195587277412 tr(WHW.T) 10036.712890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.68it/s]  6%|▋         | 2/32 [00:00<00:13,  2.26it/s]  9%|▉         | 3/32 [00:01<00:11,  2.54it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.69it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.79it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.86it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.91it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.94it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.96it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.98it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.00it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.02it/s] 41%|████      | 13/32 [00:04<00:06,  3.02it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.01it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.02it/s] 50%|█████     | 16/32 [00:05<00:05,  3.02it/s]I0624 01:39:44.406848 15520 finetune.py:68] layer 18_o @ epoch 0 new loss 0.00035599785041995347 old loss 0.00036743050441145897 BETTER
 53%|█████▎    | 17/32 [00:05<00:04,  3.03it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.01it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.99it/s] 62%|██████▎   | 20/32 [00:06<00:04,  3.00it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.00it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.01it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.00it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.00it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.02it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.02it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.02it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.02it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.02it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.00it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.01it/s]100%|██████████| 32/32 [00:10<00:00,  2.99it/s]100%|██████████| 32/32 [00:10<00:00,  2.94it/s]
I0624 01:39:56.069199 16026 finetune.py:45] layer 19_o initial loss 0.00036291347350925207
W0624 01:39:56.069621 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:39:58.339353 14510 finetune.py:68] layer 16_o @ epoch 3 new loss 0.00032924817060120404 old loss 0.00033141416497528553 BETTER
I0624 01:40:10.181483 15014 finetune.py:68] layer 17_o @ epoch 3 new loss 0.0002826963900588453 old loss 0.00028437230503186584 BETTER
I0624 01:40:17.331907 15520 finetune.py:68] layer 18_o @ epoch 1 new loss 0.00035291939275339246 old loss 0.00035599785041995347 BETTER
I0624 01:40:27.638422 16026 finetune.py:68] layer 19_o @ epoch 0 new loss 0.00035227424814365804 old loss 0.00036291347350925207 BETTER
I0624 01:40:32.812131 14510 finetune.py:68] layer 16_o @ epoch 4 new loss 0.00032735662534832954 old loss 0.00032924817060120404 BETTER
W0624 01:40:34.250704 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_o proxy err 0.010221351869404316 tr(WHW.T) 68.50062561035156
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.29s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.07s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.04s/it]I0624 01:40:42.529060 15014 finetune.py:68] layer 17_o @ epoch 4 new loss 0.0002812420134432614 old loss 0.0002826963900588453 BETTER
 22%|██▏       | 7/32 [00:07<00:25,  1.04s/it]W0624 01:40:43.835772 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:08<00:25,  1.04s/it]17_o proxy err 0.011516687460243702 tr(WHW.T) 47.30134963989258
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.04s/it]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.04s/it]  6%|▋         | 2/32 [00:02<00:33,  1.12s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.05s/it]  9%|▉         | 3/32 [00:03<00:31,  1.08s/it] 41%|████      | 13/32 [00:13<00:20,  1.05s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it]I0624 01:40:50.027072 15520 finetune.py:68] layer 18_o @ epoch 2 new loss 0.0003506774955894798 old loss 0.00035291939275339246 BETTER
 44%|████▍     | 14/32 [00:14<00:18,  1.05s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.04s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.04s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.04s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.02s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.03s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.03s/it] 41%|████      | 13/32 [00:13<00:19,  1.04s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.02s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.04s/it]I0624 01:40:59.724002 16026 finetune.py:68] layer 19_o @ epoch 1 new loss 0.00034963409416377544 old loss 0.00035227424814365804 BETTER
 75%|███████▌  | 24/32 [00:25<00:08,  1.02s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 88%|████████▊ | 28/32 [00:29<00:04,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.02s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.02s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it]100%|██████████| 32/32 [00:33<00:00,  1.02s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.01s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.01s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.01s/it]W0624 01:41:15.463000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.463000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.463000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.463000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.463000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.464000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.464000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.492000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.492000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.492000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.492000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.492000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.508000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.508000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.508000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.508000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.508000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.668000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.668000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.668000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.668000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.669000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
 94%|█████████▍| 30/32 [00:30<00:02,  1.01s/it]W0624 01:41:15.894000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.895000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.895000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.895000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.895000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.895000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.895000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.916000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.916000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.916000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.916000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.916000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.978000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.978000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.978000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.978000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:15.979000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:16.844000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:31<00:01,  1.01s/it]W0624 01:41:17.157000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.157000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.157000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.157000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.157000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.157000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.157000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.177000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.177000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.177000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.177000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.177000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.433000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.433000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.433000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.433000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.433000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:17.694000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 32/32 [00:32<00:00,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.03s/it]
I0624 01:41:22.975658 15520 finetune.py:68] layer 18_o @ epoch 3 new loss 0.0003487950307317078 old loss 0.0003506774955894798 BETTER
I0624 01:41:24.454650 14510 finetune.py:45] layer 16_up initial loss 0.0005828035064041615
W0624 01:41:24.455007 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0624 01:41:24.613000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.614000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.614000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.614000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.614000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.614000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.614000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.643000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.643000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.643000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.643000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.643000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.659000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.659000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.659000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.659000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.659000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.816000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.816000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.816000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.817000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:24.817000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.053000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.053000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.053000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.053000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.053000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.054000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.054000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.074000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.074000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.075000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.075000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.075000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.137000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.137000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.137000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.137000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:25.138000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.025000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.343000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.343000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.343000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.343000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.343000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.344000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.344000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.363000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.363000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.363000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.363000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.364000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.625000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.625000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.625000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.625000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.626000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:41:26.882000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 01:41:31.822625 16026 finetune.py:68] layer 19_o @ epoch 2 new loss 0.00034757429966703057 old loss 0.00034963409416377544 BETTER
I0624 01:41:33.162393 15014 finetune.py:45] layer 17_up initial loss 0.0005807802663184702
W0624 01:41:33.162744 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:41:56.373786 15520 finetune.py:68] layer 18_o @ epoch 4 new loss 0.00034723462886177003 old loss 0.0003487950307317078 BETTER
I0624 01:41:57.883234 14510 finetune.py:68] layer 16_up @ epoch 0 new loss 0.0005760572967119515 old loss 0.0005828035064041615 BETTER
W0624 01:41:57.903724 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_o proxy err 0.011087953113019466 tr(WHW.T) 56.93361282348633
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:40,  1.32s/it]  6%|▋         | 2/32 [00:02<00:34,  1.16s/it]  9%|▉         | 3/32 [00:03<00:32,  1.12s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.10s/it]I0624 01:42:04.152041 16026 finetune.py:68] layer 19_o @ epoch 3 new loss 0.0003460155858192593 old loss 0.00034757429966703057 BETTER
I0624 01:42:04.161692 15014 finetune.py:68] layer 17_up @ epoch 0 new loss 0.0005744252121075988 old loss 0.0005807802663184702 BETTER
 16%|█▌        | 5/32 [00:05<00:29,  1.09s/it] 19%|█▉        | 6/32 [00:06<00:28,  1.09s/it] 22%|██▏       | 7/32 [00:07<00:27,  1.08s/it] 25%|██▌       | 8/32 [00:08<00:26,  1.08s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.08s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.08s/it] 34%|███▍      | 11/32 [00:12<00:22,  1.07s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.08s/it] 41%|████      | 13/32 [00:14<00:20,  1.07s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.07s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.07s/it] 50%|█████     | 16/32 [00:17<00:17,  1.07s/it] 53%|█████▎    | 17/32 [00:18<00:16,  1.07s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.07s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.07s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.07s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.07s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.07s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.07s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.07s/it] 78%|███████▊  | 25/32 [00:27<00:07,  1.07s/it] 81%|████████▏ | 26/32 [00:28<00:06,  1.08s/it] 84%|████████▍ | 27/32 [00:29<00:05,  1.08s/it] 88%|████████▊ | 28/32 [00:30<00:04,  1.08s/it] 91%|█████████ | 29/32 [00:31<00:03,  1.09s/it] 94%|█████████▍| 30/32 [00:32<00:02,  1.09s/it]I0624 01:42:31.843309 14510 finetune.py:68] layer 16_up @ epoch 1 new loss 0.0005721785710193217 old loss 0.0005760572967119515 BETTER
 97%|█████████▋| 31/32 [00:33<00:01,  1.08s/it]100%|██████████| 32/32 [00:34<00:00,  1.07s/it]100%|██████████| 32/32 [00:34<00:00,  1.08s/it]
I0624 01:42:35.586289 15014 finetune.py:68] layer 17_up @ epoch 1 new loss 0.0005705805961042643 old loss 0.0005744252121075988 BETTER
I0624 01:42:36.285731 16026 finetune.py:68] layer 19_o @ epoch 4 new loss 0.0003447407216299325 old loss 0.0003460155858192593 BETTER
W0624 01:42:37.599991 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_o proxy err 0.011823206208646297 tr(WHW.T) 53.219852447509766
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]W0624 01:42:40.556000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.556000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.557000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.557000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.557000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.557000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.557000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.586000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.587000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.587000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.587000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.587000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.602000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.602000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.602000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.602000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.602000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.757000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.757000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.757000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.757000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.757000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.991000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.991000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.991000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.991000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.992000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.992000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:42:40.992000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:42:41.014000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:42:41.014000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:42:41.014000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:42:41.014000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:42:41.014000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:42:41.080000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:42:41.080000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:42:41.080000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:42:41.080000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:42:41.080000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:02<00:33,  1.11s/it]W0624 01:42:41.951000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  9%|▉         | 3/32 [00:03<00:30,  1.06s/it]W0624 01:42:42.257000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.257000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.257000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.258000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.258000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.258000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.258000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.279000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.279000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.279000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.279000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.280000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.531000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.531000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.531000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.532000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.532000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:42:42.800000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.01s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.01s/it]I0624 01:42:49.334706 15520 finetune.py:45] layer 18_up initial loss 0.0007108178688213229
W0624 01:42:49.335264 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:11<00:21,  1.00s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.00s/it] 41%|████      | 13/32 [00:13<00:19,  1.00s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.01s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.00s/it] 50%|█████     | 16/32 [00:16<00:16,  1.00s/it] 53%|█████▎    | 17/32 [00:17<00:14,  1.00it/s] 56%|█████▋    | 18/32 [00:18<00:14,  1.00s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.00s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.00s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.00s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.00s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it]I0624 01:43:05.732535 14510 finetune.py:68] layer 16_up @ epoch 2 new loss 0.0005689543322660029 old loss 0.0005721785710193217 BETTER
 84%|████████▍ | 27/32 [00:27<00:05,  1.03s/it]I0624 01:43:06.965772 15014 finetune.py:68] layer 17_up @ epoch 2 new loss 0.0005674410494975746 old loss 0.0005705805961042643 BETTER
 88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.02s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.02s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
W0624 01:43:18.273000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.274000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.274000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.274000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.274000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.274000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.274000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.306000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.306000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.306000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.306000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.306000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.322000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.322000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.323000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.323000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.323000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.488000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.488000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.488000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.488000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.488000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.715000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.715000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.715000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.715000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.715000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.716000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.716000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.736000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.736000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.736000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.736000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.737000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.804000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.804000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.804000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.804000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:43:18.804000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:43:19.685000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:43:19.999000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:43:19.999000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:43:19.999000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.000000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.000000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.000000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.000000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.022000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.022000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.023000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.023000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.023000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.283000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.283000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.284000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.284000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.284000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:43:20.556000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 01:43:20.835464 15520 finetune.py:68] layer 18_up @ epoch 0 new loss 0.0007032744470052421 old loss 0.0007108178688213229 BETTER
I0624 01:43:26.742800 16026 finetune.py:45] layer 19_up initial loss 0.0007633718196302652
W0624 01:43:26.743191 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:43:38.344577 15014 finetune.py:68] layer 17_up @ epoch 3 new loss 0.0005646899808198214 old loss 0.0005674410494975746 BETTER
I0624 01:43:39.682701 14510 finetune.py:68] layer 16_up @ epoch 3 new loss 0.0005661133909597993 old loss 0.0005689543322660029 BETTER
I0624 01:43:53.002647 15520 finetune.py:68] layer 18_up @ epoch 1 new loss 0.0006989582325331867 old loss 0.0007032744470052421 BETTER
I0624 01:43:56.620581 16026 finetune.py:68] layer 19_up @ epoch 0 new loss 0.0007554785115644336 old loss 0.0007633718196302652 BETTER
I0624 01:44:09.675768 15014 finetune.py:68] layer 17_up @ epoch 4 new loss 0.0005623517208732665 old loss 0.0005646899808198214 BETTER
W0624 01:44:10.926329 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_up proxy err 0.010993240401148796 tr(WHW.T) 1851.90087890625
  0%|          | 0/32 [00:00<?, ?it/s]I0624 01:44:13.170843 14510 finetune.py:68] layer 16_up @ epoch 4 new loss 0.0005636192508973181 old loss 0.0005661133909597993 BETTER
  3%|▎         | 1/32 [00:01<00:40,  1.30s/it]W0624 01:44:14.523922 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  6%|▋         | 2/32 [00:02<00:33,  1.13s/it]  9%|▉         | 3/32 [00:03<00:31,  1.07s/it]16_up proxy err 0.009867151267826557 tr(WHW.T) 1792.8076171875
  0%|          | 0/32 [00:00<?, ?it/s] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it]  9%|▉         | 3/32 [00:03<00:31,  1.09s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.07s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.02s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.06s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.06s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.05s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.05s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.04s/it]I0624 01:44:25.048120 15520 finetune.py:68] layer 18_up @ epoch 2 new loss 0.0006954250275157392 old loss 0.0006989582325331867 BETTER
 28%|██▊       | 9/32 [00:09<00:23,  1.04s/it] 41%|████      | 13/32 [00:13<00:19,  1.04s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.04s/it]I0624 01:44:27.233154 16026 finetune.py:68] layer 19_up @ epoch 1 new loss 0.0007511000148952007 old loss 0.0007554785115644336 BETTER
 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 50%|█████     | 16/32 [00:16<00:16,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.03s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.02s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.03s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.02s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.02s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.03s/it] 91%|█████████ | 29/32 [00:29<00:03,  1.02s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.03s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.02s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.03s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.01s/it] 84%|████████▍ | 27/32 [00:28<00:05,  1.03s/it]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.03s/it]
 88%|████████▊ | 28/32 [00:29<00:04,  1.03s/it] 91%|█████████ | 29/32 [00:30<00:03,  1.02s/it] 94%|█████████▍| 30/32 [00:31<00:02,  1.02s/it] 97%|█████████▋| 31/32 [00:32<00:01,  1.02s/it]100%|██████████| 32/32 [00:33<00:00,  1.02s/it]100%|██████████| 32/32 [00:33<00:00,  1.04s/it]
I0624 01:44:52.547048 15014 finetune.py:45] layer 17_gate initial loss 0.0008257845183834434
W0624 01:44:52.547507 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:44:56.964446 14510 finetune.py:45] layer 16_gate initial loss 0.0007831469993107021
W0624 01:44:56.964754 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:44:57.451770 15520 finetune.py:68] layer 18_up @ epoch 3 new loss 0.0006923926412127912 old loss 0.0006954250275157392 BETTER
I0624 01:44:58.263088 16026 finetune.py:68] layer 19_up @ epoch 2 new loss 0.0007475452730432153 old loss 0.0007511000148952007 BETTER
I0624 01:45:22.618717 15014 finetune.py:68] layer 17_gate @ epoch 0 new loss 0.0008199927979148924 old loss 0.0008257845183834434 BETTER
I0624 01:45:28.932466 16026 finetune.py:68] layer 19_up @ epoch 3 new loss 0.0007445326191373169 old loss 0.0007475452730432153 BETTER
I0624 01:45:29.249773 14510 finetune.py:68] layer 16_gate @ epoch 0 new loss 0.0007774017285555601 old loss 0.0007831469993107021 BETTER
I0624 01:45:29.714143 15520 finetune.py:68] layer 18_up @ epoch 4 new loss 0.0006897124112583697 old loss 0.0006923926412127912 BETTER
W0624 01:45:31.061119 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_up proxy err 0.011675511486828327 tr(WHW.T) 1960.49560546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.38s/it]  6%|▋         | 2/32 [00:02<00:35,  1.19s/it]  9%|▉         | 3/32 [00:03<00:32,  1.13s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.10s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.09s/it] 19%|█▉        | 6/32 [00:06<00:28,  1.08s/it] 22%|██▏       | 7/32 [00:07<00:27,  1.08s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.08s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.08s/it] 31%|███▏      | 10/32 [00:11<00:23,  1.08s/it] 34%|███▍      | 11/32 [00:12<00:22,  1.08s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.07s/it] 41%|████      | 13/32 [00:14<00:20,  1.07s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.07s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.07s/it] 50%|█████     | 16/32 [00:17<00:16,  1.06s/it] 53%|█████▎    | 17/32 [00:18<00:15,  1.06s/it] 56%|█████▋    | 18/32 [00:19<00:14,  1.06s/it] 59%|█████▉    | 19/32 [00:20<00:13,  1.06s/it]I0624 01:45:53.058913 15014 finetune.py:68] layer 17_gate @ epoch 1 new loss 0.0008173234527930617 old loss 0.0008199927979148924 BETTER
 62%|██████▎   | 20/32 [00:21<00:12,  1.06s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.06s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.07s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.07s/it] 75%|███████▌  | 24/32 [00:25<00:08,  1.07s/it] 78%|███████▊  | 25/32 [00:27<00:07,  1.08s/it]I0624 01:45:59.698126 16026 finetune.py:68] layer 19_up @ epoch 4 new loss 0.0007419344037771225 old loss 0.0007445326191373169 BETTER
 81%|████████▏ | 26/32 [00:28<00:06,  1.09s/it]W0624 01:46:00.927287 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:29<00:05,  1.09s/it]I0624 01:46:01.912258 14510 finetune.py:68] layer 16_gate @ epoch 1 new loss 0.0007748021744191647 old loss 0.0007774017285555601 BETTER
19_up proxy err 0.011902754195034504 tr(WHW.T) 2085.54736328125
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:30<00:04,  1.08s/it]  3%|▎         | 1/32 [00:01<00:40,  1.30s/it] 91%|█████████ | 29/32 [00:31<00:03,  1.08s/it]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it] 94%|█████████▍| 30/32 [00:32<00:02,  1.08s/it]  9%|▉         | 3/32 [00:03<00:31,  1.07s/it] 97%|█████████▋| 31/32 [00:33<00:01,  1.07s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it]100%|██████████| 32/32 [00:34<00:00,  1.07s/it]100%|██████████| 32/32 [00:34<00:00,  1.08s/it]
 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.02s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.01s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.00s/it]I0624 01:46:14.657942 15520 finetune.py:45] layer 18_gate initial loss 0.0010119073558598757
W0624 01:46:14.658387 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 41%|████      | 13/32 [00:13<00:19,  1.01s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.01s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.01s/it] 50%|█████     | 16/32 [00:16<00:16,  1.01s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it]I0624 01:46:23.534507 15014 finetune.py:68] layer 17_gate @ epoch 2 new loss 0.0008152027730830014 old loss 0.0008173234527930617 BETTER
 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.00s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.00s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.00s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.00s/it] 88%|████████▊ | 28/32 [00:28<00:03,  1.00it/s] 91%|█████████ | 29/32 [00:29<00:02,  1.00it/s] 94%|█████████▍| 30/32 [00:30<00:02,  1.00s/it] 97%|█████████▋| 31/32 [00:31<00:00,  1.00it/s]100%|██████████| 32/32 [00:32<00:00,  1.01it/s]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
I0624 01:46:34.807472 14510 finetune.py:68] layer 16_gate @ epoch 2 new loss 0.0007725752657279372 old loss 0.0007748021744191647 BETTER
I0624 01:46:41.954092 16026 finetune.py:45] layer 19_gate initial loss 0.0011280106846243143
W0624 01:46:41.954463 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:46:45.204729 15520 finetune.py:68] layer 18_gate @ epoch 0 new loss 0.0010048525873571634 old loss 0.0010119073558598757 BETTER
I0624 01:46:53.964146 15014 finetune.py:68] layer 17_gate @ epoch 3 new loss 0.0008132477523759007 old loss 0.0008152027730830014 BETTER
I0624 01:47:07.481526 14510 finetune.py:68] layer 16_gate @ epoch 3 new loss 0.0007705896277911961 old loss 0.0007725752657279372 BETTER
I0624 01:47:11.345867 16026 finetune.py:68] layer 19_gate @ epoch 0 new loss 0.0011208676733076572 old loss 0.0011280106846243143 BETTER
I0624 01:47:16.094257 15520 finetune.py:68] layer 18_gate @ epoch 1 new loss 0.0010020129848271608 old loss 0.0010048525873571634 BETTER
I0624 01:47:24.391422 15014 finetune.py:68] layer 17_gate @ epoch 4 new loss 0.0008115300443023443 old loss 0.0008132477523759007 BETTER
W0624 01:47:25.385057 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_gate proxy err 0.007114229258149862 tr(WHW.T) 3136.1416015625
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:52,  1.62it/s]  2%|▏         | 2/86 [00:00<00:38,  2.18it/s]  3%|▎         | 3/86 [00:01<00:33,  2.46it/s]  5%|▍         | 4/86 [00:01<00:31,  2.61it/s]  6%|▌         | 5/86 [00:02<00:29,  2.70it/s]  7%|▋         | 6/86 [00:02<00:28,  2.78it/s]  8%|▊         | 7/86 [00:02<00:27,  2.82it/s]  9%|▉         | 8/86 [00:03<00:27,  2.85it/s] 10%|█         | 9/86 [00:03<00:26,  2.87it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.88it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.88it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.89it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.89it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.89it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.84it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.86it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.87it/s] 21%|██        | 18/86 [00:06<00:23,  2.87it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.89it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.91it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.92it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.93it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.93it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.92it/s] 29%|██▉       | 25/86 [00:08<00:21,  2.90it/s] 30%|███       | 26/86 [00:09<00:20,  2.90it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.88it/s] 33%|███▎      | 28/86 [00:09<00:20,  2.88it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.85it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.86it/s] 36%|███▌      | 31/86 [00:10<00:19,  2.86it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.87it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.87it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.88it/s]I0624 01:47:40.228498 14510 finetune.py:68] layer 16_gate @ epoch 4 new loss 0.0007688338519074023 old loss 0.0007705896277911961 BETTER
 41%|████      | 35/86 [00:12<00:17,  2.88it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.89it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.90it/s]W0624 01:47:41.364674 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 01:47:41.380500 16026 finetune.py:68] layer 19_gate @ epoch 1 new loss 0.0011178737040609121 old loss 0.0011208676733076572 BETTER
 44%|████▍     | 38/86 [00:13<00:16,  2.92it/s] 45%|████▌     | 39/86 [00:13<00:15,  2.95it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.93it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.93it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.90it/s] 50%|█████     | 43/86 [00:15<00:14,  2.92it/s] 51%|█████     | 44/86 [00:15<00:14,  2.93it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.94it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.95it/s]16_gate proxy err 0.006441537290811539 tr(WHW.T) 2931.95458984375
  0%|          | 0/86 [00:00<?, ?it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.95it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.95it/s]  1%|          | 1/86 [00:00<00:51,  1.64it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.96it/s]  2%|▏         | 2/86 [00:00<00:38,  2.19it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.96it/s]  3%|▎         | 3/86 [00:01<00:33,  2.45it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.96it/s]  5%|▍         | 4/86 [00:01<00:31,  2.57it/s] 60%|██████    | 52/86 [00:18<00:11,  2.95it/s]  6%|▌         | 5/86 [00:02<00:30,  2.66it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.93it/s]  7%|▋         | 6/86 [00:02<00:29,  2.74it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.93it/s]  8%|▊         | 7/86 [00:02<00:28,  2.78it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.92it/s]I0624 01:47:47.407880 15520 finetune.py:68] layer 18_gate @ epoch 2 new loss 0.0009997055167332292 old loss 0.0010020129848271608 BETTER
  9%|▉         | 8/86 [00:03<00:27,  2.84it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.90it/s] 10%|█         | 9/86 [00:03<00:26,  2.87it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.91it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.88it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.92it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.88it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.93it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.89it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.93it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.91it/s] 71%|███████   | 61/86 [00:21<00:08,  2.94it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.93it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.93it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.94it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.92it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.93it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.93it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.92it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.94it/s] 21%|██        | 18/86 [00:06<00:23,  2.90it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.94it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.92it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.94it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.92it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.94it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.92it/s] 80%|████████  | 69/86 [00:23<00:05,  2.92it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.92it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.93it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.91it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.93it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.91it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.93it/s] 29%|██▉       | 25/86 [00:08<00:21,  2.89it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.94it/s] 30%|███       | 26/86 [00:09<00:20,  2.87it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.95it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.87it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.95it/s] 33%|███▎      | 28/86 [00:09<00:20,  2.89it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.93it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.88it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.94it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.89it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.94it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.90it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.93it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.88it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.93it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.93it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.89it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.91it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.91it/s] 41%|████      | 35/86 [00:12<00:17,  2.90it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.92it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.94it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.91it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.95it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.92it/s]100%|██████████| 86/86 [00:29<00:00,  2.93it/s]100%|██████████| 86/86 [00:29<00:00,  2.89it/s]
 44%|████▍     | 38/86 [00:13<00:16,  2.93it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.94it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.94it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.93it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.91it/s] 50%|█████     | 43/86 [00:15<00:14,  2.91it/s] 51%|█████     | 44/86 [00:15<00:14,  2.93it/s] 52%|█████▏    | 45/86 [00:15<00:14,  2.90it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.93it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.93it/s] 56%|█████▌    | 48/86 [00:16<00:13,  2.91it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.92it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.93it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.93it/s] 60%|██████    | 52/86 [00:18<00:11,  2.93it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.93it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.91it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.91it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.91it/s] 66%|██████▋   | 57/86 [00:19<00:10,  2.90it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.91it/s]W0624 01:48:04.788000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.788000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.789000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.789000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.789000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.789000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.789000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.831000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.831000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.831000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.831000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.831000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.846000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.846000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.846000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.846000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:04.847000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.021000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.021000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.021000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.021000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.022000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 69%|██████▊   | 59/86 [00:20<00:09,  2.91it/s]W0624 01:48:05.357000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.357000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.357000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.357000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.357000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.358000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.358000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.392000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.392000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.393000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.393000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.393000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 60/86 [00:20<00:08,  2.90it/s]W0624 01:48:05.466000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.466000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.467000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.467000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:05.467000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 61/86 [00:21<00:08,  2.90it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.91it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.91it/s]W0624 01:48:06.690000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:06.701000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:06.709000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:06.709000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 74%|███████▍  | 64/86 [00:22<00:07,  2.93it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.94it/s]W0624 01:48:07.166000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.167000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.167000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.167000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.167000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.167000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.167000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.198000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.198000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.198000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.198000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.198000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 66/86 [00:22<00:06,  2.94it/s]W0624 01:48:07.556000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.556000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.556000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.556000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.556000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.556000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.557000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.557000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 67/86 [00:23<00:06,  2.95it/s]W0624 01:48:07.863000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.863000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.863000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.863000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:07.863000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 68/86 [00:23<00:06,  2.97it/s]W0624 01:48:08.214000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:08.220000 139692380845888 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 69/86 [00:23<00:05,  2.95it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.94it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.95it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.95it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.91it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.93it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.91it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.90it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.94it/s]I0624 01:48:11.524652 16026 finetune.py:68] layer 19_gate @ epoch 2 new loss 0.0011154534295201302 old loss 0.0011178737040609121 BETTER
 91%|█████████ | 78/86 [00:27<00:02,  2.94it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.94it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.96it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.96it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.95it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.97it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.96it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.93it/s]100%|██████████| 86/86 [00:29<00:00,  2.92it/s]100%|██████████| 86/86 [00:29<00:00,  2.89it/s]
I0624 01:48:15.000456 15014 finetune.py:45] layer 17_down initial loss 0.0013133935863152146
W0624 01:48:15.000879 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:48:18.782268 15520 finetune.py:68] layer 18_gate @ epoch 3 new loss 0.0009976441506296396 old loss 0.0009997055167332292 BETTER
W0624 01:48:21.249000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.250000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.250000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.250000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.250000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.250000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.250000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.292000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.292000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.292000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.292000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.292000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.308000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.308000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.308000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.309000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.309000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.482000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.482000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.482000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.482000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.482000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.815000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.815000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.816000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.816000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.816000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.816000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.816000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.850000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.850000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.850000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.850000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.850000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.923000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.923000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.924000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.924000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:21.924000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.133000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.139000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.145000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.146000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.597000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.597000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.597000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.597000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.597000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.598000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.598000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.627000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.627000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.627000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.627000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.627000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.992000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.992000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.992000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.993000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.993000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.993000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.993000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:23.993000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:24.288000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:48:24.288000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:24.289000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:48:24.289000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:48:24.289000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:48:24.617000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:48:24.622000 140312612116288 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 01:48:31.649459 14510 finetune.py:45] layer 16_down initial loss 0.0012046697083860636
W0624 01:48:31.649898 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:48:41.496509 16026 finetune.py:68] layer 19_gate @ epoch 3 new loss 0.0011133552761748433 old loss 0.0011154534295201302 BETTER
I0624 01:48:43.562263 15014 finetune.py:68] layer 17_down @ epoch 0 new loss 0.0013128876453265548 old loss 0.0013133935863152146 BETTER
I0624 01:48:50.010963 15520 finetune.py:68] layer 18_gate @ epoch 4 new loss 0.0009958401788026094 old loss 0.0009976441506296396 BETTER
W0624 01:48:51.149485 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_gate proxy err 0.007687258999794722 tr(WHW.T) 3340.7822265625
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:51,  1.64it/s]  2%|▏         | 2/86 [00:00<00:38,  2.19it/s]  3%|▎         | 3/86 [00:01<00:34,  2.44it/s]  5%|▍         | 4/86 [00:01<00:32,  2.55it/s]  6%|▌         | 5/86 [00:02<00:30,  2.63it/s]  7%|▋         | 6/86 [00:02<00:29,  2.67it/s]  8%|▊         | 7/86 [00:02<00:29,  2.69it/s]  9%|▉         | 8/86 [00:03<00:28,  2.71it/s] 10%|█         | 9/86 [00:03<00:28,  2.72it/s] 12%|█▏        | 10/86 [00:03<00:27,  2.72it/s] 13%|█▎        | 11/86 [00:04<00:27,  2.73it/s] 14%|█▍        | 12/86 [00:04<00:27,  2.74it/s] 15%|█▌        | 13/86 [00:04<00:26,  2.75it/s] 16%|█▋        | 14/86 [00:05<00:26,  2.75it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.76it/s] 19%|█▊        | 16/86 [00:06<00:25,  2.77it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.76it/s] 21%|██        | 18/86 [00:06<00:24,  2.76it/s] 22%|██▏       | 19/86 [00:07<00:24,  2.76it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.75it/s] 24%|██▍       | 21/86 [00:07<00:23,  2.76it/s]I0624 01:49:02.064670 14510 finetune.py:68] layer 16_down @ epoch 0 new loss 0.0012042757589370012 old loss 0.0012046697083860636 BETTER
 26%|██▌       | 22/86 [00:08<00:23,  2.76it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.79it/s] 28%|██▊       | 24/86 [00:08<00:22,  2.79it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.78it/s] 30%|███       | 26/86 [00:09<00:21,  2.79it/s] 31%|███▏      | 27/86 [00:09<00:21,  2.78it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.79it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.80it/s] 35%|███▍      | 30/86 [00:11<00:19,  2.81it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.80it/s] 37%|███▋      | 32/86 [00:11<00:19,  2.79it/s] 38%|███▊      | 33/86 [00:12<00:18,  2.79it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.78it/s] 41%|████      | 35/86 [00:12<00:18,  2.75it/s] 42%|████▏     | 36/86 [00:13<00:18,  2.78it/s] 43%|████▎     | 37/86 [00:13<00:17,  2.78it/s] 44%|████▍     | 38/86 [00:13<00:17,  2.79it/s] 45%|████▌     | 39/86 [00:14<00:16,  2.79it/s] 47%|████▋     | 40/86 [00:14<00:16,  2.78it/s] 48%|████▊     | 41/86 [00:15<00:16,  2.79it/s] 49%|████▉     | 42/86 [00:15<00:15,  2.79it/s] 50%|█████     | 43/86 [00:15<00:15,  2.80it/s] 51%|█████     | 44/86 [00:16<00:15,  2.79it/s] 52%|█████▏    | 45/86 [00:16<00:14,  2.78it/s] 53%|█████▎    | 46/86 [00:16<00:14,  2.78it/s] 55%|█████▍    | 47/86 [00:17<00:14,  2.76it/s]I0624 01:49:11.428577 16026 finetune.py:68] layer 19_gate @ epoch 4 new loss 0.0011115235975012183 old loss 0.0011133552761748433 BETTER
 56%|█████▌    | 48/86 [00:17<00:13,  2.77it/s] 57%|█████▋    | 49/86 [00:17<00:13,  2.78it/s] 58%|█████▊    | 50/86 [00:18<00:12,  2.79it/s]W0624 01:49:12.480843 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 51/86 [00:18<00:12,  2.79it/s]I0624 01:49:12.703173 15014 finetune.py:68] layer 17_down @ epoch 1 new loss 0.001312806038185954 old loss 0.0013128876453265548 BETTER
 60%|██████    | 52/86 [00:18<00:12,  2.78it/s] 62%|██████▏   | 53/86 [00:19<00:11,  2.79it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.78it/s] 64%|██████▍   | 55/86 [00:20<00:11,  2.77it/s] 65%|██████▌   | 56/86 [00:20<00:10,  2.77it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.77it/s] 67%|██████▋   | 58/86 [00:21<00:10,  2.77it/s]19_gate proxy err 0.008446631021797657 tr(WHW.T) 3319.64208984375
  0%|          | 0/86 [00:00<?, ?it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.77it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.75it/s]  1%|          | 1/86 [00:00<00:51,  1.65it/s] 71%|███████   | 61/86 [00:22<00:09,  2.75it/s]  2%|▏         | 2/86 [00:00<00:37,  2.22it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.76it/s]  3%|▎         | 3/86 [00:01<00:33,  2.49it/s]  5%|▍         | 4/86 [00:01<00:30,  2.66it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.76it/s]  6%|▌         | 5/86 [00:01<00:29,  2.75it/s] 74%|███████▍  | 64/86 [00:23<00:07,  2.77it/s]  7%|▋         | 6/86 [00:02<00:28,  2.82it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.77it/s]  8%|▊         | 7/86 [00:02<00:27,  2.86it/s] 77%|███████▋  | 66/86 [00:24<00:07,  2.78it/s]  9%|▉         | 8/86 [00:02<00:27,  2.89it/s] 78%|███████▊  | 67/86 [00:24<00:06,  2.78it/s] 10%|█         | 9/86 [00:03<00:26,  2.91it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.76it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.92it/s] 80%|████████  | 69/86 [00:25<00:06,  2.75it/s] 13%|█▎        | 11/86 [00:04<00:25,  2.94it/s] 81%|████████▏ | 70/86 [00:25<00:05,  2.75it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.95it/s] 83%|████████▎ | 71/86 [00:25<00:05,  2.75it/s] 15%|█▌        | 13/86 [00:04<00:24,  2.93it/s] 84%|████████▎ | 72/86 [00:26<00:05,  2.72it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.92it/s] 85%|████████▍ | 73/86 [00:26<00:04,  2.74it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.93it/s] 86%|████████▌ | 74/86 [00:26<00:04,  2.75it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.92it/s] 87%|████████▋ | 75/86 [00:27<00:03,  2.75it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.94it/s] 88%|████████▊ | 76/86 [00:27<00:03,  2.78it/s] 21%|██        | 18/86 [00:06<00:23,  2.94it/s] 90%|████████▉ | 77/86 [00:28<00:03,  2.79it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.94it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.95it/s] 91%|█████████ | 78/86 [00:28<00:02,  2.79it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.95it/s] 92%|█████████▏| 79/86 [00:28<00:02,  2.79it/s] 26%|██▌       | 22/86 [00:07<00:21,  2.96it/s] 93%|█████████▎| 80/86 [00:29<00:02,  2.79it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.97it/s] 94%|█████████▍| 81/86 [00:29<00:01,  2.79it/s] 28%|██▊       | 24/86 [00:08<00:20,  2.98it/s] 95%|█████████▌| 82/86 [00:29<00:01,  2.78it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.97it/s] 97%|█████████▋| 83/86 [00:30<00:01,  2.77it/s] 30%|███       | 26/86 [00:09<00:20,  2.95it/s] 98%|█████████▊| 84/86 [00:30<00:00,  2.76it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.95it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.74it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.94it/s]100%|██████████| 86/86 [00:31<00:00,  2.76it/s]100%|██████████| 86/86 [00:31<00:00,  2.75it/s]
 34%|███▎      | 29/86 [00:10<00:19,  2.92it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.93it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.93it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.94it/s] 38%|███▊      | 33/86 [00:11<00:17,  2.95it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.96it/s] 41%|████      | 35/86 [00:12<00:17,  2.96it/s] 42%|████▏     | 36/86 [00:12<00:16,  2.96it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.96it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.96it/s] 45%|████▌     | 39/86 [00:13<00:15,  2.96it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.94it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.92it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.90it/s] 50%|█████     | 43/86 [00:14<00:14,  2.92it/s] 51%|█████     | 44/86 [00:15<00:14,  2.93it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.93it/s] 53%|█████▎    | 46/86 [00:15<00:13,  2.95it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.95it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.96it/s]W0624 01:49:32.049000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.049000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.049000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.050000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.050000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.050000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.050000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.093000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.093000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.094000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.094000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.094000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.110000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.110000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.110000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.110000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.110000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 57%|█████▋    | 49/86 [00:16<00:12,  2.97it/s]W0624 01:49:32.281000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.281000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.281000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.281000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.281000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 58%|█████▊    | 50/86 [00:17<00:12,  2.97it/s]W0624 01:49:32.612000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.612000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.612000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.613000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.613000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.613000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.613000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.648000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.648000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.648000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.648000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.648000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.721000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.721000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.722000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.722000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:32.722000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 51/86 [00:17<00:11,  2.97it/s] 60%|██████    | 52/86 [00:17<00:11,  2.96it/s]I0624 01:49:33.274030 14510 finetune.py:68] layer 16_down @ epoch 1 new loss 0.0012041714508086443 old loss 0.0012042757589370012 BETTER
 62%|██████▏   | 53/86 [00:18<00:11,  2.95it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.93it/s]W0624 01:49:33.920000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:33.933000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:33.943000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:33.943000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 64%|██████▍   | 55/86 [00:18<00:10,  2.92it/s]W0624 01:49:34.393000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.393000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.393000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.394000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.394000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.394000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.394000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.426000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.426000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.426000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.426000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.426000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 65%|██████▌   | 56/86 [00:19<00:10,  2.93it/s]W0624 01:49:34.772000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.773000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.773000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.773000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.773000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.773000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.773000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:34.773000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 66%|██████▋   | 57/86 [00:19<00:09,  2.92it/s]W0624 01:49:35.070000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:35.070000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:35.070000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:35.070000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:35.070000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 67%|██████▋   | 58/86 [00:19<00:09,  2.93it/s]W0624 01:49:35.415000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:35.420000 139985982908224 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 69%|██████▊   | 59/86 [00:20<00:09,  2.95it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.95it/s] 71%|███████   | 61/86 [00:20<00:08,  2.95it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.95it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.94it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.93it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.91it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.89it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.88it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.85it/s] 80%|████████  | 69/86 [00:23<00:05,  2.86it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.86it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.86it/s] 84%|████████▎ | 72/86 [00:24<00:04,  2.87it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.89it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.89it/s] 87%|████████▋ | 75/86 [00:25<00:03,  2.89it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.89it/s]I0624 01:49:41.852049 15014 finetune.py:68] layer 17_down @ epoch 2 new loss 0.001312743523158133 old loss 0.001312806038185954 BETTER
 90%|████████▉ | 77/86 [00:26<00:03,  2.89it/s] 91%|█████████ | 78/86 [00:26<00:02,  2.93it/s]I0624 01:49:42.243619 15520 finetune.py:45] layer 18_down initial loss 0.0015937094576656818
W0624 01:49:42.244087 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 92%|█████████▏| 79/86 [00:27<00:02,  2.90it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.91it/s] 94%|█████████▍| 81/86 [00:27<00:01,  2.90it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.91it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.93it/s] 98%|█████████▊| 84/86 [00:28<00:00,  2.95it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.96it/s]100%|██████████| 86/86 [00:29<00:00,  2.96it/s]100%|██████████| 86/86 [00:29<00:00,  2.91it/s]
W0624 01:49:51.779000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.779000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.779000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.779000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.780000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.780000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.780000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.825000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.825000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.825000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.825000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.825000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.841000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.841000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.841000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.841000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:51.841000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.021000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.021000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.021000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.021000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.021000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.351000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.351000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.351000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.351000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.351000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.351000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.351000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.380000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.380000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.380000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.381000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.381000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.451000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.452000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.452000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.452000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:52.452000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:53.652000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:53.665000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:53.673000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:53.673000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.125000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.125000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.125000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.125000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.126000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.126000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.126000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.158000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.158000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.158000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.158000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.158000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.502000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.502000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.503000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.503000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.503000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.503000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.503000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.503000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.801000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.801000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.801000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.801000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:54.802000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:49:55.137000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 01:49:55.143000 140056561846080 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 01:50:01.732388 16026 finetune.py:45] layer 19_down initial loss 0.0017831255681812763
W0624 01:50:01.732800 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:50:04.704728 14510 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0012041192967444658 old loss 0.0012041714508086443 BETTER
I0624 01:50:11.125927 15014 finetune.py:68] layer 17_down @ epoch 3 new loss 0.0013126962585374713 old loss 0.001312743523158133 BETTER
I0624 01:50:11.349929 15520 finetune.py:68] layer 18_down @ epoch 0 new loss 0.0015931553207337856 old loss 0.0015937094576656818 BETTER
I0624 01:50:29.672506 16026 finetune.py:68] layer 19_down @ epoch 0 new loss 0.0017825006507337093 old loss 0.0017831255681812763 BETTER
I0624 01:50:36.187217 14510 finetune.py:68] layer 16_down @ epoch 3 new loss 0.001204081461764872 old loss 0.0012041192967444658 BETTER
I0624 01:50:40.636207 15014 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0013126605190336704 old loss 0.0013126962585374713 BETTER
I0624 01:50:41.197873 15520 finetune.py:68] layer 18_down @ epoch 1 new loss 0.0015930465888231993 old loss 0.0015931553207337856 BETTER
W0624 01:50:41.345500 15014 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

17_down proxy err 0.014377474784851074 tr(WHW.T) 148.13186645507812
I0624 01:50:58.524665 16026 finetune.py:68] layer 19_down @ epoch 1 new loss 0.0017823496600612998 old loss 0.0017825006507337093 BETTER
I0624 01:51:08.069021 14510 finetune.py:68] layer 16_down @ epoch 4 new loss 0.0012040332658216357 old loss 0.001204081461764872 BETTER
W0624 01:51:08.828883 14510 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

16_down proxy err 0.0137582216411829 tr(WHW.T) 134.0746612548828
I0624 01:51:11.428577 15520 finetune.py:68] layer 18_down @ epoch 2 new loss 0.001592945889569819 old loss 0.0015930465888231993 BETTER
I0624 01:51:27.530973 16026 finetune.py:68] layer 19_down @ epoch 2 new loss 0.001782274222932756 old loss 0.0017823496600612998 BETTER
I0624 01:51:41.663684 15520 finetune.py:68] layer 18_down @ epoch 3 new loss 0.0015929024666547775 old loss 0.001592945889569819 BETTER
I0624 01:51:56.380983 16026 finetune.py:68] layer 19_down @ epoch 3 new loss 0.0017821985529735684 old loss 0.001782274222932756 BETTER
I0624 01:52:11.529786 15520 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0015927796484902501 old loss 0.0015929024666547775 BETTER
W0624 01:52:12.229751 15520 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

18_down proxy err 0.014385388232767582 tr(WHW.T) 175.61721801757812
I0624 01:52:16.966235 4970 quantize_finetune_llama.py:193] computed original embedding for layer 20 in 63.900327920913696s
I0624 01:52:17.419278 4970 quantize_finetune_llama.py:162] layer 21 gpu 1
I0624 01:52:19.366375 16724 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:52:19.366484 16724 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:52:19.366541 16724 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:52:19.549656 16724 config.py:58] PyTorch version 2.4.0 available.
I0624 01:52:21.913825 16724 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:52:22.271560 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:01<00:22,  1.34it/s]I0624 01:52:25.439096 16026 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0017821330111473799 old loss 0.0017821985529735684 BETTER
  9%|▉         | 3/32 [00:01<00:15,  1.81it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s]W0624 01:52:26.380748 16026 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s]19_down proxy err 0.014351990073919296 tr(WHW.T) 197.7074432373047
 22%|██▏       | 7/32 [00:03<00:09,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.87it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.90it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.98it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.03it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.04it/s] 41%|████      | 13/32 [00:05<00:06,  3.06it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.07it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.07it/s] 50%|█████     | 16/32 [00:06<00:05,  3.05it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.03it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.05it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.08it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.09it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.09it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.10it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.12it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.07it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.08it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.09it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.08it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.08it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.10it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.12it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.08it/s]100%|██████████| 32/32 [00:11<00:00,  3.11it/s]100%|██████████| 32/32 [00:11<00:00,  2.82it/s]
W0624 01:52:38.078000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.078000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.078000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.078000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.079000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.079000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.079000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.104000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.104000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.104000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.104000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.104000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.120000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.120000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.120000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.120000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.120000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.439000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.439000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.439000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.439000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:52:38.439000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.308000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.308000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.308000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.308000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.308000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.308000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.308000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.326000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.326000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.326000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.326000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.326000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.559000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.559000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.560000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.560000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:52:39.560000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.714000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.714000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.714000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.714000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.714000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.715000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.715000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.731000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.732000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.732000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.732000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:52:40.732000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:52:41.608000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:52:41.608000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:52:41.608000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:52:41.608000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:52:41.608000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 01:52:47.774272 16724 finetune.py:45] layer 20_v initial loss 0.0005139441345818341
W0624 01:52:47.774658 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:53:16.781068 4970 quantize_finetune_llama.py:193] computed original embedding for layer 21 in 58.910112142562866s
I0624 01:53:17.167717 4970 quantize_finetune_llama.py:162] layer 22 gpu 2
I0624 01:53:19.282395 17228 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:53:19.282572 17228 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:53:19.282636 17228 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:53:19.566563 17228 config.py:58] PyTorch version 2.4.0 available.
I0624 01:53:21.609048 16724 finetune.py:68] layer 20_v @ epoch 0 new loss 0.00019209487072657794 old loss 0.0005139441345818341 BETTER
I0624 01:53:21.899427 17228 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:53:22.215671 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.60s/it]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.96it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.25it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.47it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.81it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.88it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.94it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.98it/s] 41%|████      | 13/32 [00:05<00:06,  3.02it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.03it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.04it/s] 50%|█████     | 16/32 [00:06<00:05,  3.02it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.03it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.03it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.06it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.06it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.07it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.08it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.09it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.99it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  3.00it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.00it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.01it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.02it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.02it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
W0624 01:53:38.242000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.243000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.243000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.243000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.243000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.243000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.243000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.268000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.268000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.268000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.268000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.268000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.284000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.284000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.284000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.284000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.284000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.595000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.596000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.596000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.596000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:53:38.596000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.463000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.463000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.480000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.480000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.480000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.480000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.480000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.711000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.711000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.711000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.711000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:53:39.711000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.835000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.835000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.835000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.836000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.836000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.836000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.836000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.854000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.854000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.854000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.854000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:53:40.854000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:53:41.743000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:53:41.744000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:53:41.744000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:53:41.744000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:53:41.744000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 01:53:48.366878 17228 finetune.py:45] layer 21_v initial loss 0.0006588799296878278
W0624 01:53:48.367230 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:53:56.677467 16724 finetune.py:68] layer 20_v @ epoch 1 new loss 0.00016520787903573364 old loss 0.00019209487072657794 BETTER
I0624 01:54:17.107987 4970 quantize_finetune_llama.py:193] computed original embedding for layer 22 in 59.50575590133667s
I0624 01:54:17.480717 4970 quantize_finetune_llama.py:162] layer 23 gpu 3
I0624 01:54:19.558891 17734 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:54:19.559019 17734 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:54:19.559082 17734 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:54:19.745075 17734 config.py:58] PyTorch version 2.4.0 available.
I0624 01:54:20.199502 17228 finetune.py:68] layer 21_v @ epoch 0 new loss 0.0002173756656702608 old loss 0.0006588799296878278 BETTER
I0624 01:54:22.471040 17734 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:54:22.787212 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:52,  1.69s/it]  6%|▋         | 2/32 [00:02<00:26,  1.12it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.96it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.48it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.90it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.94it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.96it/s] 41%|████      | 13/32 [00:05<00:06,  3.00it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.03it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.04it/s] 50%|█████     | 16/32 [00:06<00:05,  3.02it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.01it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.05it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.06it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.07it/s]I0624 01:54:31.753063 16724 finetune.py:68] layer 20_v @ epoch 2 new loss 0.00015487569908145815 old loss 0.00016520787903573364 BETTER
 66%|██████▌   | 21/32 [00:08<00:03,  3.08it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.09it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.10it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.06it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.08it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.11it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.13it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.14it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.15it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.15it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.12it/s]100%|██████████| 32/32 [00:11<00:00,  3.07it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
W0624 01:54:38.820000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.820000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.820000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.820000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.820000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.820000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.820000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.847000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.848000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.848000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.848000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.848000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.865000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.865000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.865000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.865000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:54:38.865000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:54:39.183000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:54:39.183000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:54:39.183000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:54:39.183000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:54:39.183000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.080000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.080000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.080000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.081000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.081000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.081000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.081000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.099000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.099000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.099000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.100000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.100000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.337000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.337000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.337000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.337000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:54:40.337000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.502000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.502000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.502000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.502000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.502000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.502000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.502000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.521000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.521000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.521000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.521000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:54:41.521000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:54:42.457000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:54:42.457000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:54:42.457000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:54:42.457000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:54:42.457000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 01:54:48.440369 17734 finetune.py:45] layer 22_v initial loss 0.0007553930045105517
W0624 01:54:48.440767 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:54:53.536516 17228 finetune.py:68] layer 21_v @ epoch 1 new loss 0.00017407326959073544 old loss 0.0002173756656702608 BETTER
I0624 01:55:06.667291 16724 finetune.py:68] layer 20_v @ epoch 3 new loss 0.0001492728915764019 old loss 0.00015487569908145815 BETTER
I0624 01:55:17.129649 4970 quantize_finetune_llama.py:193] computed original embedding for layer 23 in 59.209301233291626s
I0624 01:55:17.495599 4970 quantize_finetune_llama.py:162] layer 24 gpu 0
I0624 01:55:19.629639 18240 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 01:55:19.629767 18240 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 01:55:19.629830 18240 utils.py:162] NumExpr defaulting to 16 threads.
I0624 01:55:19.818305 18240 config.py:58] PyTorch version 2.4.0 available.
I0624 01:55:20.053103 17734 finetune.py:68] layer 22_v @ epoch 0 new loss 0.00027367452275939286 old loss 0.0007553930045105517 BETTER
I0624 01:55:22.195215 18240 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 01:55:22.527541 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it]  6%|▋         | 2/32 [00:02<00:29,  1.03it/s]  9%|▉         | 3/32 [00:02<00:19,  1.47it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.83it/s]I0624 01:55:26.730690 17228 finetune.py:68] layer 21_v @ epoch 2 new loss 0.00016128044808283448 old loss 0.00017407326959073544 BETTER
 16%|█▌        | 5/32 [00:03<00:12,  2.13it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.84it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.87it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.92it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.94it/s] 50%|█████     | 16/32 [00:06<00:05,  2.95it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.94it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.96it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.96it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.97it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.96it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.96it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.92it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.95it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.99it/s]100%|██████████| 32/32 [00:12<00:00,  2.98it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
W0624 01:55:39.124000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.124000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.125000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.125000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.125000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.125000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.125000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.152000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.152000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.152000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.153000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.153000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.170000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.170000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.170000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.170000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.170000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.491000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.491000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.491000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.491000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 01:55:39.491000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.392000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.392000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.392000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.392000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.392000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.392000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.393000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.411000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.411000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.411000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.411000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.412000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.654000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.654000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.654000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.654000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 01:55:40.654000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
I0624 01:55:41.731664 16724 finetune.py:68] layer 20_v @ epoch 4 new loss 0.00014536782691720873 old loss 0.0001492728915764019 BETTER
W0624 01:55:41.837000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.837000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.837000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.837000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.837000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.838000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.838000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.855000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.855000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.855000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.855000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:55:41.855000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 01:55:42.773000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 01:55:42.773000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 01:55:42.773000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 01:55:42.773000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 01:55:42.773000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0624 01:55:43.286140 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_v proxy err 0.013691757805645466 tr(WHW.T) 992.5980224609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.59it/s]  6%|▋         | 2/32 [00:00<00:13,  2.17it/s]  9%|▉         | 3/32 [00:01<00:11,  2.46it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.75it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.80it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.85it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.88it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.92it/s]I0624 01:55:48.046373 18240 finetune.py:45] layer 23_v initial loss 0.0010169160086661577
W0624 01:55:48.046585 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.93it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.92it/s] 41%|████      | 13/32 [00:04<00:06,  2.92it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s] 50%|█████     | 16/32 [00:05<00:05,  2.92it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.93it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.95it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.95it/s]I0624 01:55:52.328797 17734 finetune.py:68] layer 22_v @ epoch 1 new loss 0.000224029878154397 old loss 0.00027367452275939286 BETTER
 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.92it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.96it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.97it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]
I0624 01:55:59.905044 17228 finetune.py:68] layer 21_v @ epoch 3 new loss 0.00015471717051696032 old loss 0.00016128044808283448 BETTER
I0624 01:56:02.443761 16724 finetune.py:45] layer 20_q initial loss 0.00020022499666083604
W0624 01:56:02.444048 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:56:19.068662 18240 finetune.py:68] layer 23_v @ epoch 0 new loss 0.00037217576755210757 old loss 0.0010169160086661577 BETTER
I0624 01:56:24.839908 17734 finetune.py:68] layer 22_v @ epoch 2 new loss 0.00020860604126937687 old loss 0.000224029878154397 BETTER
I0624 01:56:33.517202 17228 finetune.py:68] layer 21_v @ epoch 4 new loss 0.0001505285908933729 old loss 0.00015471717051696032 BETTER
W0624 01:56:35.268247 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 01:56:36.175783 16724 finetune.py:68] layer 20_q @ epoch 0 new loss 0.00019098236225545406 old loss 0.00020022499666083604 BETTER
21_v proxy err 0.014032598584890366 tr(WHW.T) 1170.4912109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.62it/s]  6%|▋         | 2/32 [00:00<00:13,  2.16it/s]  9%|▉         | 3/32 [00:01<00:12,  2.40it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.54it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.62it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.65it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.76it/s] 41%|████      | 13/32 [00:04<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:07<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.75it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.75it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.78it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.79it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
I0624 01:56:51.075892 18240 finetune.py:68] layer 23_v @ epoch 1 new loss 0.0002794158353935927 old loss 0.00037217576755210757 BETTER
I0624 01:56:55.533921 17228 finetune.py:45] layer 21_q initial loss 0.00020062510157004
W0624 01:56:55.534311 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:56:57.870255 17734 finetune.py:68] layer 22_v @ epoch 3 new loss 0.00020053129992447793 old loss 0.00020860604126937687 BETTER
I0624 01:57:11.280323 16724 finetune.py:68] layer 20_q @ epoch 1 new loss 0.00018658988119568676 old loss 0.00019098236225545406 BETTER
I0624 01:57:23.021950 18240 finetune.py:68] layer 23_v @ epoch 2 new loss 0.00025527458637952805 old loss 0.0002794158353935927 BETTER
I0624 01:57:27.869420 17228 finetune.py:68] layer 21_q @ epoch 0 new loss 0.0001910663122544065 old loss 0.00020062510157004 BETTER
I0624 01:57:30.720268 17734 finetune.py:68] layer 22_v @ epoch 4 new loss 0.0001951967424247414 old loss 0.00020053129992447793 BETTER
W0624 01:57:32.227053 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_v proxy err 0.01377524621784687 tr(WHW.T) 1255.868408203125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.65it/s]  6%|▋         | 2/32 [00:00<00:13,  2.25it/s]  9%|▉         | 3/32 [00:01<00:11,  2.55it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.74it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.85it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.91it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.94it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.97it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.00it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.02it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.01it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.99it/s] 41%|████      | 13/32 [00:04<00:06,  3.01it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.02it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.03it/s] 50%|█████     | 16/32 [00:05<00:05,  3.03it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.03it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.05it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.06it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.06it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.05it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.05it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.06it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.06it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.05it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.03it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.01it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.03it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.04it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.04it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.03it/s]100%|██████████| 32/32 [00:10<00:00,  3.05it/s]100%|██████████| 32/32 [00:10<00:00,  2.96it/s]
I0624 01:57:45.911721 16724 finetune.py:68] layer 20_q @ epoch 2 new loss 0.000183300202479586 old loss 0.00018658988119568676 BETTER
I0624 01:57:50.372420 17734 finetune.py:45] layer 22_q initial loss 0.00028347119223326445
W0624 01:57:50.372731 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:57:54.933372 18240 finetune.py:68] layer 23_v @ epoch 3 new loss 0.00024374273198191077 old loss 0.00025527458637952805 BETTER
I0624 01:58:01.084174 17228 finetune.py:68] layer 21_q @ epoch 1 new loss 0.00018692364392336458 old loss 0.0001910663122544065 BETTER
I0624 01:58:20.907600 16724 finetune.py:68] layer 20_q @ epoch 3 new loss 0.00018072250531986356 old loss 0.000183300202479586 BETTER
I0624 01:58:22.405576 17734 finetune.py:68] layer 22_q @ epoch 0 new loss 0.00026912122848443687 old loss 0.00028347119223326445 BETTER
I0624 01:58:26.822643 18240 finetune.py:68] layer 23_v @ epoch 4 new loss 0.00023682950995862484 old loss 0.00024374273198191077 BETTER
W0624 01:58:28.241842 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_v proxy err 0.014226309955120087 tr(WHW.T) 1528.427734375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.61it/s]  6%|▋         | 2/32 [00:00<00:13,  2.20it/s]  9%|▉         | 3/32 [00:01<00:11,  2.50it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.68it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.78it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.84it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.87it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.91it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.93it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.95it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.95it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.93it/s] 41%|████      | 13/32 [00:04<00:06,  2.95it/s]I0624 01:58:34.227625 17228 finetune.py:68] layer 21_q @ epoch 2 new loss 0.00018398513202555478 old loss 0.00018692364392336458 BETTER
 44%|████▍     | 14/32 [00:04<00:06,  2.97it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.98it/s] 50%|█████     | 16/32 [00:05<00:05,  2.97it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.98it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.00it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.00it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.98it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.98it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.98it/s] 72%|███████▏  | 23/32 [00:08<00:03,  3.00it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.99it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.96it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.97it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.97it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.98it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.98it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]
I0624 01:58:46.757046 18240 finetune.py:45] layer 23_q initial loss 0.0003164583758916706
W0624 01:58:46.757307 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:58:54.895709 17734 finetune.py:68] layer 22_q @ epoch 1 new loss 0.0002624940825626254 old loss 0.00026912122848443687 BETTER
I0624 01:58:55.536907 16724 finetune.py:68] layer 20_q @ epoch 4 new loss 0.00017858704086393118 old loss 0.00018072250531986356 BETTER
W0624 01:58:57.077259 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_q proxy err 0.0034002617467194796 tr(WHW.T) 6849.40625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]  6%|▋         | 2/32 [00:00<00:13,  2.21it/s]  9%|▉         | 3/32 [00:01<00:11,  2.48it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.72it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.77it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.81it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.84it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.86it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.88it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 41%|████      | 13/32 [00:04<00:06,  2.91it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.90it/s] 50%|█████     | 16/32 [00:05<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.89it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.90it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.90it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.90it/s]I0624 01:59:07.346445 17228 finetune.py:68] layer 21_q @ epoch 3 new loss 0.00018166533845942467 old loss 0.00018398513202555478 BETTER
 81%|████████▏ | 26/32 [00:09<00:02,  2.90it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.96it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.92it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]
I0624 01:59:16.249765 16724 finetune.py:45] layer 20_k initial loss 0.0002204139600507915
W0624 01:59:16.250117 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 01:59:18.169499 18240 finetune.py:68] layer 23_q @ epoch 0 new loss 0.00030072624213062227 old loss 0.0003164583758916706 BETTER
I0624 01:59:27.805205 17734 finetune.py:68] layer 22_q @ epoch 2 new loss 0.0002575546386651695 old loss 0.0002624940825626254 BETTER
I0624 01:59:40.273715 17228 finetune.py:68] layer 21_q @ epoch 4 new loss 0.0001798060693545267 old loss 0.00018166533845942467 BETTER
W0624 01:59:41.865056 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_q proxy err 0.0037450569216161966 tr(WHW.T) 6762.712890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.63it/s]  6%|▋         | 2/32 [00:00<00:14,  2.13it/s]  9%|▉         | 3/32 [00:01<00:12,  2.39it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.54it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.65it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.71it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:03<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s] 41%|████      | 13/32 [00:04<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.72it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.75it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s]I0624 01:59:50.197170 16724 finetune.py:68] layer 20_k @ epoch 0 new loss 0.00021441892022266984 old loss 0.0002204139600507915 BETTER
I0624 01:59:50.383262 18240 finetune.py:68] layer 23_q @ epoch 1 new loss 0.000293962424620986 old loss 0.00030072624213062227 BETTER
 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.80it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.75it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.77it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
I0624 02:00:00.676848 17734 finetune.py:68] layer 22_q @ epoch 3 new loss 0.0002535979438107461 old loss 0.0002575546386651695 BETTER
I0624 02:00:01.805897 17228 finetune.py:45] layer 21_k initial loss 0.00022259520483203232
W0624 02:00:01.806147 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:00:22.433568 18240 finetune.py:68] layer 23_q @ epoch 2 new loss 0.00028919088072143495 old loss 0.000293962424620986 BETTER
I0624 02:00:25.526981 16724 finetune.py:68] layer 20_k @ epoch 1 new loss 0.00021275816834531724 old loss 0.00021441892022266984 BETTER
I0624 02:00:33.605588 17734 finetune.py:68] layer 22_q @ epoch 4 new loss 0.0002503498981241137 old loss 0.0002535979438107461 BETTER
I0624 02:00:34.399766 17228 finetune.py:68] layer 21_k @ epoch 0 new loss 0.00021777171059511602 old loss 0.00022259520483203232 BETTER
W0624 02:00:34.957251 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_q proxy err 0.0037475975695997477 tr(WHW.T) 7350.3359375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:17,  1.75it/s]  6%|▋         | 2/32 [00:00<00:12,  2.32it/s]  9%|▉         | 3/32 [00:01<00:11,  2.60it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.76it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.86it/s] 19%|█▉        | 6/32 [00:02<00:08,  2.91it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.95it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.98it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.99it/s] 31%|███▏      | 10/32 [00:03<00:07,  3.00it/s] 34%|███▍      | 11/32 [00:03<00:06,  3.00it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.02it/s] 41%|████      | 13/32 [00:04<00:06,  3.05it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.06it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.04it/s] 50%|█████     | 16/32 [00:05<00:05,  3.04it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.06it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.08it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.06it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.04it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.04it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.06it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.04it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.03it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.03it/s] 81%|████████▏ | 26/32 [00:08<00:01,  3.04it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.05it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.06it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.06it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.05it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.03it/s]100%|██████████| 32/32 [00:10<00:00,  3.03it/s]100%|██████████| 32/32 [00:10<00:00,  2.97it/s]
I0624 02:00:53.078875 17734 finetune.py:45] layer 22_k initial loss 0.0003211659495718777
W0624 02:00:53.079197 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:00:54.462884 18240 finetune.py:68] layer 23_q @ epoch 3 new loss 0.0002854973135981709 old loss 0.00028919088072143495 BETTER
I0624 02:01:00.840095 16724 finetune.py:68] layer 20_k @ epoch 2 new loss 0.0002114389935741201 old loss 0.00021275816834531724 BETTER
I0624 02:01:07.400965 17228 finetune.py:68] layer 21_k @ epoch 1 new loss 0.00021623438806273043 old loss 0.00021777171059511602 BETTER
I0624 02:01:24.629949 17734 finetune.py:68] layer 22_k @ epoch 0 new loss 0.0003124493232462555 old loss 0.0003211659495718777 BETTER
I0624 02:01:26.533050 18240 finetune.py:68] layer 23_q @ epoch 4 new loss 0.0002824225521180779 old loss 0.0002854973135981709 BETTER
W0624 02:01:27.860404 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_q proxy err 0.004451265092939138 tr(WHW.T) 6995.04296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:17,  1.75it/s]  6%|▋         | 2/32 [00:00<00:13,  2.30it/s]  9%|▉         | 3/32 [00:01<00:11,  2.58it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.72it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.80it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.84it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.86it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.90it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.93it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.94it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.94it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.96it/s] 41%|████      | 13/32 [00:04<00:06,  2.99it/s] 44%|████▍     | 14/32 [00:04<00:06,  2.99it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.98it/s] 50%|█████     | 16/32 [00:05<00:05,  2.97it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.99it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.00it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.99it/s] 62%|██████▎   | 20/32 [00:06<00:04,  2.97it/s]I0624 02:01:35.978005 16724 finetune.py:68] layer 20_k @ epoch 3 new loss 0.0002104431187035516 old loss 0.0002114389935741201 BETTER
 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.99it/s] 72%|███████▏  | 23/32 [00:07<00:03,  2.99it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.98it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.97it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.97it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.99it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.01it/s] 91%|█████████ | 29/32 [00:09<00:01,  3.00it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.98it/s]100%|██████████| 32/32 [00:10<00:00,  2.99it/s]100%|██████████| 32/32 [00:10<00:00,  2.92it/s]
I0624 02:01:40.583124 17228 finetune.py:68] layer 21_k @ epoch 2 new loss 0.0002151140506612137 old loss 0.00021623438806273043 BETTER
I0624 02:01:46.385793 18240 finetune.py:45] layer 23_k initial loss 0.00035570652107708156
W0624 02:01:46.386173 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:01:57.099258 17734 finetune.py:68] layer 22_k @ epoch 1 new loss 0.0003100451431237161 old loss 0.0003124493232462555 BETTER
I0624 02:02:10.738643 16724 finetune.py:68] layer 20_k @ epoch 4 new loss 0.0002095319505315274 old loss 0.0002104431187035516 BETTER
W0624 02:02:12.212011 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_k proxy err 0.002446825383231044 tr(WHW.T) 9878.1240234375
  0%|          | 0/32 [00:00<?, ?it/s]I0624 02:02:13.766683 17228 finetune.py:68] layer 21_k @ epoch 3 new loss 0.00021427978936117142 old loss 0.0002151140506612137 BETTER
  3%|▎         | 1/32 [00:00<00:18,  1.66it/s]  6%|▋         | 2/32 [00:00<00:13,  2.27it/s]  9%|▉         | 3/32 [00:01<00:11,  2.53it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.68it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.77it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.83it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.88it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.92it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.93it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.93it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.93it/s] 41%|████      | 13/32 [00:04<00:06,  2.93it/s]I0624 02:02:17.916225 18240 finetune.py:68] layer 23_k @ epoch 0 new loss 0.0003465217596385628 old loss 0.00035570652107708156 BETTER
 44%|████▍     | 14/32 [00:04<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 50%|█████     | 16/32 [00:05<00:05,  2.91it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.93it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.93it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.94it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.93it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.94it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.94it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.93it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.93it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.94it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.95it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.95it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]
I0624 02:02:29.910736 17734 finetune.py:68] layer 22_k @ epoch 2 new loss 0.0003081813920289278 old loss 0.0003100451431237161 BETTER
I0624 02:02:31.245258 16724 finetune.py:45] layer 20_o initial loss 0.0004112370661459863
W0624 02:02:31.245572 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:02:46.665086 17228 finetune.py:68] layer 21_k @ epoch 4 new loss 0.00021352918702177703 old loss 0.00021427978936117142 BETTER
W0624 02:02:48.187696 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_k proxy err 0.0027224745135754347 tr(WHW.T) 9513.181640625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.65it/s]I0624 02:02:49.948337 18240 finetune.py:68] layer 23_k @ epoch 1 new loss 0.0003442553570494056 old loss 0.0003465217596385628 BETTER
  6%|▋         | 2/32 [00:00<00:13,  2.15it/s]  9%|▉         | 3/32 [00:01<00:12,  2.39it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.52it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.60it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 41%|████      | 13/32 [00:04<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.77it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.77it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.76it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.76it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
I0624 02:03:02.981870 17734 finetune.py:68] layer 22_k @ epoch 3 new loss 0.0003066992212552577 old loss 0.0003081813920289278 BETTER
I0624 02:03:05.319239 16724 finetune.py:68] layer 20_o @ epoch 0 new loss 0.00039882067358121276 old loss 0.0004112370661459863 BETTER
I0624 02:03:08.288059 17228 finetune.py:45] layer 21_o initial loss 0.0004200061084702611
W0624 02:03:08.288548 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:03:21.977111 18240 finetune.py:68] layer 23_k @ epoch 2 new loss 0.00034265307476744056 old loss 0.0003442553570494056 BETTER
I0624 02:03:35.930102 17734 finetune.py:68] layer 22_k @ epoch 4 new loss 0.0003054147819057107 old loss 0.0003066992212552577 BETTER
W0624 02:03:37.293707 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_k proxy err 0.002827857621014118 tr(WHW.T) 9997.89453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:17,  1.73it/s]  6%|▋         | 2/32 [00:00<00:13,  2.30it/s]  9%|▉         | 3/32 [00:01<00:11,  2.58it/s]I0624 02:03:39.884927 16724 finetune.py:68] layer 20_o @ epoch 1 new loss 0.0003957349981646985 old loss 0.00039882067358121276 BETTER
 12%|█▎        | 4/32 [00:01<00:10,  2.75it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.84it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.88it/s]I0624 02:03:40.796679 17228 finetune.py:68] layer 21_o @ epoch 0 new loss 0.0004061801009811461 old loss 0.0004200061084702611 BETTER
 22%|██▏       | 7/32 [00:02<00:08,  2.90it/s] 25%|██▌       | 8/32 [00:02<00:08,  2.94it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.94it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.95it/s] 34%|███▍      | 11/32 [00:03<00:07,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.97it/s] 41%|████      | 13/32 [00:04<00:06,  2.99it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.01it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.00it/s] 50%|█████     | 16/32 [00:05<00:05,  2.99it/s] 53%|█████▎    | 17/32 [00:05<00:05,  2.99it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.00it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.01it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.01it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.00it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.00it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.01it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.00it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.98it/s] 81%|████████▏ | 26/32 [00:08<00:02,  2.99it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.01it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.03it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.01it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.03it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.04it/s]100%|██████████| 32/32 [00:10<00:00,  3.05it/s]100%|██████████| 32/32 [00:10<00:00,  2.94it/s]
I0624 02:03:54.008125 18240 finetune.py:68] layer 23_k @ epoch 3 new loss 0.0003412960795685649 old loss 0.00034265307476744056 BETTER
I0624 02:03:55.786162 17734 finetune.py:45] layer 22_o initial loss 0.000562606961466372
W0624 02:03:55.786508 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:04:14.028476 17228 finetune.py:68] layer 21_o @ epoch 1 new loss 0.00040369099588133395 old loss 0.0004061801009811461 BETTER
I0624 02:04:14.408572 16724 finetune.py:68] layer 20_o @ epoch 2 new loss 0.0003935204294975847 old loss 0.0003957349981646985 BETTER
I0624 02:04:26.097156 18240 finetune.py:68] layer 23_k @ epoch 4 new loss 0.0003402408619876951 old loss 0.0003412960795685649 BETTER
I0624 02:04:27.434936 17734 finetune.py:68] layer 22_o @ epoch 0 new loss 0.0005468735471367836 old loss 0.000562606961466372 BETTER
W0624 02:04:27.537549 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_k proxy err 0.0033693776931613684 tr(WHW.T) 9390.2197265625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:18,  1.68it/s]  6%|▋         | 2/32 [00:00<00:13,  2.24it/s]  9%|▉         | 3/32 [00:01<00:11,  2.50it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.63it/s] 16%|█▌        | 5/32 [00:01<00:09,  2.70it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.76it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.80it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.85it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.89it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s] 41%|████      | 13/32 [00:04<00:06,  2.97it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.97it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.96it/s] 50%|█████     | 16/32 [00:05<00:05,  2.96it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.96it/s] 59%|█████▉    | 19/32 [00:06<00:04,  2.96it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:07<00:03,  2.96it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.96it/s] 78%|███████▊  | 25/32 [00:08<00:02,  2.95it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.96it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.98it/s] 88%|████████▊ | 28/32 [00:09<00:01,  2.98it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.98it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.97it/s] 97%|█████████▋| 31/32 [00:10<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  2.99it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]
I0624 02:04:45.972636 18240 finetune.py:45] layer 23_o initial loss 0.0006305722054094076
W0624 02:04:45.973116 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:04:47.119254 17228 finetune.py:68] layer 21_o @ epoch 2 new loss 0.0004020443302579224 old loss 0.00040369099588133395 BETTER
I0624 02:04:48.872601 16724 finetune.py:68] layer 20_o @ epoch 3 new loss 0.0003917060967069119 old loss 0.0003935204294975847 BETTER
I0624 02:04:59.767441 17734 finetune.py:68] layer 22_o @ epoch 1 new loss 0.0005434624035842717 old loss 0.0005468735471367836 BETTER
I0624 02:05:17.402875 18240 finetune.py:68] layer 23_o @ epoch 0 new loss 0.0006110791000537574 old loss 0.0006305722054094076 BETTER
I0624 02:05:19.952863 17228 finetune.py:68] layer 21_o @ epoch 3 new loss 0.00040074909338727593 old loss 0.0004020443302579224 BETTER
I0624 02:05:23.244338 16724 finetune.py:68] layer 20_o @ epoch 4 new loss 0.0003903212782461196 old loss 0.0003917060967069119 BETTER
W0624 02:05:24.556555 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_o proxy err 0.009512203745543957 tr(WHW.T) 71.8497543334961
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:02<00:33,  1.12s/it]  9%|▉         | 3/32 [00:03<00:31,  1.08s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it]I0624 02:05:32.038007 17734 finetune.py:68] layer 22_o @ epoch 2 new loss 0.0005410260055214167 old loss 0.0005434624035842717 BETTER
 19%|█▉        | 6/32 [00:06<00:26,  1.04s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.02s/it] 41%|████      | 13/32 [00:13<00:19,  1.02s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.02s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.03s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.03s/it]I0624 02:05:49.517598 18240 finetune.py:68] layer 23_o @ epoch 1 new loss 0.0006077424040995538 old loss 0.0006110791000537574 BETTER
 72%|███████▏  | 23/32 [00:23<00:09,  1.03s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.01s/it]I0624 02:05:53.225040 17228 finetune.py:68] layer 21_o @ epoch 4 new loss 0.00039975266554392874 old loss 0.00040074909338727593 BETTER
 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.00s/it]W0624 02:05:54.747762 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:29<00:03,  1.00s/it]21_o proxy err 0.01153130829334259 tr(WHW.T) 60.469642639160156
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:30<00:01,  1.01it/s]  3%|▎         | 1/32 [00:01<00:40,  1.31s/it] 97%|█████████▋| 31/32 [00:31<00:00,  1.02it/s]100%|██████████| 32/32 [00:32<00:00,  1.02it/s]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
  6%|▋         | 2/32 [00:02<00:34,  1.16s/it]  9%|▉         | 3/32 [00:03<00:32,  1.13s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.11s/it] 16%|█▌        | 5/32 [00:05<00:29,  1.08s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.07s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.06s/it]I0624 02:06:04.709742 17734 finetune.py:68] layer 22_o @ epoch 3 new loss 0.0005390791338868439 old loss 0.0005410260055214167 BETTER
 25%|██▌       | 8/32 [00:08<00:25,  1.05s/it]W0624 02:06:04.963000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.963000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.963000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.963000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.963000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.963000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.964000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.991000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.992000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.992000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.992000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:04.992000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.006000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.006000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.006000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.007000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.007000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.158000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.158000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.158000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.158000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.158000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.379000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.379000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.379000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.379000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.379000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.379000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.380000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.401000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.401000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.401000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.401000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.401000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.466000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.466000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.466000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.466000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:05.466000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 9/32 [00:09<00:24,  1.05s/it]W0624 02:06:06.327000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.636000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.636000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.636000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.636000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.636000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.636000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.637000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.659000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.659000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.660000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.660000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.660000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.910000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.910000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.910000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.910000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:06.910000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 10/32 [00:10<00:23,  1.05s/it]W0624 02:06:07.167000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 11/32 [00:11<00:22,  1.05s/it] 38%|███▊      | 12/32 [00:12<00:21,  1.06s/it] 41%|████      | 13/32 [00:14<00:20,  1.07s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.08s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.09s/it] 50%|█████     | 16/32 [00:17<00:17,  1.09s/it]I0624 02:06:13.783212 16724 finetune.py:45] layer 20_up initial loss 0.0008829156868159771
W0624 02:06:13.783572 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:18<00:16,  1.09s/it] 56%|█████▋    | 18/32 [00:19<00:15,  1.08s/it] 59%|█████▉    | 19/32 [00:20<00:14,  1.08s/it] 62%|██████▎   | 20/32 [00:21<00:12,  1.07s/it] 66%|██████▌   | 21/32 [00:22<00:11,  1.07s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.07s/it] 72%|███████▏  | 23/32 [00:24<00:09,  1.07s/it]I0624 02:06:21.563870 18240 finetune.py:68] layer 23_o @ epoch 2 new loss 0.0006055197445675731 old loss 0.0006077424040995538 BETTER
 75%|███████▌  | 24/32 [00:25<00:08,  1.07s/it] 78%|███████▊  | 25/32 [00:26<00:07,  1.07s/it] 81%|████████▏ | 26/32 [00:27<00:06,  1.07s/it] 84%|████████▍ | 27/32 [00:29<00:05,  1.07s/it] 88%|████████▊ | 28/32 [00:30<00:04,  1.08s/it] 91%|█████████ | 29/32 [00:31<00:03,  1.08s/it] 94%|█████████▍| 30/32 [00:32<00:02,  1.08s/it] 97%|█████████▋| 31/32 [00:33<00:01,  1.08s/it]100%|██████████| 32/32 [00:34<00:00,  1.09s/it]100%|██████████| 32/32 [00:34<00:00,  1.08s/it]
I0624 02:06:37.459135 17734 finetune.py:68] layer 22_o @ epoch 4 new loss 0.0005376448389142752 old loss 0.0005390791338868439 BETTER
W0624 02:06:37.707000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.707000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.707000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.707000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.708000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.708000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.708000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.735000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.735000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.735000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.735000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.735000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.751000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.751000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.751000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.751000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.751000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.914000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.915000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.915000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.915000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:37.915000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.142000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.142000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.142000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.142000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.142000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.142000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.142000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.161000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.161000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.162000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.162000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.162000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.228000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.228000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.228000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.228000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.228000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:38.684629 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0624 02:06:39.132000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.462000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.463000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.483000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.483000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.483000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.483000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.483000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.748000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.748000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.748000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.749000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:06:39.749000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
22_o proxy err 0.009837510995566845 tr(WHW.T) 85.11233520507812
  0%|          | 0/32 [00:00<?, ?it/s]W0624 02:06:40.023000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:01<00:39,  1.29s/it]  6%|▋         | 2/32 [00:02<00:33,  1.11s/it]  9%|▉         | 3/32 [00:03<00:30,  1.05s/it] 12%|█▎        | 4/32 [00:04<00:28,  1.02s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.00s/it] 19%|█▉        | 6/32 [00:06<00:25,  1.01it/s]I0624 02:06:46.803296 16724 finetune.py:68] layer 20_up @ epoch 0 new loss 0.0008742303471080959 old loss 0.0008829156868159771 BETTER
 22%|██▏       | 7/32 [00:07<00:24,  1.01it/s]I0624 02:06:47.000704 17228 finetune.py:45] layer 21_up initial loss 0.0009580078767612576
W0624 02:06:47.001118 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:08<00:23,  1.01it/s] 28%|██▊       | 9/32 [00:09<00:22,  1.01it/s] 31%|███▏      | 10/32 [00:10<00:21,  1.00it/s] 34%|███▍      | 11/32 [00:11<00:21,  1.01s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 41%|████      | 13/32 [00:13<00:19,  1.01s/it]I0624 02:06:53.514757 18240 finetune.py:68] layer 23_o @ epoch 3 new loss 0.0006038183346390724 old loss 0.0006055197445675731 BETTER
 44%|████▍     | 14/32 [00:14<00:18,  1.00s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.00s/it] 50%|█████     | 16/32 [00:16<00:15,  1.00it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.00it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.01it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.01it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.01it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.01it/s] 72%|███████▏  | 23/32 [00:23<00:08,  1.01it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.01it/s] 78%|███████▊  | 25/32 [00:25<00:06,  1.01it/s] 81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s] 84%|████████▍ | 27/32 [00:27<00:04,  1.01it/s] 88%|████████▊ | 28/32 [00:28<00:03,  1.02it/s] 91%|█████████ | 29/32 [00:28<00:02,  1.02it/s] 94%|█████████▍| 30/32 [00:29<00:01,  1.01it/s] 97%|█████████▋| 31/32 [00:30<00:00,  1.01it/s]100%|██████████| 32/32 [00:31<00:00,  1.01it/s]100%|██████████| 32/32 [00:31<00:00,  1.00it/s]
I0624 02:07:18.351051 17228 finetune.py:68] layer 21_up @ epoch 0 new loss 0.0009500887827016413 old loss 0.0009580078767612576 BETTER
W0624 02:07:18.438000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.438000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.438000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.439000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.439000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.439000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.439000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.468000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.468000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.468000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.468000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.468000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.486000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.486000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.486000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.486000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.486000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.648000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.648000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.648000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.648000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.648000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.878000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.878000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.878000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.878000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.878000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.878000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.879000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.899000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.900000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.900000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.900000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.900000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.967000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.967000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.967000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.967000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:07:18.968000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:07:19.850000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.159000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.159000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.159000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.160000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.160000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.160000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.160000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.181000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.181000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.181000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.181000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.181000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
I0624 02:07:20.182321 16724 finetune.py:68] layer 20_up @ epoch 1 new loss 0.0008695962023921311 old loss 0.0008742303471080959 BETTER
W0624 02:07:20.443000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.443000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.443000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.443000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.443000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:07:20.705000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 02:07:25.444832 18240 finetune.py:68] layer 23_o @ epoch 4 new loss 0.0006024270551279187 old loss 0.0006038183346390724 BETTER
W0624 02:07:26.854586 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0624 02:07:26.863276 17734 finetune.py:45] layer 22_up initial loss 0.0011844058753922582
W0624 02:07:26.863573 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

23_o proxy err 0.012921472080051899 tr(WHW.T) 73.04484558105469
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.28s/it]  6%|▋         | 2/32 [00:02<00:33,  1.13s/it]  9%|▉         | 3/32 [00:03<00:31,  1.07s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.02s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.02s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.01s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.01s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.02s/it] 41%|████      | 13/32 [00:13<00:19,  1.02s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.02s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.02s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.03s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.03s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it]I0624 02:07:50.187683 17228 finetune.py:68] layer 21_up @ epoch 1 new loss 0.0009458973072469234 old loss 0.0009500887827016413 BETTER
 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it]I0624 02:07:53.553595 16724 finetune.py:68] layer 20_up @ epoch 2 new loss 0.0008658047299832106 old loss 0.0008695962023921311 BETTER
 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.03s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.03s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.03s/it]I0624 02:07:57.574956 17734 finetune.py:68] layer 22_up @ epoch 0 new loss 0.0011765508679673076 old loss 0.0011844058753922582 BETTER
 91%|█████████ | 29/32 [00:29<00:03,  1.03s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.02s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.01s/it]100%|██████████| 32/32 [00:32<00:00,  1.03s/it]
W0624 02:08:07.570000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.570000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.571000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.571000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.571000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.572000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.572000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.604000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.604000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.604000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.604000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.604000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.620000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.620000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.621000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.621000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.621000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.780000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.780000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.781000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.781000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:08:07.781000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.015000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.016000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.016000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.016000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.016000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.016000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.016000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.036000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.037000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.037000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.037000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.037000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.104000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.104000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.104000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.104000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.104000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:08:08.996000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.315000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.315000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.315000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.315000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.315000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.316000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.316000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.337000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.337000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.337000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.337000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.337000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.592000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.592000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.592000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.592000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.592000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:08:09.856000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0624 02:08:15.910132 18240 finetune.py:45] layer 23_up initial loss 0.0013437268789857626
W0624 02:08:15.910496 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:08:21.981183 17228 finetune.py:68] layer 21_up @ epoch 2 new loss 0.0009425247553735971 old loss 0.0009458973072469234 BETTER
I0624 02:08:26.984288 16724 finetune.py:68] layer 20_up @ epoch 3 new loss 0.0008625990594737232 old loss 0.0008658047299832106 BETTER
I0624 02:08:28.870216 17734 finetune.py:68] layer 22_up @ epoch 1 new loss 0.0011720209149643779 old loss 0.0011765508679673076 BETTER
I0624 02:08:46.090994 18240 finetune.py:68] layer 23_up @ epoch 0 new loss 0.0013357718707993627 old loss 0.0013437268789857626 BETTER
I0624 02:08:54.534542 17228 finetune.py:68] layer 21_up @ epoch 3 new loss 0.0009397200774401426 old loss 0.0009425247553735971 BETTER
I0624 02:09:00.784492 17734 finetune.py:68] layer 22_up @ epoch 2 new loss 0.0011684781638905406 old loss 0.0011720209149643779 BETTER
I0624 02:09:00.981562 16724 finetune.py:68] layer 20_up @ epoch 4 new loss 0.0008598375134170055 old loss 0.0008625990594737232 BETTER
W0624 02:09:02.318975 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_up proxy err 0.01168674323707819 tr(WHW.T) 2277.25390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:42,  1.37s/it]  6%|▋         | 2/32 [00:02<00:35,  1.18s/it]  9%|▉         | 3/32 [00:03<00:32,  1.11s/it] 12%|█▎        | 4/32 [00:04<00:30,  1.07s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.05s/it] 19%|█▉        | 6/32 [00:06<00:27,  1.04s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.03s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.03s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.03s/it]I0624 02:09:17.209155 18240 finetune.py:68] layer 23_up @ epoch 1 new loss 0.0013312164228409529 old loss 0.0013357718707993627 BETTER
 41%|████      | 13/32 [00:13<00:19,  1.03s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.03s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.03s/it] 50%|█████     | 16/32 [00:16<00:16,  1.03s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.03s/it] 56%|█████▋    | 18/32 [00:18<00:14,  1.02s/it] 59%|█████▉    | 19/32 [00:19<00:13,  1.02s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.02s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.02s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.01s/it]I0624 02:09:27.366888 17228 finetune.py:68] layer 21_up @ epoch 4 new loss 0.0009373113862238824 old loss 0.0009397200774401426 BETTER
 72%|███████▏  | 23/32 [00:23<00:09,  1.01s/it] 75%|███████▌  | 24/32 [00:24<00:08,  1.01s/it]W0624 02:09:28.794883 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:25<00:07,  1.00s/it] 81%|████████▏ | 26/32 [00:26<00:05,  1.01it/s]21_up proxy err 0.01240215077996254 tr(WHW.T) 2332.48583984375
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:27<00:04,  1.02it/s]  3%|▎         | 1/32 [00:01<00:43,  1.42s/it] 88%|████████▊ | 28/32 [00:28<00:03,  1.03it/s]I0624 02:09:32.772693 17734 finetune.py:68] layer 22_up @ epoch 3 new loss 0.0011655226116999984 old loss 0.0011684781638905406 BETTER
  6%|▋         | 2/32 [00:02<00:36,  1.20s/it] 91%|█████████ | 29/32 [00:29<00:02,  1.02it/s]  9%|▉         | 3/32 [00:03<00:33,  1.15s/it] 94%|█████████▍| 30/32 [00:30<00:01,  1.03it/s] 12%|█▎        | 4/32 [00:04<00:31,  1.12s/it] 97%|█████████▋| 31/32 [00:31<00:00,  1.03it/s] 16%|█▌        | 5/32 [00:05<00:29,  1.09s/it]100%|██████████| 32/32 [00:32<00:00,  1.04it/s]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
 19%|█▉        | 6/32 [00:06<00:28,  1.08s/it] 22%|██▏       | 7/32 [00:07<00:26,  1.07s/it] 25%|██▌       | 8/32 [00:08<00:25,  1.07s/it] 28%|██▊       | 9/32 [00:09<00:24,  1.07s/it] 31%|███▏      | 10/32 [00:10<00:23,  1.06s/it] 34%|███▍      | 11/32 [00:12<00:22,  1.07s/it] 38%|███▊      | 12/32 [00:13<00:21,  1.07s/it]I0624 02:09:44.099459 16724 finetune.py:45] layer 20_gate initial loss 0.0013156302738934755
W0624 02:09:44.099789 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:14<00:20,  1.07s/it] 44%|████▍     | 14/32 [00:15<00:19,  1.07s/it] 47%|████▋     | 15/32 [00:16<00:18,  1.07s/it] 50%|█████     | 16/32 [00:17<00:17,  1.08s/it]I0624 02:09:48.066266 18240 finetune.py:68] layer 23_up @ epoch 2 new loss 0.0013275534147396684 old loss 0.0013312164228409529 BETTER
 53%|█████▎    | 17/32 [00:18<00:16,  1.09s/it] 56%|█████▋    | 18/32 [00:19<00:15,  1.09s/it] 59%|█████▉    | 19/32 [00:20<00:14,  1.09s/it] 62%|██████▎   | 20/32 [00:21<00:13,  1.09s/it] 66%|██████▌   | 21/32 [00:22<00:12,  1.10s/it] 69%|██████▉   | 22/32 [00:23<00:10,  1.09s/it] 72%|███████▏  | 23/32 [00:25<00:09,  1.09s/it] 75%|███████▌  | 24/32 [00:26<00:08,  1.09s/it] 78%|███████▊  | 25/32 [00:27<00:07,  1.08s/it] 81%|████████▏ | 26/32 [00:28<00:06,  1.09s/it] 84%|████████▍ | 27/32 [00:29<00:05,  1.09s/it] 88%|████████▊ | 28/32 [00:30<00:04,  1.09s/it] 91%|█████████ | 29/32 [00:31<00:03,  1.08s/it] 94%|█████████▍| 30/32 [00:32<00:02,  1.08s/it] 97%|█████████▋| 31/32 [00:33<00:01,  1.09s/it]I0624 02:10:04.553347 17734 finetune.py:68] layer 22_up @ epoch 4 new loss 0.0011629204964265227 old loss 0.0011655226116999984 BETTER
100%|██████████| 32/32 [00:34<00:00,  1.09s/it]100%|██████████| 32/32 [00:34<00:00,  1.09s/it]
W0624 02:10:05.765924 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_up proxy err 0.012644351460039616 tr(WHW.T) 2457.669921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:41,  1.33s/it]  6%|▋         | 2/32 [00:02<00:34,  1.14s/it]  9%|▉         | 3/32 [00:03<00:31,  1.08s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.04s/it] 16%|█▌        | 5/32 [00:05<00:27,  1.03s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.02s/it]I0624 02:10:14.162928 17228 finetune.py:45] layer 21_gate initial loss 0.0014609408099204302
W0624 02:10:14.163458 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:07<00:25,  1.01s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.01s/it]I0624 02:10:16.334872 16724 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.0013070962158963084 old loss 0.0013156302738934755 BETTER
 28%|██▊       | 9/32 [00:09<00:23,  1.02s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.02s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it]I0624 02:10:18.778462 18240 finetune.py:68] layer 23_up @ epoch 3 new loss 0.001324577839113772 old loss 0.0013275534147396684 BETTER
 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 41%|████      | 13/32 [00:13<00:19,  1.00s/it] 44%|████▍     | 14/32 [00:14<00:17,  1.00it/s] 47%|████▋     | 15/32 [00:15<00:16,  1.01it/s] 50%|█████     | 16/32 [00:16<00:15,  1.01it/s] 53%|█████▎    | 17/32 [00:17<00:14,  1.01it/s] 56%|█████▋    | 18/32 [00:18<00:13,  1.01it/s] 59%|█████▉    | 19/32 [00:19<00:12,  1.02it/s] 62%|██████▎   | 20/32 [00:20<00:11,  1.02it/s] 66%|██████▌   | 21/32 [00:21<00:10,  1.02it/s] 69%|██████▉   | 22/32 [00:22<00:09,  1.02it/s] 72%|███████▏  | 23/32 [00:23<00:08,  1.02it/s] 75%|███████▌  | 24/32 [00:24<00:07,  1.02it/s] 78%|███████▊  | 25/32 [00:25<00:06,  1.02it/s] 81%|████████▏ | 26/32 [00:26<00:05,  1.02it/s] 84%|████████▍ | 27/32 [00:27<00:04,  1.02it/s] 88%|████████▊ | 28/32 [00:28<00:03,  1.02it/s] 91%|█████████ | 29/32 [00:29<00:02,  1.02it/s] 94%|█████████▍| 30/32 [00:30<00:01,  1.02it/s] 97%|█████████▋| 31/32 [00:31<00:00,  1.02it/s]100%|██████████| 32/32 [00:32<00:00,  1.02it/s]100%|██████████| 32/32 [00:32<00:00,  1.00s/it]
I0624 02:10:44.654811 17228 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.0014538333052769303 old loss 0.0014609408099204302 BETTER
I0624 02:10:46.226872 17734 finetune.py:45] layer 22_gate initial loss 0.0017758554313331842
W0624 02:10:46.227144 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:10:48.843023 16724 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.0013038369361311197 old loss 0.0013070962158963084 BETTER
I0624 02:10:49.547891 18240 finetune.py:68] layer 23_up @ epoch 4 new loss 0.0013219318352639675 old loss 0.001324577839113772 BETTER
W0624 02:10:50.756736 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_up proxy err 0.013301825150847435 tr(WHW.T) 2553.108154296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:39,  1.29s/it]  6%|▋         | 2/32 [00:02<00:33,  1.12s/it]  9%|▉         | 3/32 [00:03<00:30,  1.06s/it] 12%|█▎        | 4/32 [00:04<00:29,  1.05s/it] 16%|█▌        | 5/32 [00:05<00:28,  1.04s/it] 19%|█▉        | 6/32 [00:06<00:26,  1.03s/it] 22%|██▏       | 7/32 [00:07<00:25,  1.03s/it] 25%|██▌       | 8/32 [00:08<00:24,  1.02s/it] 28%|██▊       | 9/32 [00:09<00:23,  1.03s/it] 31%|███▏      | 10/32 [00:10<00:22,  1.03s/it] 34%|███▍      | 11/32 [00:11<00:21,  1.02s/it] 38%|███▊      | 12/32 [00:12<00:20,  1.01s/it] 41%|████      | 13/32 [00:13<00:19,  1.01s/it] 44%|████▍     | 14/32 [00:14<00:18,  1.01s/it] 47%|████▋     | 15/32 [00:15<00:17,  1.01s/it] 50%|█████     | 16/32 [00:16<00:16,  1.00s/it] 53%|█████▎    | 17/32 [00:17<00:15,  1.00s/it] 56%|█████▋    | 18/32 [00:18<00:13,  1.00it/s] 59%|█████▉    | 19/32 [00:19<00:13,  1.00s/it] 62%|██████▎   | 20/32 [00:20<00:12,  1.01s/it] 66%|██████▌   | 21/32 [00:21<00:11,  1.01s/it] 69%|██████▉   | 22/32 [00:22<00:10,  1.02s/it] 72%|███████▏  | 23/32 [00:23<00:09,  1.02s/it]I0624 02:11:15.786474 17228 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0014507891610264778 old loss 0.0014538333052769303 BETTER
I0624 02:11:16.276909 17734 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0017685570055618882 old loss 0.0017758554313331842 BETTER
 75%|███████▌  | 24/32 [00:24<00:08,  1.02s/it] 78%|███████▊  | 25/32 [00:25<00:07,  1.02s/it] 81%|████████▏ | 26/32 [00:26<00:06,  1.01s/it] 84%|████████▍ | 27/32 [00:27<00:05,  1.01s/it] 88%|████████▊ | 28/32 [00:28<00:04,  1.01s/it]I0624 02:11:21.428529 16724 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.001301304087974131 old loss 0.0013038369361311197 BETTER
 91%|█████████ | 29/32 [00:29<00:03,  1.01s/it] 94%|█████████▍| 30/32 [00:30<00:02,  1.02s/it] 97%|█████████▋| 31/32 [00:31<00:01,  1.02s/it]100%|██████████| 32/32 [00:32<00:00,  1.03s/it]100%|██████████| 32/32 [00:32<00:00,  1.02s/it]
I0624 02:11:31.796696 18240 finetune.py:45] layer 23_gate initial loss 0.0020357770845294
W0624 02:11:31.797056 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:11:46.715232 17228 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.0014485962456092238 old loss 0.0014507891610264778 BETTER
I0624 02:11:46.857513 17734 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.0017654128605499864 old loss 0.0017685570055618882 BETTER
I0624 02:11:54.281610 16724 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.0012991082621738315 old loss 0.001301304087974131 BETTER
I0624 02:12:01.508859 18240 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.0020286787766963243 old loss 0.0020357770845294 BETTER
I0624 02:12:17.645773 17734 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0017630568472668529 old loss 0.0017654128605499864 BETTER
I0624 02:12:17.870556 17228 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.0014467022847384214 old loss 0.0014485962456092238 BETTER
I0624 02:12:26.976472 16724 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.001297143753618002 old loss 0.0012991082621738315 BETTER
W0624 02:12:28.135990 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_gate proxy err 0.008340230211615562 tr(WHW.T) 3649.4794921875
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:53,  1.58it/s]I0624 02:12:31.617331 18240 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.002025558380410075 old loss 0.0020286787766963243 BETTER
  2%|▏         | 2/86 [00:00<00:38,  2.17it/s]  3%|▎         | 3/86 [00:01<00:33,  2.46it/s]  5%|▍         | 4/86 [00:01<00:31,  2.62it/s]  6%|▌         | 5/86 [00:01<00:29,  2.73it/s]  7%|▋         | 6/86 [00:02<00:28,  2.78it/s]  8%|▊         | 7/86 [00:02<00:28,  2.82it/s]  9%|▉         | 8/86 [00:03<00:27,  2.84it/s] 10%|█         | 9/86 [00:03<00:26,  2.87it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.89it/s] 13%|█▎        | 11/86 [00:04<00:25,  2.90it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.91it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.92it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.93it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.93it/s] 19%|█▊        | 16/86 [00:05<00:23,  2.93it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.93it/s] 21%|██        | 18/86 [00:06<00:23,  2.93it/s] 22%|██▏       | 19/86 [00:06<00:22,  2.93it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.92it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.91it/s] 26%|██▌       | 22/86 [00:07<00:22,  2.90it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.90it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.92it/s] 29%|██▉       | 25/86 [00:08<00:20,  2.92it/s] 30%|███       | 26/86 [00:09<00:20,  2.93it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.93it/s] 33%|███▎      | 28/86 [00:09<00:19,  2.94it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.94it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.94it/s] 36%|███▌      | 31/86 [00:10<00:18,  2.93it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.93it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.92it/s] 40%|███▉      | 34/86 [00:11<00:17,  2.91it/s] 41%|████      | 35/86 [00:12<00:17,  2.92it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.93it/s] 43%|████▎     | 37/86 [00:12<00:16,  2.93it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.92it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.91it/s] 47%|████▋     | 40/86 [00:13<00:15,  2.91it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.92it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.92it/s] 50%|█████     | 43/86 [00:15<00:14,  2.91it/s] 51%|█████     | 44/86 [00:15<00:14,  2.92it/s] 52%|█████▏    | 45/86 [00:15<00:14,  2.91it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.89it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.89it/s] 56%|█████▌    | 48/86 [00:16<00:13,  2.88it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.89it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.88it/s]I0624 02:12:48.416793 17734 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.001761138904839754 old loss 0.0017630568472668529 BETTER
 59%|█████▉    | 51/86 [00:17<00:12,  2.88it/s]I0624 02:12:48.918694 17228 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.001445054542273283 old loss 0.0014467022847384214 BETTER
 60%|██████    | 52/86 [00:18<00:11,  2.90it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.92it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.96it/s]W0624 02:12:50.024272 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 64%|██████▍   | 55/86 [00:19<00:10,  2.99it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.97it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.96it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.94it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.94it/s] 70%|██████▉   | 60/86 [00:20<00:08,  2.94it/s] 71%|███████   | 61/86 [00:21<00:08,  2.95it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.96it/s] 73%|███████▎  | 63/86 [00:21<00:07,  2.96it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.99it/s]21_gate proxy err 0.009087247774004936 tr(WHW.T) 3702.9755859375
  0%|          | 0/86 [00:00<?, ?it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.99it/s] 77%|███████▋  | 66/86 [00:22<00:06,  3.01it/s]  1%|          | 1/86 [00:00<00:52,  1.61it/s] 78%|███████▊  | 67/86 [00:23<00:06,  3.03it/s]  2%|▏         | 2/86 [00:00<00:38,  2.17it/s] 79%|███████▉  | 68/86 [00:23<00:05,  3.05it/s]  3%|▎         | 3/86 [00:01<00:34,  2.41it/s] 80%|████████  | 69/86 [00:23<00:05,  3.04it/s]  5%|▍         | 4/86 [00:01<00:32,  2.56it/s] 81%|████████▏ | 70/86 [00:24<00:05,  3.07it/s]  6%|▌         | 5/86 [00:02<00:30,  2.66it/s] 83%|████████▎ | 71/86 [00:24<00:04,  3.07it/s]  7%|▋         | 6/86 [00:02<00:29,  2.69it/s] 84%|████████▎ | 72/86 [00:24<00:04,  3.05it/s]  8%|▊         | 7/86 [00:02<00:28,  2.74it/s] 85%|████████▍ | 73/86 [00:25<00:04,  3.06it/s] 86%|████████▌ | 74/86 [00:25<00:03,  3.04it/s]  9%|▉         | 8/86 [00:03<00:28,  2.75it/s] 87%|████████▋ | 75/86 [00:25<00:03,  3.02it/s] 10%|█         | 9/86 [00:03<00:28,  2.74it/s] 88%|████████▊ | 76/86 [00:26<00:03,  3.04it/s] 12%|█▏        | 10/86 [00:03<00:27,  2.77it/s] 90%|████████▉ | 77/86 [00:26<00:02,  3.06it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.79it/s] 91%|█████████ | 78/86 [00:26<00:02,  3.07it/s] 14%|█▍        | 12/86 [00:04<00:26,  2.80it/s] 92%|█████████▏| 79/86 [00:27<00:02,  3.09it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.82it/s] 93%|█████████▎| 80/86 [00:27<00:01,  3.10it/s] 16%|█▋        | 14/86 [00:05<00:25,  2.81it/s] 94%|█████████▍| 81/86 [00:27<00:01,  3.08it/s] 17%|█▋        | 15/86 [00:05<00:25,  2.79it/s] 95%|█████████▌| 82/86 [00:28<00:01,  3.07it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.81it/s] 97%|█████████▋| 83/86 [00:28<00:00,  3.09it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.83it/s] 98%|█████████▊| 84/86 [00:28<00:00,  3.09it/s] 21%|██        | 18/86 [00:06<00:23,  2.84it/s] 99%|█████████▉| 85/86 [00:28<00:00,  3.10it/s]100%|██████████| 86/86 [00:29<00:00,  3.11it/s]100%|██████████| 86/86 [00:29<00:00,  2.93it/s]
 22%|██▏       | 19/86 [00:07<00:23,  2.86it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.83it/s] 24%|██▍       | 21/86 [00:07<00:23,  2.78it/s] 26%|██▌       | 22/86 [00:08<00:23,  2.75it/s]I0624 02:13:01.499289 18240 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0020233013201504946 old loss 0.002025558380410075 BETTER
 27%|██▋       | 23/86 [00:08<00:22,  2.77it/s] 28%|██▊       | 24/86 [00:08<00:22,  2.75it/s] 29%|██▉       | 25/86 [00:09<00:21,  2.79it/s] 30%|███       | 26/86 [00:09<00:21,  2.80it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.82it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.81it/s] 34%|███▎      | 29/86 [00:10<00:20,  2.81it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.83it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.85it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.86it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.86it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.87it/s] 41%|████      | 35/86 [00:12<00:17,  2.87it/s] 42%|████▏     | 36/86 [00:13<00:17,  2.88it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.89it/s]W0624 02:13:06.847000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.847000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.848000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.848000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.848000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.848000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.848000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.889000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.889000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.889000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.889000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.889000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.905000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.905000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.905000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.905000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:06.906000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 38/86 [00:13<00:16,  2.88it/s]W0624 02:13:07.073000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.073000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.073000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.074000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.074000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 45%|████▌     | 39/86 [00:14<00:16,  2.86it/s]W0624 02:13:07.388000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.388000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.388000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.388000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.388000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.388000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.389000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.420000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.420000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.420000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.420000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.420000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.487000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.487000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.487000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.487000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:07.487000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 40/86 [00:14<00:16,  2.85it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.82it/s] 49%|████▉     | 42/86 [00:15<00:15,  2.83it/s]W0624 02:13:08.653000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:08.666000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:08.674000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:08.674000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 43/86 [00:15<00:15,  2.82it/s] 51%|█████     | 44/86 [00:15<00:14,  2.82it/s]W0624 02:13:09.123000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.124000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.124000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.124000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.124000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.124000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.124000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.154000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.154000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.154000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.154000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.154000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 52%|█████▏    | 45/86 [00:16<00:14,  2.84it/s]W0624 02:13:09.489000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.489000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.489000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.489000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.489000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.490000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.490000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.490000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.784000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.785000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.785000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.785000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:09.785000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 53%|█████▎    | 46/86 [00:16<00:14,  2.84it/s]W0624 02:13:10.137000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:10.142000 140297836713792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 55%|█████▍    | 47/86 [00:16<00:13,  2.82it/s] 56%|█████▌    | 48/86 [00:17<00:13,  2.82it/s] 57%|█████▋    | 49/86 [00:17<00:13,  2.84it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.85it/s] 59%|█████▉    | 51/86 [00:18<00:12,  2.86it/s] 60%|██████    | 52/86 [00:18<00:11,  2.84it/s] 62%|██████▏   | 53/86 [00:19<00:11,  2.80it/s] 63%|██████▎   | 54/86 [00:19<00:11,  2.75it/s] 64%|██████▍   | 55/86 [00:19<00:11,  2.75it/s] 65%|██████▌   | 56/86 [00:20<00:10,  2.74it/s] 66%|██████▋   | 57/86 [00:20<00:10,  2.74it/s] 67%|██████▋   | 58/86 [00:20<00:10,  2.74it/s] 69%|██████▊   | 59/86 [00:21<00:09,  2.74it/s] 70%|██████▉   | 60/86 [00:21<00:09,  2.73it/s] 71%|███████   | 61/86 [00:21<00:09,  2.73it/s] 72%|███████▏  | 62/86 [00:22<00:08,  2.73it/s] 73%|███████▎  | 63/86 [00:22<00:08,  2.73it/s] 74%|███████▍  | 64/86 [00:23<00:08,  2.72it/s] 76%|███████▌  | 65/86 [00:23<00:07,  2.71it/s]I0624 02:13:17.053111 16724 finetune.py:45] layer 20_down initial loss 0.002105934079736471
W0624 02:13:17.053498 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 77%|███████▋  | 66/86 [00:23<00:07,  2.71it/s] 78%|███████▊  | 67/86 [00:24<00:07,  2.71it/s] 79%|███████▉  | 68/86 [00:24<00:06,  2.68it/s] 80%|████████  | 69/86 [00:24<00:06,  2.71it/s] 81%|████████▏ | 70/86 [00:25<00:05,  2.72it/s]I0624 02:13:18.800509 17734 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.0017594766104593873 old loss 0.001761138904839754 BETTER
 83%|████████▎ | 71/86 [00:25<00:05,  2.72it/s] 84%|████████▎ | 72/86 [00:26<00:05,  2.71it/s] 85%|████████▍ | 73/86 [00:26<00:04,  2.71it/s]W0624 02:13:19.874542 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 86%|████████▌ | 74/86 [00:26<00:04,  2.72it/s] 87%|████████▋ | 75/86 [00:27<00:04,  2.73it/s] 88%|████████▊ | 76/86 [00:27<00:03,  2.73it/s] 90%|████████▉ | 77/86 [00:27<00:03,  2.71it/s] 91%|█████████ | 78/86 [00:28<00:02,  2.70it/s] 92%|█████████▏| 79/86 [00:28<00:02,  2.69it/s] 93%|█████████▎| 80/86 [00:28<00:02,  2.69it/s]22_gate proxy err 0.00943511351943016 tr(WHW.T) 3879.861572265625
  0%|          | 0/86 [00:00<?, ?it/s] 94%|█████████▍| 81/86 [00:29<00:01,  2.67it/s] 95%|█████████▌| 82/86 [00:29<00:01,  2.71it/s]  1%|          | 1/86 [00:00<00:52,  1.61it/s] 97%|█████████▋| 83/86 [00:30<00:01,  2.70it/s]  2%|▏         | 2/86 [00:00<00:38,  2.18it/s] 98%|█████████▊| 84/86 [00:30<00:00,  2.72it/s]  3%|▎         | 3/86 [00:01<00:33,  2.45it/s] 99%|█████████▉| 85/86 [00:30<00:00,  2.73it/s]  5%|▍         | 4/86 [00:01<00:31,  2.60it/s]100%|██████████| 86/86 [00:31<00:00,  2.74it/s]100%|██████████| 86/86 [00:31<00:00,  2.76it/s]
  6%|▌         | 5/86 [00:02<00:30,  2.69it/s]  7%|▋         | 6/86 [00:02<00:29,  2.75it/s]  8%|▊         | 7/86 [00:02<00:28,  2.77it/s]  9%|▉         | 8/86 [00:03<00:27,  2.82it/s] 10%|█         | 9/86 [00:03<00:27,  2.84it/s] 12%|█▏        | 10/86 [00:03<00:26,  2.86it/s] 13%|█▎        | 11/86 [00:04<00:25,  2.89it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.90it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.91it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.90it/s] 17%|█▋        | 15/86 [00:05<00:24,  2.90it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.90it/s] 20%|█▉        | 17/86 [00:06<00:23,  2.90it/s] 21%|██        | 18/86 [00:06<00:23,  2.89it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.89it/s] 23%|██▎       | 20/86 [00:07<00:22,  2.88it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.88it/s] 26%|██▌       | 22/86 [00:07<00:22,  2.88it/s] 27%|██▋       | 23/86 [00:08<00:22,  2.85it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.86it/s]I0624 02:13:31.381500 18240 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.0020213909447193146 old loss 0.0020233013201504946 BETTER
 29%|██▉       | 25/86 [00:08<00:21,  2.86it/s]W0624 02:13:31.511000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.512000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.512000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.512000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.512000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.512000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.512000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.552000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.552000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.552000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.552000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.552000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.568000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.568000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.568000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.569000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.569000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.752000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.752000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.752000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.752000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:31.752000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 30%|███       | 26/86 [00:09<00:20,  2.86it/s]W0624 02:13:32.085000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.085000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.085000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.086000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.086000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.086000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.086000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.121000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.121000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.121000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.121000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.121000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 27/86 [00:09<00:20,  2.87it/s]W0624 02:13:32.193000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.193000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.193000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.194000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:32.194000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 33%|███▎      | 28/86 [00:09<00:20,  2.87it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.87it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.87it/s]W0624 02:13:33.438000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.451000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.460000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.460000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 36%|███▌      | 31/86 [00:11<00:19,  2.88it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.91it/s]W0624 02:13:33.918000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.918000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.918000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.919000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.919000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.919000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.919000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.956000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.956000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.956000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.956000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:33.956000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 33/86 [00:11<00:18,  2.91it/s]W0624 02:13:34.303000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.303000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.303000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.303000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.303000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.303000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.303000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.303000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 40%|███▉      | 34/86 [00:12<00:17,  2.91it/s]W0624 02:13:34.602000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.602000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.602000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.602000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.602000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 41%|████      | 35/86 [00:12<00:17,  2.92it/s]W0624 02:13:34.946000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:34.951000 139987463874368 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 42%|████▏     | 36/86 [00:12<00:17,  2.91it/s] 43%|████▎     | 37/86 [00:13<00:16,  2.93it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.94it/s] 45%|████▌     | 39/86 [00:13<00:15,  2.94it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.96it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.98it/s] 49%|████▉     | 42/86 [00:14<00:14,  2.98it/s] 50%|█████     | 43/86 [00:15<00:14,  2.98it/s] 51%|█████     | 44/86 [00:15<00:14,  2.98it/s] 52%|█████▏    | 45/86 [00:15<00:13,  2.98it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.99it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.99it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.99it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.99it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.97it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.98it/s] 60%|██████    | 52/86 [00:18<00:11,  2.99it/s] 62%|██████▏   | 53/86 [00:18<00:11,  3.00it/s] 63%|██████▎   | 54/86 [00:18<00:10,  3.01it/s] 64%|██████▍   | 55/86 [00:19<00:10,  3.01it/s]I0624 02:13:41.803569 17228 finetune.py:45] layer 21_down initial loss 0.002316409256309271
W0624 02:13:41.804012 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 65%|██████▌   | 56/86 [00:19<00:09,  3.01it/s] 66%|██████▋   | 57/86 [00:19<00:09,  3.01it/s] 67%|██████▋   | 58/86 [00:20<00:09,  3.00it/s] 69%|██████▊   | 59/86 [00:20<00:08,  3.01it/s] 70%|██████▉   | 60/86 [00:20<00:08,  3.01it/s] 71%|███████   | 61/86 [00:21<00:08,  3.02it/s] 72%|███████▏  | 62/86 [00:21<00:07,  3.01it/s] 73%|███████▎  | 63/86 [00:21<00:07,  3.00it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.99it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.97it/s] 77%|███████▋  | 66/86 [00:22<00:06,  2.97it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.97it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.98it/s] 80%|████████  | 69/86 [00:23<00:05,  2.97it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.98it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.98it/s]I0624 02:13:47.241305 16724 finetune.py:68] layer 20_down @ epoch 0 new loss 0.0021051312796771526 old loss 0.002105934079736471 BETTER
 84%|████████▎ | 72/86 [00:24<00:04,  2.99it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.99it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.99it/s] 87%|████████▋ | 75/86 [00:25<00:03,  2.98it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.99it/s] 90%|████████▉ | 77/86 [00:26<00:03,  3.00it/s] 91%|█████████ | 78/86 [00:26<00:02,  3.00it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.97it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.97it/s] 94%|█████████▍| 81/86 [00:27<00:01,  2.99it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.99it/s] 97%|█████████▋| 83/86 [00:28<00:00,  3.00it/s] 98%|█████████▊| 84/86 [00:28<00:00,  3.01it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.99it/s]100%|██████████| 86/86 [00:29<00:00,  2.99it/s]100%|██████████| 86/86 [00:29<00:00,  2.92it/s]
W0624 02:13:58.949000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.949000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.949000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.949000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.949000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.949000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.950000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.998000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.999000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.999000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.999000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:58.999000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.015000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.015000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.015000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.015000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.015000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.192000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.192000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.192000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.192000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.192000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.529000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.529000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.529000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.529000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.530000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.530000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.530000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.564000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.564000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.564000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.564000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.565000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.637000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.637000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.637000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.637000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:13:59.637000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:00.887000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:00.899000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:00.907000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:00.907000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
I0624 02:14:01.319736 18240 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0020197387784719467 old loss 0.0020213909447193146 BETTER
W0624 02:14:01.366000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.366000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.366000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.366000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.366000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.366000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.366000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.397000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.397000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.397000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.397000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.398000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.734000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.734000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.734000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.735000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.735000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.735000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.735000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:01.735000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:02.028000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:02.028000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:02.028000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:02.028000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:02.028000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:02.315333 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0624 02:14:02.357000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:02.362000 139706711291712 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
23_gate proxy err 0.010210040956735611 tr(WHW.T) 3876.075439453125
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:53,  1.59it/s]  2%|▏         | 2/86 [00:00<00:39,  2.15it/s]  3%|▎         | 3/86 [00:01<00:34,  2.40it/s]  5%|▍         | 4/86 [00:01<00:32,  2.55it/s]  6%|▌         | 5/86 [00:02<00:30,  2.64it/s]  7%|▋         | 6/86 [00:02<00:29,  2.71it/s]  8%|▊         | 7/86 [00:02<00:28,  2.76it/s]  9%|▉         | 8/86 [00:03<00:28,  2.78it/s] 10%|█         | 9/86 [00:03<00:27,  2.82it/s]I0624 02:14:08.964976 17734 finetune.py:45] layer 22_down initial loss 0.0027575534768402576
W0624 02:14:08.965379 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▏        | 10/86 [00:03<00:26,  2.86it/s] 13%|█▎        | 11/86 [00:04<00:26,  2.88it/s] 14%|█▍        | 12/86 [00:04<00:25,  2.90it/s] 15%|█▌        | 13/86 [00:04<00:25,  2.91it/s] 16%|█▋        | 14/86 [00:05<00:24,  2.92it/s]I0624 02:14:10.674010 17228 finetune.py:68] layer 21_down @ epoch 0 new loss 0.002315645571798086 old loss 0.002316409256309271 BETTER
 17%|█▋        | 15/86 [00:05<00:24,  2.91it/s] 19%|█▊        | 16/86 [00:05<00:24,  2.92it/s] 20%|█▉        | 17/86 [00:06<00:24,  2.86it/s] 21%|██        | 18/86 [00:06<00:23,  2.85it/s] 22%|██▏       | 19/86 [00:06<00:23,  2.86it/s] 23%|██▎       | 20/86 [00:07<00:23,  2.85it/s] 24%|██▍       | 21/86 [00:07<00:22,  2.86it/s] 26%|██▌       | 22/86 [00:07<00:22,  2.87it/s] 27%|██▋       | 23/86 [00:08<00:21,  2.88it/s] 28%|██▊       | 24/86 [00:08<00:21,  2.87it/s] 29%|██▉       | 25/86 [00:08<00:21,  2.88it/s] 30%|███       | 26/86 [00:09<00:20,  2.89it/s] 31%|███▏      | 27/86 [00:09<00:20,  2.89it/s] 33%|███▎      | 28/86 [00:10<00:20,  2.88it/s] 34%|███▎      | 29/86 [00:10<00:19,  2.87it/s] 35%|███▍      | 30/86 [00:10<00:19,  2.85it/s] 36%|███▌      | 31/86 [00:11<00:19,  2.86it/s] 37%|███▋      | 32/86 [00:11<00:18,  2.86it/s] 38%|███▊      | 33/86 [00:11<00:18,  2.86it/s] 40%|███▉      | 34/86 [00:12<00:18,  2.87it/s] 41%|████      | 35/86 [00:12<00:17,  2.88it/s] 42%|████▏     | 36/86 [00:12<00:17,  2.87it/s]I0624 02:14:18.419374 16724 finetune.py:68] layer 20_down @ epoch 1 new loss 0.0021049256902188063 old loss 0.0021051312796771526 BETTER
 43%|████▎     | 37/86 [00:13<00:16,  2.88it/s] 44%|████▍     | 38/86 [00:13<00:16,  2.89it/s] 45%|████▌     | 39/86 [00:13<00:16,  2.90it/s] 47%|████▋     | 40/86 [00:14<00:15,  2.91it/s] 48%|████▊     | 41/86 [00:14<00:15,  2.90it/s] 49%|████▉     | 42/86 [00:14<00:15,  2.90it/s] 50%|█████     | 43/86 [00:15<00:14,  2.92it/s] 51%|█████     | 44/86 [00:15<00:14,  2.91it/s] 52%|█████▏    | 45/86 [00:15<00:14,  2.90it/s] 53%|█████▎    | 46/86 [00:16<00:13,  2.91it/s] 55%|█████▍    | 47/86 [00:16<00:13,  2.91it/s] 56%|█████▌    | 48/86 [00:16<00:12,  2.93it/s] 57%|█████▋    | 49/86 [00:17<00:12,  2.94it/s] 58%|█████▊    | 50/86 [00:17<00:12,  2.95it/s] 59%|█████▉    | 51/86 [00:17<00:11,  2.96it/s] 60%|██████    | 52/86 [00:18<00:11,  2.96it/s] 62%|██████▏   | 53/86 [00:18<00:11,  2.94it/s] 63%|██████▎   | 54/86 [00:18<00:10,  2.94it/s] 64%|██████▍   | 55/86 [00:19<00:10,  2.94it/s] 65%|██████▌   | 56/86 [00:19<00:10,  2.94it/s] 66%|██████▋   | 57/86 [00:19<00:09,  2.94it/s] 67%|██████▋   | 58/86 [00:20<00:09,  2.94it/s] 69%|██████▊   | 59/86 [00:20<00:09,  2.91it/s] 70%|██████▉   | 60/86 [00:21<00:08,  2.92it/s] 71%|███████   | 61/86 [00:21<00:08,  2.91it/s] 72%|███████▏  | 62/86 [00:21<00:08,  2.93it/s] 73%|███████▎  | 63/86 [00:22<00:07,  2.94it/s] 74%|███████▍  | 64/86 [00:22<00:07,  2.95it/s] 76%|███████▌  | 65/86 [00:22<00:07,  2.95it/s] 77%|███████▋  | 66/86 [00:23<00:06,  2.95it/s] 78%|███████▊  | 67/86 [00:23<00:06,  2.96it/s] 79%|███████▉  | 68/86 [00:23<00:06,  2.96it/s] 80%|████████  | 69/86 [00:24<00:05,  2.96it/s] 81%|████████▏ | 70/86 [00:24<00:05,  2.96it/s] 83%|████████▎ | 71/86 [00:24<00:05,  2.94it/s] 84%|████████▎ | 72/86 [00:25<00:04,  2.93it/s] 85%|████████▍ | 73/86 [00:25<00:04,  2.94it/s] 86%|████████▌ | 74/86 [00:25<00:04,  2.90it/s] 87%|████████▋ | 75/86 [00:26<00:03,  2.91it/s] 88%|████████▊ | 76/86 [00:26<00:03,  2.91it/s] 90%|████████▉ | 77/86 [00:26<00:03,  2.91it/s] 91%|█████████ | 78/86 [00:27<00:02,  2.93it/s] 92%|█████████▏| 79/86 [00:27<00:02,  2.94it/s] 93%|█████████▎| 80/86 [00:27<00:02,  2.93it/s] 94%|█████████▍| 81/86 [00:28<00:01,  2.92it/s] 95%|█████████▌| 82/86 [00:28<00:01,  2.92it/s] 97%|█████████▋| 83/86 [00:28<00:01,  2.92it/s] 98%|█████████▊| 84/86 [00:29<00:00,  2.90it/s] 99%|█████████▉| 85/86 [00:29<00:00,  2.90it/s]100%|██████████| 86/86 [00:29<00:00,  2.89it/s]100%|██████████| 86/86 [00:29<00:00,  2.88it/s]
I0624 02:14:37.534942 17734 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0027567336801439524 old loss 0.0027575534768402576 BETTER
I0624 02:14:40.270556 17228 finetune.py:68] layer 21_down @ epoch 1 new loss 0.0023154953960329294 old loss 0.002315645571798086 BETTER
W0624 02:14:42.445000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.446000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.446000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.446000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.446000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.446000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.446000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.486000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.486000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.486000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.486000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.486000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.501000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.501000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.502000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.502000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.502000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.671000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.671000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.671000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.671000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.671000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.997000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.998000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.998000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.998000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.998000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.998000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:14:42.998000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:43.030000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:43.030000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:43.030000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:43.030000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:43.030000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:43.102000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:43.102000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:43.103000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:43.103000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:43.103000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.307000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.313000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.319000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.319000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.765000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.765000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.765000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.765000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.765000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.765000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.765000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.796000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.796000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.796000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.796000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:44.796000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.149000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.149000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.149000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.149000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.149000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.149000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.150000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.150000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.452000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.452000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.452000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.452000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.452000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.796000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0624 02:14:45.802000 140099713591104 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0624 02:14:49.914669 16724 finetune.py:68] layer 20_down @ epoch 2 new loss 0.002104861428961158 old loss 0.0021049256902188063 BETTER
I0624 02:14:52.560540 18240 finetune.py:45] layer 23_down initial loss 0.0030976247508078814
W0624 02:14:52.560942 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:15:07.312867 17734 finetune.py:68] layer 22_down @ epoch 1 new loss 0.0027566272765398026 old loss 0.0027567336801439524 BETTER
I0624 02:15:10.308091 17228 finetune.py:68] layer 21_down @ epoch 2 new loss 0.0023154267109930515 old loss 0.0023154953960329294 BETTER
I0624 02:15:21.105892 18240 finetune.py:68] layer 23_down @ epoch 0 new loss 0.0030967965722084045 old loss 0.0030976247508078814 BETTER
I0624 02:15:21.758479 16724 finetune.py:68] layer 20_down @ epoch 3 new loss 0.002104753628373146 old loss 0.002104861428961158 BETTER
I0624 02:15:37.202039 17734 finetune.py:68] layer 22_down @ epoch 2 new loss 0.002756440080702305 old loss 0.0027566272765398026 BETTER
I0624 02:15:40.515723 17228 finetune.py:68] layer 21_down @ epoch 3 new loss 0.002315354300662875 old loss 0.0023154267109930515 BETTER
I0624 02:15:50.020915 18240 finetune.py:68] layer 23_down @ epoch 1 new loss 0.003096672473475337 old loss 0.0030967965722084045 BETTER
I0624 02:15:53.138866 16724 finetune.py:68] layer 20_down @ epoch 4 new loss 0.0021046679466962814 old loss 0.002104753628373146 BETTER
W0624 02:15:53.899403 16724 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

20_down proxy err 0.014168121851980686 tr(WHW.T) 239.271240234375
I0624 02:16:06.535338 17734 finetune.py:68] layer 22_down @ epoch 3 new loss 0.0027564046904444695 old loss 0.002756440080702305 BETTER
I0624 02:16:10.341830 17228 finetune.py:68] layer 21_down @ epoch 4 new loss 0.0023153480142354965 old loss 0.002315354300662875 BETTER
W0624 02:16:11.061905 17228 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

21_down proxy err 0.014698010869324207 tr(WHW.T) 249.3344268798828
I0624 02:16:18.819783 18240 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0030965697951614857 old loss 0.003096672473475337 BETTER
I0624 02:16:36.025580 17734 finetune.py:68] layer 22_down @ epoch 4 new loss 0.00275631807744503 old loss 0.0027564046904444695 BETTER
W0624 02:16:36.729871 17734 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

22_down proxy err 0.014993940480053425 tr(WHW.T) 279.95623779296875
I0624 02:16:47.583249 18240 finetune.py:68] layer 23_down @ epoch 3 new loss 0.003096464555710554 old loss 0.0030965697951614857 BETTER
I0624 02:17:16.359173 18240 finetune.py:68] layer 23_down @ epoch 4 new loss 0.003096359083428979 old loss 0.003096464555710554 BETTER
I0624 02:17:16.482606 4970 quantize_finetune_llama.py:193] computed original embedding for layer 24 in 61.8048300743103s
I0624 02:17:16.890537 4970 quantize_finetune_llama.py:162] layer 25 gpu 1
W0624 02:17:17.064693 18240 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:324: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

23_down proxy err 0.015401185490190983 tr(WHW.T) 296.2626953125
I0624 02:17:18.866583 25802 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 02:17:18.866684 25802 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 02:17:18.866740 25802 utils.py:162] NumExpr defaulting to 16 threads.
I0624 02:17:19.050015 25802 config.py:58] PyTorch version 2.4.0 available.
I0624 02:17:21.734867 25802 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 02:17:22.112314 25802 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.41s/it]  6%|▋         | 2/32 [00:01<00:22,  1.30it/s]  9%|▉         | 3/32 [00:02<00:16,  1.78it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.59it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.84it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.86it/s] 50%|█████     | 16/32 [00:06<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.87it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.86it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
W0624 02:17:38.874000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.874000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.874000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.874000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.874000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.875000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.875000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.900000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.900000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.901000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.901000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.901000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.917000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.917000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.917000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.917000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:17:38.917000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:17:39.243000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:17:39.243000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:17:39.243000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:17:39.243000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:17:39.243000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.126000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.126000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.126000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.126000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.126000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.126000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.126000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.144000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.144000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.144000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.144000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.144000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.381000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.381000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.381000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.381000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:17:40.382000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.568000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.568000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.568000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.568000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.568000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.568000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.568000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.586000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.586000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.586000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.587000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:17:41.587000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:17:42.485000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:17:42.486000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:17:42.486000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:17:42.486000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:17:42.486000 139935112808256 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 02:17:50.007298 25802 finetune.py:45] layer 24_v initial loss 0.000951144378632307
W0624 02:17:50.008092 25802 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:18:14.461924 4970 quantize_finetune_llama.py:193] computed original embedding for layer 25 in 57.12645149230957s
I0624 02:18:14.792773 4970 quantize_finetune_llama.py:162] layer 26 gpu 2
I0624 02:18:16.881085 26892 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 02:18:16.881222 26892 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 02:18:16.881294 26892 utils.py:162] NumExpr defaulting to 16 threads.
I0624 02:18:17.090952 26892 config.py:58] PyTorch version 2.4.0 available.
I0624 02:18:19.562264 26892 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 02:18:19.939481 26892 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:47,  1.53s/it]  6%|▋         | 2/32 [00:01<00:25,  1.20it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.96it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.21it/s]I0624 02:18:24.012011 25802 finetune.py:68] layer 24_v @ epoch 0 new loss 0.0003692641039378941 old loss 0.000951144378632307 BETTER
 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.75it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
W0624 02:18:36.811000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.812000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.812000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.812000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.812000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.812000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.812000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.839000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.839000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.839000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.839000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.839000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.856000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.857000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.857000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.857000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:18:36.857000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:18:37.181000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:18:37.181000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:18:37.181000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:18:37.181000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:18:37.181000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.085000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.085000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.085000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.085000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.085000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.085000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.086000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.105000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.105000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.105000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.105000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.105000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.357000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.357000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.357000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.357000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:18:38.357000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.560000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.560000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.560000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.560000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.560000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.560000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.560000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.578000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.579000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.579000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.579000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:18:39.579000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:18:40.478000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:18:40.478000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:18:40.478000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:18:40.479000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:18:40.479000 140494506379072 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 02:18:46.451679 26892 finetune.py:45] layer 25_v initial loss 0.0013176080537959933
W0624 02:18:46.452334 26892 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:18:59.323698 25802 finetune.py:68] layer 24_v @ epoch 1 new loss 0.00029514459311030805 old loss 0.0003692641039378941 BETTER
I0624 02:19:14.204483 4970 quantize_finetune_llama.py:193] computed original embedding for layer 26 in 58.98995542526245s
I0624 02:19:14.590695 4970 quantize_finetune_llama.py:162] layer 27 gpu 3
I0624 02:19:16.746197 28042 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 02:19:16.746382 28042 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 02:19:16.746486 28042 utils.py:162] NumExpr defaulting to 16 threads.
I0624 02:19:16.977063 28042 config.py:58] PyTorch version 2.4.0 available.
I0624 02:19:18.625163 26892 finetune.py:68] layer 25_v @ epoch 0 new loss 0.0004409397370181978 old loss 0.0013176080537959933 BETTER
I0624 02:19:19.563711 28042 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0624 02:19:20.036114 28042 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.58s/it]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s]  9%|▉         | 3/32 [00:02<00:18,  1.58it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.91it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.67it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.70it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.74it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.75it/s] 50%|█████     | 16/32 [00:07<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.80it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0624 02:19:34.628000 25802 finetune.py:68] layer 24_v @ epoch 2 new loss 0.0002716959279496223 old loss 0.00029514459311030805 BETTER
W0624 02:19:37.176000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.176000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.176000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.176000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.176000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.176000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.177000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.203000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.203000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.203000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.203000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.203000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.220000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.220000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.221000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.221000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.221000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.553000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.554000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.554000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.554000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:19:37.554000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.467000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.467000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.468000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.468000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.468000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.468000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.468000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.486000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.487000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.487000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.487000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.487000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.738000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.738000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.738000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.738000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:19:38.738000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.947000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.948000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.948000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.948000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.948000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.948000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.948000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.967000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.967000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.967000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.967000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:19:39.967000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:19:40.909000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:19:40.909000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:19:40.909000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:19:40.909000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:19:40.909000 140288563570496 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 02:19:46.793511 28042 finetune.py:45] layer 26_v initial loss 0.0014117242535576224
W0624 02:19:46.793887 28042 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:19:51.038811 26892 finetune.py:68] layer 25_v @ epoch 1 new loss 0.00031545895035378635 old loss 0.0004409397370181978 BETTER
I0624 02:20:10.452979 25802 finetune.py:68] layer 24_v @ epoch 3 new loss 0.0002597327984403819 old loss 0.0002716959279496223 BETTER
I0624 02:20:13.135106 4970 quantize_finetune_llama.py:193] computed original embedding for layer 27 in 57.876155853271484s
I0624 02:20:13.482858 4970 quantize_finetune_llama.py:162] layer 28 gpu 0
I0624 02:20:15.591137 29167 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0624 02:20:15.591253 29167 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0624 02:20:15.591322 29167 utils.py:162] NumExpr defaulting to 16 threads.
I0624 02:20:15.776212 29167 config.py:58] PyTorch version 2.4.0 available.
I0624 02:20:18.272297 29167 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0624 02:20:18.516258 28042 finetune.py:68] layer 26_v @ epoch 0 new loss 0.0005252904375083745 old loss 0.0014117242535576224 BETTER
W0624 02:20:18.726170 29167 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.
Token is valid (permission: fineGrained).
Your token has been saved to /root/.cache/huggingface/token
Login successful
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:54,  1.75s/it]  6%|▋         | 2/32 [00:02<00:27,  1.08it/s]  9%|▉         | 3/32 [00:02<00:19,  1.51it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.85it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.12it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s]I0624 02:20:23.731873 26892 finetune.py:68] layer 25_v @ epoch 2 new loss 0.00028343743178993464 old loss 0.00031545895035378635 BETTER
 25%|██▌       | 8/32 [00:04<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 50%|█████     | 16/32 [00:07<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0624 02:20:35.599000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.600000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.600000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.600000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.600000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.600000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.600000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.627000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.627000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.628000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.628000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.628000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.645000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.645000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.645000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.645000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.645000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.982000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.982000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.982000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.982000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0624 02:20:35.983000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.903000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.903000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.903000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.903000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.903000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.903000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.903000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.921000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.921000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.921000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.921000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:20:36.921000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:20:37.174000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0624 02:20:37.174000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0624 02:20:37.174000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0624 02:20:37.175000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0624 02:20:37.175000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.377000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.377000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.377000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.377000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.377000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.377000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.378000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.396000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.397000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.397000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.397000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:20:38.397000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0624 02:20:39.353000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0624 02:20:39.353000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0624 02:20:39.353000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0624 02:20:39.353000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0624 02:20:39.353000 140174081587008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0624 02:20:44.933657 29167 finetune.py:45] layer 27_v initial loss 0.0015726700657978654
W0624 02:20:44.933847 29167 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:20:45.487190 25802 finetune.py:68] layer 24_v @ epoch 4 new loss 0.00025243835989385843 old loss 0.0002597327984403819 BETTER
W0624 02:20:47.007779 25802 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_v proxy err 0.01448383554816246 tr(WHW.T) 1442.45458984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:19,  1.56it/s]  6%|▋         | 2/32 [00:00<00:13,  2.16it/s]  9%|▉         | 3/32 [00:01<00:11,  2.44it/s] 12%|█▎        | 4/32 [00:01<00:10,  2.62it/s] 16%|█▌        | 5/32 [00:02<00:09,  2.72it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.79it/s] 22%|██▏       | 7/32 [00:02<00:08,  2.84it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.87it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.90it/s]I0624 02:20:51.478886 28042 finetune.py:68] layer 26_v @ epoch 1 new loss 0.0004361722385510802 old loss 0.0005252904375083745 BETTER
 31%|███▏      | 10/32 [00:03<00:07,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.99it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.04it/s] 41%|████      | 13/32 [00:04<00:06,  3.08it/s] 44%|████▍     | 14/32 [00:04<00:05,  3.11it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.09it/s] 50%|█████     | 16/32 [00:05<00:05,  3.09it/s] 53%|█████▎    | 17/32 [00:05<00:04,  3.11it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.12it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.13it/s] 62%|██████▎   | 20/32 [00:06<00:03,  3.11it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.13it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.14it/s] 72%|███████▏  | 23/32 [00:07<00:02,  3.14it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.15it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.13it/s]I0624 02:20:56.761676 26892 finetune.py:68] layer 25_v @ epoch 3 new loss 0.00026930353487841785 old loss 0.00028343743178993464 BETTER
 81%|████████▏ | 26/32 [00:08<00:01,  3.14it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.16it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.18it/s] 91%|█████████ | 29/32 [00:09<00:00,  3.18it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.18it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.18it/s]100%|██████████| 32/32 [00:10<00:00,  3.17it/s]100%|██████████| 32/32 [00:10<00:00,  3.00it/s]
I0624 02:21:06.435019 25802 finetune.py:45] layer 24_q initial loss 0.0003415488463360816
W0624 02:21:06.435369 25802 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:21:16.501771 29167 finetune.py:68] layer 27_v @ epoch 0 new loss 0.0004913341253995895 old loss 0.0015726700657978654 BETTER
I0624 02:21:24.785552 28042 finetune.py:68] layer 26_v @ epoch 2 new loss 0.0004089906287845224 old loss 0.0004361722385510802 BETTER
I0624 02:21:29.933084 26892 finetune.py:68] layer 25_v @ epoch 4 new loss 0.0002610387746244669 old loss 0.00026930353487841785 BETTER
W0624 02:21:31.473022 26892 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

25_v proxy err 0.014875008724629879 tr(WHW.T) 1782.2060546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:20,  1.53it/s]  6%|▋         | 2/32 [00:01<00:14,  2.07it/s]  9%|▉         | 3/32 [00:01<00:12,  2.35it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.51it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.61it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.68it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.77it/s] 41%|████      | 13/32 [00:04<00:06,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.77it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s]I0624 02:21:41.523315 25802 finetune.py:68] layer 24_q @ epoch 0 new loss 0.0003263216931372881 old loss 0.0003415488463360816 BETTER
 75%|███████▌  | 24/32 [00:08<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.76it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.74it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.75it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
I0624 02:21:48.614974 29167 finetune.py:68] layer 27_v @ epoch 1 new loss 0.0004114361072424799 old loss 0.0004913341253995895 BETTER
I0624 02:21:51.205679 26892 finetune.py:45] layer 25_q initial loss 0.0003526759974192828
W0624 02:21:51.206060 26892 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0624 02:21:57.963059 28042 finetune.py:68] layer 26_v @ epoch 3 new loss 0.00039454031502828 old loss 0.0004089906287845224 BETTER
I0624 02:22:17.174016 25802 finetune.py:68] layer 24_q @ epoch 1 new loss 0.00031985031091608107 old loss 0.0003263216931372881 BETTER
