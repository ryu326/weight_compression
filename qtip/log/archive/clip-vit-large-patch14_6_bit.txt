I0416 08:14:26.257546 3179540 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:26.257649 3179540 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:26.257699 3179540 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:26.373338 3179540 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:28.554249 3179540 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

I0416 08:14:29.793117 3179540 quantize_finetune_clip.py:141] loaded model
I0416 08:14:29.793229 3179540 quantize_finetune_clip.py:143] loaded dataset and devset
I0416 08:14:29.845764 3179540 quantize_finetune_clip.py:151] vision layer 0 gpu 0
I0416 08:14:30.499042 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 0 in 0.51s
I0416 08:14:31.120570 3179540 quantize_finetune_clip.py:151] vision layer 1 gpu 0
I0416 08:14:33.593942 3182029 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:33.594138 3182029 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:33.594201 3182029 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:33.711770 3182029 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:36.047758 3182029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:03<00:27,  3.87s/it] 25%|██▌       | 2/8 [00:04<00:10,  1.80s/it] 38%|███▊      | 3/8 [00:04<00:05,  1.14s/it] 50%|█████     | 4/8 [00:04<00:03,  1.22it/s] 62%|██████▎   | 5/8 [00:05<00:01,  1.56it/s] 75%|███████▌  | 6/8 [00:05<00:01,  1.86it/s] 88%|████████▊ | 7/8 [00:05<00:00,  2.12it/s]100%|██████████| 8/8 [00:06<00:00,  2.34it/s]100%|██████████| 8/8 [00:06<00:00,  1.28it/s]
W0416 08:14:43.864627 3182029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_0_v proxy err 6.202951772138476e-05 tr(WHW.T) 28.12407684326172
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.71it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.35it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.63it/s] 50%|█████     | 4/8 [00:01<00:01,  2.79it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  2.80it/s]100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
W0416 08:14:48.178243 3182029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_0_q proxy err 1.0156800271943212e-05 tr(WHW.T) 372.64678955078125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.63it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.26it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.58it/s] 50%|█████     | 4/8 [00:01<00:01,  2.78it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.90it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  2.76it/s]
W0416 08:14:52.466727 3182029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_0_k proxy err 1.7231295714736916e-05 tr(WHW.T) 187.16102600097656
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.74it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.65it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
W0416 08:14:56.742707 3182029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_0_o proxy err 3.0724044336238876e-05 tr(WHW.T) 13.379737854003906
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:03<00:24,  3.52s/it] 25%|██▌       | 2/8 [00:03<00:09,  1.67s/it] 38%|███▊      | 3/8 [00:04<00:05,  1.07s/it] 50%|█████     | 4/8 [00:04<00:03,  1.26it/s] 62%|██████▎   | 5/8 [00:04<00:01,  1.56it/s] 75%|███████▌  | 6/8 [00:05<00:01,  1.84it/s] 88%|████████▊ | 7/8 [00:05<00:00,  2.08it/s]100%|██████████| 8/8 [00:06<00:00,  2.29it/s]100%|██████████| 8/8 [00:06<00:00,  1.32it/s]
W0416 08:15:04.278113 3182029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_0_fc1 proxy err 0.00010381131141912192 tr(WHW.T) 21288.48046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:36,  1.16s/it]  6%|▋         | 2/32 [00:01<00:20,  1.46it/s]  9%|▉         | 3/32 [00:01<00:15,  1.89it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.84it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.88it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.90it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
W0416 08:15:17.954056 3182029 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_0_fc2 proxy err 5.867925210623071e-06 tr(WHW.T) 112.2016830444336
I0416 08:15:19.851882 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 1 in 0.32s
I0416 08:15:20.022107 3179540 quantize_finetune_clip.py:151] vision layer 2 gpu 0
I0416 08:15:22.470468 3193098 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:22.470591 3193098 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:22.470655 3193098 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:22.598253 3193098 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:24.982515 3193098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.66s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.14it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.61it/s] 50%|█████     | 4/8 [00:02<00:02,  1.99it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.30it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.55it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.71it/s]100%|██████████| 8/8 [00:03<00:00,  2.81it/s]100%|██████████| 8/8 [00:03<00:00,  2.05it/s]
W0416 08:15:30.469329 3193098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_1_v proxy err 9.866024629445747e-05 tr(WHW.T) 68.71031951904297
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.74it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.65it/s] 50%|█████     | 4/8 [00:01<00:01,  2.80it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
W0416 08:15:34.707643 3193098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_1_q proxy err 1.74065244209487e-05 tr(WHW.T) 529.9490356445312
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.57it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.19it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.55it/s] 50%|█████     | 4/8 [00:01<00:01,  2.73it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.83it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.76it/s]
W0416 08:15:38.977174 3193098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_1_k proxy err 2.8947317332495004e-05 tr(WHW.T) 303.2245788574219
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.76it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.37it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.64it/s] 50%|█████     | 4/8 [00:01<00:01,  2.79it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.89it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
W0416 08:15:43.180768 3193098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_1_o proxy err 4.298039493733086e-05 tr(WHW.T) 21.233436584472656
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.32s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.32it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.76it/s] 50%|█████     | 4/8 [00:02<00:01,  2.07it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.29it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.45it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.60it/s]100%|██████████| 8/8 [00:03<00:00,  2.67it/s]100%|██████████| 8/8 [00:03<00:00,  2.12it/s]
W0416 08:15:48.516805 3193098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_1_fc1 proxy err 1.3238119208835997e-05 tr(WHW.T) 7345.1005859375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.09s/it]  6%|▋         | 2/32 [00:01<00:19,  1.52it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.84it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.94it/s] 41%|████      | 13/32 [00:05<00:06,  2.94it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.92it/s] 50%|█████     | 16/32 [00:06<00:05,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.93it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.95it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.92it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.91it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.93it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.91it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.93it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
W0416 08:16:01.753907 3193098 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_1_fc2 proxy err 1.597955633769743e-05 tr(WHW.T) 65.21305847167969
I0416 08:16:03.567409 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 2 in 0.29s
I0416 08:16:03.692157 3179540 quantize_finetune_clip.py:151] vision layer 3 gpu 0
I0416 08:16:06.204455 3203871 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:06.204596 3203871 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:06.204663 3203871 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:06.327260 3203871 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:08.634717 3203871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:12,  1.83s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.05it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.50it/s] 50%|█████     | 4/8 [00:02<00:02,  1.81it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.09it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.31it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.49it/s]100%|██████████| 8/8 [00:04<00:00,  2.55it/s]100%|██████████| 8/8 [00:04<00:00,  1.87it/s]
W0416 08:16:14.474905 3203871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_2_v proxy err 0.00016279215924441814 tr(WHW.T) 61.11924743652344
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.27it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.54it/s] 50%|█████     | 4/8 [00:01<00:01,  2.67it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.74it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.81it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.85it/s]100%|██████████| 8/8 [00:02<00:00,  2.90it/s]100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
W0416 08:16:18.822126 3203871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_2_q proxy err 8.514362707501277e-05 tr(WHW.T) 195.00669860839844
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.48it/s] 25%|██▌       | 2/8 [00:01<00:02,  2.11it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.42it/s] 50%|█████     | 4/8 [00:01<00:01,  2.60it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.71it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.78it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.84it/s]100%|██████████| 8/8 [00:03<00:00,  2.86it/s]100%|██████████| 8/8 [00:03<00:00,  2.62it/s]
W0416 08:16:23.291925 3203871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_2_k proxy err 6.960491737117991e-05 tr(WHW.T) 206.7851104736328
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.62it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.21it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.50it/s] 50%|█████     | 4/8 [00:01<00:01,  2.65it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.76it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.86it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.92it/s]100%|██████████| 8/8 [00:02<00:00,  2.92it/s]100%|██████████| 8/8 [00:02<00:00,  2.70it/s]
W0416 08:16:27.748508 3203871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_2_o proxy err 4.478358096093871e-05 tr(WHW.T) 12.587794303894043
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.30s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.32it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.74it/s] 50%|█████     | 4/8 [00:02<00:01,  2.04it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.24it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.41it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.49it/s]100%|██████████| 8/8 [00:03<00:00,  2.56it/s]100%|██████████| 8/8 [00:03<00:00,  2.07it/s]
W0416 08:16:33.312787 3203871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_2_fc1 proxy err 2.707852218009066e-05 tr(WHW.T) 11104.5673828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:37,  1.22s/it]  6%|▋         | 2/32 [00:01<00:21,  1.40it/s]  9%|▉         | 3/32 [00:01<00:16,  1.78it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.05it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.72it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.72it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.74it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0416 08:16:47.877639 3203871 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_2_fc2 proxy err 2.1910400391789153e-05 tr(WHW.T) 34.765602111816406
I0416 08:16:49.614495 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 3 in 0.17s
I0416 08:16:49.733382 3179540 quantize_finetune_clip.py:151] vision layer 4 gpu 0
I0416 08:16:52.082862 3215237 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:52.082995 3215237 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:52.083061 3215237 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:52.255103 3215237 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:54.658752 3215237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:12,  1.79s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.07it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.54it/s] 50%|█████     | 4/8 [00:02<00:02,  1.89it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.14it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.34it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.46it/s]100%|██████████| 8/8 [00:04<00:00,  2.58it/s]100%|██████████| 8/8 [00:04<00:00,  1.90it/s]
W0416 08:17:00.914277 3215237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_3_v proxy err 0.0002340648352401331 tr(WHW.T) 78.36602783203125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.26it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.54it/s] 50%|█████     | 4/8 [00:01<00:01,  2.63it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.73it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.61it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.73it/s]100%|██████████| 8/8 [00:03<00:00,  2.81it/s]100%|██████████| 8/8 [00:03<00:00,  2.63it/s]
W0416 08:17:05.435245 3215237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_3_q proxy err 8.060918480623513e-05 tr(WHW.T) 300.93536376953125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.64it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.22it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.52it/s] 50%|█████     | 4/8 [00:01<00:01,  2.38it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.57it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.71it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.81it/s]100%|██████████| 8/8 [00:03<00:00,  2.85it/s]100%|██████████| 8/8 [00:03<00:00,  2.61it/s]
W0416 08:17:09.906601 3215237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_3_k proxy err 7.504152745241299e-05 tr(WHW.T) 327.538330078125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.64it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.16it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.47it/s] 50%|█████     | 4/8 [00:01<00:01,  2.65it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.76it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.89it/s]100%|██████████| 8/8 [00:02<00:00,  2.93it/s]100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
W0416 08:17:14.461081 3215237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_3_o proxy err 5.587146370089613e-05 tr(WHW.T) 11.745527267456055
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.28s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.34it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.76it/s] 50%|█████     | 4/8 [00:02<00:01,  2.05it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.25it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.38it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.43it/s]100%|██████████| 8/8 [00:03<00:00,  2.52it/s]100%|██████████| 8/8 [00:03<00:00,  2.06it/s]
W0416 08:17:20.052805 3215237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_3_fc1 proxy err 2.4802238840493374e-05 tr(WHW.T) 6100.47265625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:34,  1.12s/it]  6%|▋         | 2/32 [00:01<00:20,  1.49it/s]  9%|▉         | 3/32 [00:01<00:15,  1.91it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.78it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
W0416 08:17:33.932827 3215237 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_3_fc2 proxy err 3.713479600264691e-05 tr(WHW.T) 40.130794525146484
I0416 08:17:35.377197 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 4 in 0.15s
I0416 08:17:35.498805 3179540 quantize_finetune_clip.py:151] vision layer 5 gpu 0
I0416 08:17:37.997211 3226896 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:37.997435 3226896 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:37.997506 3226896 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:38.128908 3226896 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:40.845366 3226896 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:12,  1.75s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.08it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.51it/s] 50%|█████     | 4/8 [00:02<00:02,  1.85it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.12it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.31it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.47it/s]100%|██████████| 8/8 [00:04<00:00,  2.57it/s]100%|██████████| 8/8 [00:04<00:00,  1.90it/s]
W0416 08:17:47.246068 3226896 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_4_v proxy err 0.0002158026909455657 tr(WHW.T) 95.10258483886719
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.65it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.18it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.27it/s] 50%|█████     | 4/8 [00:01<00:01,  2.46it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.56it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.65it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.70it/s]100%|██████████| 8/8 [00:03<00:00,  2.76it/s]100%|██████████| 8/8 [00:03<00:00,  2.54it/s]
W0416 08:17:51.947070 3226896 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_4_q proxy err 9.757776570040733e-05 tr(WHW.T) 333.9703369140625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.54it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.12it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.39it/s] 50%|█████     | 4/8 [00:01<00:01,  2.58it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.69it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.78it/s]100%|██████████| 8/8 [00:03<00:00,  2.81it/s]100%|██████████| 8/8 [00:03<00:00,  2.60it/s]
W0416 08:17:56.424175 3226896 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_4_k proxy err 8.760836499277502e-05 tr(WHW.T) 323.5253601074219
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.25it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.44it/s] 50%|█████     | 4/8 [00:01<00:01,  2.60it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.69it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.63it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.73it/s]100%|██████████| 8/8 [00:03<00:00,  2.81it/s]100%|██████████| 8/8 [00:03<00:00,  2.61it/s]
W0416 08:18:00.989763 3226896 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_4_o proxy err 5.9213150962023064e-05 tr(WHW.T) 18.242263793945312
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.33s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.30it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.68it/s] 50%|█████     | 4/8 [00:02<00:02,  1.98it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.18it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.35it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.47it/s]100%|██████████| 8/8 [00:03<00:00,  2.57it/s]100%|██████████| 8/8 [00:03<00:00,  2.04it/s]
W0416 08:18:06.652574 3226896 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_4_fc1 proxy err 4.366525536170229e-05 tr(WHW.T) 4969.951171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:35,  1.14s/it]  6%|▋         | 2/32 [00:01<00:20,  1.47it/s]  9%|▉         | 3/32 [00:01<00:15,  1.85it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.10it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.70it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.54it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.77it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.76it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0416 08:18:21.522190 3226896 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_4_fc2 proxy err 8.059709944063798e-05 tr(WHW.T) 29.563274383544922
I0416 08:18:23.125546 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 5 in 0.15s
I0416 08:18:23.318537 3179540 quantize_finetune_clip.py:151] vision layer 6 gpu 0
I0416 08:18:25.737217 3238852 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:25.737366 3238852 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:25.737432 3238852 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:25.865067 3238852 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:28.544155 3238852 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:12,  1.75s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.09it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.52it/s] 50%|█████     | 4/8 [00:02<00:02,  1.87it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.17it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.39it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.52it/s]100%|██████████| 8/8 [00:04<00:00,  2.62it/s]100%|██████████| 8/8 [00:04<00:00,  1.93it/s]
W0416 08:18:34.969185 3238852 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_5_v proxy err 0.00026181049179285765 tr(WHW.T) 151.44627380371094
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.31it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.56it/s] 50%|█████     | 4/8 [00:01<00:01,  2.68it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.76it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.81it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.78it/s]100%|██████████| 8/8 [00:03<00:00,  2.78it/s]100%|██████████| 8/8 [00:03<00:00,  2.66it/s]
W0416 08:18:39.378571 3238852 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_5_q proxy err 0.000156005538883619 tr(WHW.T) 316.5805969238281
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.58it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.14it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.40it/s] 50%|█████     | 4/8 [00:01<00:01,  2.54it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.64it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.70it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.73it/s]100%|██████████| 8/8 [00:03<00:00,  2.78it/s]100%|██████████| 8/8 [00:03<00:00,  2.57it/s]
W0416 08:18:43.973423 3238852 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_5_k proxy err 0.00015977789007592946 tr(WHW.T) 303.82757568359375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.25it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.53it/s] 50%|█████     | 4/8 [00:01<00:01,  2.69it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.81it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.87it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.90it/s]100%|██████████| 8/8 [00:02<00:00,  2.85it/s]100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
W0416 08:18:48.596822 3238852 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_5_o proxy err 8.100393461063504e-05 tr(WHW.T) 16.882675170898438
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.30s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.32it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.71it/s] 50%|█████     | 4/8 [00:02<00:02,  1.98it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.16it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.29it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.37it/s]100%|██████████| 8/8 [00:03<00:00,  2.43it/s]100%|██████████| 8/8 [00:03<00:00,  2.00it/s]
W0416 08:18:54.462495 3238852 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_5_fc1 proxy err 7.815154822310433e-05 tr(WHW.T) 4523.89453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:35,  1.14s/it]  6%|▋         | 2/32 [00:01<00:20,  1.46it/s]  9%|▉         | 3/32 [00:01<00:15,  1.86it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.70it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
W0416 08:19:08.876117 3238852 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_5_fc2 proxy err 0.00013456681335810572 tr(WHW.T) 25.434589385986328
I0416 08:19:10.468965 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 6 in 0.17s
I0416 08:19:10.594755 3179540 quantize_finetune_clip.py:151] vision layer 7 gpu 0
I0416 08:19:12.998309 3250605 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:12.998494 3250605 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:12.998564 3250605 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:13.132661 3250605 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:16.020108 3250605 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.70s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.11it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.51it/s] 50%|█████     | 4/8 [00:02<00:02,  1.79it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.07it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.31it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.50it/s]100%|██████████| 8/8 [00:04<00:00,  2.59it/s]100%|██████████| 8/8 [00:04<00:00,  1.90it/s]
W0416 08:19:22.471871 3250605 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_6_v proxy err 0.00042197623406536877 tr(WHW.T) 170.1556854248047
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.63it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.25it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.55it/s] 50%|█████     | 4/8 [00:01<00:01,  2.70it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.79it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.85it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.86it/s]100%|██████████| 8/8 [00:02<00:00,  2.89it/s]100%|██████████| 8/8 [00:02<00:00,  2.70it/s]
W0416 08:19:27.487020 3250605 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_6_q proxy err 0.0001886087702587247 tr(WHW.T) 456.3508605957031
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.28it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.50it/s] 50%|█████     | 4/8 [00:01<00:01,  2.63it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.72it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.82it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.87it/s]100%|██████████| 8/8 [00:02<00:00,  2.91it/s]100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
W0416 08:19:32.408703 3250605 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_6_k proxy err 0.00016097156913019717 tr(WHW.T) 510.3419189453125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.67it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.25it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.54it/s] 50%|█████     | 4/8 [00:01<00:01,  2.68it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.80it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.88it/s]100%|██████████| 8/8 [00:02<00:00,  2.87it/s]100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
W0416 08:19:37.035478 3250605 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_6_o proxy err 0.0001409349642926827 tr(WHW.T) 14.702461242675781
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.28s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.35it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.74it/s] 50%|█████     | 4/8 [00:02<00:01,  2.02it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.23it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.39it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.49it/s]100%|██████████| 8/8 [00:03<00:00,  2.58it/s]100%|██████████| 8/8 [00:03<00:00,  2.08it/s]
W0416 08:19:42.499775 3250605 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_6_fc1 proxy err 0.00012725224951282144 tr(WHW.T) 3307.18994140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.10s/it]  6%|▋         | 2/32 [00:01<00:19,  1.50it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.72it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.75it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.76it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.76it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.76it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.78it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.76it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.79it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0416 08:19:56.489380 3250605 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_6_fc2 proxy err 0.00011445215932326391 tr(WHW.T) 37.4859619140625
I0416 08:19:58.205778 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 7 in 0.19s
I0416 08:19:58.331835 3179540 quantize_finetune_clip.py:151] vision layer 8 gpu 0
I0416 08:20:00.628754 3258090 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:20:00.628890 3258090 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:20:00.628949 3258090 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:20:00.743378 3258090 config.py:58] PyTorch version 2.4.0 available.
W0416 08:20:03.263057 3258090 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:12,  1.85s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.04it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.46it/s] 50%|█████     | 4/8 [00:02<00:02,  1.82it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.11it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.34it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.53it/s]100%|██████████| 8/8 [00:04<00:00,  2.68it/s]100%|██████████| 8/8 [00:04<00:00,  1.90it/s]
W0416 08:20:09.768991 3258090 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_7_v proxy err 0.0004515734617598355 tr(WHW.T) 231.85989379882812
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.32it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.62it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.88it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
W0416 08:20:14.145741 3258090 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_7_q proxy err 0.00026466866256669164 tr(WHW.T) 471.35491943359375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.64it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.27it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.61it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.89it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
W0416 08:20:18.408133 3258090 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_7_k proxy err 0.00028731959173455834 tr(WHW.T) 440.3499755859375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.75it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.62it/s] 50%|█████     | 4/8 [00:01<00:01,  2.51it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.64it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.75it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.86it/s]100%|██████████| 8/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.70it/s]
W0416 08:20:22.775639 3258090 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_7_o proxy err 0.00014757242752239108 tr(WHW.T) 16.805908203125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.29s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.35it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.76it/s] 50%|█████     | 4/8 [00:02<00:01,  2.07it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.29it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.44it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.56it/s]100%|██████████| 8/8 [00:03<00:00,  2.63it/s]100%|██████████| 8/8 [00:03<00:00,  2.11it/s]
W0416 08:20:28.106079 3258090 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_7_fc1 proxy err 0.00014517067756969482 tr(WHW.T) 3763.969970703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.06s/it]  6%|▋         | 2/32 [00:01<00:19,  1.53it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.80it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.77it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.73it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
W0416 08:20:41.941174 3258090 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_7_fc2 proxy err 0.00014204993203748018 tr(WHW.T) 36.481971740722656
I0416 08:20:43.451904 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 8 in 0.14s
I0416 08:20:43.596486 3179540 quantize_finetune_clip.py:151] vision layer 9 gpu 0
I0416 08:20:45.972890 3263877 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:20:45.973056 3263877 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:20:45.973158 3263877 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:20:46.101031 3263877 config.py:58] PyTorch version 2.4.0 available.
W0416 08:20:48.775662 3263877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.70s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.12it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.56it/s] 50%|█████     | 4/8 [00:02<00:02,  1.93it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.22it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.43it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.58it/s]100%|██████████| 8/8 [00:04<00:00,  2.70it/s]100%|██████████| 8/8 [00:04<00:00,  1.98it/s]
W0416 08:20:54.963529 3263877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_8_v proxy err 0.0004225230950396508 tr(WHW.T) 293.3328857421875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.74it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.34it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.63it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
W0416 08:20:59.408769 3263877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_8_q proxy err 0.0003292192704975605 tr(WHW.T) 492.7628173828125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.26it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.56it/s] 50%|█████     | 4/8 [00:01<00:01,  2.76it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.89it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
W0416 08:21:03.674133 3263877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_8_k proxy err 0.00032102741533890367 tr(WHW.T) 478.1852722167969
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.67it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.26it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.57it/s] 50%|█████     | 4/8 [00:01<00:01,  2.73it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.82it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.88it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
W0416 08:21:08.034978 3263877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_8_o proxy err 0.00016329027130268514 tr(WHW.T) 21.60403823852539
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.35s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.29it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.71it/s] 50%|█████     | 4/8 [00:02<00:01,  2.01it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.25it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.43it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.56it/s]100%|██████████| 8/8 [00:03<00:00,  2.64it/s]100%|██████████| 8/8 [00:03<00:00,  2.08it/s]
W0416 08:21:13.567277 3263877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_8_fc1 proxy err 0.00015374532085843384 tr(WHW.T) 4153.708984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.08s/it]  6%|▋         | 2/32 [00:01<00:19,  1.53it/s]  9%|▉         | 3/32 [00:01<00:15,  1.90it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.77it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
W0416 08:21:27.656685 3263877 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_8_fc2 proxy err 0.00017274411220569164 tr(WHW.T) 36.89303207397461
I0416 08:21:29.222422 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 9 in 0.16s
I0416 08:21:29.340786 3179540 quantize_finetune_clip.py:151] vision layer 10 gpu 0
I0416 08:21:31.644855 3269713 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:21:31.644984 3269713 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:21:31.645045 3269713 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:21:31.783273 3269713 config.py:58] PyTorch version 2.4.0 available.
W0416 08:21:34.770380 3269713 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.63s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.15it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.61it/s] 50%|█████     | 4/8 [00:02<00:02,  1.96it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.26it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.48it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.64it/s]100%|██████████| 8/8 [00:03<00:00,  2.75it/s]100%|██████████| 8/8 [00:03<00:00,  2.03it/s]
W0416 08:21:41.031313 3269713 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_9_v proxy err 0.0004471441498026252 tr(WHW.T) 314.0332336425781
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.74it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.62it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
W0416 08:21:45.699643 3269713 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_9_q proxy err 0.00035140628460794687 tr(WHW.T) 482.75921630859375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.67it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.30it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.59it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
W0416 08:21:50.182103 3269713 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_9_k proxy err 0.00033522796002216637 tr(WHW.T) 495.71539306640625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.31it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.60it/s] 50%|█████     | 4/8 [00:01<00:01,  2.78it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.88it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
W0416 08:21:54.422351 3269713 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_9_o proxy err 0.0001779890590114519 tr(WHW.T) 20.151992797851562
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.26s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.37it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.79it/s] 50%|█████     | 4/8 [00:02<00:01,  2.09it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.30it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.46it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.57it/s]100%|██████████| 8/8 [00:03<00:00,  2.63it/s]100%|██████████| 8/8 [00:03<00:00,  2.13it/s]
W0416 08:21:59.777876 3269713 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_9_fc1 proxy err 0.00016637564112897962 tr(WHW.T) 4800.1669921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.08s/it]  6%|▋         | 2/32 [00:01<00:19,  1.54it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.80it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.67it/s]
W0416 08:22:13.378460 3269713 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_9_fc2 proxy err 0.00020687479991465807 tr(WHW.T) 32.10093688964844
I0416 08:22:14.894315 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 10 in 0.15s
I0416 08:22:15.015246 3179540 quantize_finetune_clip.py:151] vision layer 11 gpu 0
I0416 08:22:17.285349 3273380 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:22:17.285474 3273380 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:22:17.285532 3273380 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:22:17.400802 3273380 config.py:58] PyTorch version 2.4.0 available.
W0416 08:22:19.649092 3273380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.69s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.12it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.57it/s] 50%|█████     | 4/8 [00:02<00:02,  1.95it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.26it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.49it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.66it/s]100%|██████████| 8/8 [00:03<00:00,  2.79it/s]100%|██████████| 8/8 [00:03<00:00,  2.02it/s]
W0416 08:22:25.067553 3273380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_10_v proxy err 0.0005400259979069233 tr(WHW.T) 327.5375671386719
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.76it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.60it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
W0416 08:22:29.307573 3273380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_10_q proxy err 0.0004397815209813416 tr(WHW.T) 541.7548217773438
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.75it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.37it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.83it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.86it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.91it/s]100%|██████████| 8/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
W0416 08:22:33.552608 3273380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_10_k proxy err 0.00043325958540663123 tr(WHW.T) 543.0531005859375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.76it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.38it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.69it/s] 50%|█████     | 4/8 [00:01<00:01,  2.84it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.95it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  3.06it/s]100%|██████████| 8/8 [00:02<00:00,  2.86it/s]
W0416 08:22:37.717098 3273380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_10_o proxy err 0.0002179869043175131 tr(WHW.T) 19.04693031311035
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.26s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.38it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.81it/s] 50%|█████     | 4/8 [00:02<00:01,  2.12it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.34it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.49it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.61it/s]100%|██████████| 8/8 [00:03<00:00,  2.68it/s]100%|██████████| 8/8 [00:03<00:00,  2.16it/s]
W0416 08:22:42.935589 3273380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_10_fc1 proxy err 0.0001534284820081666 tr(WHW.T) 6493.8876953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.06s/it]  6%|▋         | 2/32 [00:01<00:19,  1.54it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.78it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
W0416 08:22:56.623513 3273380 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_10_fc2 proxy err 0.0002226772194262594 tr(WHW.T) 29.747730255126953
I0416 08:22:58.071798 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 11 in 0.15s
I0416 08:22:58.201769 3179540 quantize_finetune_clip.py:151] vision layer 12 gpu 0
I0416 08:23:00.490179 3276848 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:23:00.490313 3276848 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:23:00.490376 3276848 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:23:00.612900 3276848 config.py:58] PyTorch version 2.4.0 available.
W0416 08:23:02.854798 3276848 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.66s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.14it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.60it/s] 50%|█████     | 4/8 [00:02<00:02,  1.98it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.26it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.48it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.66it/s]100%|██████████| 8/8 [00:03<00:00,  2.77it/s]100%|██████████| 8/8 [00:03<00:00,  2.03it/s]
W0416 08:23:08.190274 3276848 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_11_v proxy err 0.0006226520054042339 tr(WHW.T) 309.68670654296875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.34it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.64it/s] 50%|█████     | 4/8 [00:01<00:01,  2.79it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.83it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.88it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
W0416 08:23:13.187749 3276848 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_11_q proxy err 0.0004196914378553629 tr(WHW.T) 642.15869140625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.61it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.23it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.54it/s] 50%|█████     | 4/8 [00:01<00:01,  2.70it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.84it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
W0416 08:23:18.751209 3276848 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_11_k proxy err 0.00041262892773374915 tr(WHW.T) 620.22705078125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.76it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.64it/s] 50%|█████     | 4/8 [00:01<00:01,  2.75it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.82it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]100%|██████████| 8/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
W0416 08:23:23.426236 3276848 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_11_o proxy err 0.00023883608810137957 tr(WHW.T) 19.117759704589844
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.27s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.36it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.78it/s] 50%|█████     | 4/8 [00:02<00:01,  2.08it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.29it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.44it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.53it/s]100%|██████████| 8/8 [00:03<00:00,  2.62it/s]100%|██████████| 8/8 [00:03<00:00,  2.12it/s]
W0416 08:23:28.843986 3276848 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_11_fc1 proxy err 0.00016138522187247872 tr(WHW.T) 7438.43212890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.09s/it]  6%|▋         | 2/32 [00:01<00:19,  1.53it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.83it/s] 50%|█████     | 16/32 [00:06<00:05,  2.85it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
W0416 08:23:42.570315 3276848 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_11_fc2 proxy err 0.00018423906294628978 tr(WHW.T) 33.167724609375
I0416 08:23:43.996712 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 12 in 0.15s
I0416 08:23:44.128840 3179540 quantize_finetune_clip.py:151] vision layer 13 gpu 0
I0416 08:23:46.423204 3280407 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:23:46.423348 3280407 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:23:46.423409 3280407 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:23:46.540091 3280407 config.py:58] PyTorch version 2.4.0 available.
W0416 08:23:48.861645 3280407 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.63s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.15it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.61it/s] 50%|█████     | 4/8 [00:02<00:02,  1.96it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.23it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.43it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.60it/s]100%|██████████| 8/8 [00:03<00:00,  2.72it/s]100%|██████████| 8/8 [00:03<00:00,  2.01it/s]
W0416 08:23:54.313461 3280407 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_12_v proxy err 0.0007133936160244048 tr(WHW.T) 274.07757568359375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.74it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.34it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.64it/s] 50%|█████     | 4/8 [00:01<00:01,  2.79it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.90it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.90it/s]100%|██████████| 8/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
W0416 08:23:58.577689 3280407 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_12_q proxy err 0.000472038023872301 tr(WHW.T) 615.548828125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.31it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.59it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.84it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.91it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
W0416 08:24:02.813668 3280407 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_12_k proxy err 0.0004763667529914528 tr(WHW.T) 612.162109375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.63it/s] 50%|█████     | 4/8 [00:01<00:01,  2.79it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.89it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
W0416 08:24:07.070502 3280407 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_12_o proxy err 0.00023881370725575835 tr(WHW.T) 13.876964569091797
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.27s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.36it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.78it/s] 50%|█████     | 4/8 [00:02<00:01,  2.07it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.24it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.38it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.50it/s]100%|██████████| 8/8 [00:03<00:00,  2.58it/s]100%|██████████| 8/8 [00:03<00:00,  2.10it/s]
W0416 08:24:12.501159 3280407 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_12_fc1 proxy err 0.00017944356659427285 tr(WHW.T) 7523.9033203125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.08s/it]  6%|▋         | 2/32 [00:01<00:19,  1.54it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.85it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.85it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.87it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.88it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
W0416 08:24:26.322714 3280407 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_12_fc2 proxy err 3.824735540547408e-05 tr(WHW.T) 199.962646484375
I0416 08:24:27.735336 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 13 in 0.15s
I0416 08:24:27.852094 3179540 quantize_finetune_clip.py:151] vision layer 14 gpu 0
I0416 08:24:30.109516 3283975 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:24:30.109637 3283975 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:24:30.109695 3283975 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:24:30.242222 3283975 config.py:58] PyTorch version 2.4.0 available.
W0416 08:24:33.088385 3283975 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.63s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.16it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.63it/s] 50%|█████     | 4/8 [00:02<00:01,  2.01it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.31it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.53it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.70it/s]100%|██████████| 8/8 [00:03<00:00,  2.83it/s]100%|██████████| 8/8 [00:03<00:00,  2.07it/s]
W0416 08:24:39.091398 3283975 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_13_v proxy err 0.0008119037956930697 tr(WHW.T) 298.0824890136719
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.31it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.59it/s] 50%|█████     | 4/8 [00:01<00:01,  2.75it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  2.76it/s]
W0416 08:24:44.048350 3283975 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_13_q proxy err 0.0004922557855024934 tr(WHW.T) 650.8009033203125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.64it/s] 50%|█████     | 4/8 [00:01<00:01,  2.80it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.90it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
W0416 08:24:48.604167 3283975 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_13_k proxy err 0.00048146568587981164 tr(WHW.T) 669.0814819335938
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.74it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.37it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.68it/s] 50%|█████     | 4/8 [00:01<00:01,  2.85it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.96it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]100%|██████████| 8/8 [00:02<00:00,  3.11it/s]100%|██████████| 8/8 [00:02<00:00,  2.88it/s]
W0416 08:24:52.802288 3283975 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_13_o proxy err 0.000328969385009259 tr(WHW.T) 9.406055450439453
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.27s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.35it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.77it/s] 50%|█████     | 4/8 [00:02<00:01,  2.06it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.29it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.45it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.57it/s]100%|██████████| 8/8 [00:03<00:00,  2.65it/s]100%|██████████| 8/8 [00:03<00:00,  2.12it/s]
W0416 08:24:58.178828 3283975 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_13_fc1 proxy err 0.00019973232701886445 tr(WHW.T) 7018.62451171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:34,  1.11s/it]  6%|▋         | 2/32 [00:01<00:19,  1.52it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.78it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.87it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.89it/s] 50%|█████     | 16/32 [00:06<00:05,  2.91it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.94it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.92it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.91it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.90it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.90it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
W0416 08:25:11.729351 3283975 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_13_fc2 proxy err 0.00030122892349027097 tr(WHW.T) 20.70946502685547
I0416 08:25:13.207902 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 14 in 0.17s
I0416 08:25:13.325345 3179540 quantize_finetune_clip.py:151] vision layer 15 gpu 0
I0416 08:25:15.682312 3287543 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:25:15.682549 3287543 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:25:15.682612 3287543 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:25:15.803206 3287543 config.py:58] PyTorch version 2.4.0 available.
W0416 08:25:18.102944 3287543 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.62s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.16it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.62it/s] 50%|█████     | 4/8 [00:02<00:02,  2.00it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.28it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.51it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.67it/s]100%|██████████| 8/8 [00:03<00:00,  2.78it/s]100%|██████████| 8/8 [00:03<00:00,  2.05it/s]
W0416 08:25:23.519664 3287543 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_14_v proxy err 0.0008707046508789062 tr(WHW.T) 284.17218017578125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.31it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.61it/s] 50%|█████     | 4/8 [00:01<00:01,  2.75it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
W0416 08:25:27.836173 3287543 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_14_q proxy err 0.0005643717013299465 tr(WHW.T) 660.6932373046875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.31it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.60it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
W0416 08:25:32.137543 3287543 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_14_k proxy err 0.0005168752977624536 tr(WHW.T) 724.1702880859375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.35it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.64it/s] 50%|█████     | 4/8 [00:01<00:01,  2.80it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.88it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
W0416 08:25:36.436985 3287543 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_14_o proxy err 0.0003205331158824265 tr(WHW.T) 8.858718872070312
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.28s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.35it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.75it/s] 50%|█████     | 4/8 [00:02<00:01,  2.04it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.27it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.41it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.54it/s]100%|██████████| 8/8 [00:03<00:00,  2.63it/s]100%|██████████| 8/8 [00:03<00:00,  2.11it/s]
W0416 08:25:41.871802 3287543 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_14_fc1 proxy err 0.00021607305097859353 tr(WHW.T) 8141.02294921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.10s/it]  6%|▋         | 2/32 [00:01<00:19,  1.52it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.65it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.83it/s] 41%|████      | 13/32 [00:05<00:06,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.93it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.93it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.91it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.90it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
W0416 08:25:55.461077 3287543 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_14_fc2 proxy err 0.0002842455287463963 tr(WHW.T) 22.776063919067383
I0416 08:25:56.977581 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 15 in 0.18s
I0416 08:25:57.107756 3179540 quantize_finetune_clip.py:151] vision layer 16 gpu 0
I0416 08:25:59.555169 3291100 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:25:59.555318 3291100 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:25:59.555382 3291100 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:25:59.689142 3291100 config.py:58] PyTorch version 2.4.0 available.
W0416 08:26:02.506534 3291100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.65s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.14it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.60it/s] 50%|█████     | 4/8 [00:02<00:02,  1.96it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.21it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.43it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.63it/s]100%|██████████| 8/8 [00:03<00:00,  2.76it/s]100%|██████████| 8/8 [00:03<00:00,  2.02it/s]
W0416 08:26:08.541366 3291100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_15_v proxy err 0.0008728501852601767 tr(WHW.T) 285.29705810546875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.75it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.39it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.70it/s] 50%|█████     | 4/8 [00:01<00:01,  2.87it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.93it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  3.10it/s]100%|██████████| 8/8 [00:02<00:00,  2.87it/s]
W0416 08:26:12.890773 3291100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_15_q proxy err 0.0005872739711776376 tr(WHW.T) 650.9993286132812
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.71it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.35it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.66it/s] 50%|█████     | 4/8 [00:01<00:01,  2.82it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.93it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  2.84it/s]
W0416 08:26:17.089403 3291100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_15_k proxy err 0.0005106036551296711 tr(WHW.T) 748.6654052734375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.78it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.39it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.68it/s] 50%|█████     | 4/8 [00:01<00:01,  2.85it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.94it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  3.06it/s]100%|██████████| 8/8 [00:02<00:00,  2.85it/s]
W0416 08:26:21.322643 3291100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_15_o proxy err 0.00027190844411961734 tr(WHW.T) 9.187652587890625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.26s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.37it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.78it/s] 50%|█████     | 4/8 [00:02<00:01,  2.07it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.29it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.44it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.51it/s]100%|██████████| 8/8 [00:03<00:00,  2.58it/s]100%|██████████| 8/8 [00:03<00:00,  2.11it/s]
W0416 08:26:26.672701 3291100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_15_fc1 proxy err 0.00023490660532843322 tr(WHW.T) 8731.9501953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.09s/it]  6%|▋         | 2/32 [00:01<00:19,  1.53it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.87it/s] 50%|█████     | 16/32 [00:06<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.89it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.89it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.89it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.89it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.90it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.89it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.70it/s]
W0416 08:26:40.063788 3291100 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_15_fc2 proxy err 0.00034391923691146076 tr(WHW.T) 21.015804290771484
I0416 08:26:41.611919 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 16 in 0.15s
I0416 08:26:41.737975 3179540 quantize_finetune_clip.py:151] vision layer 17 gpu 0
I0416 08:26:44.003371 3294648 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:26:44.003629 3294648 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:26:44.003691 3294648 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:26:44.125888 3294648 config.py:58] PyTorch version 2.4.0 available.
W0416 08:26:46.529236 3294648 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.62s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.17it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.64it/s] 50%|█████     | 4/8 [00:02<00:01,  2.01it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.31it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.53it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.70it/s]100%|██████████| 8/8 [00:03<00:00,  2.80it/s]100%|██████████| 8/8 [00:03<00:00,  2.07it/s]
W0416 08:26:51.913516 3294648 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_16_v proxy err 0.0009762296685948968 tr(WHW.T) 323.1610412597656
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.76it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.37it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.65it/s] 50%|█████     | 4/8 [00:01<00:01,  2.82it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.92it/s]100%|██████████| 8/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
W0416 08:26:56.169523 3294648 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_16_q proxy err 0.0007012067944742739 tr(WHW.T) 631.683349609375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.31it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.62it/s] 50%|█████     | 4/8 [00:01<00:01,  2.78it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.88it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.95it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
W0416 08:27:00.430649 3294648 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_16_k proxy err 0.000496193184517324 tr(WHW.T) 873.5093383789062
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.76it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.37it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.83it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.94it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  3.09it/s]100%|██████████| 8/8 [00:02<00:00,  2.86it/s]
W0416 08:27:04.656320 3294648 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_16_o proxy err 0.0003071865066885948 tr(WHW.T) 8.326766967773438
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.29s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.34it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.75it/s] 50%|█████     | 4/8 [00:02<00:01,  2.04it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.27it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.44it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.55it/s]100%|██████████| 8/8 [00:03<00:00,  2.61it/s]100%|██████████| 8/8 [00:03<00:00,  2.10it/s]
W0416 08:27:10.043244 3294648 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_16_fc1 proxy err 0.00023529725149273872 tr(WHW.T) 12186.9990234375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.09s/it]  6%|▋         | 2/32 [00:01<00:19,  1.53it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
W0416 08:27:23.631089 3294648 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_16_fc2 proxy err 0.00043196234037168324 tr(WHW.T) 18.154970169067383
I0416 08:27:25.082233 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 17 in 0.15s
I0416 08:27:25.238001 3179540 quantize_finetune_clip.py:151] vision layer 18 gpu 0
I0416 08:27:27.515108 3298050 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:27:27.515240 3298050 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:27:27.515300 3298050 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:27:27.633617 3298050 config.py:58] PyTorch version 2.4.0 available.
W0416 08:27:29.904078 3298050 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.62s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.16it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.64it/s] 50%|█████     | 4/8 [00:02<00:01,  2.01it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.30it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.54it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.70it/s]100%|██████████| 8/8 [00:03<00:00,  2.83it/s]100%|██████████| 8/8 [00:03<00:00,  2.07it/s]
W0416 08:27:35.156275 3298050 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_17_v proxy err 0.0010884947841987014 tr(WHW.T) 311.25750732421875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.65it/s] 50%|█████     | 4/8 [00:01<00:01,  2.81it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.92it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
W0416 08:27:39.339290 3298050 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_17_q proxy err 0.0007526507833972573 tr(WHW.T) 640.507080078125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.65it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.28it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.58it/s] 50%|█████     | 4/8 [00:01<00:01,  2.78it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.89it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
W0416 08:27:43.550155 3298050 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_17_k proxy err 0.0005458284867927432 tr(WHW.T) 881.98974609375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.78it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.41it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.69it/s] 50%|█████     | 4/8 [00:01<00:01,  2.87it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.03it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]100%|██████████| 8/8 [00:02<00:00,  3.10it/s]100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
W0416 08:27:47.691793 3298050 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_17_o proxy err 0.0003089917008765042 tr(WHW.T) 7.44087028503418
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.27s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.38it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.82it/s] 50%|█████     | 4/8 [00:02<00:01,  2.14it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.36it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.53it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.63it/s]100%|██████████| 8/8 [00:03<00:00,  2.71it/s]100%|██████████| 8/8 [00:03<00:00,  2.17it/s]
W0416 08:27:52.858103 3298050 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_17_fc1 proxy err 0.0002625442575663328 tr(WHW.T) 15504.220703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.08s/it]  6%|▋         | 2/32 [00:01<00:19,  1.50it/s]  9%|▉         | 3/32 [00:01<00:15,  1.87it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.12it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.84it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.83it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s] 53%|█████▎    | 17/32 [00:07<00:06,  2.32it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.45it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0416 08:28:06.667123 3298050 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_17_fc2 proxy err 0.0005444116541184485 tr(WHW.T) 18.253433227539062
I0416 08:28:08.076176 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 18 in 0.14s
I0416 08:28:08.198274 3179540 quantize_finetune_clip.py:151] vision layer 19 gpu 0
I0416 08:28:10.510297 3301507 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:28:10.510421 3301507 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:28:10.510483 3301507 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:28:10.629969 3301507 config.py:58] PyTorch version 2.4.0 available.
W0416 08:28:12.933474 3301507 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.63s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.16it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.62it/s] 50%|█████     | 4/8 [00:02<00:02,  1.99it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.29it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.49it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.66it/s]100%|██████████| 8/8 [00:03<00:00,  2.74it/s]100%|██████████| 8/8 [00:03<00:00,  2.04it/s]
W0416 08:28:18.274679 3301507 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_18_v proxy err 0.0011849036673083901 tr(WHW.T) 355.8935546875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.78it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.38it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.69it/s] 50%|█████     | 4/8 [00:01<00:01,  2.84it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.04it/s]100%|██████████| 8/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  2.86it/s]
W0416 08:28:22.395606 3301507 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_18_q proxy err 0.0007304237806238234 tr(WHW.T) 765.724365234375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.83it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  3.04it/s]100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
W0416 08:28:26.564011 3301507 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_18_k proxy err 0.0005683415220119059 tr(WHW.T) 952.23486328125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.78it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.41it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.71it/s] 50%|█████     | 4/8 [00:01<00:01,  2.85it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.03it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]100%|██████████| 8/8 [00:02<00:00,  3.11it/s]100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
W0416 08:28:30.674775 3301507 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_18_o proxy err 0.0003199038910679519 tr(WHW.T) 8.747404098510742
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.27s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.37it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.81it/s] 50%|█████     | 4/8 [00:02<00:01,  2.11it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.34it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.49it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.60it/s]100%|██████████| 8/8 [00:03<00:00,  2.69it/s]100%|██████████| 8/8 [00:03<00:00,  2.16it/s]
W0416 08:28:35.930746 3301507 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_18_fc1 proxy err 0.0002829775621648878 tr(WHW.T) 17885.8984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.07s/it]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.46it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.60it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.72it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.85it/s] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.90it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.91it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.88it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
W0416 08:28:49.388725 3301507 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_18_fc2 proxy err 0.0005789852584712207 tr(WHW.T) 21.267833709716797
I0416 08:28:50.824091 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 19 in 0.16s
I0416 08:28:50.934844 3179540 quantize_finetune_clip.py:151] vision layer 20 gpu 0
I0416 08:28:53.390444 3304921 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:28:53.390600 3304921 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:28:53.390661 3304921 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:28:53.508694 3304921 config.py:58] PyTorch version 2.4.0 available.
W0416 08:28:56.305748 3304921 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.69s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.11it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.56it/s] 50%|█████     | 4/8 [00:02<00:02,  1.92it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.21it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.41it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.57it/s]100%|██████████| 8/8 [00:04<00:00,  2.70it/s]100%|██████████| 8/8 [00:04<00:00,  1.98it/s]
W0416 08:29:02.533292 3304921 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_19_v proxy err 0.0012177065946161747 tr(WHW.T) 348.79376220703125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.62it/s] 50%|█████     | 4/8 [00:01<00:01,  2.76it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
W0416 08:29:06.898849 3304921 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_19_q proxy err 0.0008378875791095197 tr(WHW.T) 673.1248779296875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.29it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.59it/s] 50%|█████     | 4/8 [00:01<00:01,  2.76it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
W0416 08:29:11.172387 3304921 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_19_k proxy err 0.0005477015511132777 tr(WHW.T) 986.3826293945312
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.78it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.38it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.83it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  2.84it/s]
W0416 08:29:15.355846 3304921 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_19_o proxy err 0.000326499663060531 tr(WHW.T) 9.281452178955078
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.27s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.36it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.78it/s] 50%|█████     | 4/8 [00:02<00:01,  2.09it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.30it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.45it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.56it/s]100%|██████████| 8/8 [00:03<00:00,  2.62it/s]100%|██████████| 8/8 [00:03<00:00,  2.12it/s]
W0416 08:29:20.685360 3304921 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_19_fc1 proxy err 0.00027756867348216474 tr(WHW.T) 20043.716796875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.07s/it]  6%|▋         | 2/32 [00:01<00:19,  1.52it/s]  9%|▉         | 3/32 [00:01<00:15,  1.91it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.66it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.77it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.83it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.77it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
W0416 08:29:34.288556 3304921 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_19_fc2 proxy err 0.00042095902608707547 tr(WHW.T) 25.913047790527344
I0416 08:29:35.813153 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 20 in 0.15s
I0416 08:29:35.939383 3179540 quantize_finetune_clip.py:151] vision layer 21 gpu 0
I0416 08:29:38.265690 3308475 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:29:38.265817 3308475 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:29:38.265876 3308475 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:29:38.379183 3308475 config.py:58] PyTorch version 2.4.0 available.
W0416 08:29:40.602562 3308475 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.70s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.12it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.59it/s] 50%|█████     | 4/8 [00:02<00:02,  1.97it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.26it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.47it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.59it/s]100%|██████████| 8/8 [00:03<00:00,  2.73it/s]100%|██████████| 8/8 [00:03<00:00,  2.00it/s]
W0416 08:29:46.287601 3308475 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_20_v proxy err 0.0012332763290032744 tr(WHW.T) 375.24658203125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.29it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.59it/s] 50%|█████     | 4/8 [00:01<00:01,  2.74it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.80it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.88it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]100%|██████████| 8/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
W0416 08:29:50.775533 3308475 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_20_q proxy err 0.0007459691842086613 tr(WHW.T) 722.5367431640625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.63it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.21it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.54it/s] 50%|█████     | 4/8 [00:01<00:01,  2.68it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.81it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.86it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.91it/s]100%|██████████| 8/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
W0416 08:29:55.164397 3308475 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_20_k proxy err 0.00046891917008906603 tr(WHW.T) 1119.14111328125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.30it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.59it/s] 50%|█████     | 4/8 [00:01<00:01,  2.76it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.95it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
W0416 08:29:59.481760 3308475 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_20_o proxy err 0.00028895100695081055 tr(WHW.T) 13.241490364074707
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.27s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.36it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.77it/s] 50%|█████     | 4/8 [00:02<00:01,  2.08it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.30it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.46it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.55it/s]100%|██████████| 8/8 [00:03<00:00,  2.63it/s]100%|██████████| 8/8 [00:03<00:00,  2.12it/s]
W0416 08:30:04.814725 3308475 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_20_fc1 proxy err 0.0002496094966772944 tr(WHW.T) 28492.39453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.08s/it]  6%|▋         | 2/32 [00:01<00:19,  1.52it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.78it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.82it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
W0416 08:30:18.568445 3308475 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_20_fc2 proxy err 0.00013090440188534558 tr(WHW.T) 105.08917999267578
I0416 08:30:20.005277 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 21 in 0.16s
I0416 08:30:20.128333 3179540 quantize_finetune_clip.py:151] vision layer 22 gpu 0
I0416 08:30:22.457331 3312020 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:30:22.457469 3312020 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:30:22.457528 3312020 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:30:22.573535 3312020 config.py:58] PyTorch version 2.4.0 available.
W0416 08:30:24.848326 3312020 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.61s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.17it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.62it/s] 50%|█████     | 4/8 [00:02<00:02,  1.99it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.27it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.46it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.59it/s]100%|██████████| 8/8 [00:03<00:00,  2.72it/s]100%|██████████| 8/8 [00:03<00:00,  2.03it/s]
W0416 08:30:30.215043 3312020 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_21_v proxy err 0.0011944117723032832 tr(WHW.T) 334.0597229003906
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.75it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.65it/s] 50%|█████     | 4/8 [00:01<00:01,  2.80it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.91it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
W0416 08:30:34.393245 3312020 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_21_q proxy err 4.723289384855889e-05 tr(WHW.T) 10014.0400390625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.65it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.28it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.58it/s] 50%|█████     | 4/8 [00:01<00:01,  2.75it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  2.76it/s]
W0416 08:30:38.636602 3312020 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_21_k proxy err 0.0004114315961487591 tr(WHW.T) 998.8802490234375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.77it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.38it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.83it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.92it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  3.07it/s]100%|██████████| 8/8 [00:02<00:00,  2.85it/s]
W0416 08:30:42.795396 3312020 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_21_o proxy err 0.0003053818072658032 tr(WHW.T) 17.47711181640625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.25s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.37it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.77it/s] 50%|█████     | 4/8 [00:02<00:01,  2.06it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.29it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.44it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.55it/s]100%|██████████| 8/8 [00:03<00:00,  2.61it/s]100%|██████████| 8/8 [00:03<00:00,  2.12it/s]
W0416 08:30:48.066573 3312020 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_21_fc1 proxy err 0.0002200588205596432 tr(WHW.T) 26967.453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:34,  1.11s/it]  6%|▋         | 2/32 [00:01<00:20,  1.47it/s]  9%|▉         | 3/32 [00:01<00:15,  1.87it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.67it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.75it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.88it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.90it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]
W0416 08:31:01.631371 3312020 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_21_fc2 proxy err 0.00017901530372910202 tr(WHW.T) 76.61061096191406
I0416 08:31:03.030290 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 22 in 0.15s
I0416 08:31:03.148658 3179540 quantize_finetune_clip.py:151] vision layer 23 gpu 0
I0416 08:31:05.670531 3315491 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:31:05.670682 3315491 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:31:05.670741 3315491 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:31:05.788136 3315491 config.py:58] PyTorch version 2.4.0 available.
W0416 08:31:08.645832 3315491 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.70s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.12it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.59it/s] 50%|█████     | 4/8 [00:02<00:02,  1.95it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.25it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.47it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.65it/s]100%|██████████| 8/8 [00:03<00:00,  2.77it/s]100%|██████████| 8/8 [00:03<00:00,  2.01it/s]
W0416 08:31:14.960299 3315491 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_22_v proxy err 0.0014823239762336016 tr(WHW.T) 295.9122619628906
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.71it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.32it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.63it/s] 50%|█████     | 4/8 [00:01<00:01,  2.79it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.90it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
W0416 08:31:19.347167 3315491 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_22_q proxy err 0.00017700949683785439 tr(WHW.T) 127158.65625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.62it/s] 50%|█████     | 4/8 [00:01<00:01,  2.79it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.04it/s]100%|██████████| 8/8 [00:02<00:00,  3.07it/s]100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
W0416 08:31:23.597247 3315491 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_22_k proxy err 8.91057716216892e-05 tr(WHW.T) 7097.814453125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.77it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.39it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.70it/s] 50%|█████     | 4/8 [00:01<00:01,  2.86it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.94it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  3.04it/s]100%|██████████| 8/8 [00:02<00:00,  2.85it/s]
W0416 08:31:27.774055 3315491 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_22_o proxy err 0.00038282095920294523 tr(WHW.T) 19.675331115722656
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.29s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.34it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.75it/s] 50%|█████     | 4/8 [00:02<00:01,  2.05it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.26it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.43it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.55it/s]100%|██████████| 8/8 [00:03<00:00,  2.64it/s]100%|██████████| 8/8 [00:03<00:00,  2.11it/s]
W0416 08:31:33.119765 3315491 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_22_fc1 proxy err 0.0001907650294015184 tr(WHW.T) 20578.845703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.08s/it]  6%|▋         | 2/32 [00:01<00:19,  1.52it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.84it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]
W0416 08:31:46.714701 3315491 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_22_fc2 proxy err 0.00015653831360395998 tr(WHW.T) 57.731483459472656
I0416 08:31:48.283496 3179540 quantize_finetune_clip.py:168] computed original embedding for vision layer 23 in 0.18s
I0416 08:31:50.682184 3319044 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:31:50.682313 3319044 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:31:50.682370 3319044 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:31:50.797287 3319044 config.py:58] PyTorch version 2.4.0 available.
W0416 08:31:53.157681 3319044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.62s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.17it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.64it/s] 50%|█████     | 4/8 [00:02<00:01,  2.02it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.30it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.52it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.68it/s]100%|██████████| 8/8 [00:03<00:00,  2.80it/s]100%|██████████| 8/8 [00:03<00:00,  2.06it/s]
W0416 08:31:58.495863 3319044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_23_v proxy err 0.0014648506185039878 tr(WHW.T) 353.4617919921875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.63it/s] 50%|█████     | 4/8 [00:01<00:01,  2.78it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
W0416 08:32:02.686615 3319044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_23_q proxy err 0.0007836767472326756 tr(WHW.T) 414.4307861328125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.67it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.31it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.56it/s] 50%|█████     | 4/8 [00:01<00:01,  2.73it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.81it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.86it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.92it/s]100%|██████████| 8/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
W0416 08:32:06.941015 3319044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_23_k proxy err 0.00035647276672534645 tr(WHW.T) 884.6561279296875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.62it/s] 50%|█████     | 4/8 [00:01<00:01,  2.80it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  2.83it/s]
W0416 08:32:11.119444 3319044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_23_o proxy err 0.00019652491027954966 tr(WHW.T) 40.9009895324707
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.27s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.37it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.79it/s] 50%|█████     | 4/8 [00:02<00:01,  2.09it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.32it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.47it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.58it/s]100%|██████████| 8/8 [00:03<00:00,  2.65it/s]100%|██████████| 8/8 [00:03<00:00,  2.14it/s]
W0416 08:32:16.373210 3319044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_23_fc1 proxy err 0.0002895374200306833 tr(WHW.T) 5847.96337890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.09s/it]  6%|▋         | 2/32 [00:01<00:19,  1.53it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.61it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.80it/s] 41%|████      | 13/32 [00:05<00:06,  2.82it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.84it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.77it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.78it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]
W0416 08:32:29.939696 3319044 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_23_fc2 proxy err 5.5017055274220183e-05 tr(WHW.T) 125.12049102783203
I0416 08:32:31.008005 3179540 quantize_finetune_clip.py:151] text layer 0 gpu 0
I0416 08:32:31.525258 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 0 in 0.23s
I0416 08:32:31.635771 3179540 quantize_finetune_clip.py:151] text layer 1 gpu 0
I0416 08:32:33.998890 3322668 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:32:33.999051 3322668 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:32:33.999111 3322668 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:32:34.120921 3322668 config.py:58] PyTorch version 2.4.0 available.
W0416 08:32:36.464085 3322668 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:03<00:18,  3.69s/it] 33%|███▎      | 2/6 [00:04<00:06,  1.71s/it] 50%|█████     | 3/6 [00:04<00:03,  1.07s/it] 67%|██████▋   | 4/6 [00:04<00:01,  1.30it/s] 83%|████████▎ | 5/6 [00:04<00:00,  1.66it/s]100%|██████████| 6/6 [00:05<00:00,  1.99it/s]100%|██████████| 6/6 [00:05<00:00,  1.14it/s]
W0416 08:32:43.134389 3322668 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_0_v proxy err 0.0011654503177851439 tr(WHW.T) 75.04249572753906
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.80it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.45it/s] 50%|█████     | 3/6 [00:01<00:01,  2.74it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.92it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.03it/s]100%|██████████| 6/6 [00:02<00:00,  3.10it/s]100%|██████████| 6/6 [00:02<00:00,  2.86it/s]
W0416 08:32:46.551562 3322668 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_0_q proxy err 0.00043288792949169874 tr(WHW.T) 563.9849853515625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.72it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.39it/s] 50%|█████     | 3/6 [00:01<00:01,  2.73it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.92it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.05it/s]100%|██████████| 6/6 [00:02<00:00,  3.13it/s]100%|██████████| 6/6 [00:02<00:00,  2.86it/s]
W0416 08:32:49.972283 3322668 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_0_k proxy err 0.00043984875082969666 tr(WHW.T) 544.4554443359375
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.45it/s] 50%|█████     | 3/6 [00:01<00:01,  2.76it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.93it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.07it/s]100%|██████████| 6/6 [00:02<00:00,  3.15it/s]100%|██████████| 6/6 [00:02<00:00,  2.89it/s]
W0416 08:32:53.349277 3322668 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_0_o proxy err 0.0001871822460088879 tr(WHW.T) 5.182252883911133
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.24s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.41it/s] 50%|█████     | 3/6 [00:01<00:01,  1.85it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.15it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.39it/s]100%|██████████| 6/6 [00:02<00:00,  2.55it/s]100%|██████████| 6/6 [00:02<00:00,  2.04it/s]
W0416 08:32:57.717911 3322668 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_0_fc1 proxy err 1.372701080981642e-05 tr(WHW.T) 36859.8359375
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.06s/it]  8%|▊         | 2/24 [00:01<00:13,  1.58it/s] 12%|█▎        | 3/24 [00:01<00:10,  2.02it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.32it/s] 21%|██        | 5/24 [00:02<00:07,  2.53it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.65it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.71it/s] 33%|███▎      | 8/24 [00:03<00:06,  2.39it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.56it/s] 42%|████▏     | 10/24 [00:04<00:05,  2.67it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.76it/s] 50%|█████     | 12/24 [00:04<00:04,  2.84it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.90it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.93it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.95it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.97it/s] 71%|███████   | 17/24 [00:06<00:02,  2.95it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.99it/s] 79%|███████▉  | 19/24 [00:07<00:01,  3.02it/s] 83%|████████▎ | 20/24 [00:07<00:01,  3.00it/s] 88%|████████▊ | 21/24 [00:07<00:00,  3.01it/s] 92%|█████████▏| 22/24 [00:08<00:00,  3.02it/s] 96%|█████████▌| 23/24 [00:08<00:00,  3.03it/s]100%|██████████| 24/24 [00:08<00:00,  3.04it/s]100%|██████████| 24/24 [00:08<00:00,  2.69it/s]
W0416 08:33:08.041734 3322668 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_0_fc2 proxy err 1.3511287306755548e-06 tr(WHW.T) 2877.713134765625
I0416 08:33:09.781549 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 1 in 0.27s
I0416 08:33:09.891311 3179540 quantize_finetune_clip.py:151] text layer 2 gpu 0
I0416 08:33:12.251173 3325845 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:33:12.251341 3325845 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:33:12.251405 3325845 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:33:12.372602 3325845 config.py:58] PyTorch version 2.4.0 available.
W0416 08:33:14.770058 3325845 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.80s/it] 33%|███▎      | 2/6 [00:02<00:03,  1.08it/s] 50%|█████     | 3/6 [00:02<00:01,  1.55it/s] 67%|██████▋   | 4/6 [00:02<00:01,  1.96it/s] 83%|████████▎ | 5/6 [00:03<00:00,  2.29it/s]100%|██████████| 6/6 [00:03<00:00,  2.55it/s]100%|██████████| 6/6 [00:03<00:00,  1.80it/s]
W0416 08:33:19.555756 3325845 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_1_v proxy err 0.00023129626060836017 tr(WHW.T) 73.62937927246094
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.39it/s] 50%|█████     | 3/6 [00:01<00:01,  2.72it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.88it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  3.06it/s]100%|██████████| 6/6 [00:02<00:00,  2.82it/s]
W0416 08:33:23.025154 3325845 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_1_q proxy err 0.00010886773088714108 tr(WHW.T) 623.6982421875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.77it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.40it/s] 50%|█████     | 3/6 [00:01<00:01,  2.69it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.87it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  3.05it/s]100%|██████████| 6/6 [00:02<00:00,  2.81it/s]
W0416 08:33:26.505976 3325845 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_1_k proxy err 0.00014890875900164247 tr(WHW.T) 389.26483154296875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.80it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.43it/s] 50%|█████     | 3/6 [00:01<00:01,  2.74it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.90it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.00it/s]100%|██████████| 6/6 [00:02<00:00,  3.07it/s]100%|██████████| 6/6 [00:02<00:00,  2.84it/s]
W0416 08:33:29.950518 3325845 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_1_o proxy err 7.35418580006808e-05 tr(WHW.T) 3.8104982376098633
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.28s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.36it/s] 50%|█████     | 3/6 [00:01<00:01,  1.78it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.11it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.35it/s]100%|██████████| 6/6 [00:03<00:00,  2.52it/s]100%|██████████| 6/6 [00:03<00:00,  1.99it/s]
W0416 08:33:34.391589 3325845 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_1_fc1 proxy err 2.355774267925881e-05 tr(WHW.T) 24829.86328125
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.06s/it]  8%|▊         | 2/24 [00:01<00:13,  1.58it/s] 12%|█▎        | 3/24 [00:01<00:10,  2.02it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.33it/s] 21%|██        | 5/24 [00:02<00:07,  2.55it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.65it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.75it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.84it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.83it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.87it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.91it/s] 50%|█████     | 12/24 [00:04<00:04,  2.91it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.94it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.98it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.98it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.97it/s] 71%|███████   | 17/24 [00:06<00:02,  2.98it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.99it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.99it/s] 83%|████████▎ | 20/24 [00:07<00:01,  3.00it/s] 88%|████████▊ | 21/24 [00:07<00:00,  3.01it/s] 92%|█████████▏| 22/24 [00:08<00:00,  3.00it/s] 96%|█████████▌| 23/24 [00:08<00:00,  3.01it/s]100%|██████████| 24/24 [00:08<00:00,  2.97it/s]100%|██████████| 24/24 [00:08<00:00,  2.74it/s]
W0416 08:33:44.570422 3325845 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_1_fc2 proxy err 6.803785800002515e-05 tr(WHW.T) 2.0527796745300293
I0416 08:33:46.250271 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 2 in 0.27s
I0416 08:33:46.336147 3179540 quantize_finetune_clip.py:151] text layer 3 gpu 0
I0416 08:33:48.579105 3329019 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:33:48.579260 3329019 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:33:48.579319 3329019 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:33:48.707163 3329019 config.py:58] PyTorch version 2.4.0 available.
W0416 08:33:51.097298 3329019 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.69s/it] 33%|███▎      | 2/6 [00:02<00:03,  1.13it/s] 50%|█████     | 3/6 [00:02<00:01,  1.61it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.01it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.31it/s]100%|██████████| 6/6 [00:03<00:00,  2.54it/s]100%|██████████| 6/6 [00:03<00:00,  1.84it/s]
W0416 08:33:55.934502 3329019 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_2_v proxy err 0.00023645340115763247 tr(WHW.T) 129.4635009765625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.81it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.46it/s] 50%|█████     | 3/6 [00:01<00:01,  2.75it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.92it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.03it/s]100%|██████████| 6/6 [00:02<00:00,  3.10it/s]100%|██████████| 6/6 [00:02<00:00,  2.86it/s]
W0416 08:33:59.481556 3329019 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_2_q proxy err 9.814283839659765e-05 tr(WHW.T) 725.2421264648438
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.72it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.33it/s] 50%|█████     | 3/6 [00:01<00:01,  2.66it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.84it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.95it/s]100%|██████████| 6/6 [00:02<00:00,  3.02it/s]100%|██████████| 6/6 [00:02<00:00,  2.77it/s]
W0416 08:34:03.107987 3329019 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_2_k proxy err 0.00014108170580584556 tr(WHW.T) 564.4356079101562
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.77it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.41it/s] 50%|█████     | 3/6 [00:01<00:01,  2.68it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.87it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.97it/s]100%|██████████| 6/6 [00:02<00:00,  3.03it/s]100%|██████████| 6/6 [00:02<00:00,  2.80it/s]
W0416 08:34:06.686652 3329019 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_2_o proxy err 0.00011839851504191756 tr(WHW.T) 3.46923828125
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.24s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.40it/s] 50%|█████     | 3/6 [00:01<00:01,  1.83it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.14it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.34it/s]100%|██████████| 6/6 [00:02<00:00,  2.50it/s]100%|██████████| 6/6 [00:02<00:00,  2.01it/s]
W0416 08:34:11.170751 3329019 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_2_fc1 proxy err 2.934752410510555e-05 tr(WHW.T) 20103.25390625
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:25,  1.09s/it]  8%|▊         | 2/24 [00:01<00:14,  1.54it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.98it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.28it/s] 21%|██        | 5/24 [00:02<00:07,  2.49it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.63it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.72it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.76it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.81it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.83it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.82it/s] 50%|█████     | 12/24 [00:04<00:04,  2.86it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.89it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.91it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.94it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.95it/s] 71%|███████   | 17/24 [00:06<00:02,  2.96it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.98it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.99it/s] 83%|████████▎ | 20/24 [00:07<00:01,  3.00it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.98it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.98it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.96it/s]100%|██████████| 24/24 [00:08<00:00,  2.96it/s]100%|██████████| 24/24 [00:08<00:00,  2.70it/s]
W0416 08:34:21.533424 3329019 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_2_fc2 proxy err 5.500044790096581e-05 tr(WHW.T) 4.276344299316406
I0416 08:34:22.953970 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 3 in 0.14s
I0416 08:34:23.064168 3179540 quantize_finetune_clip.py:151] text layer 4 gpu 0
I0416 08:34:25.414408 3332032 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:34:25.414572 3332032 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:34:25.414633 3332032 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:34:25.536261 3332032 config.py:58] PyTorch version 2.4.0 available.
W0416 08:34:27.910551 3332032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.79s/it] 33%|███▎      | 2/6 [00:02<00:03,  1.08it/s] 50%|█████     | 3/6 [00:02<00:01,  1.54it/s] 67%|██████▋   | 4/6 [00:02<00:01,  1.92it/s] 83%|████████▎ | 5/6 [00:03<00:00,  2.25it/s]100%|██████████| 6/6 [00:03<00:00,  2.50it/s]100%|██████████| 6/6 [00:03<00:00,  1.78it/s]
W0416 08:34:32.732075 3332032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_3_v proxy err 0.0002574028621893376 tr(WHW.T) 130.8394775390625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.80it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.43it/s] 50%|█████     | 3/6 [00:01<00:01,  2.74it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.88it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  3.06it/s]100%|██████████| 6/6 [00:02<00:00,  2.83it/s]
W0416 08:34:36.204994 3332032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_3_q proxy err 0.0001368884404655546 tr(WHW.T) 616.7564697265625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.70it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.35it/s] 50%|█████     | 3/6 [00:01<00:01,  2.68it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.87it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.97it/s]100%|██████████| 6/6 [00:02<00:00,  3.05it/s]100%|██████████| 6/6 [00:02<00:00,  2.79it/s]
W0416 08:34:39.666610 3332032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_3_k proxy err 0.00012858146510552615 tr(WHW.T) 697.400146484375
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.81it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.42it/s] 50%|█████     | 3/6 [00:01<00:01,  2.73it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.88it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.98it/s]100%|██████████| 6/6 [00:02<00:00,  3.05it/s]100%|██████████| 6/6 [00:02<00:00,  2.83it/s]
W0416 08:34:43.128519 3332032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_3_o proxy err 8.444397826679051e-05 tr(WHW.T) 5.515538215637207
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.26s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.39it/s] 50%|█████     | 3/6 [00:01<00:01,  1.82it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.12it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.34it/s]100%|██████████| 6/6 [00:02<00:00,  2.51it/s]100%|██████████| 6/6 [00:02<00:00,  2.00it/s]
W0416 08:34:47.529919 3332032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_3_fc1 proxy err 2.960739584523253e-05 tr(WHW.T) 14064.40625
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.07s/it]  8%|▊         | 2/24 [00:01<00:14,  1.56it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.97it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.27it/s] 21%|██        | 5/24 [00:02<00:07,  2.48it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.59it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.68it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.76it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.79it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.82it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.84it/s] 50%|█████     | 12/24 [00:04<00:04,  2.89it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.92it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.92it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.90it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.90it/s] 71%|███████   | 17/24 [00:06<00:02,  2.90it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.93it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.93it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.93it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.93it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.90it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.92it/s]100%|██████████| 24/24 [00:08<00:00,  2.93it/s]100%|██████████| 24/24 [00:08<00:00,  2.68it/s]
W0416 08:34:57.905689 3332032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_3_fc2 proxy err 5.496133235283196e-05 tr(WHW.T) 6.90947151184082
I0416 08:34:59.325612 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 4 in 0.14s
I0416 08:34:59.437268 3179540 quantize_finetune_clip.py:151] text layer 5 gpu 0
I0416 08:35:01.847355 3335048 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:35:01.847521 3335048 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:35:01.847581 3335048 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:35:01.976454 3335048 config.py:58] PyTorch version 2.4.0 available.
W0416 08:35:04.355974 3335048 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:09,  1.81s/it] 33%|███▎      | 2/6 [00:02<00:03,  1.07it/s] 50%|█████     | 3/6 [00:02<00:01,  1.53it/s] 67%|██████▋   | 4/6 [00:02<00:01,  1.92it/s] 83%|████████▎ | 5/6 [00:03<00:00,  2.24it/s]100%|██████████| 6/6 [00:03<00:00,  2.47it/s]100%|██████████| 6/6 [00:03<00:00,  1.76it/s]
W0416 08:35:09.216521 3335048 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_4_v proxy err 0.0003010413493029773 tr(WHW.T) 180.7900848388672
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.77it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.40it/s] 50%|█████     | 3/6 [00:01<00:01,  2.64it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.81it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.93it/s]100%|██████████| 6/6 [00:02<00:00,  3.00it/s]100%|██████████| 6/6 [00:02<00:00,  2.77it/s]
W0416 08:35:12.704216 3335048 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_4_q proxy err 0.00011980097042396665 tr(WHW.T) 742.8862915039062
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.73it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.38it/s] 50%|█████     | 3/6 [00:01<00:01,  2.70it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.86it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.96it/s]100%|██████████| 6/6 [00:02<00:00,  3.03it/s]100%|██████████| 6/6 [00:02<00:00,  2.79it/s]
W0416 08:35:16.169944 3335048 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_4_k proxy err 0.0001050437058438547 tr(WHW.T) 890.5114135742188
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.80it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.43it/s] 50%|█████     | 3/6 [00:01<00:01,  2.72it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.89it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  3.05it/s]100%|██████████| 6/6 [00:02<00:00,  2.82it/s]
W0416 08:35:19.610672 3335048 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_4_o proxy err 0.00013092027802485973 tr(WHW.T) 7.300278663635254
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.24s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.39it/s] 50%|█████     | 3/6 [00:01<00:01,  1.82it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.13it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.35it/s]100%|██████████| 6/6 [00:02<00:00,  2.48it/s]100%|██████████| 6/6 [00:02<00:00,  2.00it/s]
W0416 08:35:24.027627 3335048 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_4_fc1 proxy err 5.2498253353405744e-05 tr(WHW.T) 9402.642578125
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.06s/it]  8%|▊         | 2/24 [00:01<00:14,  1.55it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.98it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.27it/s] 21%|██        | 5/24 [00:02<00:07,  2.45it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.58it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.66it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.72it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.79it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.82it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.86it/s] 50%|█████     | 12/24 [00:04<00:04,  2.88it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.87it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.90it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.92it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.93it/s] 71%|███████   | 17/24 [00:06<00:02,  2.94it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.92it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.93it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.94it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.94it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.94it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.95it/s]100%|██████████| 24/24 [00:08<00:00,  2.95it/s]100%|██████████| 24/24 [00:08<00:00,  2.68it/s]
W0416 08:35:34.401943 3335048 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_4_fc2 proxy err 7.930988795123994e-05 tr(WHW.T) 11.091764450073242
I0416 08:35:35.915719 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 5 in 0.16s
I0416 08:35:36.024570 3179540 quantize_finetune_clip.py:151] text layer 6 gpu 0
I0416 08:35:38.374051 3338220 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:35:38.374211 3338220 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:35:38.374276 3338220 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:35:38.499082 3338220 config.py:58] PyTorch version 2.4.0 available.
W0416 08:35:40.896225 3338220 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.69s/it] 33%|███▎      | 2/6 [00:02<00:03,  1.14it/s] 50%|█████     | 3/6 [00:02<00:01,  1.61it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.01it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.31it/s]100%|██████████| 6/6 [00:03<00:00,  2.52it/s]100%|██████████| 6/6 [00:03<00:00,  1.84it/s]
W0416 08:35:45.600353 3338220 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_5_v proxy err 0.00019998745119664818 tr(WHW.T) 249.72238159179688
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.80it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.44it/s] 50%|█████     | 3/6 [00:01<00:01,  2.74it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.91it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.01it/s]100%|██████████| 6/6 [00:02<00:00,  3.09it/s]100%|██████████| 6/6 [00:02<00:00,  2.85it/s]
W0416 08:35:49.025407 3338220 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_5_q proxy err 0.00010430143447592854 tr(WHW.T) 855.363525390625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.73it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.40it/s] 50%|█████     | 3/6 [00:01<00:01,  2.72it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.87it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  3.07it/s]100%|██████████| 6/6 [00:02<00:00,  2.82it/s]
W0416 08:35:52.440380 3338220 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_5_k proxy err 0.00012236063776072115 tr(WHW.T) 789.1260986328125
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.79it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.43it/s] 50%|█████     | 3/6 [00:01<00:01,  2.75it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.91it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.00it/s]100%|██████████| 6/6 [00:02<00:00,  3.08it/s]100%|██████████| 6/6 [00:02<00:00,  2.85it/s]
W0416 08:35:55.839334 3338220 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_5_o proxy err 0.00013647325977217406 tr(WHW.T) 11.59481430053711
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.24s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.40it/s] 50%|█████     | 3/6 [00:01<00:01,  1.83it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.14it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.37it/s]100%|██████████| 6/6 [00:02<00:00,  2.52it/s]100%|██████████| 6/6 [00:02<00:00,  2.02it/s]
W0416 08:36:00.240203 3338220 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_5_fc1 proxy err 4.6442666643997654e-05 tr(WHW.T) 11533.93359375
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.07s/it]  8%|▊         | 2/24 [00:01<00:14,  1.56it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.99it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.28it/s] 21%|██        | 5/24 [00:02<00:07,  2.49it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.63it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.69it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.75it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.77it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.83it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.86it/s] 50%|█████     | 12/24 [00:04<00:04,  2.89it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.90it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.93it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.94it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.95it/s] 71%|███████   | 17/24 [00:06<00:02,  2.96it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.96it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.98it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.99it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.97it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.98it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.99it/s]100%|██████████| 24/24 [00:08<00:00,  2.96it/s]100%|██████████| 24/24 [00:08<00:00,  2.71it/s]
W0416 08:36:10.541964 3338220 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_5_fc2 proxy err 5.969447738607414e-05 tr(WHW.T) 12.426437377929688
I0416 08:36:12.175739 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 6 in 0.35s
I0416 08:36:12.309616 3179540 quantize_finetune_clip.py:151] text layer 7 gpu 0
I0416 08:36:14.493347 3341315 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:36:14.493477 3341315 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:36:14.493548 3341315 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:36:14.605420 3341315 config.py:58] PyTorch version 2.4.0 available.
W0416 08:36:16.861130 3341315 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.63s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.16it/s] 50%|█████     | 3/6 [00:02<00:01,  1.62it/s] 67%|██████▋   | 4/6 [00:02<00:01,  1.99it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.29it/s]100%|██████████| 6/6 [00:03<00:00,  2.50it/s]100%|██████████| 6/6 [00:03<00:00,  1.84it/s]
W0416 08:36:21.668705 3341315 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_6_v proxy err 0.00027832385967485607 tr(WHW.T) 246.55203247070312
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.72it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.35it/s] 50%|█████     | 3/6 [00:01<00:01,  2.66it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.82it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.93it/s]100%|██████████| 6/6 [00:02<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  2.76it/s]
W0416 08:36:25.184314 3341315 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_6_q proxy err 0.00014093404752202332 tr(WHW.T) 831.7105102539062
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:03,  1.63it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.26it/s] 50%|█████     | 3/6 [00:01<00:01,  2.57it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.74it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.82it/s]100%|██████████| 6/6 [00:02<00:00,  2.90it/s]100%|██████████| 6/6 [00:02<00:00,  2.67it/s]
W0416 08:36:28.785792 3341315 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_6_k proxy err 0.00013628765009343624 tr(WHW.T) 706.4376831054688
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.74it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.34it/s] 50%|█████     | 3/6 [00:01<00:01,  2.64it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.80it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.89it/s]100%|██████████| 6/6 [00:02<00:00,  2.96it/s]100%|██████████| 6/6 [00:02<00:00,  2.73it/s]
W0416 08:36:32.300943 3341315 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_6_o proxy err 0.00014547980390489101 tr(WHW.T) 7.770773410797119
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.28s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.35it/s] 50%|█████     | 3/6 [00:02<00:01,  1.76it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.06it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.28it/s]100%|██████████| 6/6 [00:03<00:00,  2.39it/s]100%|██████████| 6/6 [00:03<00:00,  1.94it/s]
W0416 08:36:36.858019 3341315 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_6_fc1 proxy err 4.975730553269386e-05 tr(WHW.T) 10551.373046875
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:25,  1.11s/it]  8%|▊         | 2/24 [00:01<00:14,  1.52it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.93it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.23it/s] 21%|██        | 5/24 [00:02<00:07,  2.42it/s] 25%|██▌       | 6/24 [00:02<00:07,  2.56it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.65it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.67it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.73it/s] 42%|████▏     | 10/24 [00:04<00:05,  2.77it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.75it/s] 50%|█████     | 12/24 [00:04<00:04,  2.75it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.77it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.80it/s] 62%|██████▎   | 15/24 [00:06<00:03,  2.82it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.84it/s] 71%|███████   | 17/24 [00:06<00:02,  2.86it/s] 75%|███████▌  | 18/24 [00:07<00:02,  2.88it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.88it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.89it/s] 88%|████████▊ | 21/24 [00:08<00:01,  2.89it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.89it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.88it/s]100%|██████████| 24/24 [00:09<00:00,  2.86it/s]100%|██████████| 24/24 [00:09<00:00,  2.62it/s]
W0416 08:36:47.541777 3341315 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_6_fc2 proxy err 6.309330638032407e-05 tr(WHW.T) 15.198512077331543
I0416 08:36:49.180261 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 7 in 0.21s
I0416 08:36:49.291665 3179540 quantize_finetune_clip.py:151] text layer 8 gpu 0
I0416 08:36:51.615720 3344316 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:36:51.615965 3344316 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:36:51.616026 3344316 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:36:51.730390 3344316 config.py:58] PyTorch version 2.4.0 available.
W0416 08:36:54.028411 3344316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.63s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.16it/s] 50%|█████     | 3/6 [00:02<00:01,  1.63it/s] 67%|██████▋   | 4/6 [00:02<00:01,  1.98it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.28it/s]100%|██████████| 6/6 [00:03<00:00,  2.49it/s]100%|██████████| 6/6 [00:03<00:00,  1.84it/s]
W0416 08:36:58.760893 3344316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_7_v proxy err 0.00025446555810049176 tr(WHW.T) 263.7965087890625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.40it/s] 50%|█████     | 3/6 [00:01<00:01,  2.70it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.86it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.96it/s]100%|██████████| 6/6 [00:02<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  2.78it/s]
W0416 08:37:02.318976 3344316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_7_q proxy err 0.0001346550416201353 tr(WHW.T) 725.1131591796875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.72it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.33it/s] 50%|█████     | 3/6 [00:01<00:01,  2.64it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.82it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.93it/s]100%|██████████| 6/6 [00:02<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  2.75it/s]
W0416 08:37:05.905552 3344316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_7_k proxy err 0.00014562130672857165 tr(WHW.T) 651.355712890625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.35it/s] 50%|█████     | 3/6 [00:01<00:01,  2.66it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.82it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.92it/s]100%|██████████| 6/6 [00:02<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  2.76it/s]
W0416 08:37:09.494794 3344316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_7_o proxy err 0.00012879635323770344 tr(WHW.T) 10.79519271850586
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.25s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.38it/s] 50%|█████     | 3/6 [00:01<00:01,  1.77it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.08it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.31it/s]100%|██████████| 6/6 [00:03<00:00,  2.45it/s]100%|██████████| 6/6 [00:03<00:00,  1.97it/s]
W0416 08:37:14.064723 3344316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_7_fc1 proxy err 4.940996223012917e-05 tr(WHW.T) 12009.625
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.07s/it]  8%|▊         | 2/24 [00:01<00:14,  1.55it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.97it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.25it/s] 21%|██        | 5/24 [00:02<00:07,  2.42it/s] 25%|██▌       | 6/24 [00:02<00:07,  2.56it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.63it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.69it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.75it/s] 42%|████▏     | 10/24 [00:04<00:05,  2.80it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.83it/s] 50%|█████     | 12/24 [00:04<00:04,  2.82it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.81it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.84it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.86it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.88it/s] 71%|███████   | 17/24 [00:06<00:02,  2.90it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.90it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.90it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.91it/s] 88%|████████▊ | 21/24 [00:08<00:01,  2.93it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.93it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.91it/s]100%|██████████| 24/24 [00:09<00:00,  2.90it/s]100%|██████████| 24/24 [00:09<00:00,  2.65it/s]
W0416 08:37:24.649581 3344316 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_7_fc2 proxy err 6.326898437691852e-05 tr(WHW.T) 11.257575988769531
I0416 08:37:26.363045 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 8 in 0.21s
I0416 08:37:26.473988 3179540 quantize_finetune_clip.py:151] text layer 9 gpu 0
I0416 08:37:28.753784 3347582 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:37:28.753915 3347582 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:37:28.753976 3347582 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:37:28.869531 3347582 config.py:58] PyTorch version 2.4.0 available.
W0416 08:37:31.109802 3347582 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.62s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.17it/s] 50%|█████     | 3/6 [00:02<00:01,  1.63it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.02it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.32it/s]100%|██████████| 6/6 [00:03<00:00,  2.55it/s]100%|██████████| 6/6 [00:03<00:00,  1.87it/s]
W0416 08:37:35.723412 3347582 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_8_v proxy err 0.00021500563889276236 tr(WHW.T) 368.84979248046875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.77it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.40it/s] 50%|█████     | 3/6 [00:01<00:01,  2.66it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.84it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.91it/s]100%|██████████| 6/6 [00:02<00:00,  3.01it/s]100%|██████████| 6/6 [00:02<00:00,  2.78it/s]
W0416 08:37:39.199913 3347582 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_8_q proxy err 0.00014191288209985942 tr(WHW.T) 895.6217041015625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.72it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.37it/s] 50%|█████     | 3/6 [00:01<00:01,  2.66it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.84it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.95it/s]100%|██████████| 6/6 [00:02<00:00,  3.03it/s]100%|██████████| 6/6 [00:02<00:00,  2.78it/s]
W0416 08:37:43.950989 3347582 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_8_k proxy err 0.0001437708706362173 tr(WHW.T) 747.58984375
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.73it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.38it/s] 50%|█████     | 3/6 [00:01<00:01,  2.70it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.84it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.95it/s]100%|██████████| 6/6 [00:02<00:00,  3.02it/s]100%|██████████| 6/6 [00:02<00:00,  2.79it/s]
W0416 08:37:48.307885 3347582 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_8_o proxy err 0.00010639664105838165 tr(WHW.T) 10.459186553955078
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.26s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.38it/s] 50%|█████     | 3/6 [00:01<00:01,  1.79it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.08it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.32it/s]100%|██████████| 6/6 [00:03<00:00,  2.48it/s]100%|██████████| 6/6 [00:03<00:00,  1.99it/s]
W0416 08:37:52.855126 3347582 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_8_fc1 proxy err 4.954204996465705e-05 tr(WHW.T) 10972.4697265625
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.09s/it]  8%|▊         | 2/24 [00:01<00:14,  1.52it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.94it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.25it/s] 21%|██        | 5/24 [00:02<00:07,  2.45it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.60it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.68it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.74it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.79it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.83it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.86it/s] 50%|█████     | 12/24 [00:04<00:04,  2.87it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.90it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.91it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.93it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.93it/s] 71%|███████   | 17/24 [00:06<00:02,  2.93it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.93it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.94it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.95it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.95it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.94it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.94it/s]100%|██████████| 24/24 [00:08<00:00,  2.95it/s]100%|██████████| 24/24 [00:08<00:00,  2.68it/s]
W0416 08:38:03.264873 3347582 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_8_fc2 proxy err 5.7374250900465995e-05 tr(WHW.T) 20.137924194335938
I0416 08:38:04.817860 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 9 in 0.18s
I0416 08:38:04.929534 3179540 quantize_finetune_clip.py:151] text layer 10 gpu 0
I0416 08:38:07.213983 3350698 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:38:07.214232 3350698 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:38:07.214293 3350698 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:38:07.331744 3350698 config.py:58] PyTorch version 2.4.0 available.
W0416 08:38:09.590042 3350698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.64s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.15it/s] 50%|█████     | 3/6 [00:02<00:01,  1.61it/s] 67%|██████▋   | 4/6 [00:02<00:01,  1.97it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.24it/s]100%|██████████| 6/6 [00:03<00:00,  2.44it/s]100%|██████████| 6/6 [00:03<00:00,  1.81it/s]
W0416 08:38:14.381345 3350698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_9_v proxy err 0.0002157378039555624 tr(WHW.T) 361.0838623046875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.75it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.36it/s] 50%|█████     | 3/6 [00:01<00:01,  2.66it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.77it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.87it/s]100%|██████████| 6/6 [00:02<00:00,  2.92it/s]100%|██████████| 6/6 [00:02<00:00,  2.72it/s]
W0416 08:38:17.975989 3350698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_9_q proxy err 0.0001299437863053754 tr(WHW.T) 794.7393188476562
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.72it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.35it/s] 50%|█████     | 3/6 [00:01<00:01,  2.65it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.81it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.92it/s]100%|██████████| 6/6 [00:02<00:00,  2.98it/s]100%|██████████| 6/6 [00:02<00:00,  2.75it/s]
W0416 08:38:21.552673 3350698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_9_k proxy err 0.00014201272279024124 tr(WHW.T) 691.6158447265625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.76it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.37it/s] 50%|█████     | 3/6 [00:01<00:01,  2.66it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.82it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.92it/s]100%|██████████| 6/6 [00:02<00:00,  2.97it/s]100%|██████████| 6/6 [00:02<00:00,  2.76it/s]
W0416 08:38:25.148715 3350698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_9_o proxy err 0.00013194649363867939 tr(WHW.T) 24.63672637939453
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.26s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.35it/s] 50%|█████     | 3/6 [00:01<00:01,  1.77it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.08it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.29it/s]100%|██████████| 6/6 [00:03<00:00,  2.42it/s]100%|██████████| 6/6 [00:03<00:00,  1.95it/s]
W0416 08:38:29.763092 3350698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_9_fc1 proxy err 6.839708657935262e-05 tr(WHW.T) 10332.9814453125
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.09s/it]  8%|▊         | 2/24 [00:01<00:14,  1.52it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.94it/s] 17%|█▋        | 4/24 [00:02<00:09,  2.19it/s] 21%|██        | 5/24 [00:02<00:08,  2.37it/s] 25%|██▌       | 6/24 [00:02<00:07,  2.51it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.56it/s] 33%|███▎      | 8/24 [00:03<00:06,  2.61it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.67it/s] 42%|████▏     | 10/24 [00:04<00:05,  2.72it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.75it/s] 50%|█████     | 12/24 [00:05<00:04,  2.77it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.80it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.80it/s] 62%|██████▎   | 15/24 [00:06<00:03,  2.79it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.79it/s] 71%|███████   | 17/24 [00:06<00:02,  2.81it/s] 75%|███████▌  | 18/24 [00:07<00:02,  2.84it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.84it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.83it/s] 88%|████████▊ | 21/24 [00:08<00:01,  2.83it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.82it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.83it/s]100%|██████████| 24/24 [00:09<00:00,  2.85it/s]100%|██████████| 24/24 [00:09<00:00,  2.59it/s]
W0416 08:38:40.677594 3350698 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_9_fc2 proxy err 7.26452490198426e-05 tr(WHW.T) 20.32598304748535
I0416 08:38:43.391156 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 10 in 0.22s
I0416 08:38:43.504833 3179540 quantize_finetune_clip.py:151] text layer 11 gpu 0
I0416 08:38:45.741783 3353880 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:38:45.741919 3353880 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:38:45.741979 3353880 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:38:45.858660 3353880 config.py:58] PyTorch version 2.4.0 available.
W0416 08:38:48.415439 3353880 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.63s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.15it/s] 50%|█████     | 3/6 [00:02<00:01,  1.58it/s] 67%|██████▋   | 4/6 [00:02<00:01,  1.93it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.22it/s]100%|██████████| 6/6 [00:03<00:00,  2.42it/s]100%|██████████| 6/6 [00:03<00:00,  1.80it/s]
W0416 08:38:53.270565 3353880 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_10_v proxy err 0.0002701675985008478 tr(WHW.T) 381.42498779296875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.73it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.32it/s] 50%|█████     | 3/6 [00:01<00:01,  2.60it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.76it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.80it/s]100%|██████████| 6/6 [00:02<00:00,  2.87it/s]100%|██████████| 6/6 [00:02<00:00,  2.67it/s]
W0416 08:38:56.858145 3353880 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_10_q proxy err 0.00013235044025350362 tr(WHW.T) 717.8143310546875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.71it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.33it/s] 50%|█████     | 3/6 [00:01<00:01,  2.62it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.78it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.88it/s]100%|██████████| 6/6 [00:02<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  2.72it/s]
W0416 08:39:00.384796 3353880 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_10_k proxy err 0.00018489733338356018 tr(WHW.T) 629.68408203125
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.74it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.33it/s] 50%|█████     | 3/6 [00:01<00:01,  2.62it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.77it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.86it/s]100%|██████████| 6/6 [00:02<00:00,  2.91it/s]100%|██████████| 6/6 [00:02<00:00,  2.70it/s]
W0416 08:39:03.948512 3353880 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_10_o proxy err 0.00013546401169151068 tr(WHW.T) 33.12681579589844
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.25s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.38it/s] 50%|█████     | 3/6 [00:01<00:01,  1.77it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.05it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.26it/s]100%|██████████| 6/6 [00:03<00:00,  2.41it/s]100%|██████████| 6/6 [00:03<00:00,  1.95it/s]
W0416 08:39:08.515926 3353880 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_10_fc1 proxy err 5.843704639119096e-05 tr(WHW.T) 9721.9580078125
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.09s/it]  8%|▊         | 2/24 [00:01<00:14,  1.53it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.94it/s] 17%|█▋        | 4/24 [00:02<00:09,  2.21it/s] 21%|██        | 5/24 [00:02<00:07,  2.39it/s] 25%|██▌       | 6/24 [00:02<00:07,  2.50it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.60it/s] 33%|███▎      | 8/24 [00:03<00:06,  2.63it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.66it/s] 42%|████▏     | 10/24 [00:04<00:05,  2.70it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.73it/s] 50%|█████     | 12/24 [00:05<00:04,  2.76it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.77it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.77it/s] 62%|██████▎   | 15/24 [00:06<00:03,  2.79it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.81it/s] 71%|███████   | 17/24 [00:06<00:02,  2.82it/s] 75%|███████▌  | 18/24 [00:07<00:02,  2.79it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.81it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.82it/s] 88%|████████▊ | 21/24 [00:08<00:01,  2.81it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.81it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.81it/s]100%|██████████| 24/24 [00:09<00:00,  2.81it/s]100%|██████████| 24/24 [00:09<00:00,  2.58it/s]
W0416 08:39:19.302373 3353880 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_10_fc2 proxy err 4.824714415008202e-05 tr(WHW.T) 42.26137924194336
I0416 08:39:21.134542 3179540 quantize_finetune_clip.py:168] computed original embedding for text layer 11 in 0.24s
I0416 08:39:23.565240 3357146 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:39:23.565456 3357146 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:39:23.565519 3357146 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:39:23.683253 3357146 config.py:58] PyTorch version 2.4.0 available.
W0416 08:39:26.409769 3357146 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:09,  1.86s/it] 33%|███▎      | 2/6 [00:02<00:03,  1.04it/s] 50%|█████     | 3/6 [00:02<00:02,  1.48it/s] 67%|██████▋   | 4/6 [00:02<00:01,  1.86it/s] 83%|████████▎ | 5/6 [00:03<00:00,  2.17it/s]100%|██████████| 6/6 [00:03<00:00,  2.40it/s]100%|██████████| 6/6 [00:03<00:00,  1.71it/s]
W0416 08:39:32.130562 3357146 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_11_v proxy err 0.0003213656600564718 tr(WHW.T) 514.353271484375
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.73it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.34it/s] 50%|█████     | 3/6 [00:01<00:01,  2.64it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.79it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.88it/s]100%|██████████| 6/6 [00:02<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  2.72it/s]
W0416 08:39:35.897669 3357146 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_11_q proxy err 0.00015704560792073607 tr(WHW.T) 626.7928466796875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.71it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.33it/s] 50%|█████     | 3/6 [00:01<00:01,  2.59it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.78it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.89it/s]100%|██████████| 6/6 [00:02<00:00,  2.97it/s]100%|██████████| 6/6 [00:02<00:00,  2.73it/s]
W0416 08:39:39.624477 3357146 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_11_k proxy err 0.00013936457980889827 tr(WHW.T) 589.9808349609375
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.77it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.35it/s] 50%|█████     | 3/6 [00:01<00:01,  2.65it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.80it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.90it/s]100%|██████████| 6/6 [00:02<00:00,  2.95it/s]100%|██████████| 6/6 [00:02<00:00,  2.74it/s]
W0416 08:39:43.250127 3357146 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_11_o proxy err 0.00011208740033907816 tr(WHW.T) 93.02412414550781
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.26s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.37it/s] 50%|█████     | 3/6 [00:01<00:01,  1.78it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.07it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.23it/s]100%|██████████| 6/6 [00:03<00:00,  2.39it/s]100%|██████████| 6/6 [00:03<00:00,  1.94it/s]
W0416 08:39:47.898386 3357146 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_11_fc1 proxy err 5.7583514717407525e-05 tr(WHW.T) 6430.23974609375
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.06s/it]  8%|▊         | 2/24 [00:01<00:14,  1.53it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.94it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.24it/s] 21%|██        | 5/24 [00:02<00:07,  2.43it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.57it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.67it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.73it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.71it/s] 42%|████▏     | 10/24 [00:04<00:05,  2.74it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.78it/s] 50%|█████     | 12/24 [00:04<00:04,  2.77it/s] 54%|█████▍    | 13/24 [00:05<00:04,  2.75it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.74it/s] 62%|██████▎   | 15/24 [00:06<00:03,  2.73it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.72it/s] 71%|███████   | 17/24 [00:06<00:02,  2.72it/s] 75%|███████▌  | 18/24 [00:07<00:02,  2.70it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.69it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.73it/s] 88%|████████▊ | 21/24 [00:08<00:01,  2.74it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.72it/s] 96%|█████████▌| 23/24 [00:09<00:00,  2.71it/s]100%|██████████| 24/24 [00:09<00:00,  2.69it/s]100%|██████████| 24/24 [00:09<00:00,  2.56it/s]
W0416 08:39:58.907547 3357146 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_11_fc2 proxy err 2.8487847885116935e-05 tr(WHW.T) 66.53541564941406
I0416 08:40:05.802656 3360304 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:40:05.802841 3360304 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:40:05.802932 3360304 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:40:05.926112 3360304 config.py:58] PyTorch version 2.4.0 available.
W0416 08:40:07.565234 3360304 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0416 08:40:07.565948 3360304 hfize_clip.py:43] CLIPConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/openai--clip-vit-large-patch14",
  "architectures": [
    "CLIPModel"
  ],
  "initializer_factor": 1.0,
  "logit_scale_init_value": 2.6592,
  "model_type": "clip",
  "projection_dim": 768,
  "quip_params": {
    "K": 6,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "text_config": {
    "dropout": 0.0,
    "hidden_size": 768,
    "intermediate_size": 3072,
    "model_type": "clip_text_model",
    "num_attention_heads": 12,
    "projection_dim": 768,
    "torch_dtype": "float32"
  },
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "vision_config": {
    "dropout": 0.0,
    "hidden_size": 1024,
    "intermediate_size": 4096,
    "model_type": "clip_vision_model",
    "num_attention_heads": 16,
    "num_hidden_layers": 24,
    "patch_size": 14,
    "projection_dim": 768,
    "torch_dtype": "float32"
  }
}

Some weights of the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 were not used when initializing CLIPModel: ['text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing CLIPModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CLIPModel were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 and are newly initialized: ['text_model.encoder.layers.0.mlp.fc1.SU', 'text_model.encoder.layers.0.mlp.fc1.SV', 'text_model.encoder.layers.0.mlp.fc1.rcp', 'text_model.encoder.layers.0.mlp.fc1.tlut', 'text_model.encoder.layers.0.mlp.fc1.tp_rank', 'text_model.encoder.layers.0.mlp.fc1.trellis', 'text_model.encoder.layers.0.mlp.fc2.SU', 'text_model.encoder.layers.0.mlp.fc2.SV', 'text_model.encoder.layers.0.mlp.fc2.rcp', 'text_model.encoder.layers.0.mlp.fc2.tlut', 'text_model.encoder.layers.0.mlp.fc2.tp_rank', 'text_model.encoder.layers.0.mlp.fc2.trellis', 'text_model.encoder.layers.0.self_attn.k_proj.SU', 'text_model.encoder.layers.0.self_attn.k_proj.SV', 'text_model.encoder.layers.0.self_attn.k_proj.rcp', 'text_model.encoder.layers.0.self_attn.k_proj.tlut', 'text_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.k_proj.trellis', 'text_model.encoder.layers.0.self_attn.out_proj.SU', 'text_model.encoder.layers.0.self_attn.out_proj.SV', 'text_model.encoder.layers.0.self_attn.out_proj.rcp', 'text_model.encoder.layers.0.self_attn.out_proj.tlut', 'text_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.out_proj.trellis', 'text_model.encoder.layers.0.self_attn.q_proj.SU', 'text_model.encoder.layers.0.self_attn.q_proj.SV', 'text_model.encoder.layers.0.self_attn.q_proj.rcp', 'text_model.encoder.layers.0.self_attn.q_proj.tlut', 'text_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.q_proj.trellis', 'text_model.encoder.layers.0.self_attn.v_proj.SU', 'text_model.encoder.layers.0.self_attn.v_proj.SV', 'text_model.encoder.layers.0.self_attn.v_proj.rcp', 'text_model.encoder.layers.0.self_attn.v_proj.tlut', 'text_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.v_proj.trellis', 'text_model.encoder.layers.1.mlp.fc1.SU', 'text_model.encoder.layers.1.mlp.fc1.SV', 'text_model.encoder.layers.1.mlp.fc1.rcp', 'text_model.encoder.layers.1.mlp.fc1.tlut', 'text_model.encoder.layers.1.mlp.fc1.tp_rank', 'text_model.encoder.layers.1.mlp.fc1.trellis', 'text_model.encoder.layers.1.mlp.fc2.SU', 'text_model.encoder.layers.1.mlp.fc2.SV', 'text_model.encoder.layers.1.mlp.fc2.rcp', 'text_model.encoder.layers.1.mlp.fc2.tlut', 'text_model.encoder.layers.1.mlp.fc2.tp_rank', 'text_model.encoder.layers.1.mlp.fc2.trellis', 'text_model.encoder.layers.1.self_attn.k_proj.SU', 'text_model.encoder.layers.1.self_attn.k_proj.SV', 'text_model.encoder.layers.1.self_attn.k_proj.rcp', 'text_model.encoder.layers.1.self_attn.k_proj.tlut', 'text_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.k_proj.trellis', 'text_model.encoder.layers.1.self_attn.out_proj.SU', 'text_model.encoder.layers.1.self_attn.out_proj.SV', 'text_model.encoder.layers.1.self_attn.out_proj.rcp', 'text_model.encoder.layers.1.self_attn.out_proj.tlut', 'text_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.out_proj.trellis', 'text_model.encoder.layers.1.self_attn.q_proj.SU', 'text_model.encoder.layers.1.self_attn.q_proj.SV', 'text_model.encoder.layers.1.self_attn.q_proj.rcp', 'text_model.encoder.layers.1.self_attn.q_proj.tlut', 'text_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.q_proj.trellis', 'text_model.encoder.layers.1.self_attn.v_proj.SU', 'text_model.encoder.layers.1.self_attn.v_proj.SV', 'text_model.encoder.layers.1.self_attn.v_proj.rcp', 'text_model.encoder.layers.1.self_attn.v_proj.tlut', 'text_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.v_proj.trellis', 'text_model.encoder.layers.10.mlp.fc1.SU', 'text_model.encoder.layers.10.mlp.fc1.SV', 'text_model.encoder.layers.10.mlp.fc1.rcp', 'text_model.encoder.layers.10.mlp.fc1.tlut', 'text_model.encoder.layers.10.mlp.fc1.tp_rank', 'text_model.encoder.layers.10.mlp.fc1.trellis', 'text_model.encoder.layers.10.mlp.fc2.SU', 'text_model.encoder.layers.10.mlp.fc2.SV', 'text_model.encoder.layers.10.mlp.fc2.rcp', 'text_model.encoder.layers.10.mlp.fc2.tlut', 'text_model.encoder.layers.10.mlp.fc2.tp_rank', 'text_model.encoder.layers.10.mlp.fc2.trellis', 'text_model.encoder.layers.10.self_attn.k_proj.SU', 'text_model.encoder.layers.10.self_attn.k_proj.SV', 'text_model.encoder.layers.10.self_attn.k_proj.rcp', 'text_model.encoder.layers.10.self_attn.k_proj.tlut', 'text_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.k_proj.trellis', 'text_model.encoder.layers.10.self_attn.out_proj.SU', 'text_model.encoder.layers.10.self_attn.out_proj.SV', 'text_model.encoder.layers.10.self_attn.out_proj.rcp', 'text_model.encoder.layers.10.self_attn.out_proj.tlut', 'text_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.out_proj.trellis', 'text_model.encoder.layers.10.self_attn.q_proj.SU', 'text_model.encoder.layers.10.self_attn.q_proj.SV', 'text_model.encoder.layers.10.self_attn.q_proj.rcp', 'text_model.encoder.layers.10.self_attn.q_proj.tlut', 'text_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.q_proj.trellis', 'text_model.encoder.layers.10.self_attn.v_proj.SU', 'text_model.encoder.layers.10.self_attn.v_proj.SV', 'text_model.encoder.layers.10.self_attn.v_proj.rcp', 'text_model.encoder.layers.10.self_attn.v_proj.tlut', 'text_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.v_proj.trellis', 'text_model.encoder.layers.11.mlp.fc1.SU', 'text_model.encoder.layers.11.mlp.fc1.SV', 'text_model.encoder.layers.11.mlp.fc1.rcp', 'text_model.encoder.layers.11.mlp.fc1.tlut', 'text_model.encoder.layers.11.mlp.fc1.tp_rank', 'text_model.encoder.layers.11.mlp.fc1.trellis', 'text_model.encoder.layers.11.mlp.fc2.SU', 'text_model.encoder.layers.11.mlp.fc2.SV', 'text_model.encoder.layers.11.mlp.fc2.rcp', 'text_model.encoder.layers.11.mlp.fc2.tlut', 'text_model.encoder.layers.11.mlp.fc2.tp_rank', 'text_model.encoder.layers.11.mlp.fc2.trellis', 'text_model.encoder.layers.11.self_attn.k_proj.SU', 'text_model.encoder.layers.11.self_attn.k_proj.SV', 'text_model.encoder.layers.11.self_attn.k_proj.rcp', 'text_model.encoder.layers.11.self_attn.k_proj.tlut', 'text_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.k_proj.trellis', 'text_model.encoder.layers.11.self_attn.out_proj.SU', 'text_model.encoder.layers.11.self_attn.out_proj.SV', 'text_model.encoder.layers.11.self_attn.out_proj.rcp', 'text_model.encoder.layers.11.self_attn.out_proj.tlut', 'text_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.out_proj.trellis', 'text_model.encoder.layers.11.self_attn.q_proj.SU', 'text_model.encoder.layers.11.self_attn.q_proj.SV', 'text_model.encoder.layers.11.self_attn.q_proj.rcp', 'text_model.encoder.layers.11.self_attn.q_proj.tlut', 'text_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.q_proj.trellis', 'text_model.encoder.layers.11.self_attn.v_proj.SU', 'text_model.encoder.layers.11.self_attn.v_proj.SV', 'text_model.encoder.layers.11.self_attn.v_proj.rcp', 'text_model.encoder.layers.11.self_attn.v_proj.tlut', 'text_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.v_proj.trellis', 'text_model.encoder.layers.2.mlp.fc1.SU', 'text_model.encoder.layers.2.mlp.fc1.SV', 'text_model.encoder.layers.2.mlp.fc1.rcp', 'text_model.encoder.layers.2.mlp.fc1.tlut', 'text_model.encoder.layers.2.mlp.fc1.tp_rank', 'text_model.encoder.layers.2.mlp.fc1.trellis', 'text_model.encoder.layers.2.mlp.fc2.SU', 'text_model.encoder.layers.2.mlp.fc2.SV', 'text_model.encoder.layers.2.mlp.fc2.rcp', 'text_model.encoder.layers.2.mlp.fc2.tlut', 'text_model.encoder.layers.2.mlp.fc2.tp_rank', 'text_model.encoder.layers.2.mlp.fc2.trellis', 'text_model.encoder.layers.2.self_attn.k_proj.SU', 'text_model.encoder.layers.2.self_attn.k_proj.SV', 'text_model.encoder.layers.2.self_attn.k_proj.rcp', 'text_model.encoder.layers.2.self_attn.k_proj.tlut', 'text_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.k_proj.trellis', 'text_model.encoder.layers.2.self_attn.out_proj.SU', 'text_model.encoder.layers.2.self_attn.out_proj.SV', 'text_model.encoder.layers.2.self_attn.out_proj.rcp', 'text_model.encoder.layers.2.self_attn.out_proj.tlut', 'text_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.out_proj.trellis', 'text_model.encoder.layers.2.self_attn.q_proj.SU', 'text_model.encoder.layers.2.self_attn.q_proj.SV', 'text_model.encoder.layers.2.self_attn.q_proj.rcp', 'text_model.encoder.layers.2.self_attn.q_proj.tlut', 'text_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.q_proj.trellis', 'text_model.encoder.layers.2.self_attn.v_proj.SU', 'text_model.encoder.layers.2.self_attn.v_proj.SV', 'text_model.encoder.layers.2.self_attn.v_proj.rcp', 'text_model.encoder.layers.2.self_attn.v_proj.tlut', 'text_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.v_proj.trellis', 'text_model.encoder.layers.3.mlp.fc1.SU', 'text_model.encoder.layers.3.mlp.fc1.SV', 'text_model.encoder.layers.3.mlp.fc1.rcp', 'text_model.encoder.layers.3.mlp.fc1.tlut', 'text_model.encoder.layers.3.mlp.fc1.tp_rank', 'text_model.encoder.layers.3.mlp.fc1.trellis', 'text_model.encoder.layers.3.mlp.fc2.SU', 'text_model.encoder.layers.3.mlp.fc2.SV', 'text_model.encoder.layers.3.mlp.fc2.rcp', 'text_model.encoder.layers.3.mlp.fc2.tlut', 'text_model.encoder.layers.3.mlp.fc2.tp_rank', 'text_model.encoder.layers.3.mlp.fc2.trellis', 'text_model.encoder.layers.3.self_attn.k_proj.SU', 'text_model.encoder.layers.3.self_attn.k_proj.SV', 'text_model.encoder.layers.3.self_attn.k_proj.rcp', 'text_model.encoder.layers.3.self_attn.k_proj.tlut', 'text_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.k_proj.trellis', 'text_model.encoder.layers.3.self_attn.out_proj.SU', 'text_model.encoder.layers.3.self_attn.out_proj.SV', 'text_model.encoder.layers.3.self_attn.out_proj.rcp', 'text_model.encoder.layers.3.self_attn.out_proj.tlut', 'text_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.out_proj.trellis', 'text_model.encoder.layers.3.self_attn.q_proj.SU', 'text_model.encoder.layers.3.self_attn.q_proj.SV', 'text_model.encoder.layers.3.self_attn.q_proj.rcp', 'text_model.encoder.layers.3.self_attn.q_proj.tlut', 'text_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.q_proj.trellis', 'text_model.encoder.layers.3.self_attn.v_proj.SU', 'text_model.encoder.layers.3.self_attn.v_proj.SV', 'text_model.encoder.layers.3.self_attn.v_proj.rcp', 'text_model.encoder.layers.3.self_attn.v_proj.tlut', 'text_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.v_proj.trellis', 'text_model.encoder.layers.4.mlp.fc1.SU', 'text_model.encoder.layers.4.mlp.fc1.SV', 'text_model.encoder.layers.4.mlp.fc1.rcp', 'text_model.encoder.layers.4.mlp.fc1.tlut', 'text_model.encoder.layers.4.mlp.fc1.tp_rank', 'text_model.encoder.layers.4.mlp.fc1.trellis', 'text_model.encoder.layers.4.mlp.fc2.SU', 'text_model.encoder.layers.4.mlp.fc2.SV', 'text_model.encoder.layers.4.mlp.fc2.rcp', 'text_model.encoder.layers.4.mlp.fc2.tlut', 'text_model.encoder.layers.4.mlp.fc2.tp_rank', 'text_model.encoder.layers.4.mlp.fc2.trellis', 'text_model.encoder.layers.4.self_attn.k_proj.SU', 'text_model.encoder.layers.4.self_attn.k_proj.SV', 'text_model.encoder.layers.4.self_attn.k_proj.rcp', 'text_model.encoder.layers.4.self_attn.k_proj.tlut', 'text_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.k_proj.trellis', 'text_model.encoder.layers.4.self_attn.out_proj.SU', 'text_model.encoder.layers.4.self_attn.out_proj.SV', 'text_model.encoder.layers.4.self_attn.out_proj.rcp', 'text_model.encoder.layers.4.self_attn.out_proj.tlut', 'text_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.out_proj.trellis', 'text_model.encoder.layers.4.self_attn.q_proj.SU', 'text_model.encoder.layers.4.self_attn.q_proj.SV', 'text_model.encoder.layers.4.self_attn.q_proj.rcp', 'text_model.encoder.layers.4.self_attn.q_proj.tlut', 'text_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.q_proj.trellis', 'text_model.encoder.layers.4.self_attn.v_proj.SU', 'text_model.encoder.layers.4.self_attn.v_proj.SV', 'text_model.encoder.layers.4.self_attn.v_proj.rcp', 'text_model.encoder.layers.4.self_attn.v_proj.tlut', 'text_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.v_proj.trellis', 'text_model.encoder.layers.5.mlp.fc1.SU', 'text_model.encoder.layers.5.mlp.fc1.SV', 'text_model.encoder.layers.5.mlp.fc1.rcp', 'text_model.encoder.layers.5.mlp.fc1.tlut', 'text_model.encoder.layers.5.mlp.fc1.tp_rank', 'text_model.encoder.layers.5.mlp.fc1.trellis', 'text_model.encoder.layers.5.mlp.fc2.SU', 'text_model.encoder.layers.5.mlp.fc2.SV', 'text_model.encoder.layers.5.mlp.fc2.rcp', 'text_model.encoder.layers.5.mlp.fc2.tlut', 'text_model.encoder.layers.5.mlp.fc2.tp_rank', 'text_model.encoder.layers.5.mlp.fc2.trellis', 'text_model.encoder.layers.5.self_attn.k_proj.SU', 'text_model.encoder.layers.5.self_attn.k_proj.SV', 'text_model.encoder.layers.5.self_attn.k_proj.rcp', 'text_model.encoder.layers.5.self_attn.k_proj.tlut', 'text_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.k_proj.trellis', 'text_model.encoder.layers.5.self_attn.out_proj.SU', 'text_model.encoder.layers.5.self_attn.out_proj.SV', 'text_model.encoder.layers.5.self_attn.out_proj.rcp', 'text_model.encoder.layers.5.self_attn.out_proj.tlut', 'text_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.out_proj.trellis', 'text_model.encoder.layers.5.self_attn.q_proj.SU', 'text_model.encoder.layers.5.self_attn.q_proj.SV', 'text_model.encoder.layers.5.self_attn.q_proj.rcp', 'text_model.encoder.layers.5.self_attn.q_proj.tlut', 'text_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.q_proj.trellis', 'text_model.encoder.layers.5.self_attn.v_proj.SU', 'text_model.encoder.layers.5.self_attn.v_proj.SV', 'text_model.encoder.layers.5.self_attn.v_proj.rcp', 'text_model.encoder.layers.5.self_attn.v_proj.tlut', 'text_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.v_proj.trellis', 'text_model.encoder.layers.6.mlp.fc1.SU', 'text_model.encoder.layers.6.mlp.fc1.SV', 'text_model.encoder.layers.6.mlp.fc1.rcp', 'text_model.encoder.layers.6.mlp.fc1.tlut', 'text_model.encoder.layers.6.mlp.fc1.tp_rank', 'text_model.encoder.layers.6.mlp.fc1.trellis', 'text_model.encoder.layers.6.mlp.fc2.SU', 'text_model.encoder.layers.6.mlp.fc2.SV', 'text_model.encoder.layers.6.mlp.fc2.rcp', 'text_model.encoder.layers.6.mlp.fc2.tlut', 'text_model.encoder.layers.6.mlp.fc2.tp_rank', 'text_model.encoder.layers.6.mlp.fc2.trellis', 'text_model.encoder.layers.6.self_attn.k_proj.SU', 'text_model.encoder.layers.6.self_attn.k_proj.SV', 'text_model.encoder.layers.6.self_attn.k_proj.rcp', 'text_model.encoder.layers.6.self_attn.k_proj.tlut', 'text_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.k_proj.trellis', 'text_model.encoder.layers.6.self_attn.out_proj.SU', 'text_model.encoder.layers.6.self_attn.out_proj.SV', 'text_model.encoder.layers.6.self_attn.out_proj.rcp', 'text_model.encoder.layers.6.self_attn.out_proj.tlut', 'text_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.out_proj.trellis', 'text_model.encoder.layers.6.self_attn.q_proj.SU', 'text_model.encoder.layers.6.self_attn.q_proj.SV', 'text_model.encoder.layers.6.self_attn.q_proj.rcp', 'text_model.encoder.layers.6.self_attn.q_proj.tlut', 'text_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.q_proj.trellis', 'text_model.encoder.layers.6.self_attn.v_proj.SU', 'text_model.encoder.layers.6.self_attn.v_proj.SV', 'text_model.encoder.layers.6.self_attn.v_proj.rcp', 'text_model.encoder.layers.6.self_attn.v_proj.tlut', 'text_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.v_proj.trellis', 'text_model.encoder.layers.7.mlp.fc1.SU', 'text_model.encoder.layers.7.mlp.fc1.SV', 'text_model.encoder.layers.7.mlp.fc1.rcp', 'text_model.encoder.layers.7.mlp.fc1.tlut', 'text_model.encoder.layers.7.mlp.fc1.tp_rank', 'text_model.encoder.layers.7.mlp.fc1.trellis', 'text_model.encoder.layers.7.mlp.fc2.SU', 'text_model.encoder.layers.7.mlp.fc2.SV', 'text_model.encoder.layers.7.mlp.fc2.rcp', 'text_model.encoder.layers.7.mlp.fc2.tlut', 'text_model.encoder.layers.7.mlp.fc2.tp_rank', 'text_model.encoder.layers.7.mlp.fc2.trellis', 'text_model.encoder.layers.7.self_attn.k_proj.SU', 'text_model.encoder.layers.7.self_attn.k_proj.SV', 'text_model.encoder.layers.7.self_attn.k_proj.rcp', 'text_model.encoder.layers.7.self_attn.k_proj.tlut', 'text_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.k_proj.trellis', 'text_model.encoder.layers.7.self_attn.out_proj.SU', 'text_model.encoder.layers.7.self_attn.out_proj.SV', 'text_model.encoder.layers.7.self_attn.out_proj.rcp', 'text_model.encoder.layers.7.self_attn.out_proj.tlut', 'text_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.out_proj.trellis', 'text_model.encoder.layers.7.self_attn.q_proj.SU', 'text_model.encoder.layers.7.self_attn.q_proj.SV', 'text_model.encoder.layers.7.self_attn.q_proj.rcp', 'text_model.encoder.layers.7.self_attn.q_proj.tlut', 'text_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.q_proj.trellis', 'text_model.encoder.layers.7.self_attn.v_proj.SU', 'text_model.encoder.layers.7.self_attn.v_proj.SV', 'text_model.encoder.layers.7.self_attn.v_proj.rcp', 'text_model.encoder.layers.7.self_attn.v_proj.tlut', 'text_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.v_proj.trellis', 'text_model.encoder.layers.8.mlp.fc1.SU', 'text_model.encoder.layers.8.mlp.fc1.SV', 'text_model.encoder.layers.8.mlp.fc1.rcp', 'text_model.encoder.layers.8.mlp.fc1.tlut', 'text_model.encoder.layers.8.mlp.fc1.tp_rank', 'text_model.encoder.layers.8.mlp.fc1.trellis', 'text_model.encoder.layers.8.mlp.fc2.SU', 'text_model.encoder.layers.8.mlp.fc2.SV', 'text_model.encoder.layers.8.mlp.fc2.rcp', 'text_model.encoder.layers.8.mlp.fc2.tlut', 'text_model.encoder.layers.8.mlp.fc2.tp_rank', 'text_model.encoder.layers.8.mlp.fc2.trellis', 'text_model.encoder.layers.8.self_attn.k_proj.SU', 'text_model.encoder.layers.8.self_attn.k_proj.SV', 'text_model.encoder.layers.8.self_attn.k_proj.rcp', 'text_model.encoder.layers.8.self_attn.k_proj.tlut', 'text_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.k_proj.trellis', 'text_model.encoder.layers.8.self_attn.out_proj.SU', 'text_model.encoder.layers.8.self_attn.out_proj.SV', 'text_model.encoder.layers.8.self_attn.out_proj.rcp', 'text_model.encoder.layers.8.self_attn.out_proj.tlut', 'text_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.out_proj.trellis', 'text_model.encoder.layers.8.self_attn.q_proj.SU', 'text_model.encoder.layers.8.self_attn.q_proj.SV', 'text_model.encoder.layers.8.self_attn.q_proj.rcp', 'text_model.encoder.layers.8.self_attn.q_proj.tlut', 'text_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.q_proj.trellis', 'text_model.encoder.layers.8.self_attn.v_proj.SU', 'text_model.encoder.layers.8.self_attn.v_proj.SV', 'text_model.encoder.layers.8.self_attn.v_proj.rcp', 'text_model.encoder.layers.8.self_attn.v_proj.tlut', 'text_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.v_proj.trellis', 'text_model.encoder.layers.9.mlp.fc1.SU', 'text_model.encoder.layers.9.mlp.fc1.SV', 'text_model.encoder.layers.9.mlp.fc1.rcp', 'text_model.encoder.layers.9.mlp.fc1.tlut', 'text_model.encoder.layers.9.mlp.fc1.tp_rank', 'text_model.encoder.layers.9.mlp.fc1.trellis', 'text_model.encoder.layers.9.mlp.fc2.SU', 'text_model.encoder.layers.9.mlp.fc2.SV', 'text_model.encoder.layers.9.mlp.fc2.rcp', 'text_model.encoder.layers.9.mlp.fc2.tlut', 'text_model.encoder.layers.9.mlp.fc2.tp_rank', 'text_model.encoder.layers.9.mlp.fc2.trellis', 'text_model.encoder.layers.9.self_attn.k_proj.SU', 'text_model.encoder.layers.9.self_attn.k_proj.SV', 'text_model.encoder.layers.9.self_attn.k_proj.rcp', 'text_model.encoder.layers.9.self_attn.k_proj.tlut', 'text_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.k_proj.trellis', 'text_model.encoder.layers.9.self_attn.out_proj.SU', 'text_model.encoder.layers.9.self_attn.out_proj.SV', 'text_model.encoder.layers.9.self_attn.out_proj.rcp', 'text_model.encoder.layers.9.self_attn.out_proj.tlut', 'text_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.out_proj.trellis', 'text_model.encoder.layers.9.self_attn.q_proj.SU', 'text_model.encoder.layers.9.self_attn.q_proj.SV', 'text_model.encoder.layers.9.self_attn.q_proj.rcp', 'text_model.encoder.layers.9.self_attn.q_proj.tlut', 'text_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.q_proj.trellis', 'text_model.encoder.layers.9.self_attn.v_proj.SU', 'text_model.encoder.layers.9.self_attn.v_proj.SV', 'text_model.encoder.layers.9.self_attn.v_proj.rcp', 'text_model.encoder.layers.9.self_attn.v_proj.tlut', 'text_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.v_proj.trellis', 'vision_model.encoder.layers.0.mlp.fc1.SU', 'vision_model.encoder.layers.0.mlp.fc1.SV', 'vision_model.encoder.layers.0.mlp.fc1.rcp', 'vision_model.encoder.layers.0.mlp.fc1.tlut', 'vision_model.encoder.layers.0.mlp.fc1.tp_rank', 'vision_model.encoder.layers.0.mlp.fc1.trellis', 'vision_model.encoder.layers.0.mlp.fc2.SU', 'vision_model.encoder.layers.0.mlp.fc2.SV', 'vision_model.encoder.layers.0.mlp.fc2.rcp', 'vision_model.encoder.layers.0.mlp.fc2.tlut', 'vision_model.encoder.layers.0.mlp.fc2.tp_rank', 'vision_model.encoder.layers.0.mlp.fc2.trellis', 'vision_model.encoder.layers.0.self_attn.k_proj.SU', 'vision_model.encoder.layers.0.self_attn.k_proj.SV', 'vision_model.encoder.layers.0.self_attn.k_proj.rcp', 'vision_model.encoder.layers.0.self_attn.k_proj.tlut', 'vision_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.k_proj.trellis', 'vision_model.encoder.layers.0.self_attn.out_proj.SU', 'vision_model.encoder.layers.0.self_attn.out_proj.SV', 'vision_model.encoder.layers.0.self_attn.out_proj.rcp', 'vision_model.encoder.layers.0.self_attn.out_proj.tlut', 'vision_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.out_proj.trellis', 'vision_model.encoder.layers.0.self_attn.q_proj.SU', 'vision_model.encoder.layers.0.self_attn.q_proj.SV', 'vision_model.encoder.layers.0.self_attn.q_proj.rcp', 'vision_model.encoder.layers.0.self_attn.q_proj.tlut', 'vision_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.q_proj.trellis', 'vision_model.encoder.layers.0.self_attn.v_proj.SU', 'vision_model.encoder.layers.0.self_attn.v_proj.SV', 'vision_model.encoder.layers.0.self_attn.v_proj.rcp', 'vision_model.encoder.layers.0.self_attn.v_proj.tlut', 'vision_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.v_proj.trellis', 'vision_model.encoder.layers.1.mlp.fc1.SU', 'vision_model.encoder.layers.1.mlp.fc1.SV', 'vision_model.encoder.layers.1.mlp.fc1.rcp', 'vision_model.encoder.layers.1.mlp.fc1.tlut', 'vision_model.encoder.layers.1.mlp.fc1.tp_rank', 'vision_model.encoder.layers.1.mlp.fc1.trellis', 'vision_model.encoder.layers.1.mlp.fc2.SU', 'vision_model.encoder.layers.1.mlp.fc2.SV', 'vision_model.encoder.layers.1.mlp.fc2.rcp', 'vision_model.encoder.layers.1.mlp.fc2.tlut', 'vision_model.encoder.layers.1.mlp.fc2.tp_rank', 'vision_model.encoder.layers.1.mlp.fc2.trellis', 'vision_model.encoder.layers.1.self_attn.k_proj.SU', 'vision_model.encoder.layers.1.self_attn.k_proj.SV', 'vision_model.encoder.layers.1.self_attn.k_proj.rcp', 'vision_model.encoder.layers.1.self_attn.k_proj.tlut', 'vision_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.k_proj.trellis', 'vision_model.encoder.layers.1.self_attn.out_proj.SU', 'vision_model.encoder.layers.1.self_attn.out_proj.SV', 'vision_model.encoder.layers.1.self_attn.out_proj.rcp', 'vision_model.encoder.layers.1.self_attn.out_proj.tlut', 'vision_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.out_proj.trellis', 'vision_model.encoder.layers.1.self_attn.q_proj.SU', 'vision_model.encoder.layers.1.self_attn.q_proj.SV', 'vision_model.encoder.layers.1.self_attn.q_proj.rcp', 'vision_model.encoder.layers.1.self_attn.q_proj.tlut', 'vision_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.q_proj.trellis', 'vision_model.encoder.layers.1.self_attn.v_proj.SU', 'vision_model.encoder.layers.1.self_attn.v_proj.SV', 'vision_model.encoder.layers.1.self_attn.v_proj.rcp', 'vision_model.encoder.layers.1.self_attn.v_proj.tlut', 'vision_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.v_proj.trellis', 'vision_model.encoder.layers.10.mlp.fc1.SU', 'vision_model.encoder.layers.10.mlp.fc1.SV', 'vision_model.encoder.layers.10.mlp.fc1.rcp', 'vision_model.encoder.layers.10.mlp.fc1.tlut', 'vision_model.encoder.layers.10.mlp.fc1.tp_rank', 'vision_model.encoder.layers.10.mlp.fc1.trellis', 'vision_model.encoder.layers.10.mlp.fc2.SU', 'vision_model.encoder.layers.10.mlp.fc2.SV', 'vision_model.encoder.layers.10.mlp.fc2.rcp', 'vision_model.encoder.layers.10.mlp.fc2.tlut', 'vision_model.encoder.layers.10.mlp.fc2.tp_rank', 'vision_model.encoder.layers.10.mlp.fc2.trellis', 'vision_model.encoder.layers.10.self_attn.k_proj.SU', 'vision_model.encoder.layers.10.self_attn.k_proj.SV', 'vision_model.encoder.layers.10.self_attn.k_proj.rcp', 'vision_model.encoder.layers.10.self_attn.k_proj.tlut', 'vision_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.k_proj.trellis', 'vision_model.encoder.layers.10.self_attn.out_proj.SU', 'vision_model.encoder.layers.10.self_attn.out_proj.SV', 'vision_model.encoder.layers.10.self_attn.out_proj.rcp', 'vision_model.encoder.layers.10.self_attn.out_proj.tlut', 'vision_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.out_proj.trellis', 'vision_model.encoder.layers.10.self_attn.q_proj.SU', 'vision_model.encoder.layers.10.self_attn.q_proj.SV', 'vision_model.encoder.layers.10.self_attn.q_proj.rcp', 'vision_model.encoder.layers.10.self_attn.q_proj.tlut', 'vision_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.q_proj.trellis', 'vision_model.encoder.layers.10.self_attn.v_proj.SU', 'vision_model.encoder.layers.10.self_attn.v_proj.SV', 'vision_model.encoder.layers.10.self_attn.v_proj.rcp', 'vision_model.encoder.layers.10.self_attn.v_proj.tlut', 'vision_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.v_proj.trellis', 'vision_model.encoder.layers.11.mlp.fc1.SU', 'vision_model.encoder.layers.11.mlp.fc1.SV', 'vision_model.encoder.layers.11.mlp.fc1.rcp', 'vision_model.encoder.layers.11.mlp.fc1.tlut', 'vision_model.encoder.layers.11.mlp.fc1.tp_rank', 'vision_model.encoder.layers.11.mlp.fc1.trellis', 'vision_model.encoder.layers.11.mlp.fc2.SU', 'vision_model.encoder.layers.11.mlp.fc2.SV', 'vision_model.encoder.layers.11.mlp.fc2.rcp', 'vision_model.encoder.layers.11.mlp.fc2.tlut', 'vision_model.encoder.layers.11.mlp.fc2.tp_rank', 'vision_model.encoder.layers.11.mlp.fc2.trellis', 'vision_model.encoder.layers.11.self_attn.k_proj.SU', 'vision_model.encoder.layers.11.self_attn.k_proj.SV', 'vision_model.encoder.layers.11.self_attn.k_proj.rcp', 'vision_model.encoder.layers.11.self_attn.k_proj.tlut', 'vision_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.k_proj.trellis', 'vision_model.encoder.layers.11.self_attn.out_proj.SU', 'vision_model.encoder.layers.11.self_attn.out_proj.SV', 'vision_model.encoder.layers.11.self_attn.out_proj.rcp', 'vision_model.encoder.layers.11.self_attn.out_proj.tlut', 'vision_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.out_proj.trellis', 'vision_model.encoder.layers.11.self_attn.q_proj.SU', 'vision_model.encoder.layers.11.self_attn.q_proj.SV', 'vision_model.encoder.layers.11.self_attn.q_proj.rcp', 'vision_model.encoder.layers.11.self_attn.q_proj.tlut', 'vision_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.q_proj.trellis', 'vision_model.encoder.layers.11.self_attn.v_proj.SU', 'vision_model.encoder.layers.11.self_attn.v_proj.SV', 'vision_model.encoder.layers.11.self_attn.v_proj.rcp', 'vision_model.encoder.layers.11.self_attn.v_proj.tlut', 'vision_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.v_proj.trellis', 'vision_model.encoder.layers.12.mlp.fc1.SU', 'vision_model.encoder.layers.12.mlp.fc1.SV', 'vision_model.encoder.layers.12.mlp.fc1.rcp', 'vision_model.encoder.layers.12.mlp.fc1.tlut', 'vision_model.encoder.layers.12.mlp.fc1.tp_rank', 'vision_model.encoder.layers.12.mlp.fc1.trellis', 'vision_model.encoder.layers.12.mlp.fc2.SU', 'vision_model.encoder.layers.12.mlp.fc2.SV', 'vision_model.encoder.layers.12.mlp.fc2.rcp', 'vision_model.encoder.layers.12.mlp.fc2.tlut', 'vision_model.encoder.layers.12.mlp.fc2.tp_rank', 'vision_model.encoder.layers.12.mlp.fc2.trellis', 'vision_model.encoder.layers.12.self_attn.k_proj.SU', 'vision_model.encoder.layers.12.self_attn.k_proj.SV', 'vision_model.encoder.layers.12.self_attn.k_proj.rcp', 'vision_model.encoder.layers.12.self_attn.k_proj.tlut', 'vision_model.encoder.layers.12.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.k_proj.trellis', 'vision_model.encoder.layers.12.self_attn.out_proj.SU', 'vision_model.encoder.layers.12.self_attn.out_proj.SV', 'vision_model.encoder.layers.12.self_attn.out_proj.rcp', 'vision_model.encoder.layers.12.self_attn.out_proj.tlut', 'vision_model.encoder.layers.12.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.out_proj.trellis', 'vision_model.encoder.layers.12.self_attn.q_proj.SU', 'vision_model.encoder.layers.12.self_attn.q_proj.SV', 'vision_model.encoder.layers.12.self_attn.q_proj.rcp', 'vision_model.encoder.layers.12.self_attn.q_proj.tlut', 'vision_model.encoder.layers.12.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.q_proj.trellis', 'vision_model.encoder.layers.12.self_attn.v_proj.SU', 'vision_model.encoder.layers.12.self_attn.v_proj.SV', 'vision_model.encoder.layers.12.self_attn.v_proj.rcp', 'vision_model.encoder.layers.12.self_attn.v_proj.tlut', 'vision_model.encoder.layers.12.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.v_proj.trellis', 'vision_model.encoder.layers.13.mlp.fc1.SU', 'vision_model.encoder.layers.13.mlp.fc1.SV', 'vision_model.encoder.layers.13.mlp.fc1.rcp', 'vision_model.encoder.layers.13.mlp.fc1.tlut', 'vision_model.encoder.layers.13.mlp.fc1.tp_rank', 'vision_model.encoder.layers.13.mlp.fc1.trellis', 'vision_model.encoder.layers.13.mlp.fc2.SU', 'vision_model.encoder.layers.13.mlp.fc2.SV', 'vision_model.encoder.layers.13.mlp.fc2.rcp', 'vision_model.encoder.layers.13.mlp.fc2.tlut', 'vision_model.encoder.layers.13.mlp.fc2.tp_rank', 'vision_model.encoder.layers.13.mlp.fc2.trellis', 'vision_model.encoder.layers.13.self_attn.k_proj.SU', 'vision_model.encoder.layers.13.self_attn.k_proj.SV', 'vision_model.encoder.layers.13.self_attn.k_proj.rcp', 'vision_model.encoder.layers.13.self_attn.k_proj.tlut', 'vision_model.encoder.layers.13.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.k_proj.trellis', 'vision_model.encoder.layers.13.self_attn.out_proj.SU', 'vision_model.encoder.layers.13.self_attn.out_proj.SV', 'vision_model.encoder.layers.13.self_attn.out_proj.rcp', 'vision_model.encoder.layers.13.self_attn.out_proj.tlut', 'vision_model.encoder.layers.13.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.out_proj.trellis', 'vision_model.encoder.layers.13.self_attn.q_proj.SU', 'vision_model.encoder.layers.13.self_attn.q_proj.SV', 'vision_model.encoder.layers.13.self_attn.q_proj.rcp', 'vision_model.encoder.layers.13.self_attn.q_proj.tlut', 'vision_model.encoder.layers.13.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.q_proj.trellis', 'vision_model.encoder.layers.13.self_attn.v_proj.SU', 'vision_model.encoder.layers.13.self_attn.v_proj.SV', 'vision_model.encoder.layers.13.self_attn.v_proj.rcp', 'vision_model.encoder.layers.13.self_attn.v_proj.tlut', 'vision_model.encoder.layers.13.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.v_proj.trellis', 'vision_model.encoder.layers.14.mlp.fc1.SU', 'vision_model.encoder.layers.14.mlp.fc1.SV', 'vision_model.encoder.layers.14.mlp.fc1.rcp', 'vision_model.encoder.layers.14.mlp.fc1.tlut', 'vision_model.encoder.layers.14.mlp.fc1.tp_rank', 'vision_model.encoder.layers.14.mlp.fc1.trellis', 'vision_model.encoder.layers.14.mlp.fc2.SU', 'vision_model.encoder.layers.14.mlp.fc2.SV', 'vision_model.encoder.layers.14.mlp.fc2.rcp', 'vision_model.encoder.layers.14.mlp.fc2.tlut', 'vision_model.encoder.layers.14.mlp.fc2.tp_rank', 'vision_model.encoder.layers.14.mlp.fc2.trellis', 'vision_model.encoder.layers.14.self_attn.k_proj.SU', 'vision_model.encoder.layers.14.self_attn.k_proj.SV', 'vision_model.encoder.layers.14.self_attn.k_proj.rcp', 'vision_model.encoder.layers.14.self_attn.k_proj.tlut', 'vision_model.encoder.layers.14.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.k_proj.trellis', 'vision_model.encoder.layers.14.self_attn.out_proj.SU', 'vision_model.encoder.layers.14.self_attn.out_proj.SV', 'vision_model.encoder.layers.14.self_attn.out_proj.rcp', 'vision_model.encoder.layers.14.self_attn.out_proj.tlut', 'vision_model.encoder.layers.14.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.out_proj.trellis', 'vision_model.encoder.layers.14.self_attn.q_proj.SU', 'vision_model.encoder.layers.14.self_attn.q_proj.SV', 'vision_model.encoder.layers.14.self_attn.q_proj.rcp', 'vision_model.encoder.layers.14.self_attn.q_proj.tlut', 'vision_model.encoder.layers.14.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.q_proj.trellis', 'vision_model.encoder.layers.14.self_attn.v_proj.SU', 'vision_model.encoder.layers.14.self_attn.v_proj.SV', 'vision_model.encoder.layers.14.self_attn.v_proj.rcp', 'vision_model.encoder.layers.14.self_attn.v_proj.tlut', 'vision_model.encoder.layers.14.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.v_proj.trellis', 'vision_model.encoder.layers.15.mlp.fc1.SU', 'vision_model.encoder.layers.15.mlp.fc1.SV', 'vision_model.encoder.layers.15.mlp.fc1.rcp', 'vision_model.encoder.layers.15.mlp.fc1.tlut', 'vision_model.encoder.layers.15.mlp.fc1.tp_rank', 'vision_model.encoder.layers.15.mlp.fc1.trellis', 'vision_model.encoder.layers.15.mlp.fc2.SU', 'vision_model.encoder.layers.15.mlp.fc2.SV', 'vision_model.encoder.layers.15.mlp.fc2.rcp', 'vision_model.encoder.layers.15.mlp.fc2.tlut', 'vision_model.encoder.layers.15.mlp.fc2.tp_rank', 'vision_model.encoder.layers.15.mlp.fc2.trellis', 'vision_model.encoder.layers.15.self_attn.k_proj.SU', 'vision_model.encoder.layers.15.self_attn.k_proj.SV', 'vision_model.encoder.layers.15.self_attn.k_proj.rcp', 'vision_model.encoder.layers.15.self_attn.k_proj.tlut', 'vision_model.encoder.layers.15.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.k_proj.trellis', 'vision_model.encoder.layers.15.self_attn.out_proj.SU', 'vision_model.encoder.layers.15.self_attn.out_proj.SV', 'vision_model.encoder.layers.15.self_attn.out_proj.rcp', 'vision_model.encoder.layers.15.self_attn.out_proj.tlut', 'vision_model.encoder.layers.15.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.out_proj.trellis', 'vision_model.encoder.layers.15.self_attn.q_proj.SU', 'vision_model.encoder.layers.15.self_attn.q_proj.SV', 'vision_model.encoder.layers.15.self_attn.q_proj.rcp', 'vision_model.encoder.layers.15.self_attn.q_proj.tlut', 'vision_model.encoder.layers.15.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.q_proj.trellis', 'vision_model.encoder.layers.15.self_attn.v_proj.SU', 'vision_model.encoder.layers.15.self_attn.v_proj.SV', 'vision_model.encoder.layers.15.self_attn.v_proj.rcp', 'vision_model.encoder.layers.15.self_attn.v_proj.tlut', 'vision_model.encoder.layers.15.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.v_proj.trellis', 'vision_model.encoder.layers.16.mlp.fc1.SU', 'vision_model.encoder.layers.16.mlp.fc1.SV', 'vision_model.encoder.layers.16.mlp.fc1.rcp', 'vision_model.encoder.layers.16.mlp.fc1.tlut', 'vision_model.encoder.layers.16.mlp.fc1.tp_rank', 'vision_model.encoder.layers.16.mlp.fc1.trellis', 'vision_model.encoder.layers.16.mlp.fc2.SU', 'vision_model.encoder.layers.16.mlp.fc2.SV', 'vision_model.encoder.layers.16.mlp.fc2.rcp', 'vision_model.encoder.layers.16.mlp.fc2.tlut', 'vision_model.encoder.layers.16.mlp.fc2.tp_rank', 'vision_model.encoder.layers.16.mlp.fc2.trellis', 'vision_model.encoder.layers.16.self_attn.k_proj.SU', 'vision_model.encoder.layers.16.self_attn.k_proj.SV', 'vision_model.encoder.layers.16.self_attn.k_proj.rcp', 'vision_model.encoder.layers.16.self_attn.k_proj.tlut', 'vision_model.encoder.layers.16.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.k_proj.trellis', 'vision_model.encoder.layers.16.self_attn.out_proj.SU', 'vision_model.encoder.layers.16.self_attn.out_proj.SV', 'vision_model.encoder.layers.16.self_attn.out_proj.rcp', 'vision_model.encoder.layers.16.self_attn.out_proj.tlut', 'vision_model.encoder.layers.16.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.out_proj.trellis', 'vision_model.encoder.layers.16.self_attn.q_proj.SU', 'vision_model.encoder.layers.16.self_attn.q_proj.SV', 'vision_model.encoder.layers.16.self_attn.q_proj.rcp', 'vision_model.encoder.layers.16.self_attn.q_proj.tlut', 'vision_model.encoder.layers.16.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.q_proj.trellis', 'vision_model.encoder.layers.16.self_attn.v_proj.SU', 'vision_model.encoder.layers.16.self_attn.v_proj.SV', 'vision_model.encoder.layers.16.self_attn.v_proj.rcp', 'vision_model.encoder.layers.16.self_attn.v_proj.tlut', 'vision_model.encoder.layers.16.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.v_proj.trellis', 'vision_model.encoder.layers.17.mlp.fc1.SU', 'vision_model.encoder.layers.17.mlp.fc1.SV', 'vision_model.encoder.layers.17.mlp.fc1.rcp', 'vision_model.encoder.layers.17.mlp.fc1.tlut', 'vision_model.encoder.layers.17.mlp.fc1.tp_rank', 'vision_model.encoder.layers.17.mlp.fc1.trellis', 'vision_model.encoder.layers.17.mlp.fc2.SU', 'vision_model.encoder.layers.17.mlp.fc2.SV', 'vision_model.encoder.layers.17.mlp.fc2.rcp', 'vision_model.encoder.layers.17.mlp.fc2.tlut', 'vision_model.encoder.layers.17.mlp.fc2.tp_rank', 'vision_model.encoder.layers.17.mlp.fc2.trellis', 'vision_model.encoder.layers.17.self_attn.k_proj.SU', 'vision_model.encoder.layers.17.self_attn.k_proj.SV', 'vision_model.encoder.layers.17.self_attn.k_proj.rcp', 'vision_model.encoder.layers.17.self_attn.k_proj.tlut', 'vision_model.encoder.layers.17.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.k_proj.trellis', 'vision_model.encoder.layers.17.self_attn.out_proj.SU', 'vision_model.encoder.layers.17.self_attn.out_proj.SV', 'vision_model.encoder.layers.17.self_attn.out_proj.rcp', 'vision_model.encoder.layers.17.self_attn.out_proj.tlut', 'vision_model.encoder.layers.17.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.out_proj.trellis', 'vision_model.encoder.layers.17.self_attn.q_proj.SU', 'vision_model.encoder.layers.17.self_attn.q_proj.SV', 'vision_model.encoder.layers.17.self_attn.q_proj.rcp', 'vision_model.encoder.layers.17.self_attn.q_proj.tlut', 'vision_model.encoder.layers.17.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.q_proj.trellis', 'vision_model.encoder.layers.17.self_attn.v_proj.SU', 'vision_model.encoder.layers.17.self_attn.v_proj.SV', 'vision_model.encoder.layers.17.self_attn.v_proj.rcp', 'vision_model.encoder.layers.17.self_attn.v_proj.tlut', 'vision_model.encoder.layers.17.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.v_proj.trellis', 'vision_model.encoder.layers.18.mlp.fc1.SU', 'vision_model.encoder.layers.18.mlp.fc1.SV', 'vision_model.encoder.layers.18.mlp.fc1.rcp', 'vision_model.encoder.layers.18.mlp.fc1.tlut', 'vision_model.encoder.layers.18.mlp.fc1.tp_rank', 'vision_model.encoder.layers.18.mlp.fc1.trellis', 'vision_model.encoder.layers.18.mlp.fc2.SU', 'vision_model.encoder.layers.18.mlp.fc2.SV', 'vision_model.encoder.layers.18.mlp.fc2.rcp', 'vision_model.encoder.layers.18.mlp.fc2.tlut', 'vision_model.encoder.layers.18.mlp.fc2.tp_rank', 'vision_model.encoder.layers.18.mlp.fc2.trellis', 'vision_model.encoder.layers.18.self_attn.k_proj.SU', 'vision_model.encoder.layers.18.self_attn.k_proj.SV', 'vision_model.encoder.layers.18.self_attn.k_proj.rcp', 'vision_model.encoder.layers.18.self_attn.k_proj.tlut', 'vision_model.encoder.layers.18.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.k_proj.trellis', 'vision_model.encoder.layers.18.self_attn.out_proj.SU', 'vision_model.encoder.layers.18.self_attn.out_proj.SV', 'vision_model.encoder.layers.18.self_attn.out_proj.rcp', 'vision_model.encoder.layers.18.self_attn.out_proj.tlut', 'vision_model.encoder.layers.18.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.out_proj.trellis', 'vision_model.encoder.layers.18.self_attn.q_proj.SU', 'vision_model.encoder.layers.18.self_attn.q_proj.SV', 'vision_model.encoder.layers.18.self_attn.q_proj.rcp', 'vision_model.encoder.layers.18.self_attn.q_proj.tlut', 'vision_model.encoder.layers.18.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.q_proj.trellis', 'vision_model.encoder.layers.18.self_attn.v_proj.SU', 'vision_model.encoder.layers.18.self_attn.v_proj.SV', 'vision_model.encoder.layers.18.self_attn.v_proj.rcp', 'vision_model.encoder.layers.18.self_attn.v_proj.tlut', 'vision_model.encoder.layers.18.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.v_proj.trellis', 'vision_model.encoder.layers.19.mlp.fc1.SU', 'vision_model.encoder.layers.19.mlp.fc1.SV', 'vision_model.encoder.layers.19.mlp.fc1.rcp', 'vision_model.encoder.layers.19.mlp.fc1.tlut', 'vision_model.encoder.layers.19.mlp.fc1.tp_rank', 'vision_model.encoder.layers.19.mlp.fc1.trellis', 'vision_model.encoder.layers.19.mlp.fc2.SU', 'vision_model.encoder.layers.19.mlp.fc2.SV', 'vision_model.encoder.layers.19.mlp.fc2.rcp', 'vision_model.encoder.layers.19.mlp.fc2.tlut', 'vision_model.encoder.layers.19.mlp.fc2.tp_rank', 'vision_model.encoder.layers.19.mlp.fc2.trellis', 'vision_model.encoder.layers.19.self_attn.k_proj.SU', 'vision_model.encoder.layers.19.self_attn.k_proj.SV', 'vision_model.encoder.layers.19.self_attn.k_proj.rcp', 'vision_model.encoder.layers.19.self_attn.k_proj.tlut', 'vision_model.encoder.layers.19.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.k_proj.trellis', 'vision_model.encoder.layers.19.self_attn.out_proj.SU', 'vision_model.encoder.layers.19.self_attn.out_proj.SV', 'vision_model.encoder.layers.19.self_attn.out_proj.rcp', 'vision_model.encoder.layers.19.self_attn.out_proj.tlut', 'vision_model.encoder.layers.19.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.out_proj.trellis', 'vision_model.encoder.layers.19.self_attn.q_proj.SU', 'vision_model.encoder.layers.19.self_attn.q_proj.SV', 'vision_model.encoder.layers.19.self_attn.q_proj.rcp', 'vision_model.encoder.layers.19.self_attn.q_proj.tlut', 'vision_model.encoder.layers.19.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.q_proj.trellis', 'vision_model.encoder.layers.19.self_attn.v_proj.SU', 'vision_model.encoder.layers.19.self_attn.v_proj.SV', 'vision_model.encoder.layers.19.self_attn.v_proj.rcp', 'vision_model.encoder.layers.19.self_attn.v_proj.tlut', 'vision_model.encoder.layers.19.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.v_proj.trellis', 'vision_model.encoder.layers.2.mlp.fc1.SU', 'vision_model.encoder.layers.2.mlp.fc1.SV', 'vision_model.encoder.layers.2.mlp.fc1.rcp', 'vision_model.encoder.layers.2.mlp.fc1.tlut', 'vision_model.encoder.layers.2.mlp.fc1.tp_rank', 'vision_model.encoder.layers.2.mlp.fc1.trellis', 'vision_model.encoder.layers.2.mlp.fc2.SU', 'vision_model.encoder.layers.2.mlp.fc2.SV', 'vision_model.encoder.layers.2.mlp.fc2.rcp', 'vision_model.encoder.layers.2.mlp.fc2.tlut', 'vision_model.encoder.layers.2.mlp.fc2.tp_rank', 'vision_model.encoder.layers.2.mlp.fc2.trellis', 'vision_model.encoder.layers.2.self_attn.k_proj.SU', 'vision_model.encoder.layers.2.self_attn.k_proj.SV', 'vision_model.encoder.layers.2.self_attn.k_proj.rcp', 'vision_model.encoder.layers.2.self_attn.k_proj.tlut', 'vision_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.k_proj.trellis', 'vision_model.encoder.layers.2.self_attn.out_proj.SU', 'vision_model.encoder.layers.2.self_attn.out_proj.SV', 'vision_model.encoder.layers.2.self_attn.out_proj.rcp', 'vision_model.encoder.layers.2.self_attn.out_proj.tlut', 'vision_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.out_proj.trellis', 'vision_model.encoder.layers.2.self_attn.q_proj.SU', 'vision_model.encoder.layers.2.self_attn.q_proj.SV', 'vision_model.encoder.layers.2.self_attn.q_proj.rcp', 'vision_model.encoder.layers.2.self_attn.q_proj.tlut', 'vision_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.q_proj.trellis', 'vision_model.encoder.layers.2.self_attn.v_proj.SU', 'vision_model.encoder.layers.2.self_attn.v_proj.SV', 'vision_model.encoder.layers.2.self_attn.v_proj.rcp', 'vision_model.encoder.layers.2.self_attn.v_proj.tlut', 'vision_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.v_proj.trellis', 'vision_model.encoder.layers.20.mlp.fc1.SU', 'vision_model.encoder.layers.20.mlp.fc1.SV', 'vision_model.encoder.layers.20.mlp.fc1.rcp', 'vision_model.encoder.layers.20.mlp.fc1.tlut', 'vision_model.encoder.layers.20.mlp.fc1.tp_rank', 'vision_model.encoder.layers.20.mlp.fc1.trellis', 'vision_model.encoder.layers.20.mlp.fc2.SU', 'vision_model.encoder.layers.20.mlp.fc2.SV', 'vision_model.encoder.layers.20.mlp.fc2.rcp', 'vision_model.encoder.layers.20.mlp.fc2.tlut', 'vision_model.encoder.layers.20.mlp.fc2.tp_rank', 'vision_model.encoder.layers.20.mlp.fc2.trellis', 'vision_model.encoder.layers.20.self_attn.k_proj.SU', 'vision_model.encoder.layers.20.self_attn.k_proj.SV', 'vision_model.encoder.layers.20.self_attn.k_proj.rcp', 'vision_model.encoder.layers.20.self_attn.k_proj.tlut', 'vision_model.encoder.layers.20.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.k_proj.trellis', 'vision_model.encoder.layers.20.self_attn.out_proj.SU', 'vision_model.encoder.layers.20.self_attn.out_proj.SV', 'vision_model.encoder.layers.20.self_attn.out_proj.rcp', 'vision_model.encoder.layers.20.self_attn.out_proj.tlut', 'vision_model.encoder.layers.20.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.out_proj.trellis', 'vision_model.encoder.layers.20.self_attn.q_proj.SU', 'vision_model.encoder.layers.20.self_attn.q_proj.SV', 'vision_model.encoder.layers.20.self_attn.q_proj.rcp', 'vision_model.encoder.layers.20.self_attn.q_proj.tlut', 'vision_model.encoder.layers.20.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.q_proj.trellis', 'vision_model.encoder.layers.20.self_attn.v_proj.SU', 'vision_model.encoder.layers.20.self_attn.v_proj.SV', 'vision_model.encoder.layers.20.self_attn.v_proj.rcp', 'vision_model.encoder.layers.20.self_attn.v_proj.tlut', 'vision_model.encoder.layers.20.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.v_proj.trellis', 'vision_model.encoder.layers.21.mlp.fc1.SU', 'vision_model.encoder.layers.21.mlp.fc1.SV', 'vision_model.encoder.layers.21.mlp.fc1.rcp', 'vision_model.encoder.layers.21.mlp.fc1.tlut', 'vision_model.encoder.layers.21.mlp.fc1.tp_rank', 'vision_model.encoder.layers.21.mlp.fc1.trellis', 'vision_model.encoder.layers.21.mlp.fc2.SU', 'vision_model.encoder.layers.21.mlp.fc2.SV', 'vision_model.encoder.layers.21.mlp.fc2.rcp', 'vision_model.encoder.layers.21.mlp.fc2.tlut', 'vision_model.encoder.layers.21.mlp.fc2.tp_rank', 'vision_model.encoder.layers.21.mlp.fc2.trellis', 'vision_model.encoder.layers.21.self_attn.k_proj.SU', 'vision_model.encoder.layers.21.self_attn.k_proj.SV', 'vision_model.encoder.layers.21.self_attn.k_proj.rcp', 'vision_model.encoder.layers.21.self_attn.k_proj.tlut', 'vision_model.encoder.layers.21.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.k_proj.trellis', 'vision_model.encoder.layers.21.self_attn.out_proj.SU', 'vision_model.encoder.layers.21.self_attn.out_proj.SV', 'vision_model.encoder.layers.21.self_attn.out_proj.rcp', 'vision_model.encoder.layers.21.self_attn.out_proj.tlut', 'vision_model.encoder.layers.21.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.out_proj.trellis', 'vision_model.encoder.layers.21.self_attn.q_proj.SU', 'vision_model.encoder.layers.21.self_attn.q_proj.SV', 'vision_model.encoder.layers.21.self_attn.q_proj.rcp', 'vision_model.encoder.layers.21.self_attn.q_proj.tlut', 'vision_model.encoder.layers.21.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.q_proj.trellis', 'vision_model.encoder.layers.21.self_attn.v_proj.SU', 'vision_model.encoder.layers.21.self_attn.v_proj.SV', 'vision_model.encoder.layers.21.self_attn.v_proj.rcp', 'vision_model.encoder.layers.21.self_attn.v_proj.tlut', 'vision_model.encoder.layers.21.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.v_proj.trellis', 'vision_model.encoder.layers.22.mlp.fc1.SU', 'vision_model.encoder.layers.22.mlp.fc1.SV', 'vision_model.encoder.layers.22.mlp.fc1.rcp', 'vision_model.encoder.layers.22.mlp.fc1.tlut', 'vision_model.encoder.layers.22.mlp.fc1.tp_rank', 'vision_model.encoder.layers.22.mlp.fc1.trellis', 'vision_model.encoder.layers.22.mlp.fc2.SU', 'vision_model.encoder.layers.22.mlp.fc2.SV', 'vision_model.encoder.layers.22.mlp.fc2.rcp', 'vision_model.encoder.layers.22.mlp.fc2.tlut', 'vision_model.encoder.layers.22.mlp.fc2.tp_rank', 'vision_model.encoder.layers.22.mlp.fc2.trellis', 'vision_model.encoder.layers.22.self_attn.k_proj.SU', 'vision_model.encoder.layers.22.self_attn.k_proj.SV', 'vision_model.encoder.layers.22.self_attn.k_proj.rcp', 'vision_model.encoder.layers.22.self_attn.k_proj.tlut', 'vision_model.encoder.layers.22.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.k_proj.trellis', 'vision_model.encoder.layers.22.self_attn.out_proj.SU', 'vision_model.encoder.layers.22.self_attn.out_proj.SV', 'vision_model.encoder.layers.22.self_attn.out_proj.rcp', 'vision_model.encoder.layers.22.self_attn.out_proj.tlut', 'vision_model.encoder.layers.22.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.out_proj.trellis', 'vision_model.encoder.layers.22.self_attn.q_proj.SU', 'vision_model.encoder.layers.22.self_attn.q_proj.SV', 'vision_model.encoder.layers.22.self_attn.q_proj.rcp', 'vision_model.encoder.layers.22.self_attn.q_proj.tlut', 'vision_model.encoder.layers.22.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.q_proj.trellis', 'vision_model.encoder.layers.22.self_attn.v_proj.SU', 'vision_model.encoder.layers.22.self_attn.v_proj.SV', 'vision_model.encoder.layers.22.self_attn.v_proj.rcp', 'vision_model.encoder.layers.22.self_attn.v_proj.tlut', 'vision_model.encoder.layers.22.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.v_proj.trellis', 'vision_model.encoder.layers.23.mlp.fc1.SU', 'vision_model.encoder.layers.23.mlp.fc1.SV', 'vision_model.encoder.layers.23.mlp.fc1.rcp', 'vision_model.encoder.layers.23.mlp.fc1.tlut', 'vision_model.encoder.layers.23.mlp.fc1.tp_rank', 'vision_model.encoder.layers.23.mlp.fc1.trellis', 'vision_model.encoder.layers.23.mlp.fc2.SU', 'vision_model.encoder.layers.23.mlp.fc2.SV', 'vision_model.encoder.layers.23.mlp.fc2.rcp', 'vision_model.encoder.layers.23.mlp.fc2.tlut', 'vision_model.encoder.layers.23.mlp.fc2.tp_rank', 'vision_model.encoder.layers.23.mlp.fc2.trellis', 'vision_model.encoder.layers.23.self_attn.k_proj.SU', 'vision_model.encoder.layers.23.self_attn.k_proj.SV', 'vision_model.encoder.layers.23.self_attn.k_proj.rcp', 'vision_model.encoder.layers.23.self_attn.k_proj.tlut', 'vision_model.encoder.layers.23.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.k_proj.trellis', 'vision_model.encoder.layers.23.self_attn.out_proj.SU', 'vision_model.encoder.layers.23.self_attn.out_proj.SV', 'vision_model.encoder.layers.23.self_attn.out_proj.rcp', 'vision_model.encoder.layers.23.self_attn.out_proj.tlut', 'vision_model.encoder.layers.23.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.out_proj.trellis', 'vision_model.encoder.layers.23.self_attn.q_proj.SU', 'vision_model.encoder.layers.23.self_attn.q_proj.SV', 'vision_model.encoder.layers.23.self_attn.q_proj.rcp', 'vision_model.encoder.layers.23.self_attn.q_proj.tlut', 'vision_model.encoder.layers.23.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.q_proj.trellis', 'vision_model.encoder.layers.23.self_attn.v_proj.SU', 'vision_model.encoder.layers.23.self_attn.v_proj.SV', 'vision_model.encoder.layers.23.self_attn.v_proj.rcp', 'vision_model.encoder.layers.23.self_attn.v_proj.tlut', 'vision_model.encoder.layers.23.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.v_proj.trellis', 'vision_model.encoder.layers.3.mlp.fc1.SU', 'vision_model.encoder.layers.3.mlp.fc1.SV', 'vision_model.encoder.layers.3.mlp.fc1.rcp', 'vision_model.encoder.layers.3.mlp.fc1.tlut', 'vision_model.encoder.layers.3.mlp.fc1.tp_rank', 'vision_model.encoder.layers.3.mlp.fc1.trellis', 'vision_model.encoder.layers.3.mlp.fc2.SU', 'vision_model.encoder.layers.3.mlp.fc2.SV', 'vision_model.encoder.layers.3.mlp.fc2.rcp', 'vision_model.encoder.layers.3.mlp.fc2.tlut', 'vision_model.encoder.layers.3.mlp.fc2.tp_rank', 'vision_model.encoder.layers.3.mlp.fc2.trellis', 'vision_model.encoder.layers.3.self_attn.k_proj.SU', 'vision_model.encoder.layers.3.self_attn.k_proj.SV', 'vision_model.encoder.layers.3.self_attn.k_proj.rcp', 'vision_model.encoder.layers.3.self_attn.k_proj.tlut', 'vision_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.k_proj.trellis', 'vision_model.encoder.layers.3.self_attn.out_proj.SU', 'vision_model.encoder.layers.3.self_attn.out_proj.SV', 'vision_model.encoder.layers.3.self_attn.out_proj.rcp', 'vision_model.encoder.layers.3.self_attn.out_proj.tlut', 'vision_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.out_proj.trellis', 'vision_model.encoder.layers.3.self_attn.q_proj.SU', 'vision_model.encoder.layers.3.self_attn.q_proj.SV', 'vision_model.encoder.layers.3.self_attn.q_proj.rcp', 'vision_model.encoder.layers.3.self_attn.q_proj.tlut', 'vision_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.q_proj.trellis', 'vision_model.encoder.layers.3.self_attn.v_proj.SU', 'vision_model.encoder.layers.3.self_attn.v_proj.SV', 'vision_model.encoder.layers.3.self_attn.v_proj.rcp', 'vision_model.encoder.layers.3.self_attn.v_proj.tlut', 'vision_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.v_proj.trellis', 'vision_model.encoder.layers.4.mlp.fc1.SU', 'vision_model.encoder.layers.4.mlp.fc1.SV', 'vision_model.encoder.layers.4.mlp.fc1.rcp', 'vision_model.encoder.layers.4.mlp.fc1.tlut', 'vision_model.encoder.layers.4.mlp.fc1.tp_rank', 'vision_model.encoder.layers.4.mlp.fc1.trellis', 'vision_model.encoder.layers.4.mlp.fc2.SU', 'vision_model.encoder.layers.4.mlp.fc2.SV', 'vision_model.encoder.layers.4.mlp.fc2.rcp', 'vision_model.encoder.layers.4.mlp.fc2.tlut', 'vision_model.encoder.layers.4.mlp.fc2.tp_rank', 'vision_model.encoder.layers.4.mlp.fc2.trellis', 'vision_model.encoder.layers.4.self_attn.k_proj.SU', 'vision_model.encoder.layers.4.self_attn.k_proj.SV', 'vision_model.encoder.layers.4.self_attn.k_proj.rcp', 'vision_model.encoder.layers.4.self_attn.k_proj.tlut', 'vision_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.k_proj.trellis', 'vision_model.encoder.layers.4.self_attn.out_proj.SU', 'vision_model.encoder.layers.4.self_attn.out_proj.SV', 'vision_model.encoder.layers.4.self_attn.out_proj.rcp', 'vision_model.encoder.layers.4.self_attn.out_proj.tlut', 'vision_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.out_proj.trellis', 'vision_model.encoder.layers.4.self_attn.q_proj.SU', 'vision_model.encoder.layers.4.self_attn.q_proj.SV', 'vision_model.encoder.layers.4.self_attn.q_proj.rcp', 'vision_model.encoder.layers.4.self_attn.q_proj.tlut', 'vision_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.q_proj.trellis', 'vision_model.encoder.layers.4.self_attn.v_proj.SU', 'vision_model.encoder.layers.4.self_attn.v_proj.SV', 'vision_model.encoder.layers.4.self_attn.v_proj.rcp', 'vision_model.encoder.layers.4.self_attn.v_proj.tlut', 'vision_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.v_proj.trellis', 'vision_model.encoder.layers.5.mlp.fc1.SU', 'vision_model.encoder.layers.5.mlp.fc1.SV', 'vision_model.encoder.layers.5.mlp.fc1.rcp', 'vision_model.encoder.layers.5.mlp.fc1.tlut', 'vision_model.encoder.layers.5.mlp.fc1.tp_rank', 'vision_model.encoder.layers.5.mlp.fc1.trellis', 'vision_model.encoder.layers.5.mlp.fc2.SU', 'vision_model.encoder.layers.5.mlp.fc2.SV', 'vision_model.encoder.layers.5.mlp.fc2.rcp', 'vision_model.encoder.layers.5.mlp.fc2.tlut', 'vision_model.encoder.layers.5.mlp.fc2.tp_rank', 'vision_model.encoder.layers.5.mlp.fc2.trellis', 'vision_model.encoder.layers.5.self_attn.k_proj.SU', 'vision_model.encoder.layers.5.self_attn.k_proj.SV', 'vision_model.encoder.layers.5.self_attn.k_proj.rcp', 'vision_model.encoder.layers.5.self_attn.k_proj.tlut', 'vision_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.k_proj.trellis', 'vision_model.encoder.layers.5.self_attn.out_proj.SU', 'vision_model.encoder.layers.5.self_attn.out_proj.SV', 'vision_model.encoder.layers.5.self_attn.out_proj.rcp', 'vision_model.encoder.layers.5.self_attn.out_proj.tlut', 'vision_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.out_proj.trellis', 'vision_model.encoder.layers.5.self_attn.q_proj.SU', 'vision_model.encoder.layers.5.self_attn.q_proj.SV', 'vision_model.encoder.layers.5.self_attn.q_proj.rcp', 'vision_model.encoder.layers.5.self_attn.q_proj.tlut', 'vision_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.q_proj.trellis', 'vision_model.encoder.layers.5.self_attn.v_proj.SU', 'vision_model.encoder.layers.5.self_attn.v_proj.SV', 'vision_model.encoder.layers.5.self_attn.v_proj.rcp', 'vision_model.encoder.layers.5.self_attn.v_proj.tlut', 'vision_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.v_proj.trellis', 'vision_model.encoder.layers.6.mlp.fc1.SU', 'vision_model.encoder.layers.6.mlp.fc1.SV', 'vision_model.encoder.layers.6.mlp.fc1.rcp', 'vision_model.encoder.layers.6.mlp.fc1.tlut', 'vision_model.encoder.layers.6.mlp.fc1.tp_rank', 'vision_model.encoder.layers.6.mlp.fc1.trellis', 'vision_model.encoder.layers.6.mlp.fc2.SU', 'vision_model.encoder.layers.6.mlp.fc2.SV', 'vision_model.encoder.layers.6.mlp.fc2.rcp', 'vision_model.encoder.layers.6.mlp.fc2.tlut', 'vision_model.encoder.layers.6.mlp.fc2.tp_rank', 'vision_model.encoder.layers.6.mlp.fc2.trellis', 'vision_model.encoder.layers.6.self_attn.k_proj.SU', 'vision_model.encoder.layers.6.self_attn.k_proj.SV', 'vision_model.encoder.layers.6.self_attn.k_proj.rcp', 'vision_model.encoder.layers.6.self_attn.k_proj.tlut', 'vision_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.k_proj.trellis', 'vision_model.encoder.layers.6.self_attn.out_proj.SU', 'vision_model.encoder.layers.6.self_attn.out_proj.SV', 'vision_model.encoder.layers.6.self_attn.out_proj.rcp', 'vision_model.encoder.layers.6.self_attn.out_proj.tlut', 'vision_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.out_proj.trellis', 'vision_model.encoder.layers.6.self_attn.q_proj.SU', 'vision_model.encoder.layers.6.self_attn.q_proj.SV', 'vision_model.encoder.layers.6.self_attn.q_proj.rcp', 'vision_model.encoder.layers.6.self_attn.q_proj.tlut', 'vision_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.q_proj.trellis', 'vision_model.encoder.layers.6.self_attn.v_proj.SU', 'vision_model.encoder.layers.6.self_attn.v_proj.SV', 'vision_model.encoder.layers.6.self_attn.v_proj.rcp', 'vision_model.encoder.layers.6.self_attn.v_proj.tlut', 'vision_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.v_proj.trellis', 'vision_model.encoder.layers.7.mlp.fc1.SU', 'vision_model.encoder.layers.7.mlp.fc1.SV', 'vision_model.encoder.layers.7.mlp.fc1.rcp', 'vision_model.encoder.layers.7.mlp.fc1.tlut', 'vision_model.encoder.layers.7.mlp.fc1.tp_rank', 'vision_model.encoder.layers.7.mlp.fc1.trellis', 'vision_model.encoder.layers.7.mlp.fc2.SU', 'vision_model.encoder.layers.7.mlp.fc2.SV', 'vision_model.encoder.layers.7.mlp.fc2.rcp', 'vision_model.encoder.layers.7.mlp.fc2.tlut', 'vision_model.encoder.layers.7.mlp.fc2.tp_rank', 'vision_model.encoder.layers.7.mlp.fc2.trellis', 'vision_model.encoder.layers.7.self_attn.k_proj.SU', 'vision_model.encoder.layers.7.self_attn.k_proj.SV', 'vision_model.encoder.layers.7.self_attn.k_proj.rcp', 'vision_model.encoder.layers.7.self_attn.k_proj.tlut', 'vision_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.k_proj.trellis', 'vision_model.encoder.layers.7.self_attn.out_proj.SU', 'vision_model.encoder.layers.7.self_attn.out_proj.SV', 'vision_model.encoder.layers.7.self_attn.out_proj.rcp', 'vision_model.encoder.layers.7.self_attn.out_proj.tlut', 'vision_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.out_proj.trellis', 'vision_model.encoder.layers.7.self_attn.q_proj.SU', 'vision_model.encoder.layers.7.self_attn.q_proj.SV', 'vision_model.encoder.layers.7.self_attn.q_proj.rcp', 'vision_model.encoder.layers.7.self_attn.q_proj.tlut', 'vision_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.q_proj.trellis', 'vision_model.encoder.layers.7.self_attn.v_proj.SU', 'vision_model.encoder.layers.7.self_attn.v_proj.SV', 'vision_model.encoder.layers.7.self_attn.v_proj.rcp', 'vision_model.encoder.layers.7.self_attn.v_proj.tlut', 'vision_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.v_proj.trellis', 'vision_model.encoder.layers.8.mlp.fc1.SU', 'vision_model.encoder.layers.8.mlp.fc1.SV', 'vision_model.encoder.layers.8.mlp.fc1.rcp', 'vision_model.encoder.layers.8.mlp.fc1.tlut', 'vision_model.encoder.layers.8.mlp.fc1.tp_rank', 'vision_model.encoder.layers.8.mlp.fc1.trellis', 'vision_model.encoder.layers.8.mlp.fc2.SU', 'vision_model.encoder.layers.8.mlp.fc2.SV', 'vision_model.encoder.layers.8.mlp.fc2.rcp', 'vision_model.encoder.layers.8.mlp.fc2.tlut', 'vision_model.encoder.layers.8.mlp.fc2.tp_rank', 'vision_model.encoder.layers.8.mlp.fc2.trellis', 'vision_model.encoder.layers.8.self_attn.k_proj.SU', 'vision_model.encoder.layers.8.self_attn.k_proj.SV', 'vision_model.encoder.layers.8.self_attn.k_proj.rcp', 'vision_model.encoder.layers.8.self_attn.k_proj.tlut', 'vision_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.k_proj.trellis', 'vision_model.encoder.layers.8.self_attn.out_proj.SU', 'vision_model.encoder.layers.8.self_attn.out_proj.SV', 'vision_model.encoder.layers.8.self_attn.out_proj.rcp', 'vision_model.encoder.layers.8.self_attn.out_proj.tlut', 'vision_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.out_proj.trellis', 'vision_model.encoder.layers.8.self_attn.q_proj.SU', 'vision_model.encoder.layers.8.self_attn.q_proj.SV', 'vision_model.encoder.layers.8.self_attn.q_proj.rcp', 'vision_model.encoder.layers.8.self_attn.q_proj.tlut', 'vision_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.q_proj.trellis', 'vision_model.encoder.layers.8.self_attn.v_proj.SU', 'vision_model.encoder.layers.8.self_attn.v_proj.SV', 'vision_model.encoder.layers.8.self_attn.v_proj.rcp', 'vision_model.encoder.layers.8.self_attn.v_proj.tlut', 'vision_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.v_proj.trellis', 'vision_model.encoder.layers.9.mlp.fc1.SU', 'vision_model.encoder.layers.9.mlp.fc1.SV', 'vision_model.encoder.layers.9.mlp.fc1.rcp', 'vision_model.encoder.layers.9.mlp.fc1.tlut', 'vision_model.encoder.layers.9.mlp.fc1.tp_rank', 'vision_model.encoder.layers.9.mlp.fc1.trellis', 'vision_model.encoder.layers.9.mlp.fc2.SU', 'vision_model.encoder.layers.9.mlp.fc2.SV', 'vision_model.encoder.layers.9.mlp.fc2.rcp', 'vision_model.encoder.layers.9.mlp.fc2.tlut', 'vision_model.encoder.layers.9.mlp.fc2.tp_rank', 'vision_model.encoder.layers.9.mlp.fc2.trellis', 'vision_model.encoder.layers.9.self_attn.k_proj.SU', 'vision_model.encoder.layers.9.self_attn.k_proj.SV', 'vision_model.encoder.layers.9.self_attn.k_proj.rcp', 'vision_model.encoder.layers.9.self_attn.k_proj.tlut', 'vision_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.k_proj.trellis', 'vision_model.encoder.layers.9.self_attn.out_proj.SU', 'vision_model.encoder.layers.9.self_attn.out_proj.SV', 'vision_model.encoder.layers.9.self_attn.out_proj.rcp', 'vision_model.encoder.layers.9.self_attn.out_proj.tlut', 'vision_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.out_proj.trellis', 'vision_model.encoder.layers.9.self_attn.q_proj.SU', 'vision_model.encoder.layers.9.self_attn.q_proj.SV', 'vision_model.encoder.layers.9.self_attn.q_proj.rcp', 'vision_model.encoder.layers.9.self_attn.q_proj.tlut', 'vision_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.q_proj.trellis', 'vision_model.encoder.layers.9.self_attn.v_proj.SU', 'vision_model.encoder.layers.9.self_attn.v_proj.SV', 'vision_model.encoder.layers.9.self_attn.v_proj.rcp', 'vision_model.encoder.layers.9.self_attn.v_proj.tlut', 'vision_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0416 08:40:10.956036 3360304 hfize_clip.py:65] Loading text layer 0
W0416 08:40:10.956363 3360304 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(ln_path, map_location=cpu)

W0416 08:40:10.957759 3360304 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved = torch.load(f'{path_prefix}/{full_key}.pt', map_location='cpu')

I0416 08:40:10.985311 3360304 hfize_clip.py:65] Loading text layer 1
I0416 08:40:11.006175 3360304 hfize_clip.py:65] Loading text layer 2
I0416 08:40:11.031948 3360304 hfize_clip.py:65] Loading text layer 3
I0416 08:40:11.087192 3360304 hfize_clip.py:65] Loading text layer 4
I0416 08:40:11.109278 3360304 hfize_clip.py:65] Loading text layer 5
I0416 08:40:11.133363 3360304 hfize_clip.py:65] Loading text layer 6
I0416 08:40:11.149748 3360304 hfize_clip.py:65] Loading text layer 7
I0416 08:40:11.165566 3360304 hfize_clip.py:65] Loading text layer 8
I0416 08:40:11.182681 3360304 hfize_clip.py:65] Loading text layer 9
I0416 08:40:11.201174 3360304 hfize_clip.py:65] Loading text layer 10
I0416 08:40:11.216869 3360304 hfize_clip.py:65] Loading text layer 11
I0416 08:40:11.265652 3360304 hfize_clip.py:65] Loading vision layer 0
I0416 08:40:11.334644 3360304 hfize_clip.py:65] Loading vision layer 1
I0416 08:40:11.388258 3360304 hfize_clip.py:65] Loading vision layer 2
I0416 08:40:11.413831 3360304 hfize_clip.py:65] Loading vision layer 3
I0416 08:40:11.474219 3360304 hfize_clip.py:65] Loading vision layer 4
I0416 08:40:11.546533 3360304 hfize_clip.py:65] Loading vision layer 5
I0416 08:40:11.586515 3360304 hfize_clip.py:65] Loading vision layer 6
I0416 08:40:11.640105 3360304 hfize_clip.py:65] Loading vision layer 7
I0416 08:40:11.699172 3360304 hfize_clip.py:65] Loading vision layer 8
I0416 08:40:11.768098 3360304 hfize_clip.py:65] Loading vision layer 9
I0416 08:40:11.801197 3360304 hfize_clip.py:65] Loading vision layer 10
I0416 08:40:11.820211 3360304 hfize_clip.py:65] Loading vision layer 11
I0416 08:40:11.864110 3360304 hfize_clip.py:65] Loading vision layer 12
I0416 08:40:11.879063 3360304 hfize_clip.py:65] Loading vision layer 13
I0416 08:40:11.907936 3360304 hfize_clip.py:65] Loading vision layer 14
I0416 08:40:11.921695 3360304 hfize_clip.py:65] Loading vision layer 15
I0416 08:40:11.934892 3360304 hfize_clip.py:65] Loading vision layer 16
I0416 08:40:11.954906 3360304 hfize_clip.py:65] Loading vision layer 17
I0416 08:40:11.965802 3360304 hfize_clip.py:65] Loading vision layer 18
I0416 08:40:11.975528 3360304 hfize_clip.py:65] Loading vision layer 19
I0416 08:40:11.992199 3360304 hfize_clip.py:65] Loading vision layer 20
I0416 08:40:12.001895 3360304 hfize_clip.py:65] Loading vision layer 21
I0416 08:40:12.011724 3360304 hfize_clip.py:65] Loading vision layer 22
I0416 08:40:12.025303 3360304 hfize_clip.py:65] Loading vision layer 23
I0416 08:40:12.039796 3360304 hfize_clip.py:95] Copying logit_scale
I0416 08:40:12.039957 3360304 hfize_clip.py:90] Copying submodule: visual_projection
I0416 08:40:12.040959 3360304 hfize_clip.py:90] Copying submodule: text_projection
I0416 08:40:12.041807 3360304 hfize_clip.py:90] Copying submodule: text token_embedding
I0416 08:40:12.066258 3360304 hfize_clip.py:90] Copying submodule: text position_embedding
I0416 08:40:12.076812 3360304 hfize_clip.py:90] Copying submodule: text final_layer_norm
I0416 08:40:12.076998 3360304 hfize_clip.py:90] Copying submodule: vision patch_embedding
I0416 08:40:12.083964 3360304 hfize_clip.py:90] Copying submodule: vision position_embedding
I0416 08:40:12.096275 3360304 hfize_clip.py:90] Copying submodule: vision post_layernorm
I0416 08:40:12.096538 3360304 hfize_clip.py:121] Saving model to ../hf_model_comp/qtip/hf/clip-vit-large-patch14_6bit...
  0%|          | 0/196 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  1%|          | 1/196 [00:11<35:59, 11.08s/it]  1%|          | 2/196 [00:13<19:26,  6.01s/it]  2%|▏         | 3/196 [00:15<14:06,  4.39s/it]  2%|▏         | 4/196 [00:18<11:36,  3.63s/it]  3%|▎         | 5/196 [00:20<10:13,  3.21s/it]  3%|▎         | 6/196 [00:23<09:22,  2.96s/it]  4%|▎         | 7/196 [00:25<08:48,  2.80s/it]  4%|▍         | 8/196 [00:28<08:25,  2.69s/it]  5%|▍         | 9/196 [00:30<08:10,  2.62s/it]  5%|▌         | 10/196 [00:33<07:58,  2.57s/it]  6%|▌         | 11/196 [00:35<07:49,  2.54s/it]  6%|▌         | 12/196 [00:38<07:43,  2.52s/it]  7%|▋         | 13/196 [00:40<07:37,  2.50s/it]  7%|▋         | 14/196 [00:43<07:33,  2.49s/it]  8%|▊         | 15/196 [00:45<07:29,  2.48s/it]  8%|▊         | 16/196 [00:48<07:25,  2.48s/it]  9%|▊         | 17/196 [00:50<07:22,  2.47s/it]  9%|▉         | 18/196 [00:52<07:20,  2.47s/it] 10%|▉         | 19/196 [00:55<07:18,  2.48s/it] 10%|█         | 20/196 [00:57<07:15,  2.47s/it] 11%|█         | 21/196 [01:00<07:13,  2.48s/it] 11%|█         | 22/196 [01:02<07:11,  2.48s/it] 12%|█▏        | 23/196 [01:05<07:08,  2.48s/it] 12%|█▏        | 24/196 [01:07<07:07,  2.48s/it] 13%|█▎        | 25/196 [01:10<07:05,  2.49s/it] 13%|█▎        | 26/196 [01:12<07:02,  2.49s/it] 14%|█▍        | 27/196 [01:15<06:59,  2.48s/it] 14%|█▍        | 28/196 [01:17<06:57,  2.48s/it] 15%|█▍        | 29/196 [01:20<06:54,  2.48s/it] 15%|█▌        | 30/196 [01:22<06:51,  2.48s/it] 16%|█▌        | 31/196 [01:25<06:48,  2.47s/it] 16%|█▋        | 32/196 [01:27<06:46,  2.48s/it] 17%|█▋        | 33/196 [01:30<06:43,  2.47s/it] 17%|█▋        | 34/196 [01:32<06:40,  2.47s/it] 18%|█▊        | 35/196 [01:35<06:37,  2.47s/it] 18%|█▊        | 36/196 [01:37<06:34,  2.47s/it] 19%|█▉        | 37/196 [01:40<06:31,  2.47s/it] 19%|█▉        | 38/196 [01:42<06:29,  2.47s/it] 20%|█▉        | 39/196 [01:44<06:27,  2.47s/it] 20%|██        | 40/196 [01:47<06:24,  2.47s/it] 21%|██        | 41/196 [01:49<06:21,  2.46s/it] 21%|██▏       | 42/196 [01:52<06:19,  2.47s/it] 22%|██▏       | 43/196 [01:54<06:17,  2.47s/it] 22%|██▏       | 44/196 [01:57<06:15,  2.47s/it] 23%|██▎       | 45/196 [01:59<06:13,  2.47s/it] 23%|██▎       | 46/196 [02:02<06:11,  2.48s/it] 24%|██▍       | 47/196 [02:04<06:08,  2.47s/it] 24%|██▍       | 48/196 [02:07<06:05,  2.47s/it] 25%|██▌       | 49/196 [02:09<06:03,  2.47s/it] 26%|██▌       | 50/196 [02:12<06:01,  2.47s/it] 26%|██▌       | 51/196 [02:14<05:58,  2.48s/it] 27%|██▋       | 52/196 [02:17<05:56,  2.48s/it] 27%|██▋       | 53/196 [02:19<05:54,  2.48s/it] 28%|██▊       | 54/196 [02:22<05:51,  2.48s/it] 28%|██▊       | 55/196 [02:24<05:48,  2.47s/it] 29%|██▊       | 56/196 [02:27<05:46,  2.47s/it] 29%|██▉       | 57/196 [02:29<05:42,  2.47s/it] 30%|██▉       | 58/196 [02:31<05:40,  2.47s/it] 30%|███       | 59/196 [02:34<05:37,  2.47s/it] 31%|███       | 60/196 [02:36<05:36,  2.47s/it] 31%|███       | 61/196 [02:39<05:33,  2.47s/it] 32%|███▏      | 62/196 [02:41<05:31,  2.47s/it] 32%|███▏      | 63/196 [02:44<05:28,  2.47s/it] 33%|███▎      | 64/196 [02:46<05:26,  2.47s/it] 33%|███▎      | 65/196 [02:49<05:23,  2.47s/it] 34%|███▎      | 66/196 [02:51<05:21,  2.47s/it] 34%|███▍      | 67/196 [02:54<05:31,  2.57s/it] 35%|███▍      | 68/196 [02:56<05:25,  2.54s/it] 35%|███▌      | 69/196 [02:59<05:19,  2.52s/it] 36%|███▌      | 70/196 [03:01<05:15,  2.50s/it] 36%|███▌      | 71/196 [03:04<05:11,  2.49s/it] 37%|███▋      | 72/196 [03:06<05:08,  2.48s/it] 37%|███▋      | 73/196 [03:09<05:05,  2.48s/it] 38%|███▊      | 74/196 [03:11<05:02,  2.48s/it] 38%|███▊      | 75/196 [03:14<04:59,  2.48s/it] 39%|███▉      | 76/196 [03:16<04:56,  2.47s/it] 39%|███▉      | 77/196 [03:19<04:53,  2.47s/it] 40%|███▉      | 78/196 [03:21<04:51,  2.47s/it] 40%|████      | 79/196 [03:24<04:48,  2.47s/it] 41%|████      | 80/196 [03:26<04:46,  2.47s/it] 41%|████▏     | 81/196 [03:29<04:44,  2.47s/it] 42%|████▏     | 82/196 [03:31<04:42,  2.47s/it] 42%|████▏     | 83/196 [03:34<04:39,  2.48s/it] 43%|████▎     | 84/196 [03:36<04:37,  2.48s/it] 43%|████▎     | 85/196 [03:38<04:34,  2.47s/it] 44%|████▍     | 86/196 [03:41<04:31,  2.47s/it] 44%|████▍     | 87/196 [03:43<04:29,  2.47s/it] 45%|████▍     | 88/196 [03:46<04:26,  2.47s/it] 45%|████▌     | 89/196 [03:48<04:23,  2.47s/it] 46%|████▌     | 90/196 [03:51<04:21,  2.47s/it] 46%|████▋     | 91/196 [03:53<04:18,  2.47s/it] 47%|████▋     | 92/196 [03:56<04:16,  2.47s/it] 47%|████▋     | 93/196 [03:58<04:14,  2.47s/it] 48%|████▊     | 94/196 [04:01<04:12,  2.47s/it] 48%|████▊     | 95/196 [04:03<04:09,  2.47s/it] 49%|████▉     | 96/196 [04:06<04:07,  2.47s/it] 49%|████▉     | 97/196 [04:08<04:04,  2.47s/it] 50%|█████     | 98/196 [04:11<04:01,  2.47s/it] 51%|█████     | 99/196 [04:13<03:59,  2.47s/it] 51%|█████     | 100/196 [04:16<03:57,  2.47s/it] 52%|█████▏    | 101/196 [04:18<03:55,  2.47s/it] 52%|█████▏    | 102/196 [04:20<03:52,  2.48s/it] 53%|█████▎    | 103/196 [04:23<03:50,  2.48s/it] 53%|█████▎    | 104/196 [04:25<03:47,  2.47s/it] 54%|█████▎    | 105/196 [04:28<03:44,  2.47s/it] 54%|█████▍    | 106/196 [04:30<03:42,  2.47s/it] 55%|█████▍    | 107/196 [04:33<03:40,  2.47s/it] 55%|█████▌    | 108/196 [04:35<03:38,  2.48s/it] 56%|█████▌    | 109/196 [04:38<03:35,  2.48s/it] 56%|█████▌    | 110/196 [04:40<03:33,  2.48s/it] 57%|█████▋    | 111/196 [04:43<03:30,  2.48s/it] 57%|█████▋    | 112/196 [04:45<03:28,  2.48s/it] 58%|█████▊    | 113/196 [04:48<03:25,  2.48s/it] 58%|█████▊    | 114/196 [04:50<03:23,  2.48s/it] 59%|█████▊    | 115/196 [04:53<03:21,  2.48s/it] 59%|█████▉    | 116/196 [04:55<03:18,  2.48s/it] 60%|█████▉    | 117/196 [04:58<03:16,  2.48s/it] 60%|██████    | 118/196 [05:00<03:13,  2.48s/it] 61%|██████    | 119/196 [05:03<03:11,  2.49s/it] 61%|██████    | 120/196 [05:05<03:08,  2.48s/it] 62%|██████▏   | 121/196 [05:08<03:06,  2.48s/it] 62%|██████▏   | 122/196 [05:10<03:04,  2.50s/it] 63%|██████▎   | 123/196 [05:13<03:01,  2.49s/it] 63%|██████▎   | 124/196 [05:15<02:59,  2.49s/it] 64%|██████▍   | 125/196 [05:18<02:56,  2.48s/it] 64%|██████▍   | 126/196 [05:20<02:53,  2.48s/it] 65%|██████▍   | 127/196 [05:23<02:51,  2.48s/it] 65%|██████▌   | 128/196 [05:25<02:48,  2.48s/it] 66%|██████▌   | 129/196 [05:27<02:45,  2.47s/it] 66%|██████▋   | 130/196 [05:30<02:43,  2.48s/it] 67%|██████▋   | 131/196 [05:32<02:40,  2.47s/it] 67%|██████▋   | 132/196 [05:35<02:38,  2.47s/it] 68%|██████▊   | 133/196 [05:37<02:35,  2.47s/it] 68%|██████▊   | 134/196 [05:40<02:32,  2.47s/it] 69%|██████▉   | 135/196 [05:42<02:30,  2.47s/it] 69%|██████▉   | 136/196 [05:45<02:28,  2.48s/it] 70%|██████▉   | 137/196 [05:47<02:26,  2.48s/it] 70%|███████   | 138/196 [05:50<02:23,  2.48s/it] 71%|███████   | 139/196 [05:52<02:21,  2.48s/it] 71%|███████▏  | 140/196 [05:55<02:18,  2.48s/it] 72%|███████▏  | 141/196 [05:57<02:16,  2.48s/it] 72%|███████▏  | 142/196 [06:00<02:14,  2.48s/it] 73%|███████▎  | 143/196 [06:02<02:11,  2.48s/it] 73%|███████▎  | 144/196 [06:05<02:08,  2.48s/it] 74%|███████▍  | 145/196 [06:07<02:06,  2.47s/it] 74%|███████▍  | 146/196 [06:10<02:03,  2.47s/it] 75%|███████▌  | 147/196 [06:12<02:01,  2.47s/it] 76%|███████▌  | 148/196 [06:14<01:58,  2.47s/it] 76%|███████▌  | 149/196 [06:17<01:56,  2.47s/it] 77%|███████▋  | 150/196 [06:19<01:53,  2.47s/it] 77%|███████▋  | 151/196 [06:22<01:51,  2.47s/it] 78%|███████▊  | 152/196 [06:24<01:48,  2.47s/it] 78%|███████▊  | 153/196 [06:27<01:46,  2.47s/it] 79%|███████▊  | 154/196 [06:29<01:43,  2.47s/it] 79%|███████▉  | 155/196 [06:32<01:41,  2.47s/it] 80%|███████▉  | 156/196 [06:34<01:38,  2.47s/it] 80%|████████  | 157/196 [06:37<01:36,  2.47s/it] 81%|████████  | 158/196 [06:39<01:36,  2.53s/it] 81%|████████  | 159/196 [06:42<01:32,  2.51s/it] 82%|████████▏ | 160/196 [06:44<01:29,  2.50s/it] 82%|████████▏ | 161/196 [06:47<01:27,  2.49s/it] 83%|████████▎ | 162/196 [06:49<01:24,  2.49s/it] 83%|████████▎ | 163/196 [06:52<01:22,  2.49s/it] 84%|████████▎ | 164/196 [06:54<01:19,  2.48s/it] 84%|████████▍ | 165/196 [06:57<01:16,  2.48s/it] 85%|████████▍ | 166/196 [06:59<01:14,  2.47s/it] 85%|████████▌ | 167/196 [07:02<01:11,  2.47s/it] 86%|████████▌ | 168/196 [07:04<01:09,  2.47s/it] 86%|████████▌ | 169/196 [07:07<01:06,  2.47s/it] 87%|████████▋ | 170/196 [07:09<01:04,  2.47s/it] 87%|████████▋ | 171/196 [07:12<01:01,  2.47s/it] 88%|████████▊ | 172/196 [07:14<00:59,  2.47s/it] 88%|████████▊ | 173/196 [07:16<00:56,  2.47s/it] 89%|████████▉ | 174/196 [07:19<00:54,  2.47s/it] 89%|████████▉ | 175/196 [07:21<00:51,  2.47s/it] 90%|████████▉ | 176/196 [07:24<00:49,  2.47s/it] 90%|█████████ | 177/196 [07:26<00:46,  2.47s/it] 91%|█████████ | 178/196 [07:29<00:44,  2.47s/it] 91%|█████████▏| 179/196 [07:31<00:41,  2.47s/it] 92%|█████████▏| 180/196 [07:34<00:39,  2.47s/it] 92%|█████████▏| 181/196 [07:36<00:37,  2.47s/it] 93%|█████████▎| 182/196 [07:39<00:34,  2.47s/it] 93%|█████████▎| 183/196 [07:41<00:32,  2.47s/it] 94%|█████████▍| 184/196 [07:44<00:29,  2.47s/it] 94%|█████████▍| 185/196 [07:46<00:27,  2.47s/it] 95%|█████████▍| 186/196 [07:49<00:24,  2.48s/it] 95%|█████████▌| 187/196 [07:51<00:22,  2.48s/it] 96%|█████████▌| 188/196 [07:54<00:19,  2.47s/it] 96%|█████████▋| 189/196 [07:56<00:17,  2.47s/it] 97%|█████████▋| 190/196 [07:58<00:14,  2.48s/it] 97%|█████████▋| 191/196 [08:01<00:12,  2.48s/it] 98%|█████████▊| 192/196 [08:03<00:09,  2.48s/it] 98%|█████████▊| 193/196 [08:06<00:07,  2.47s/it] 99%|█████████▉| 194/196 [08:08<00:04,  2.47s/it] 99%|█████████▉| 195/196 [08:11<00:02,  2.46s/it]100%|██████████| 196/196 [08:13<00:00,  2.31s/it]100%|██████████| 196/196 [08:13<00:00,  2.52s/it]
Top-1 Accuracy: 0.10%
Top-5 Accuracy: 0.57%
I0429 17:17:33.972407 1121119 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0429 17:17:33.972543 1121119 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0429 17:17:33.972595 1121119 utils.py:162] NumExpr defaulting to 16 threads.
I0429 17:17:34.092579 1121119 config.py:58] PyTorch version 2.4.0 available.
W0429 17:17:35.682162 1121119 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0429 17:17:35.683205 1121119 hfize_clip.py:43] CLIPConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/openai--clip-vit-large-patch14",
  "architectures": [
    "CLIPModel"
  ],
  "initializer_factor": 1.0,
  "logit_scale_init_value": 2.6592,
  "model_type": "clip",
  "projection_dim": 768,
  "quip_params": {
    "K": 6,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "text_config": {
    "dropout": 0.0,
    "hidden_size": 768,
    "intermediate_size": 3072,
    "model_type": "clip_text_model",
    "num_attention_heads": 12,
    "projection_dim": 768,
    "torch_dtype": "float32"
  },
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "vision_config": {
    "dropout": 0.0,
    "hidden_size": 1024,
    "intermediate_size": 4096,
    "model_type": "clip_vision_model",
    "num_attention_heads": 16,
    "num_hidden_layers": 24,
    "patch_size": 14,
    "projection_dim": 768,
    "torch_dtype": "float32"
  }
}

Some weights of the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 were not used when initializing CLIPModel: ['text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing CLIPModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CLIPModel were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 and are newly initialized: ['text_model.encoder.layers.0.mlp.fc1.SU', 'text_model.encoder.layers.0.mlp.fc1.SV', 'text_model.encoder.layers.0.mlp.fc1.rcp', 'text_model.encoder.layers.0.mlp.fc1.tlut', 'text_model.encoder.layers.0.mlp.fc1.tp_rank', 'text_model.encoder.layers.0.mlp.fc1.trellis', 'text_model.encoder.layers.0.mlp.fc2.SU', 'text_model.encoder.layers.0.mlp.fc2.SV', 'text_model.encoder.layers.0.mlp.fc2.rcp', 'text_model.encoder.layers.0.mlp.fc2.tlut', 'text_model.encoder.layers.0.mlp.fc2.tp_rank', 'text_model.encoder.layers.0.mlp.fc2.trellis', 'text_model.encoder.layers.0.self_attn.k_proj.SU', 'text_model.encoder.layers.0.self_attn.k_proj.SV', 'text_model.encoder.layers.0.self_attn.k_proj.rcp', 'text_model.encoder.layers.0.self_attn.k_proj.tlut', 'text_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.k_proj.trellis', 'text_model.encoder.layers.0.self_attn.out_proj.SU', 'text_model.encoder.layers.0.self_attn.out_proj.SV', 'text_model.encoder.layers.0.self_attn.out_proj.rcp', 'text_model.encoder.layers.0.self_attn.out_proj.tlut', 'text_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.out_proj.trellis', 'text_model.encoder.layers.0.self_attn.q_proj.SU', 'text_model.encoder.layers.0.self_attn.q_proj.SV', 'text_model.encoder.layers.0.self_attn.q_proj.rcp', 'text_model.encoder.layers.0.self_attn.q_proj.tlut', 'text_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.q_proj.trellis', 'text_model.encoder.layers.0.self_attn.v_proj.SU', 'text_model.encoder.layers.0.self_attn.v_proj.SV', 'text_model.encoder.layers.0.self_attn.v_proj.rcp', 'text_model.encoder.layers.0.self_attn.v_proj.tlut', 'text_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.v_proj.trellis', 'text_model.encoder.layers.1.mlp.fc1.SU', 'text_model.encoder.layers.1.mlp.fc1.SV', 'text_model.encoder.layers.1.mlp.fc1.rcp', 'text_model.encoder.layers.1.mlp.fc1.tlut', 'text_model.encoder.layers.1.mlp.fc1.tp_rank', 'text_model.encoder.layers.1.mlp.fc1.trellis', 'text_model.encoder.layers.1.mlp.fc2.SU', 'text_model.encoder.layers.1.mlp.fc2.SV', 'text_model.encoder.layers.1.mlp.fc2.rcp', 'text_model.encoder.layers.1.mlp.fc2.tlut', 'text_model.encoder.layers.1.mlp.fc2.tp_rank', 'text_model.encoder.layers.1.mlp.fc2.trellis', 'text_model.encoder.layers.1.self_attn.k_proj.SU', 'text_model.encoder.layers.1.self_attn.k_proj.SV', 'text_model.encoder.layers.1.self_attn.k_proj.rcp', 'text_model.encoder.layers.1.self_attn.k_proj.tlut', 'text_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.k_proj.trellis', 'text_model.encoder.layers.1.self_attn.out_proj.SU', 'text_model.encoder.layers.1.self_attn.out_proj.SV', 'text_model.encoder.layers.1.self_attn.out_proj.rcp', 'text_model.encoder.layers.1.self_attn.out_proj.tlut', 'text_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.out_proj.trellis', 'text_model.encoder.layers.1.self_attn.q_proj.SU', 'text_model.encoder.layers.1.self_attn.q_proj.SV', 'text_model.encoder.layers.1.self_attn.q_proj.rcp', 'text_model.encoder.layers.1.self_attn.q_proj.tlut', 'text_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.q_proj.trellis', 'text_model.encoder.layers.1.self_attn.v_proj.SU', 'text_model.encoder.layers.1.self_attn.v_proj.SV', 'text_model.encoder.layers.1.self_attn.v_proj.rcp', 'text_model.encoder.layers.1.self_attn.v_proj.tlut', 'text_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.v_proj.trellis', 'text_model.encoder.layers.10.mlp.fc1.SU', 'text_model.encoder.layers.10.mlp.fc1.SV', 'text_model.encoder.layers.10.mlp.fc1.rcp', 'text_model.encoder.layers.10.mlp.fc1.tlut', 'text_model.encoder.layers.10.mlp.fc1.tp_rank', 'text_model.encoder.layers.10.mlp.fc1.trellis', 'text_model.encoder.layers.10.mlp.fc2.SU', 'text_model.encoder.layers.10.mlp.fc2.SV', 'text_model.encoder.layers.10.mlp.fc2.rcp', 'text_model.encoder.layers.10.mlp.fc2.tlut', 'text_model.encoder.layers.10.mlp.fc2.tp_rank', 'text_model.encoder.layers.10.mlp.fc2.trellis', 'text_model.encoder.layers.10.self_attn.k_proj.SU', 'text_model.encoder.layers.10.self_attn.k_proj.SV', 'text_model.encoder.layers.10.self_attn.k_proj.rcp', 'text_model.encoder.layers.10.self_attn.k_proj.tlut', 'text_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.k_proj.trellis', 'text_model.encoder.layers.10.self_attn.out_proj.SU', 'text_model.encoder.layers.10.self_attn.out_proj.SV', 'text_model.encoder.layers.10.self_attn.out_proj.rcp', 'text_model.encoder.layers.10.self_attn.out_proj.tlut', 'text_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.out_proj.trellis', 'text_model.encoder.layers.10.self_attn.q_proj.SU', 'text_model.encoder.layers.10.self_attn.q_proj.SV', 'text_model.encoder.layers.10.self_attn.q_proj.rcp', 'text_model.encoder.layers.10.self_attn.q_proj.tlut', 'text_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.q_proj.trellis', 'text_model.encoder.layers.10.self_attn.v_proj.SU', 'text_model.encoder.layers.10.self_attn.v_proj.SV', 'text_model.encoder.layers.10.self_attn.v_proj.rcp', 'text_model.encoder.layers.10.self_attn.v_proj.tlut', 'text_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.v_proj.trellis', 'text_model.encoder.layers.11.mlp.fc1.SU', 'text_model.encoder.layers.11.mlp.fc1.SV', 'text_model.encoder.layers.11.mlp.fc1.rcp', 'text_model.encoder.layers.11.mlp.fc1.tlut', 'text_model.encoder.layers.11.mlp.fc1.tp_rank', 'text_model.encoder.layers.11.mlp.fc1.trellis', 'text_model.encoder.layers.11.mlp.fc2.SU', 'text_model.encoder.layers.11.mlp.fc2.SV', 'text_model.encoder.layers.11.mlp.fc2.rcp', 'text_model.encoder.layers.11.mlp.fc2.tlut', 'text_model.encoder.layers.11.mlp.fc2.tp_rank', 'text_model.encoder.layers.11.mlp.fc2.trellis', 'text_model.encoder.layers.11.self_attn.k_proj.SU', 'text_model.encoder.layers.11.self_attn.k_proj.SV', 'text_model.encoder.layers.11.self_attn.k_proj.rcp', 'text_model.encoder.layers.11.self_attn.k_proj.tlut', 'text_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.k_proj.trellis', 'text_model.encoder.layers.11.self_attn.out_proj.SU', 'text_model.encoder.layers.11.self_attn.out_proj.SV', 'text_model.encoder.layers.11.self_attn.out_proj.rcp', 'text_model.encoder.layers.11.self_attn.out_proj.tlut', 'text_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.out_proj.trellis', 'text_model.encoder.layers.11.self_attn.q_proj.SU', 'text_model.encoder.layers.11.self_attn.q_proj.SV', 'text_model.encoder.layers.11.self_attn.q_proj.rcp', 'text_model.encoder.layers.11.self_attn.q_proj.tlut', 'text_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.q_proj.trellis', 'text_model.encoder.layers.11.self_attn.v_proj.SU', 'text_model.encoder.layers.11.self_attn.v_proj.SV', 'text_model.encoder.layers.11.self_attn.v_proj.rcp', 'text_model.encoder.layers.11.self_attn.v_proj.tlut', 'text_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.v_proj.trellis', 'text_model.encoder.layers.2.mlp.fc1.SU', 'text_model.encoder.layers.2.mlp.fc1.SV', 'text_model.encoder.layers.2.mlp.fc1.rcp', 'text_model.encoder.layers.2.mlp.fc1.tlut', 'text_model.encoder.layers.2.mlp.fc1.tp_rank', 'text_model.encoder.layers.2.mlp.fc1.trellis', 'text_model.encoder.layers.2.mlp.fc2.SU', 'text_model.encoder.layers.2.mlp.fc2.SV', 'text_model.encoder.layers.2.mlp.fc2.rcp', 'text_model.encoder.layers.2.mlp.fc2.tlut', 'text_model.encoder.layers.2.mlp.fc2.tp_rank', 'text_model.encoder.layers.2.mlp.fc2.trellis', 'text_model.encoder.layers.2.self_attn.k_proj.SU', 'text_model.encoder.layers.2.self_attn.k_proj.SV', 'text_model.encoder.layers.2.self_attn.k_proj.rcp', 'text_model.encoder.layers.2.self_attn.k_proj.tlut', 'text_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.k_proj.trellis', 'text_model.encoder.layers.2.self_attn.out_proj.SU', 'text_model.encoder.layers.2.self_attn.out_proj.SV', 'text_model.encoder.layers.2.self_attn.out_proj.rcp', 'text_model.encoder.layers.2.self_attn.out_proj.tlut', 'text_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.out_proj.trellis', 'text_model.encoder.layers.2.self_attn.q_proj.SU', 'text_model.encoder.layers.2.self_attn.q_proj.SV', 'text_model.encoder.layers.2.self_attn.q_proj.rcp', 'text_model.encoder.layers.2.self_attn.q_proj.tlut', 'text_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.q_proj.trellis', 'text_model.encoder.layers.2.self_attn.v_proj.SU', 'text_model.encoder.layers.2.self_attn.v_proj.SV', 'text_model.encoder.layers.2.self_attn.v_proj.rcp', 'text_model.encoder.layers.2.self_attn.v_proj.tlut', 'text_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.v_proj.trellis', 'text_model.encoder.layers.3.mlp.fc1.SU', 'text_model.encoder.layers.3.mlp.fc1.SV', 'text_model.encoder.layers.3.mlp.fc1.rcp', 'text_model.encoder.layers.3.mlp.fc1.tlut', 'text_model.encoder.layers.3.mlp.fc1.tp_rank', 'text_model.encoder.layers.3.mlp.fc1.trellis', 'text_model.encoder.layers.3.mlp.fc2.SU', 'text_model.encoder.layers.3.mlp.fc2.SV', 'text_model.encoder.layers.3.mlp.fc2.rcp', 'text_model.encoder.layers.3.mlp.fc2.tlut', 'text_model.encoder.layers.3.mlp.fc2.tp_rank', 'text_model.encoder.layers.3.mlp.fc2.trellis', 'text_model.encoder.layers.3.self_attn.k_proj.SU', 'text_model.encoder.layers.3.self_attn.k_proj.SV', 'text_model.encoder.layers.3.self_attn.k_proj.rcp', 'text_model.encoder.layers.3.self_attn.k_proj.tlut', 'text_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.k_proj.trellis', 'text_model.encoder.layers.3.self_attn.out_proj.SU', 'text_model.encoder.layers.3.self_attn.out_proj.SV', 'text_model.encoder.layers.3.self_attn.out_proj.rcp', 'text_model.encoder.layers.3.self_attn.out_proj.tlut', 'text_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.out_proj.trellis', 'text_model.encoder.layers.3.self_attn.q_proj.SU', 'text_model.encoder.layers.3.self_attn.q_proj.SV', 'text_model.encoder.layers.3.self_attn.q_proj.rcp', 'text_model.encoder.layers.3.self_attn.q_proj.tlut', 'text_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.q_proj.trellis', 'text_model.encoder.layers.3.self_attn.v_proj.SU', 'text_model.encoder.layers.3.self_attn.v_proj.SV', 'text_model.encoder.layers.3.self_attn.v_proj.rcp', 'text_model.encoder.layers.3.self_attn.v_proj.tlut', 'text_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.v_proj.trellis', 'text_model.encoder.layers.4.mlp.fc1.SU', 'text_model.encoder.layers.4.mlp.fc1.SV', 'text_model.encoder.layers.4.mlp.fc1.rcp', 'text_model.encoder.layers.4.mlp.fc1.tlut', 'text_model.encoder.layers.4.mlp.fc1.tp_rank', 'text_model.encoder.layers.4.mlp.fc1.trellis', 'text_model.encoder.layers.4.mlp.fc2.SU', 'text_model.encoder.layers.4.mlp.fc2.SV', 'text_model.encoder.layers.4.mlp.fc2.rcp', 'text_model.encoder.layers.4.mlp.fc2.tlut', 'text_model.encoder.layers.4.mlp.fc2.tp_rank', 'text_model.encoder.layers.4.mlp.fc2.trellis', 'text_model.encoder.layers.4.self_attn.k_proj.SU', 'text_model.encoder.layers.4.self_attn.k_proj.SV', 'text_model.encoder.layers.4.self_attn.k_proj.rcp', 'text_model.encoder.layers.4.self_attn.k_proj.tlut', 'text_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.k_proj.trellis', 'text_model.encoder.layers.4.self_attn.out_proj.SU', 'text_model.encoder.layers.4.self_attn.out_proj.SV', 'text_model.encoder.layers.4.self_attn.out_proj.rcp', 'text_model.encoder.layers.4.self_attn.out_proj.tlut', 'text_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.out_proj.trellis', 'text_model.encoder.layers.4.self_attn.q_proj.SU', 'text_model.encoder.layers.4.self_attn.q_proj.SV', 'text_model.encoder.layers.4.self_attn.q_proj.rcp', 'text_model.encoder.layers.4.self_attn.q_proj.tlut', 'text_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.q_proj.trellis', 'text_model.encoder.layers.4.self_attn.v_proj.SU', 'text_model.encoder.layers.4.self_attn.v_proj.SV', 'text_model.encoder.layers.4.self_attn.v_proj.rcp', 'text_model.encoder.layers.4.self_attn.v_proj.tlut', 'text_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.v_proj.trellis', 'text_model.encoder.layers.5.mlp.fc1.SU', 'text_model.encoder.layers.5.mlp.fc1.SV', 'text_model.encoder.layers.5.mlp.fc1.rcp', 'text_model.encoder.layers.5.mlp.fc1.tlut', 'text_model.encoder.layers.5.mlp.fc1.tp_rank', 'text_model.encoder.layers.5.mlp.fc1.trellis', 'text_model.encoder.layers.5.mlp.fc2.SU', 'text_model.encoder.layers.5.mlp.fc2.SV', 'text_model.encoder.layers.5.mlp.fc2.rcp', 'text_model.encoder.layers.5.mlp.fc2.tlut', 'text_model.encoder.layers.5.mlp.fc2.tp_rank', 'text_model.encoder.layers.5.mlp.fc2.trellis', 'text_model.encoder.layers.5.self_attn.k_proj.SU', 'text_model.encoder.layers.5.self_attn.k_proj.SV', 'text_model.encoder.layers.5.self_attn.k_proj.rcp', 'text_model.encoder.layers.5.self_attn.k_proj.tlut', 'text_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.k_proj.trellis', 'text_model.encoder.layers.5.self_attn.out_proj.SU', 'text_model.encoder.layers.5.self_attn.out_proj.SV', 'text_model.encoder.layers.5.self_attn.out_proj.rcp', 'text_model.encoder.layers.5.self_attn.out_proj.tlut', 'text_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.out_proj.trellis', 'text_model.encoder.layers.5.self_attn.q_proj.SU', 'text_model.encoder.layers.5.self_attn.q_proj.SV', 'text_model.encoder.layers.5.self_attn.q_proj.rcp', 'text_model.encoder.layers.5.self_attn.q_proj.tlut', 'text_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.q_proj.trellis', 'text_model.encoder.layers.5.self_attn.v_proj.SU', 'text_model.encoder.layers.5.self_attn.v_proj.SV', 'text_model.encoder.layers.5.self_attn.v_proj.rcp', 'text_model.encoder.layers.5.self_attn.v_proj.tlut', 'text_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.v_proj.trellis', 'text_model.encoder.layers.6.mlp.fc1.SU', 'text_model.encoder.layers.6.mlp.fc1.SV', 'text_model.encoder.layers.6.mlp.fc1.rcp', 'text_model.encoder.layers.6.mlp.fc1.tlut', 'text_model.encoder.layers.6.mlp.fc1.tp_rank', 'text_model.encoder.layers.6.mlp.fc1.trellis', 'text_model.encoder.layers.6.mlp.fc2.SU', 'text_model.encoder.layers.6.mlp.fc2.SV', 'text_model.encoder.layers.6.mlp.fc2.rcp', 'text_model.encoder.layers.6.mlp.fc2.tlut', 'text_model.encoder.layers.6.mlp.fc2.tp_rank', 'text_model.encoder.layers.6.mlp.fc2.trellis', 'text_model.encoder.layers.6.self_attn.k_proj.SU', 'text_model.encoder.layers.6.self_attn.k_proj.SV', 'text_model.encoder.layers.6.self_attn.k_proj.rcp', 'text_model.encoder.layers.6.self_attn.k_proj.tlut', 'text_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.k_proj.trellis', 'text_model.encoder.layers.6.self_attn.out_proj.SU', 'text_model.encoder.layers.6.self_attn.out_proj.SV', 'text_model.encoder.layers.6.self_attn.out_proj.rcp', 'text_model.encoder.layers.6.self_attn.out_proj.tlut', 'text_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.out_proj.trellis', 'text_model.encoder.layers.6.self_attn.q_proj.SU', 'text_model.encoder.layers.6.self_attn.q_proj.SV', 'text_model.encoder.layers.6.self_attn.q_proj.rcp', 'text_model.encoder.layers.6.self_attn.q_proj.tlut', 'text_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.q_proj.trellis', 'text_model.encoder.layers.6.self_attn.v_proj.SU', 'text_model.encoder.layers.6.self_attn.v_proj.SV', 'text_model.encoder.layers.6.self_attn.v_proj.rcp', 'text_model.encoder.layers.6.self_attn.v_proj.tlut', 'text_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.v_proj.trellis', 'text_model.encoder.layers.7.mlp.fc1.SU', 'text_model.encoder.layers.7.mlp.fc1.SV', 'text_model.encoder.layers.7.mlp.fc1.rcp', 'text_model.encoder.layers.7.mlp.fc1.tlut', 'text_model.encoder.layers.7.mlp.fc1.tp_rank', 'text_model.encoder.layers.7.mlp.fc1.trellis', 'text_model.encoder.layers.7.mlp.fc2.SU', 'text_model.encoder.layers.7.mlp.fc2.SV', 'text_model.encoder.layers.7.mlp.fc2.rcp', 'text_model.encoder.layers.7.mlp.fc2.tlut', 'text_model.encoder.layers.7.mlp.fc2.tp_rank', 'text_model.encoder.layers.7.mlp.fc2.trellis', 'text_model.encoder.layers.7.self_attn.k_proj.SU', 'text_model.encoder.layers.7.self_attn.k_proj.SV', 'text_model.encoder.layers.7.self_attn.k_proj.rcp', 'text_model.encoder.layers.7.self_attn.k_proj.tlut', 'text_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.k_proj.trellis', 'text_model.encoder.layers.7.self_attn.out_proj.SU', 'text_model.encoder.layers.7.self_attn.out_proj.SV', 'text_model.encoder.layers.7.self_attn.out_proj.rcp', 'text_model.encoder.layers.7.self_attn.out_proj.tlut', 'text_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.out_proj.trellis', 'text_model.encoder.layers.7.self_attn.q_proj.SU', 'text_model.encoder.layers.7.self_attn.q_proj.SV', 'text_model.encoder.layers.7.self_attn.q_proj.rcp', 'text_model.encoder.layers.7.self_attn.q_proj.tlut', 'text_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.q_proj.trellis', 'text_model.encoder.layers.7.self_attn.v_proj.SU', 'text_model.encoder.layers.7.self_attn.v_proj.SV', 'text_model.encoder.layers.7.self_attn.v_proj.rcp', 'text_model.encoder.layers.7.self_attn.v_proj.tlut', 'text_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.v_proj.trellis', 'text_model.encoder.layers.8.mlp.fc1.SU', 'text_model.encoder.layers.8.mlp.fc1.SV', 'text_model.encoder.layers.8.mlp.fc1.rcp', 'text_model.encoder.layers.8.mlp.fc1.tlut', 'text_model.encoder.layers.8.mlp.fc1.tp_rank', 'text_model.encoder.layers.8.mlp.fc1.trellis', 'text_model.encoder.layers.8.mlp.fc2.SU', 'text_model.encoder.layers.8.mlp.fc2.SV', 'text_model.encoder.layers.8.mlp.fc2.rcp', 'text_model.encoder.layers.8.mlp.fc2.tlut', 'text_model.encoder.layers.8.mlp.fc2.tp_rank', 'text_model.encoder.layers.8.mlp.fc2.trellis', 'text_model.encoder.layers.8.self_attn.k_proj.SU', 'text_model.encoder.layers.8.self_attn.k_proj.SV', 'text_model.encoder.layers.8.self_attn.k_proj.rcp', 'text_model.encoder.layers.8.self_attn.k_proj.tlut', 'text_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.k_proj.trellis', 'text_model.encoder.layers.8.self_attn.out_proj.SU', 'text_model.encoder.layers.8.self_attn.out_proj.SV', 'text_model.encoder.layers.8.self_attn.out_proj.rcp', 'text_model.encoder.layers.8.self_attn.out_proj.tlut', 'text_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.out_proj.trellis', 'text_model.encoder.layers.8.self_attn.q_proj.SU', 'text_model.encoder.layers.8.self_attn.q_proj.SV', 'text_model.encoder.layers.8.self_attn.q_proj.rcp', 'text_model.encoder.layers.8.self_attn.q_proj.tlut', 'text_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.q_proj.trellis', 'text_model.encoder.layers.8.self_attn.v_proj.SU', 'text_model.encoder.layers.8.self_attn.v_proj.SV', 'text_model.encoder.layers.8.self_attn.v_proj.rcp', 'text_model.encoder.layers.8.self_attn.v_proj.tlut', 'text_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.v_proj.trellis', 'text_model.encoder.layers.9.mlp.fc1.SU', 'text_model.encoder.layers.9.mlp.fc1.SV', 'text_model.encoder.layers.9.mlp.fc1.rcp', 'text_model.encoder.layers.9.mlp.fc1.tlut', 'text_model.encoder.layers.9.mlp.fc1.tp_rank', 'text_model.encoder.layers.9.mlp.fc1.trellis', 'text_model.encoder.layers.9.mlp.fc2.SU', 'text_model.encoder.layers.9.mlp.fc2.SV', 'text_model.encoder.layers.9.mlp.fc2.rcp', 'text_model.encoder.layers.9.mlp.fc2.tlut', 'text_model.encoder.layers.9.mlp.fc2.tp_rank', 'text_model.encoder.layers.9.mlp.fc2.trellis', 'text_model.encoder.layers.9.self_attn.k_proj.SU', 'text_model.encoder.layers.9.self_attn.k_proj.SV', 'text_model.encoder.layers.9.self_attn.k_proj.rcp', 'text_model.encoder.layers.9.self_attn.k_proj.tlut', 'text_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.k_proj.trellis', 'text_model.encoder.layers.9.self_attn.out_proj.SU', 'text_model.encoder.layers.9.self_attn.out_proj.SV', 'text_model.encoder.layers.9.self_attn.out_proj.rcp', 'text_model.encoder.layers.9.self_attn.out_proj.tlut', 'text_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.out_proj.trellis', 'text_model.encoder.layers.9.self_attn.q_proj.SU', 'text_model.encoder.layers.9.self_attn.q_proj.SV', 'text_model.encoder.layers.9.self_attn.q_proj.rcp', 'text_model.encoder.layers.9.self_attn.q_proj.tlut', 'text_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.q_proj.trellis', 'text_model.encoder.layers.9.self_attn.v_proj.SU', 'text_model.encoder.layers.9.self_attn.v_proj.SV', 'text_model.encoder.layers.9.self_attn.v_proj.rcp', 'text_model.encoder.layers.9.self_attn.v_proj.tlut', 'text_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.v_proj.trellis', 'vision_model.encoder.layers.0.mlp.fc1.SU', 'vision_model.encoder.layers.0.mlp.fc1.SV', 'vision_model.encoder.layers.0.mlp.fc1.rcp', 'vision_model.encoder.layers.0.mlp.fc1.tlut', 'vision_model.encoder.layers.0.mlp.fc1.tp_rank', 'vision_model.encoder.layers.0.mlp.fc1.trellis', 'vision_model.encoder.layers.0.mlp.fc2.SU', 'vision_model.encoder.layers.0.mlp.fc2.SV', 'vision_model.encoder.layers.0.mlp.fc2.rcp', 'vision_model.encoder.layers.0.mlp.fc2.tlut', 'vision_model.encoder.layers.0.mlp.fc2.tp_rank', 'vision_model.encoder.layers.0.mlp.fc2.trellis', 'vision_model.encoder.layers.0.self_attn.k_proj.SU', 'vision_model.encoder.layers.0.self_attn.k_proj.SV', 'vision_model.encoder.layers.0.self_attn.k_proj.rcp', 'vision_model.encoder.layers.0.self_attn.k_proj.tlut', 'vision_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.k_proj.trellis', 'vision_model.encoder.layers.0.self_attn.out_proj.SU', 'vision_model.encoder.layers.0.self_attn.out_proj.SV', 'vision_model.encoder.layers.0.self_attn.out_proj.rcp', 'vision_model.encoder.layers.0.self_attn.out_proj.tlut', 'vision_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.out_proj.trellis', 'vision_model.encoder.layers.0.self_attn.q_proj.SU', 'vision_model.encoder.layers.0.self_attn.q_proj.SV', 'vision_model.encoder.layers.0.self_attn.q_proj.rcp', 'vision_model.encoder.layers.0.self_attn.q_proj.tlut', 'vision_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.q_proj.trellis', 'vision_model.encoder.layers.0.self_attn.v_proj.SU', 'vision_model.encoder.layers.0.self_attn.v_proj.SV', 'vision_model.encoder.layers.0.self_attn.v_proj.rcp', 'vision_model.encoder.layers.0.self_attn.v_proj.tlut', 'vision_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.v_proj.trellis', 'vision_model.encoder.layers.1.mlp.fc1.SU', 'vision_model.encoder.layers.1.mlp.fc1.SV', 'vision_model.encoder.layers.1.mlp.fc1.rcp', 'vision_model.encoder.layers.1.mlp.fc1.tlut', 'vision_model.encoder.layers.1.mlp.fc1.tp_rank', 'vision_model.encoder.layers.1.mlp.fc1.trellis', 'vision_model.encoder.layers.1.mlp.fc2.SU', 'vision_model.encoder.layers.1.mlp.fc2.SV', 'vision_model.encoder.layers.1.mlp.fc2.rcp', 'vision_model.encoder.layers.1.mlp.fc2.tlut', 'vision_model.encoder.layers.1.mlp.fc2.tp_rank', 'vision_model.encoder.layers.1.mlp.fc2.trellis', 'vision_model.encoder.layers.1.self_attn.k_proj.SU', 'vision_model.encoder.layers.1.self_attn.k_proj.SV', 'vision_model.encoder.layers.1.self_attn.k_proj.rcp', 'vision_model.encoder.layers.1.self_attn.k_proj.tlut', 'vision_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.k_proj.trellis', 'vision_model.encoder.layers.1.self_attn.out_proj.SU', 'vision_model.encoder.layers.1.self_attn.out_proj.SV', 'vision_model.encoder.layers.1.self_attn.out_proj.rcp', 'vision_model.encoder.layers.1.self_attn.out_proj.tlut', 'vision_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.out_proj.trellis', 'vision_model.encoder.layers.1.self_attn.q_proj.SU', 'vision_model.encoder.layers.1.self_attn.q_proj.SV', 'vision_model.encoder.layers.1.self_attn.q_proj.rcp', 'vision_model.encoder.layers.1.self_attn.q_proj.tlut', 'vision_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.q_proj.trellis', 'vision_model.encoder.layers.1.self_attn.v_proj.SU', 'vision_model.encoder.layers.1.self_attn.v_proj.SV', 'vision_model.encoder.layers.1.self_attn.v_proj.rcp', 'vision_model.encoder.layers.1.self_attn.v_proj.tlut', 'vision_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.v_proj.trellis', 'vision_model.encoder.layers.10.mlp.fc1.SU', 'vision_model.encoder.layers.10.mlp.fc1.SV', 'vision_model.encoder.layers.10.mlp.fc1.rcp', 'vision_model.encoder.layers.10.mlp.fc1.tlut', 'vision_model.encoder.layers.10.mlp.fc1.tp_rank', 'vision_model.encoder.layers.10.mlp.fc1.trellis', 'vision_model.encoder.layers.10.mlp.fc2.SU', 'vision_model.encoder.layers.10.mlp.fc2.SV', 'vision_model.encoder.layers.10.mlp.fc2.rcp', 'vision_model.encoder.layers.10.mlp.fc2.tlut', 'vision_model.encoder.layers.10.mlp.fc2.tp_rank', 'vision_model.encoder.layers.10.mlp.fc2.trellis', 'vision_model.encoder.layers.10.self_attn.k_proj.SU', 'vision_model.encoder.layers.10.self_attn.k_proj.SV', 'vision_model.encoder.layers.10.self_attn.k_proj.rcp', 'vision_model.encoder.layers.10.self_attn.k_proj.tlut', 'vision_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.k_proj.trellis', 'vision_model.encoder.layers.10.self_attn.out_proj.SU', 'vision_model.encoder.layers.10.self_attn.out_proj.SV', 'vision_model.encoder.layers.10.self_attn.out_proj.rcp', 'vision_model.encoder.layers.10.self_attn.out_proj.tlut', 'vision_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.out_proj.trellis', 'vision_model.encoder.layers.10.self_attn.q_proj.SU', 'vision_model.encoder.layers.10.self_attn.q_proj.SV', 'vision_model.encoder.layers.10.self_attn.q_proj.rcp', 'vision_model.encoder.layers.10.self_attn.q_proj.tlut', 'vision_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.q_proj.trellis', 'vision_model.encoder.layers.10.self_attn.v_proj.SU', 'vision_model.encoder.layers.10.self_attn.v_proj.SV', 'vision_model.encoder.layers.10.self_attn.v_proj.rcp', 'vision_model.encoder.layers.10.self_attn.v_proj.tlut', 'vision_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.v_proj.trellis', 'vision_model.encoder.layers.11.mlp.fc1.SU', 'vision_model.encoder.layers.11.mlp.fc1.SV', 'vision_model.encoder.layers.11.mlp.fc1.rcp', 'vision_model.encoder.layers.11.mlp.fc1.tlut', 'vision_model.encoder.layers.11.mlp.fc1.tp_rank', 'vision_model.encoder.layers.11.mlp.fc1.trellis', 'vision_model.encoder.layers.11.mlp.fc2.SU', 'vision_model.encoder.layers.11.mlp.fc2.SV', 'vision_model.encoder.layers.11.mlp.fc2.rcp', 'vision_model.encoder.layers.11.mlp.fc2.tlut', 'vision_model.encoder.layers.11.mlp.fc2.tp_rank', 'vision_model.encoder.layers.11.mlp.fc2.trellis', 'vision_model.encoder.layers.11.self_attn.k_proj.SU', 'vision_model.encoder.layers.11.self_attn.k_proj.SV', 'vision_model.encoder.layers.11.self_attn.k_proj.rcp', 'vision_model.encoder.layers.11.self_attn.k_proj.tlut', 'vision_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.k_proj.trellis', 'vision_model.encoder.layers.11.self_attn.out_proj.SU', 'vision_model.encoder.layers.11.self_attn.out_proj.SV', 'vision_model.encoder.layers.11.self_attn.out_proj.rcp', 'vision_model.encoder.layers.11.self_attn.out_proj.tlut', 'vision_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.out_proj.trellis', 'vision_model.encoder.layers.11.self_attn.q_proj.SU', 'vision_model.encoder.layers.11.self_attn.q_proj.SV', 'vision_model.encoder.layers.11.self_attn.q_proj.rcp', 'vision_model.encoder.layers.11.self_attn.q_proj.tlut', 'vision_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.q_proj.trellis', 'vision_model.encoder.layers.11.self_attn.v_proj.SU', 'vision_model.encoder.layers.11.self_attn.v_proj.SV', 'vision_model.encoder.layers.11.self_attn.v_proj.rcp', 'vision_model.encoder.layers.11.self_attn.v_proj.tlut', 'vision_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.v_proj.trellis', 'vision_model.encoder.layers.12.mlp.fc1.SU', 'vision_model.encoder.layers.12.mlp.fc1.SV', 'vision_model.encoder.layers.12.mlp.fc1.rcp', 'vision_model.encoder.layers.12.mlp.fc1.tlut', 'vision_model.encoder.layers.12.mlp.fc1.tp_rank', 'vision_model.encoder.layers.12.mlp.fc1.trellis', 'vision_model.encoder.layers.12.mlp.fc2.SU', 'vision_model.encoder.layers.12.mlp.fc2.SV', 'vision_model.encoder.layers.12.mlp.fc2.rcp', 'vision_model.encoder.layers.12.mlp.fc2.tlut', 'vision_model.encoder.layers.12.mlp.fc2.tp_rank', 'vision_model.encoder.layers.12.mlp.fc2.trellis', 'vision_model.encoder.layers.12.self_attn.k_proj.SU', 'vision_model.encoder.layers.12.self_attn.k_proj.SV', 'vision_model.encoder.layers.12.self_attn.k_proj.rcp', 'vision_model.encoder.layers.12.self_attn.k_proj.tlut', 'vision_model.encoder.layers.12.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.k_proj.trellis', 'vision_model.encoder.layers.12.self_attn.out_proj.SU', 'vision_model.encoder.layers.12.self_attn.out_proj.SV', 'vision_model.encoder.layers.12.self_attn.out_proj.rcp', 'vision_model.encoder.layers.12.self_attn.out_proj.tlut', 'vision_model.encoder.layers.12.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.out_proj.trellis', 'vision_model.encoder.layers.12.self_attn.q_proj.SU', 'vision_model.encoder.layers.12.self_attn.q_proj.SV', 'vision_model.encoder.layers.12.self_attn.q_proj.rcp', 'vision_model.encoder.layers.12.self_attn.q_proj.tlut', 'vision_model.encoder.layers.12.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.q_proj.trellis', 'vision_model.encoder.layers.12.self_attn.v_proj.SU', 'vision_model.encoder.layers.12.self_attn.v_proj.SV', 'vision_model.encoder.layers.12.self_attn.v_proj.rcp', 'vision_model.encoder.layers.12.self_attn.v_proj.tlut', 'vision_model.encoder.layers.12.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.v_proj.trellis', 'vision_model.encoder.layers.13.mlp.fc1.SU', 'vision_model.encoder.layers.13.mlp.fc1.SV', 'vision_model.encoder.layers.13.mlp.fc1.rcp', 'vision_model.encoder.layers.13.mlp.fc1.tlut', 'vision_model.encoder.layers.13.mlp.fc1.tp_rank', 'vision_model.encoder.layers.13.mlp.fc1.trellis', 'vision_model.encoder.layers.13.mlp.fc2.SU', 'vision_model.encoder.layers.13.mlp.fc2.SV', 'vision_model.encoder.layers.13.mlp.fc2.rcp', 'vision_model.encoder.layers.13.mlp.fc2.tlut', 'vision_model.encoder.layers.13.mlp.fc2.tp_rank', 'vision_model.encoder.layers.13.mlp.fc2.trellis', 'vision_model.encoder.layers.13.self_attn.k_proj.SU', 'vision_model.encoder.layers.13.self_attn.k_proj.SV', 'vision_model.encoder.layers.13.self_attn.k_proj.rcp', 'vision_model.encoder.layers.13.self_attn.k_proj.tlut', 'vision_model.encoder.layers.13.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.k_proj.trellis', 'vision_model.encoder.layers.13.self_attn.out_proj.SU', 'vision_model.encoder.layers.13.self_attn.out_proj.SV', 'vision_model.encoder.layers.13.self_attn.out_proj.rcp', 'vision_model.encoder.layers.13.self_attn.out_proj.tlut', 'vision_model.encoder.layers.13.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.out_proj.trellis', 'vision_model.encoder.layers.13.self_attn.q_proj.SU', 'vision_model.encoder.layers.13.self_attn.q_proj.SV', 'vision_model.encoder.layers.13.self_attn.q_proj.rcp', 'vision_model.encoder.layers.13.self_attn.q_proj.tlut', 'vision_model.encoder.layers.13.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.q_proj.trellis', 'vision_model.encoder.layers.13.self_attn.v_proj.SU', 'vision_model.encoder.layers.13.self_attn.v_proj.SV', 'vision_model.encoder.layers.13.self_attn.v_proj.rcp', 'vision_model.encoder.layers.13.self_attn.v_proj.tlut', 'vision_model.encoder.layers.13.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.v_proj.trellis', 'vision_model.encoder.layers.14.mlp.fc1.SU', 'vision_model.encoder.layers.14.mlp.fc1.SV', 'vision_model.encoder.layers.14.mlp.fc1.rcp', 'vision_model.encoder.layers.14.mlp.fc1.tlut', 'vision_model.encoder.layers.14.mlp.fc1.tp_rank', 'vision_model.encoder.layers.14.mlp.fc1.trellis', 'vision_model.encoder.layers.14.mlp.fc2.SU', 'vision_model.encoder.layers.14.mlp.fc2.SV', 'vision_model.encoder.layers.14.mlp.fc2.rcp', 'vision_model.encoder.layers.14.mlp.fc2.tlut', 'vision_model.encoder.layers.14.mlp.fc2.tp_rank', 'vision_model.encoder.layers.14.mlp.fc2.trellis', 'vision_model.encoder.layers.14.self_attn.k_proj.SU', 'vision_model.encoder.layers.14.self_attn.k_proj.SV', 'vision_model.encoder.layers.14.self_attn.k_proj.rcp', 'vision_model.encoder.layers.14.self_attn.k_proj.tlut', 'vision_model.encoder.layers.14.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.k_proj.trellis', 'vision_model.encoder.layers.14.self_attn.out_proj.SU', 'vision_model.encoder.layers.14.self_attn.out_proj.SV', 'vision_model.encoder.layers.14.self_attn.out_proj.rcp', 'vision_model.encoder.layers.14.self_attn.out_proj.tlut', 'vision_model.encoder.layers.14.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.out_proj.trellis', 'vision_model.encoder.layers.14.self_attn.q_proj.SU', 'vision_model.encoder.layers.14.self_attn.q_proj.SV', 'vision_model.encoder.layers.14.self_attn.q_proj.rcp', 'vision_model.encoder.layers.14.self_attn.q_proj.tlut', 'vision_model.encoder.layers.14.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.q_proj.trellis', 'vision_model.encoder.layers.14.self_attn.v_proj.SU', 'vision_model.encoder.layers.14.self_attn.v_proj.SV', 'vision_model.encoder.layers.14.self_attn.v_proj.rcp', 'vision_model.encoder.layers.14.self_attn.v_proj.tlut', 'vision_model.encoder.layers.14.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.v_proj.trellis', 'vision_model.encoder.layers.15.mlp.fc1.SU', 'vision_model.encoder.layers.15.mlp.fc1.SV', 'vision_model.encoder.layers.15.mlp.fc1.rcp', 'vision_model.encoder.layers.15.mlp.fc1.tlut', 'vision_model.encoder.layers.15.mlp.fc1.tp_rank', 'vision_model.encoder.layers.15.mlp.fc1.trellis', 'vision_model.encoder.layers.15.mlp.fc2.SU', 'vision_model.encoder.layers.15.mlp.fc2.SV', 'vision_model.encoder.layers.15.mlp.fc2.rcp', 'vision_model.encoder.layers.15.mlp.fc2.tlut', 'vision_model.encoder.layers.15.mlp.fc2.tp_rank', 'vision_model.encoder.layers.15.mlp.fc2.trellis', 'vision_model.encoder.layers.15.self_attn.k_proj.SU', 'vision_model.encoder.layers.15.self_attn.k_proj.SV', 'vision_model.encoder.layers.15.self_attn.k_proj.rcp', 'vision_model.encoder.layers.15.self_attn.k_proj.tlut', 'vision_model.encoder.layers.15.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.k_proj.trellis', 'vision_model.encoder.layers.15.self_attn.out_proj.SU', 'vision_model.encoder.layers.15.self_attn.out_proj.SV', 'vision_model.encoder.layers.15.self_attn.out_proj.rcp', 'vision_model.encoder.layers.15.self_attn.out_proj.tlut', 'vision_model.encoder.layers.15.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.out_proj.trellis', 'vision_model.encoder.layers.15.self_attn.q_proj.SU', 'vision_model.encoder.layers.15.self_attn.q_proj.SV', 'vision_model.encoder.layers.15.self_attn.q_proj.rcp', 'vision_model.encoder.layers.15.self_attn.q_proj.tlut', 'vision_model.encoder.layers.15.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.q_proj.trellis', 'vision_model.encoder.layers.15.self_attn.v_proj.SU', 'vision_model.encoder.layers.15.self_attn.v_proj.SV', 'vision_model.encoder.layers.15.self_attn.v_proj.rcp', 'vision_model.encoder.layers.15.self_attn.v_proj.tlut', 'vision_model.encoder.layers.15.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.v_proj.trellis', 'vision_model.encoder.layers.16.mlp.fc1.SU', 'vision_model.encoder.layers.16.mlp.fc1.SV', 'vision_model.encoder.layers.16.mlp.fc1.rcp', 'vision_model.encoder.layers.16.mlp.fc1.tlut', 'vision_model.encoder.layers.16.mlp.fc1.tp_rank', 'vision_model.encoder.layers.16.mlp.fc1.trellis', 'vision_model.encoder.layers.16.mlp.fc2.SU', 'vision_model.encoder.layers.16.mlp.fc2.SV', 'vision_model.encoder.layers.16.mlp.fc2.rcp', 'vision_model.encoder.layers.16.mlp.fc2.tlut', 'vision_model.encoder.layers.16.mlp.fc2.tp_rank', 'vision_model.encoder.layers.16.mlp.fc2.trellis', 'vision_model.encoder.layers.16.self_attn.k_proj.SU', 'vision_model.encoder.layers.16.self_attn.k_proj.SV', 'vision_model.encoder.layers.16.self_attn.k_proj.rcp', 'vision_model.encoder.layers.16.self_attn.k_proj.tlut', 'vision_model.encoder.layers.16.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.k_proj.trellis', 'vision_model.encoder.layers.16.self_attn.out_proj.SU', 'vision_model.encoder.layers.16.self_attn.out_proj.SV', 'vision_model.encoder.layers.16.self_attn.out_proj.rcp', 'vision_model.encoder.layers.16.self_attn.out_proj.tlut', 'vision_model.encoder.layers.16.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.out_proj.trellis', 'vision_model.encoder.layers.16.self_attn.q_proj.SU', 'vision_model.encoder.layers.16.self_attn.q_proj.SV', 'vision_model.encoder.layers.16.self_attn.q_proj.rcp', 'vision_model.encoder.layers.16.self_attn.q_proj.tlut', 'vision_model.encoder.layers.16.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.q_proj.trellis', 'vision_model.encoder.layers.16.self_attn.v_proj.SU', 'vision_model.encoder.layers.16.self_attn.v_proj.SV', 'vision_model.encoder.layers.16.self_attn.v_proj.rcp', 'vision_model.encoder.layers.16.self_attn.v_proj.tlut', 'vision_model.encoder.layers.16.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.v_proj.trellis', 'vision_model.encoder.layers.17.mlp.fc1.SU', 'vision_model.encoder.layers.17.mlp.fc1.SV', 'vision_model.encoder.layers.17.mlp.fc1.rcp', 'vision_model.encoder.layers.17.mlp.fc1.tlut', 'vision_model.encoder.layers.17.mlp.fc1.tp_rank', 'vision_model.encoder.layers.17.mlp.fc1.trellis', 'vision_model.encoder.layers.17.mlp.fc2.SU', 'vision_model.encoder.layers.17.mlp.fc2.SV', 'vision_model.encoder.layers.17.mlp.fc2.rcp', 'vision_model.encoder.layers.17.mlp.fc2.tlut', 'vision_model.encoder.layers.17.mlp.fc2.tp_rank', 'vision_model.encoder.layers.17.mlp.fc2.trellis', 'vision_model.encoder.layers.17.self_attn.k_proj.SU', 'vision_model.encoder.layers.17.self_attn.k_proj.SV', 'vision_model.encoder.layers.17.self_attn.k_proj.rcp', 'vision_model.encoder.layers.17.self_attn.k_proj.tlut', 'vision_model.encoder.layers.17.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.k_proj.trellis', 'vision_model.encoder.layers.17.self_attn.out_proj.SU', 'vision_model.encoder.layers.17.self_attn.out_proj.SV', 'vision_model.encoder.layers.17.self_attn.out_proj.rcp', 'vision_model.encoder.layers.17.self_attn.out_proj.tlut', 'vision_model.encoder.layers.17.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.out_proj.trellis', 'vision_model.encoder.layers.17.self_attn.q_proj.SU', 'vision_model.encoder.layers.17.self_attn.q_proj.SV', 'vision_model.encoder.layers.17.self_attn.q_proj.rcp', 'vision_model.encoder.layers.17.self_attn.q_proj.tlut', 'vision_model.encoder.layers.17.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.q_proj.trellis', 'vision_model.encoder.layers.17.self_attn.v_proj.SU', 'vision_model.encoder.layers.17.self_attn.v_proj.SV', 'vision_model.encoder.layers.17.self_attn.v_proj.rcp', 'vision_model.encoder.layers.17.self_attn.v_proj.tlut', 'vision_model.encoder.layers.17.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.v_proj.trellis', 'vision_model.encoder.layers.18.mlp.fc1.SU', 'vision_model.encoder.layers.18.mlp.fc1.SV', 'vision_model.encoder.layers.18.mlp.fc1.rcp', 'vision_model.encoder.layers.18.mlp.fc1.tlut', 'vision_model.encoder.layers.18.mlp.fc1.tp_rank', 'vision_model.encoder.layers.18.mlp.fc1.trellis', 'vision_model.encoder.layers.18.mlp.fc2.SU', 'vision_model.encoder.layers.18.mlp.fc2.SV', 'vision_model.encoder.layers.18.mlp.fc2.rcp', 'vision_model.encoder.layers.18.mlp.fc2.tlut', 'vision_model.encoder.layers.18.mlp.fc2.tp_rank', 'vision_model.encoder.layers.18.mlp.fc2.trellis', 'vision_model.encoder.layers.18.self_attn.k_proj.SU', 'vision_model.encoder.layers.18.self_attn.k_proj.SV', 'vision_model.encoder.layers.18.self_attn.k_proj.rcp', 'vision_model.encoder.layers.18.self_attn.k_proj.tlut', 'vision_model.encoder.layers.18.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.k_proj.trellis', 'vision_model.encoder.layers.18.self_attn.out_proj.SU', 'vision_model.encoder.layers.18.self_attn.out_proj.SV', 'vision_model.encoder.layers.18.self_attn.out_proj.rcp', 'vision_model.encoder.layers.18.self_attn.out_proj.tlut', 'vision_model.encoder.layers.18.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.out_proj.trellis', 'vision_model.encoder.layers.18.self_attn.q_proj.SU', 'vision_model.encoder.layers.18.self_attn.q_proj.SV', 'vision_model.encoder.layers.18.self_attn.q_proj.rcp', 'vision_model.encoder.layers.18.self_attn.q_proj.tlut', 'vision_model.encoder.layers.18.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.q_proj.trellis', 'vision_model.encoder.layers.18.self_attn.v_proj.SU', 'vision_model.encoder.layers.18.self_attn.v_proj.SV', 'vision_model.encoder.layers.18.self_attn.v_proj.rcp', 'vision_model.encoder.layers.18.self_attn.v_proj.tlut', 'vision_model.encoder.layers.18.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.v_proj.trellis', 'vision_model.encoder.layers.19.mlp.fc1.SU', 'vision_model.encoder.layers.19.mlp.fc1.SV', 'vision_model.encoder.layers.19.mlp.fc1.rcp', 'vision_model.encoder.layers.19.mlp.fc1.tlut', 'vision_model.encoder.layers.19.mlp.fc1.tp_rank', 'vision_model.encoder.layers.19.mlp.fc1.trellis', 'vision_model.encoder.layers.19.mlp.fc2.SU', 'vision_model.encoder.layers.19.mlp.fc2.SV', 'vision_model.encoder.layers.19.mlp.fc2.rcp', 'vision_model.encoder.layers.19.mlp.fc2.tlut', 'vision_model.encoder.layers.19.mlp.fc2.tp_rank', 'vision_model.encoder.layers.19.mlp.fc2.trellis', 'vision_model.encoder.layers.19.self_attn.k_proj.SU', 'vision_model.encoder.layers.19.self_attn.k_proj.SV', 'vision_model.encoder.layers.19.self_attn.k_proj.rcp', 'vision_model.encoder.layers.19.self_attn.k_proj.tlut', 'vision_model.encoder.layers.19.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.k_proj.trellis', 'vision_model.encoder.layers.19.self_attn.out_proj.SU', 'vision_model.encoder.layers.19.self_attn.out_proj.SV', 'vision_model.encoder.layers.19.self_attn.out_proj.rcp', 'vision_model.encoder.layers.19.self_attn.out_proj.tlut', 'vision_model.encoder.layers.19.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.out_proj.trellis', 'vision_model.encoder.layers.19.self_attn.q_proj.SU', 'vision_model.encoder.layers.19.self_attn.q_proj.SV', 'vision_model.encoder.layers.19.self_attn.q_proj.rcp', 'vision_model.encoder.layers.19.self_attn.q_proj.tlut', 'vision_model.encoder.layers.19.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.q_proj.trellis', 'vision_model.encoder.layers.19.self_attn.v_proj.SU', 'vision_model.encoder.layers.19.self_attn.v_proj.SV', 'vision_model.encoder.layers.19.self_attn.v_proj.rcp', 'vision_model.encoder.layers.19.self_attn.v_proj.tlut', 'vision_model.encoder.layers.19.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.v_proj.trellis', 'vision_model.encoder.layers.2.mlp.fc1.SU', 'vision_model.encoder.layers.2.mlp.fc1.SV', 'vision_model.encoder.layers.2.mlp.fc1.rcp', 'vision_model.encoder.layers.2.mlp.fc1.tlut', 'vision_model.encoder.layers.2.mlp.fc1.tp_rank', 'vision_model.encoder.layers.2.mlp.fc1.trellis', 'vision_model.encoder.layers.2.mlp.fc2.SU', 'vision_model.encoder.layers.2.mlp.fc2.SV', 'vision_model.encoder.layers.2.mlp.fc2.rcp', 'vision_model.encoder.layers.2.mlp.fc2.tlut', 'vision_model.encoder.layers.2.mlp.fc2.tp_rank', 'vision_model.encoder.layers.2.mlp.fc2.trellis', 'vision_model.encoder.layers.2.self_attn.k_proj.SU', 'vision_model.encoder.layers.2.self_attn.k_proj.SV', 'vision_model.encoder.layers.2.self_attn.k_proj.rcp', 'vision_model.encoder.layers.2.self_attn.k_proj.tlut', 'vision_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.k_proj.trellis', 'vision_model.encoder.layers.2.self_attn.out_proj.SU', 'vision_model.encoder.layers.2.self_attn.out_proj.SV', 'vision_model.encoder.layers.2.self_attn.out_proj.rcp', 'vision_model.encoder.layers.2.self_attn.out_proj.tlut', 'vision_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.out_proj.trellis', 'vision_model.encoder.layers.2.self_attn.q_proj.SU', 'vision_model.encoder.layers.2.self_attn.q_proj.SV', 'vision_model.encoder.layers.2.self_attn.q_proj.rcp', 'vision_model.encoder.layers.2.self_attn.q_proj.tlut', 'vision_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.q_proj.trellis', 'vision_model.encoder.layers.2.self_attn.v_proj.SU', 'vision_model.encoder.layers.2.self_attn.v_proj.SV', 'vision_model.encoder.layers.2.self_attn.v_proj.rcp', 'vision_model.encoder.layers.2.self_attn.v_proj.tlut', 'vision_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.v_proj.trellis', 'vision_model.encoder.layers.20.mlp.fc1.SU', 'vision_model.encoder.layers.20.mlp.fc1.SV', 'vision_model.encoder.layers.20.mlp.fc1.rcp', 'vision_model.encoder.layers.20.mlp.fc1.tlut', 'vision_model.encoder.layers.20.mlp.fc1.tp_rank', 'vision_model.encoder.layers.20.mlp.fc1.trellis', 'vision_model.encoder.layers.20.mlp.fc2.SU', 'vision_model.encoder.layers.20.mlp.fc2.SV', 'vision_model.encoder.layers.20.mlp.fc2.rcp', 'vision_model.encoder.layers.20.mlp.fc2.tlut', 'vision_model.encoder.layers.20.mlp.fc2.tp_rank', 'vision_model.encoder.layers.20.mlp.fc2.trellis', 'vision_model.encoder.layers.20.self_attn.k_proj.SU', 'vision_model.encoder.layers.20.self_attn.k_proj.SV', 'vision_model.encoder.layers.20.self_attn.k_proj.rcp', 'vision_model.encoder.layers.20.self_attn.k_proj.tlut', 'vision_model.encoder.layers.20.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.k_proj.trellis', 'vision_model.encoder.layers.20.self_attn.out_proj.SU', 'vision_model.encoder.layers.20.self_attn.out_proj.SV', 'vision_model.encoder.layers.20.self_attn.out_proj.rcp', 'vision_model.encoder.layers.20.self_attn.out_proj.tlut', 'vision_model.encoder.layers.20.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.out_proj.trellis', 'vision_model.encoder.layers.20.self_attn.q_proj.SU', 'vision_model.encoder.layers.20.self_attn.q_proj.SV', 'vision_model.encoder.layers.20.self_attn.q_proj.rcp', 'vision_model.encoder.layers.20.self_attn.q_proj.tlut', 'vision_model.encoder.layers.20.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.q_proj.trellis', 'vision_model.encoder.layers.20.self_attn.v_proj.SU', 'vision_model.encoder.layers.20.self_attn.v_proj.SV', 'vision_model.encoder.layers.20.self_attn.v_proj.rcp', 'vision_model.encoder.layers.20.self_attn.v_proj.tlut', 'vision_model.encoder.layers.20.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.v_proj.trellis', 'vision_model.encoder.layers.21.mlp.fc1.SU', 'vision_model.encoder.layers.21.mlp.fc1.SV', 'vision_model.encoder.layers.21.mlp.fc1.rcp', 'vision_model.encoder.layers.21.mlp.fc1.tlut', 'vision_model.encoder.layers.21.mlp.fc1.tp_rank', 'vision_model.encoder.layers.21.mlp.fc1.trellis', 'vision_model.encoder.layers.21.mlp.fc2.SU', 'vision_model.encoder.layers.21.mlp.fc2.SV', 'vision_model.encoder.layers.21.mlp.fc2.rcp', 'vision_model.encoder.layers.21.mlp.fc2.tlut', 'vision_model.encoder.layers.21.mlp.fc2.tp_rank', 'vision_model.encoder.layers.21.mlp.fc2.trellis', 'vision_model.encoder.layers.21.self_attn.k_proj.SU', 'vision_model.encoder.layers.21.self_attn.k_proj.SV', 'vision_model.encoder.layers.21.self_attn.k_proj.rcp', 'vision_model.encoder.layers.21.self_attn.k_proj.tlut', 'vision_model.encoder.layers.21.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.k_proj.trellis', 'vision_model.encoder.layers.21.self_attn.out_proj.SU', 'vision_model.encoder.layers.21.self_attn.out_proj.SV', 'vision_model.encoder.layers.21.self_attn.out_proj.rcp', 'vision_model.encoder.layers.21.self_attn.out_proj.tlut', 'vision_model.encoder.layers.21.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.out_proj.trellis', 'vision_model.encoder.layers.21.self_attn.q_proj.SU', 'vision_model.encoder.layers.21.self_attn.q_proj.SV', 'vision_model.encoder.layers.21.self_attn.q_proj.rcp', 'vision_model.encoder.layers.21.self_attn.q_proj.tlut', 'vision_model.encoder.layers.21.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.q_proj.trellis', 'vision_model.encoder.layers.21.self_attn.v_proj.SU', 'vision_model.encoder.layers.21.self_attn.v_proj.SV', 'vision_model.encoder.layers.21.self_attn.v_proj.rcp', 'vision_model.encoder.layers.21.self_attn.v_proj.tlut', 'vision_model.encoder.layers.21.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.v_proj.trellis', 'vision_model.encoder.layers.22.mlp.fc1.SU', 'vision_model.encoder.layers.22.mlp.fc1.SV', 'vision_model.encoder.layers.22.mlp.fc1.rcp', 'vision_model.encoder.layers.22.mlp.fc1.tlut', 'vision_model.encoder.layers.22.mlp.fc1.tp_rank', 'vision_model.encoder.layers.22.mlp.fc1.trellis', 'vision_model.encoder.layers.22.mlp.fc2.SU', 'vision_model.encoder.layers.22.mlp.fc2.SV', 'vision_model.encoder.layers.22.mlp.fc2.rcp', 'vision_model.encoder.layers.22.mlp.fc2.tlut', 'vision_model.encoder.layers.22.mlp.fc2.tp_rank', 'vision_model.encoder.layers.22.mlp.fc2.trellis', 'vision_model.encoder.layers.22.self_attn.k_proj.SU', 'vision_model.encoder.layers.22.self_attn.k_proj.SV', 'vision_model.encoder.layers.22.self_attn.k_proj.rcp', 'vision_model.encoder.layers.22.self_attn.k_proj.tlut', 'vision_model.encoder.layers.22.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.k_proj.trellis', 'vision_model.encoder.layers.22.self_attn.out_proj.SU', 'vision_model.encoder.layers.22.self_attn.out_proj.SV', 'vision_model.encoder.layers.22.self_attn.out_proj.rcp', 'vision_model.encoder.layers.22.self_attn.out_proj.tlut', 'vision_model.encoder.layers.22.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.out_proj.trellis', 'vision_model.encoder.layers.22.self_attn.q_proj.SU', 'vision_model.encoder.layers.22.self_attn.q_proj.SV', 'vision_model.encoder.layers.22.self_attn.q_proj.rcp', 'vision_model.encoder.layers.22.self_attn.q_proj.tlut', 'vision_model.encoder.layers.22.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.q_proj.trellis', 'vision_model.encoder.layers.22.self_attn.v_proj.SU', 'vision_model.encoder.layers.22.self_attn.v_proj.SV', 'vision_model.encoder.layers.22.self_attn.v_proj.rcp', 'vision_model.encoder.layers.22.self_attn.v_proj.tlut', 'vision_model.encoder.layers.22.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.v_proj.trellis', 'vision_model.encoder.layers.23.mlp.fc1.SU', 'vision_model.encoder.layers.23.mlp.fc1.SV', 'vision_model.encoder.layers.23.mlp.fc1.rcp', 'vision_model.encoder.layers.23.mlp.fc1.tlut', 'vision_model.encoder.layers.23.mlp.fc1.tp_rank', 'vision_model.encoder.layers.23.mlp.fc1.trellis', 'vision_model.encoder.layers.23.mlp.fc2.SU', 'vision_model.encoder.layers.23.mlp.fc2.SV', 'vision_model.encoder.layers.23.mlp.fc2.rcp', 'vision_model.encoder.layers.23.mlp.fc2.tlut', 'vision_model.encoder.layers.23.mlp.fc2.tp_rank', 'vision_model.encoder.layers.23.mlp.fc2.trellis', 'vision_model.encoder.layers.23.self_attn.k_proj.SU', 'vision_model.encoder.layers.23.self_attn.k_proj.SV', 'vision_model.encoder.layers.23.self_attn.k_proj.rcp', 'vision_model.encoder.layers.23.self_attn.k_proj.tlut', 'vision_model.encoder.layers.23.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.k_proj.trellis', 'vision_model.encoder.layers.23.self_attn.out_proj.SU', 'vision_model.encoder.layers.23.self_attn.out_proj.SV', 'vision_model.encoder.layers.23.self_attn.out_proj.rcp', 'vision_model.encoder.layers.23.self_attn.out_proj.tlut', 'vision_model.encoder.layers.23.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.out_proj.trellis', 'vision_model.encoder.layers.23.self_attn.q_proj.SU', 'vision_model.encoder.layers.23.self_attn.q_proj.SV', 'vision_model.encoder.layers.23.self_attn.q_proj.rcp', 'vision_model.encoder.layers.23.self_attn.q_proj.tlut', 'vision_model.encoder.layers.23.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.q_proj.trellis', 'vision_model.encoder.layers.23.self_attn.v_proj.SU', 'vision_model.encoder.layers.23.self_attn.v_proj.SV', 'vision_model.encoder.layers.23.self_attn.v_proj.rcp', 'vision_model.encoder.layers.23.self_attn.v_proj.tlut', 'vision_model.encoder.layers.23.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.v_proj.trellis', 'vision_model.encoder.layers.3.mlp.fc1.SU', 'vision_model.encoder.layers.3.mlp.fc1.SV', 'vision_model.encoder.layers.3.mlp.fc1.rcp', 'vision_model.encoder.layers.3.mlp.fc1.tlut', 'vision_model.encoder.layers.3.mlp.fc1.tp_rank', 'vision_model.encoder.layers.3.mlp.fc1.trellis', 'vision_model.encoder.layers.3.mlp.fc2.SU', 'vision_model.encoder.layers.3.mlp.fc2.SV', 'vision_model.encoder.layers.3.mlp.fc2.rcp', 'vision_model.encoder.layers.3.mlp.fc2.tlut', 'vision_model.encoder.layers.3.mlp.fc2.tp_rank', 'vision_model.encoder.layers.3.mlp.fc2.trellis', 'vision_model.encoder.layers.3.self_attn.k_proj.SU', 'vision_model.encoder.layers.3.self_attn.k_proj.SV', 'vision_model.encoder.layers.3.self_attn.k_proj.rcp', 'vision_model.encoder.layers.3.self_attn.k_proj.tlut', 'vision_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.k_proj.trellis', 'vision_model.encoder.layers.3.self_attn.out_proj.SU', 'vision_model.encoder.layers.3.self_attn.out_proj.SV', 'vision_model.encoder.layers.3.self_attn.out_proj.rcp', 'vision_model.encoder.layers.3.self_attn.out_proj.tlut', 'vision_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.out_proj.trellis', 'vision_model.encoder.layers.3.self_attn.q_proj.SU', 'vision_model.encoder.layers.3.self_attn.q_proj.SV', 'vision_model.encoder.layers.3.self_attn.q_proj.rcp', 'vision_model.encoder.layers.3.self_attn.q_proj.tlut', 'vision_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.q_proj.trellis', 'vision_model.encoder.layers.3.self_attn.v_proj.SU', 'vision_model.encoder.layers.3.self_attn.v_proj.SV', 'vision_model.encoder.layers.3.self_attn.v_proj.rcp', 'vision_model.encoder.layers.3.self_attn.v_proj.tlut', 'vision_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.v_proj.trellis', 'vision_model.encoder.layers.4.mlp.fc1.SU', 'vision_model.encoder.layers.4.mlp.fc1.SV', 'vision_model.encoder.layers.4.mlp.fc1.rcp', 'vision_model.encoder.layers.4.mlp.fc1.tlut', 'vision_model.encoder.layers.4.mlp.fc1.tp_rank', 'vision_model.encoder.layers.4.mlp.fc1.trellis', 'vision_model.encoder.layers.4.mlp.fc2.SU', 'vision_model.encoder.layers.4.mlp.fc2.SV', 'vision_model.encoder.layers.4.mlp.fc2.rcp', 'vision_model.encoder.layers.4.mlp.fc2.tlut', 'vision_model.encoder.layers.4.mlp.fc2.tp_rank', 'vision_model.encoder.layers.4.mlp.fc2.trellis', 'vision_model.encoder.layers.4.self_attn.k_proj.SU', 'vision_model.encoder.layers.4.self_attn.k_proj.SV', 'vision_model.encoder.layers.4.self_attn.k_proj.rcp', 'vision_model.encoder.layers.4.self_attn.k_proj.tlut', 'vision_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.k_proj.trellis', 'vision_model.encoder.layers.4.self_attn.out_proj.SU', 'vision_model.encoder.layers.4.self_attn.out_proj.SV', 'vision_model.encoder.layers.4.self_attn.out_proj.rcp', 'vision_model.encoder.layers.4.self_attn.out_proj.tlut', 'vision_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.out_proj.trellis', 'vision_model.encoder.layers.4.self_attn.q_proj.SU', 'vision_model.encoder.layers.4.self_attn.q_proj.SV', 'vision_model.encoder.layers.4.self_attn.q_proj.rcp', 'vision_model.encoder.layers.4.self_attn.q_proj.tlut', 'vision_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.q_proj.trellis', 'vision_model.encoder.layers.4.self_attn.v_proj.SU', 'vision_model.encoder.layers.4.self_attn.v_proj.SV', 'vision_model.encoder.layers.4.self_attn.v_proj.rcp', 'vision_model.encoder.layers.4.self_attn.v_proj.tlut', 'vision_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.v_proj.trellis', 'vision_model.encoder.layers.5.mlp.fc1.SU', 'vision_model.encoder.layers.5.mlp.fc1.SV', 'vision_model.encoder.layers.5.mlp.fc1.rcp', 'vision_model.encoder.layers.5.mlp.fc1.tlut', 'vision_model.encoder.layers.5.mlp.fc1.tp_rank', 'vision_model.encoder.layers.5.mlp.fc1.trellis', 'vision_model.encoder.layers.5.mlp.fc2.SU', 'vision_model.encoder.layers.5.mlp.fc2.SV', 'vision_model.encoder.layers.5.mlp.fc2.rcp', 'vision_model.encoder.layers.5.mlp.fc2.tlut', 'vision_model.encoder.layers.5.mlp.fc2.tp_rank', 'vision_model.encoder.layers.5.mlp.fc2.trellis', 'vision_model.encoder.layers.5.self_attn.k_proj.SU', 'vision_model.encoder.layers.5.self_attn.k_proj.SV', 'vision_model.encoder.layers.5.self_attn.k_proj.rcp', 'vision_model.encoder.layers.5.self_attn.k_proj.tlut', 'vision_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.k_proj.trellis', 'vision_model.encoder.layers.5.self_attn.out_proj.SU', 'vision_model.encoder.layers.5.self_attn.out_proj.SV', 'vision_model.encoder.layers.5.self_attn.out_proj.rcp', 'vision_model.encoder.layers.5.self_attn.out_proj.tlut', 'vision_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.out_proj.trellis', 'vision_model.encoder.layers.5.self_attn.q_proj.SU', 'vision_model.encoder.layers.5.self_attn.q_proj.SV', 'vision_model.encoder.layers.5.self_attn.q_proj.rcp', 'vision_model.encoder.layers.5.self_attn.q_proj.tlut', 'vision_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.q_proj.trellis', 'vision_model.encoder.layers.5.self_attn.v_proj.SU', 'vision_model.encoder.layers.5.self_attn.v_proj.SV', 'vision_model.encoder.layers.5.self_attn.v_proj.rcp', 'vision_model.encoder.layers.5.self_attn.v_proj.tlut', 'vision_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.v_proj.trellis', 'vision_model.encoder.layers.6.mlp.fc1.SU', 'vision_model.encoder.layers.6.mlp.fc1.SV', 'vision_model.encoder.layers.6.mlp.fc1.rcp', 'vision_model.encoder.layers.6.mlp.fc1.tlut', 'vision_model.encoder.layers.6.mlp.fc1.tp_rank', 'vision_model.encoder.layers.6.mlp.fc1.trellis', 'vision_model.encoder.layers.6.mlp.fc2.SU', 'vision_model.encoder.layers.6.mlp.fc2.SV', 'vision_model.encoder.layers.6.mlp.fc2.rcp', 'vision_model.encoder.layers.6.mlp.fc2.tlut', 'vision_model.encoder.layers.6.mlp.fc2.tp_rank', 'vision_model.encoder.layers.6.mlp.fc2.trellis', 'vision_model.encoder.layers.6.self_attn.k_proj.SU', 'vision_model.encoder.layers.6.self_attn.k_proj.SV', 'vision_model.encoder.layers.6.self_attn.k_proj.rcp', 'vision_model.encoder.layers.6.self_attn.k_proj.tlut', 'vision_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.k_proj.trellis', 'vision_model.encoder.layers.6.self_attn.out_proj.SU', 'vision_model.encoder.layers.6.self_attn.out_proj.SV', 'vision_model.encoder.layers.6.self_attn.out_proj.rcp', 'vision_model.encoder.layers.6.self_attn.out_proj.tlut', 'vision_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.out_proj.trellis', 'vision_model.encoder.layers.6.self_attn.q_proj.SU', 'vision_model.encoder.layers.6.self_attn.q_proj.SV', 'vision_model.encoder.layers.6.self_attn.q_proj.rcp', 'vision_model.encoder.layers.6.self_attn.q_proj.tlut', 'vision_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.q_proj.trellis', 'vision_model.encoder.layers.6.self_attn.v_proj.SU', 'vision_model.encoder.layers.6.self_attn.v_proj.SV', 'vision_model.encoder.layers.6.self_attn.v_proj.rcp', 'vision_model.encoder.layers.6.self_attn.v_proj.tlut', 'vision_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.v_proj.trellis', 'vision_model.encoder.layers.7.mlp.fc1.SU', 'vision_model.encoder.layers.7.mlp.fc1.SV', 'vision_model.encoder.layers.7.mlp.fc1.rcp', 'vision_model.encoder.layers.7.mlp.fc1.tlut', 'vision_model.encoder.layers.7.mlp.fc1.tp_rank', 'vision_model.encoder.layers.7.mlp.fc1.trellis', 'vision_model.encoder.layers.7.mlp.fc2.SU', 'vision_model.encoder.layers.7.mlp.fc2.SV', 'vision_model.encoder.layers.7.mlp.fc2.rcp', 'vision_model.encoder.layers.7.mlp.fc2.tlut', 'vision_model.encoder.layers.7.mlp.fc2.tp_rank', 'vision_model.encoder.layers.7.mlp.fc2.trellis', 'vision_model.encoder.layers.7.self_attn.k_proj.SU', 'vision_model.encoder.layers.7.self_attn.k_proj.SV', 'vision_model.encoder.layers.7.self_attn.k_proj.rcp', 'vision_model.encoder.layers.7.self_attn.k_proj.tlut', 'vision_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.k_proj.trellis', 'vision_model.encoder.layers.7.self_attn.out_proj.SU', 'vision_model.encoder.layers.7.self_attn.out_proj.SV', 'vision_model.encoder.layers.7.self_attn.out_proj.rcp', 'vision_model.encoder.layers.7.self_attn.out_proj.tlut', 'vision_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.out_proj.trellis', 'vision_model.encoder.layers.7.self_attn.q_proj.SU', 'vision_model.encoder.layers.7.self_attn.q_proj.SV', 'vision_model.encoder.layers.7.self_attn.q_proj.rcp', 'vision_model.encoder.layers.7.self_attn.q_proj.tlut', 'vision_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.q_proj.trellis', 'vision_model.encoder.layers.7.self_attn.v_proj.SU', 'vision_model.encoder.layers.7.self_attn.v_proj.SV', 'vision_model.encoder.layers.7.self_attn.v_proj.rcp', 'vision_model.encoder.layers.7.self_attn.v_proj.tlut', 'vision_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.v_proj.trellis', 'vision_model.encoder.layers.8.mlp.fc1.SU', 'vision_model.encoder.layers.8.mlp.fc1.SV', 'vision_model.encoder.layers.8.mlp.fc1.rcp', 'vision_model.encoder.layers.8.mlp.fc1.tlut', 'vision_model.encoder.layers.8.mlp.fc1.tp_rank', 'vision_model.encoder.layers.8.mlp.fc1.trellis', 'vision_model.encoder.layers.8.mlp.fc2.SU', 'vision_model.encoder.layers.8.mlp.fc2.SV', 'vision_model.encoder.layers.8.mlp.fc2.rcp', 'vision_model.encoder.layers.8.mlp.fc2.tlut', 'vision_model.encoder.layers.8.mlp.fc2.tp_rank', 'vision_model.encoder.layers.8.mlp.fc2.trellis', 'vision_model.encoder.layers.8.self_attn.k_proj.SU', 'vision_model.encoder.layers.8.self_attn.k_proj.SV', 'vision_model.encoder.layers.8.self_attn.k_proj.rcp', 'vision_model.encoder.layers.8.self_attn.k_proj.tlut', 'vision_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.k_proj.trellis', 'vision_model.encoder.layers.8.self_attn.out_proj.SU', 'vision_model.encoder.layers.8.self_attn.out_proj.SV', 'vision_model.encoder.layers.8.self_attn.out_proj.rcp', 'vision_model.encoder.layers.8.self_attn.out_proj.tlut', 'vision_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.out_proj.trellis', 'vision_model.encoder.layers.8.self_attn.q_proj.SU', 'vision_model.encoder.layers.8.self_attn.q_proj.SV', 'vision_model.encoder.layers.8.self_attn.q_proj.rcp', 'vision_model.encoder.layers.8.self_attn.q_proj.tlut', 'vision_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.q_proj.trellis', 'vision_model.encoder.layers.8.self_attn.v_proj.SU', 'vision_model.encoder.layers.8.self_attn.v_proj.SV', 'vision_model.encoder.layers.8.self_attn.v_proj.rcp', 'vision_model.encoder.layers.8.self_attn.v_proj.tlut', 'vision_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.v_proj.trellis', 'vision_model.encoder.layers.9.mlp.fc1.SU', 'vision_model.encoder.layers.9.mlp.fc1.SV', 'vision_model.encoder.layers.9.mlp.fc1.rcp', 'vision_model.encoder.layers.9.mlp.fc1.tlut', 'vision_model.encoder.layers.9.mlp.fc1.tp_rank', 'vision_model.encoder.layers.9.mlp.fc1.trellis', 'vision_model.encoder.layers.9.mlp.fc2.SU', 'vision_model.encoder.layers.9.mlp.fc2.SV', 'vision_model.encoder.layers.9.mlp.fc2.rcp', 'vision_model.encoder.layers.9.mlp.fc2.tlut', 'vision_model.encoder.layers.9.mlp.fc2.tp_rank', 'vision_model.encoder.layers.9.mlp.fc2.trellis', 'vision_model.encoder.layers.9.self_attn.k_proj.SU', 'vision_model.encoder.layers.9.self_attn.k_proj.SV', 'vision_model.encoder.layers.9.self_attn.k_proj.rcp', 'vision_model.encoder.layers.9.self_attn.k_proj.tlut', 'vision_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.k_proj.trellis', 'vision_model.encoder.layers.9.self_attn.out_proj.SU', 'vision_model.encoder.layers.9.self_attn.out_proj.SV', 'vision_model.encoder.layers.9.self_attn.out_proj.rcp', 'vision_model.encoder.layers.9.self_attn.out_proj.tlut', 'vision_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.out_proj.trellis', 'vision_model.encoder.layers.9.self_attn.q_proj.SU', 'vision_model.encoder.layers.9.self_attn.q_proj.SV', 'vision_model.encoder.layers.9.self_attn.q_proj.rcp', 'vision_model.encoder.layers.9.self_attn.q_proj.tlut', 'vision_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.q_proj.trellis', 'vision_model.encoder.layers.9.self_attn.v_proj.SU', 'vision_model.encoder.layers.9.self_attn.v_proj.SV', 'vision_model.encoder.layers.9.self_attn.v_proj.rcp', 'vision_model.encoder.layers.9.self_attn.v_proj.tlut', 'vision_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0429 17:17:49.145178 1121119 hfize_clip.py:65] Loading text layer 0
W0429 17:17:49.145414 1121119 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved = torch.load(f'{path_prefix}/{full_key}.pt', map_location='cpu')

I0429 17:17:49.378537 1121119 hfize_clip.py:65] Loading text layer 1
I0429 17:17:49.708112 1121119 hfize_clip.py:65] Loading text layer 2
I0429 17:17:50.082151 1121119 hfize_clip.py:65] Loading text layer 3
I0429 17:17:50.438307 1121119 hfize_clip.py:65] Loading text layer 4
I0429 17:17:50.771098 1121119 hfize_clip.py:65] Loading text layer 5
I0429 17:17:51.116018 1121119 hfize_clip.py:65] Loading text layer 6
I0429 17:17:51.507445 1121119 hfize_clip.py:65] Loading text layer 7
I0429 17:17:51.858491 1121119 hfize_clip.py:65] Loading text layer 8
I0429 17:17:52.219578 1121119 hfize_clip.py:65] Loading text layer 9
I0429 17:17:52.592942 1121119 hfize_clip.py:65] Loading text layer 10
I0429 17:17:52.952172 1121119 hfize_clip.py:65] Loading text layer 11
I0429 17:17:53.321230 1121119 hfize_clip.py:65] Loading vision layer 0
I0429 17:17:53.667708 1121119 hfize_clip.py:65] Loading vision layer 1
I0429 17:17:54.005358 1121119 hfize_clip.py:65] Loading vision layer 2
I0429 17:17:54.382008 1121119 hfize_clip.py:65] Loading vision layer 3
I0429 17:17:54.730962 1121119 hfize_clip.py:65] Loading vision layer 4
I0429 17:17:55.086511 1121119 hfize_clip.py:65] Loading vision layer 5
I0429 17:17:55.456639 1121119 hfize_clip.py:65] Loading vision layer 6
I0429 17:17:55.844117 1121119 hfize_clip.py:65] Loading vision layer 7
I0429 17:17:56.196493 1121119 hfize_clip.py:65] Loading vision layer 8
I0429 17:17:56.569509 1121119 hfize_clip.py:65] Loading vision layer 9
I0429 17:17:56.886809 1121119 hfize_clip.py:65] Loading vision layer 10
I0429 17:17:57.265812 1121119 hfize_clip.py:65] Loading vision layer 11
I0429 17:17:57.620913 1121119 hfize_clip.py:65] Loading vision layer 12
I0429 17:17:58.006267 1121119 hfize_clip.py:65] Loading vision layer 13
I0429 17:17:58.348292 1121119 hfize_clip.py:65] Loading vision layer 14
I0429 17:17:58.708778 1121119 hfize_clip.py:65] Loading vision layer 15
I0429 17:17:59.119609 1121119 hfize_clip.py:65] Loading vision layer 16
I0429 17:17:59.460030 1121119 hfize_clip.py:65] Loading vision layer 17
I0429 17:17:59.824156 1121119 hfize_clip.py:65] Loading vision layer 18
I0429 17:18:00.204162 1121119 hfize_clip.py:65] Loading vision layer 19
I0429 17:18:00.578834 1121119 hfize_clip.py:65] Loading vision layer 20
I0429 17:18:00.940099 1121119 hfize_clip.py:65] Loading vision layer 21
I0429 17:18:01.305404 1121119 hfize_clip.py:65] Loading vision layer 22
I0429 17:18:01.702708 1121119 hfize_clip.py:65] Loading vision layer 23
I0429 17:18:02.048153 1121119 hfize_clip.py:95] Copying logit_scale
I0429 17:18:02.048310 1121119 hfize_clip.py:90] Copying submodule: visual_projection
I0429 17:18:02.089073 1121119 hfize_clip.py:90] Copying submodule: text_projection
I0429 17:18:02.103207 1121119 hfize_clip.py:90] Copying submodule: text token_embedding
I0429 17:18:02.188266 1121119 hfize_clip.py:90] Copying submodule: text position_embedding
I0429 17:18:02.194734 1121119 hfize_clip.py:90] Copying submodule: text final_layer_norm
I0429 17:18:02.195146 1121119 hfize_clip.py:90] Copying submodule: vision patch_embedding
I0429 17:18:02.207105 1121119 hfize_clip.py:90] Copying submodule: vision position_embedding
I0429 17:18:02.225783 1121119 hfize_clip.py:90] Copying submodule: vision post_layernorm
I0429 17:18:02.226038 1121119 hfize_clip.py:121] Saving model to ../hf_model_comp/qtip/hf/clip-vit-large-patch14_6bit...
  0%|          | 0/196 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  1%|          | 1/196 [00:11<38:41, 11.91s/it]  1%|          | 2/196 [00:14<20:52,  6.46s/it]  2%|▏         | 3/196 [00:17<15:07,  4.70s/it]  2%|▏         | 4/196 [00:19<12:23,  3.87s/it]  3%|▎         | 5/196 [00:22<10:50,  3.40s/it]  3%|▎         | 6/196 [00:24<09:53,  3.12s/it]  4%|▎         | 7/196 [00:27<09:17,  2.95s/it]  4%|▍         | 8/196 [00:30<08:54,  2.84s/it]  5%|▍         | 9/196 [00:32<08:36,  2.76s/it]  5%|▌         | 10/196 [00:35<08:23,  2.71s/it]  6%|▌         | 11/196 [00:37<08:13,  2.67s/it]  6%|▌         | 12/196 [00:40<08:06,  2.64s/it]  7%|▋         | 13/196 [00:43<08:00,  2.62s/it]  7%|▋         | 14/196 [00:45<07:55,  2.61s/it]  8%|▊         | 15/196 [00:48<07:54,  2.62s/it]  8%|▊         | 16/196 [00:50<07:51,  2.62s/it]  9%|▊         | 17/196 [00:53<07:46,  2.61s/it]  9%|▉         | 18/196 [00:56<07:43,  2.60s/it] 10%|▉         | 19/196 [00:58<07:41,  2.60s/it] 10%|█         | 20/196 [01:01<07:37,  2.60s/it] 11%|█         | 21/196 [01:03<07:34,  2.59s/it] 11%|█         | 22/196 [01:06<07:30,  2.59s/it] 12%|█▏        | 23/196 [01:08<07:26,  2.58s/it] 12%|█▏        | 24/196 [01:11<07:24,  2.58s/it] 13%|█▎        | 25/196 [01:14<07:20,  2.58s/it] 13%|█▎        | 26/196 [01:16<07:18,  2.58s/it] 14%|█▍        | 27/196 [01:19<07:15,  2.57s/it] 14%|█▍        | 28/196 [01:21<07:13,  2.58s/it] 15%|█▍        | 29/196 [01:24<07:11,  2.58s/it] 15%|█▌        | 30/196 [01:27<07:10,  2.59s/it] 16%|█▌        | 31/196 [01:29<07:06,  2.58s/it] 16%|█▋        | 32/196 [01:32<07:04,  2.59s/it] 17%|█▋        | 33/196 [01:34<07:02,  2.59s/it] 17%|█▋        | 34/196 [01:37<07:02,  2.61s/it] 18%|█▊        | 35/196 [01:40<06:58,  2.60s/it] 18%|█▊        | 36/196 [01:42<06:54,  2.59s/it] 19%|█▉        | 37/196 [01:45<06:51,  2.59s/it] 19%|█▉        | 38/196 [01:47<06:48,  2.59s/it] 20%|█▉        | 39/196 [01:50<06:45,  2.58s/it] 20%|██        | 40/196 [01:52<06:42,  2.58s/it] 21%|██        | 41/196 [01:55<06:39,  2.58s/it] 21%|██▏       | 42/196 [01:58<06:36,  2.57s/it] 22%|██▏       | 43/196 [02:00<06:34,  2.58s/it] 22%|██▏       | 44/196 [02:03<06:31,  2.58s/it] 23%|██▎       | 45/196 [02:05<06:28,  2.57s/it] 23%|██▎       | 46/196 [02:08<06:26,  2.57s/it] 24%|██▍       | 47/196 [02:10<06:23,  2.58s/it] 24%|██▍       | 48/196 [02:13<06:22,  2.58s/it] 25%|██▌       | 49/196 [02:16<06:20,  2.59s/it] 26%|██▌       | 50/196 [02:18<06:20,  2.60s/it] 26%|██▌       | 51/196 [02:21<06:17,  2.61s/it] 27%|██▋       | 52/196 [02:24<06:14,  2.60s/it] 27%|██▋       | 53/196 [02:26<06:12,  2.61s/it] 28%|██▊       | 54/196 [02:29<06:11,  2.62s/it] 28%|██▊       | 55/196 [02:31<06:07,  2.61s/it] 29%|██▊       | 56/196 [02:34<06:06,  2.62s/it] 29%|██▉       | 57/196 [02:37<06:01,  2.60s/it] 30%|██▉       | 58/196 [02:39<05:57,  2.59s/it] 30%|███       | 59/196 [02:42<05:53,  2.58s/it] 31%|███       | 60/196 [02:44<05:51,  2.58s/it] 31%|███       | 61/196 [02:47<05:48,  2.58s/it] 32%|███▏      | 62/196 [02:49<05:46,  2.59s/it] 32%|███▏      | 63/196 [02:52<05:44,  2.59s/it] 33%|███▎      | 64/196 [02:55<05:41,  2.59s/it] 33%|███▎      | 65/196 [02:57<05:39,  2.59s/it] 34%|███▎      | 66/196 [03:00<05:37,  2.59s/it] 34%|███▍      | 67/196 [03:02<05:35,  2.60s/it] 35%|███▍      | 68/196 [03:05<05:33,  2.60s/it] 35%|███▌      | 69/196 [03:08<05:30,  2.60s/it] 36%|███▌      | 70/196 [03:10<05:28,  2.61s/it] 36%|███▌      | 71/196 [03:13<05:25,  2.60s/it] 37%|███▋      | 72/196 [03:15<05:23,  2.60s/it] 37%|███▋      | 73/196 [03:18<05:22,  2.63s/it] 38%|███▊      | 74/196 [03:21<05:19,  2.62s/it] 38%|███▊      | 75/196 [03:23<05:15,  2.61s/it] 39%|███▉      | 76/196 [03:26<05:13,  2.61s/it] 39%|███▉      | 77/196 [03:29<05:10,  2.61s/it] 40%|███▉      | 78/196 [03:31<05:07,  2.61s/it] 40%|████      | 79/196 [03:34<05:04,  2.61s/it] 41%|████      | 80/196 [03:36<05:03,  2.61s/it] 41%|████▏     | 81/196 [03:39<04:59,  2.60s/it] 42%|████▏     | 82/196 [03:42<04:56,  2.60s/it] 42%|████▏     | 83/196 [03:44<04:53,  2.59s/it] 43%|████▎     | 84/196 [03:47<04:50,  2.59s/it] 43%|████▎     | 85/196 [03:49<04:47,  2.59s/it] 44%|████▍     | 86/196 [03:52<04:45,  2.60s/it] 44%|████▍     | 87/196 [03:54<04:42,  2.59s/it] 45%|████▍     | 88/196 [03:57<04:40,  2.59s/it] 45%|████▌     | 89/196 [04:00<04:37,  2.59s/it] 46%|████▌     | 90/196 [04:02<04:34,  2.59s/it] 46%|████▋     | 91/196 [04:05<04:31,  2.59s/it] 47%|████▋     | 92/196 [04:07<04:30,  2.60s/it] 47%|████▋     | 93/196 [04:10<04:28,  2.60s/it] 48%|████▊     | 94/196 [04:13<04:24,  2.60s/it] 48%|████▊     | 95/196 [04:15<04:21,  2.59s/it] 49%|████▉     | 96/196 [04:18<04:18,  2.59s/it] 49%|████▉     | 97/196 [04:20<04:16,  2.59s/it] 50%|█████     | 98/196 [04:23<04:13,  2.59s/it] 51%|█████     | 99/196 [04:26<04:11,  2.59s/it] 51%|█████     | 100/196 [04:28<04:08,  2.59s/it] 52%|█████▏    | 101/196 [04:31<04:06,  2.59s/it] 52%|█████▏    | 102/196 [04:33<04:03,  2.59s/it] 53%|█████▎    | 103/196 [04:36<04:00,  2.59s/it] 53%|█████▎    | 104/196 [04:39<03:58,  2.60s/it] 54%|█████▎    | 105/196 [04:41<03:56,  2.59s/it] 54%|█████▍    | 106/196 [04:44<03:53,  2.59s/it] 55%|█████▍    | 107/196 [04:46<03:50,  2.59s/it] 55%|█████▌    | 108/196 [04:49<03:48,  2.59s/it] 56%|█████▌    | 109/196 [04:52<03:45,  2.60s/it] 56%|█████▌    | 110/196 [04:54<03:43,  2.60s/it] 57%|█████▋    | 111/196 [04:57<03:40,  2.60s/it] 57%|█████▋    | 112/196 [04:59<03:39,  2.61s/it] 58%|█████▊    | 113/196 [05:02<03:36,  2.61s/it] 58%|█████▊    | 114/196 [05:05<03:33,  2.60s/it] 59%|█████▊    | 115/196 [05:07<03:31,  2.61s/it] 59%|█████▉    | 116/196 [05:10<03:28,  2.61s/it] 60%|█████▉    | 117/196 [05:12<03:26,  2.61s/it] 60%|██████    | 118/196 [05:15<03:23,  2.61s/it] 61%|██████    | 119/196 [05:18<03:20,  2.61s/it] 61%|██████    | 120/196 [05:20<03:18,  2.61s/it] 62%|██████▏   | 121/196 [05:23<03:15,  2.61s/it] 62%|██████▏   | 122/196 [05:25<03:12,  2.61s/it] 63%|██████▎   | 123/196 [05:28<03:10,  2.60s/it] 63%|██████▎   | 124/196 [05:31<03:07,  2.61s/it] 64%|██████▍   | 125/196 [05:33<03:04,  2.60s/it] 64%|██████▍   | 126/196 [05:36<03:02,  2.61s/it] 65%|██████▍   | 127/196 [05:38<02:59,  2.60s/it] 65%|██████▌   | 128/196 [05:41<02:57,  2.62s/it] 66%|██████▌   | 129/196 [05:44<02:54,  2.61s/it] 66%|██████▋   | 130/196 [05:46<02:52,  2.61s/it] 67%|██████▋   | 131/196 [05:49<02:50,  2.62s/it] 67%|██████▋   | 132/196 [05:52<02:48,  2.63s/it] 68%|██████▊   | 133/196 [05:54<02:45,  2.62s/it] 68%|██████▊   | 134/196 [05:57<02:42,  2.61s/it] 69%|██████▉   | 135/196 [05:59<02:39,  2.61s/it] 69%|██████▉   | 136/196 [06:02<02:36,  2.61s/it] 70%|██████▉   | 137/196 [06:05<02:33,  2.61s/it] 70%|███████   | 138/196 [06:07<02:31,  2.61s/it] 71%|███████   | 139/196 [06:10<02:28,  2.60s/it] 71%|███████▏  | 140/196 [06:12<02:25,  2.60s/it] 72%|███████▏  | 141/196 [06:15<02:22,  2.60s/it] 72%|███████▏  | 142/196 [06:18<02:20,  2.59s/it] 73%|███████▎  | 143/196 [06:20<02:17,  2.60s/it] 73%|███████▎  | 144/196 [06:23<02:15,  2.61s/it] 74%|███████▍  | 145/196 [06:25<02:12,  2.60s/it] 74%|███████▍  | 146/196 [06:28<02:10,  2.60s/it] 75%|███████▌  | 147/196 [06:31<02:07,  2.60s/it] 76%|███████▌  | 148/196 [06:33<02:04,  2.60s/it] 76%|███████▌  | 149/196 [06:36<02:02,  2.60s/it] 77%|███████▋  | 150/196 [06:38<02:00,  2.62s/it] 77%|███████▋  | 151/196 [06:41<01:57,  2.61s/it] 78%|███████▊  | 152/196 [06:44<01:54,  2.61s/it] 78%|███████▊  | 153/196 [06:46<01:51,  2.60s/it] 79%|███████▊  | 154/196 [06:49<01:48,  2.59s/it] 79%|███████▉  | 155/196 [06:51<01:46,  2.59s/it] 80%|███████▉  | 156/196 [06:54<01:43,  2.58s/it] 80%|████████  | 157/196 [06:57<01:40,  2.58s/it] 81%|████████  | 158/196 [06:59<01:38,  2.59s/it] 81%|████████  | 159/196 [07:02<01:35,  2.58s/it] 82%|████████▏ | 160/196 [07:04<01:33,  2.59s/it] 82%|████████▏ | 161/196 [07:07<01:30,  2.59s/it] 83%|████████▎ | 162/196 [07:10<01:28,  2.59s/it] 83%|████████▎ | 163/196 [07:12<01:25,  2.59s/it] 84%|████████▎ | 164/196 [07:15<01:22,  2.59s/it] 84%|████████▍ | 165/196 [07:17<01:20,  2.59s/it] 85%|████████▍ | 166/196 [07:20<01:17,  2.59s/it] 85%|████████▌ | 167/196 [07:22<01:15,  2.59s/it] 86%|████████▌ | 168/196 [07:25<01:12,  2.59s/it] 86%|████████▌ | 169/196 [07:28<01:10,  2.60s/it] 87%|████████▋ | 170/196 [07:30<01:07,  2.61s/it] 87%|████████▋ | 171/196 [07:33<01:05,  2.61s/it] 88%|████████▊ | 172/196 [07:36<01:02,  2.61s/it] 88%|████████▊ | 173/196 [07:38<00:59,  2.60s/it] 89%|████████▉ | 174/196 [07:41<00:57,  2.60s/it] 89%|████████▉ | 175/196 [07:43<00:54,  2.60s/it] 90%|████████▉ | 176/196 [07:46<00:52,  2.61s/it] 90%|█████████ | 177/196 [07:49<00:49,  2.61s/it] 91%|█████████ | 178/196 [07:51<00:46,  2.60s/it] 91%|█████████▏| 179/196 [07:54<00:44,  2.61s/it] 92%|█████████▏| 180/196 [07:56<00:41,  2.61s/it] 92%|█████████▏| 181/196 [07:59<00:38,  2.60s/it] 93%|█████████▎| 182/196 [08:02<00:36,  2.60s/it] 93%|█████████▎| 183/196 [08:04<00:33,  2.61s/it] 94%|█████████▍| 184/196 [08:07<00:31,  2.63s/it] 94%|█████████▍| 185/196 [08:09<00:28,  2.61s/it] 95%|█████████▍| 186/196 [08:12<00:26,  2.61s/it] 95%|█████████▌| 187/196 [08:15<00:23,  2.59s/it] 96%|█████████▌| 188/196 [08:17<00:20,  2.60s/it] 96%|█████████▋| 189/196 [08:20<00:18,  2.61s/it] 97%|█████████▋| 190/196 [08:22<00:15,  2.62s/it] 97%|█████████▋| 191/196 [08:25<00:13,  2.63s/it] 98%|█████████▊| 192/196 [08:28<00:10,  2.65s/it] 98%|█████████▊| 193/196 [08:30<00:07,  2.64s/it] 99%|█████████▉| 194/196 [08:33<00:05,  2.63s/it] 99%|█████████▉| 195/196 [08:36<00:02,  2.62s/it]100%|██████████| 196/196 [08:38<00:00,  2.47s/it]100%|██████████| 196/196 [08:38<00:00,  2.64s/it]
Top-1 Accuracy: 70.23%
Top-5 Accuracy: 91.18%
