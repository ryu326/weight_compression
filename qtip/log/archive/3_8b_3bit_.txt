Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/3_8b_3bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 74, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 29, in main
    model, model_str = model_from_hf_path(args.hf_path, max_mem_ratio=args.max_mem_ratio)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/unsafe_import.py", line 16, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/3_8b_3bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/3_8b_3bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 74, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/eval/eval_ppl.py", line 29, in main
    model, model_str = model_from_hf_path(args.hf_path, max_mem_ratio=args.max_mem_ratio)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/lib/utils/unsafe_import.py", line 16, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/3_8b_3bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
I0315 02:55:14.497356 257804 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 02:55:14.497479 257804 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 02:55:14.497525 257804 utils.py:162] NumExpr defaulting to 16 threads.
I0315 02:55:14.678127 257804 config.py:58] PyTorch version 2.4.0 available.
I0315 03:10:23.262482 273992 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 03:10:23.262624 273992 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 03:10:23.262673 273992 utils.py:162] NumExpr defaulting to 16 threads.
I0315 03:10:23.442673 273992 config.py:58] PyTorch version 2.4.0 available.
W0315 03:10:25.591959 273992 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  6.32it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.02it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  7.34it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  7.60it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  7.83it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  7.86it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.02it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  7.69it/s]
I0315 03:10:27.341738 273992 quantize_finetune_llama.py:135] loaded model
I0315 03:10:47.431248 273992 quantize_finetune_llama.py:139] loaded dataset and devset
I0315 03:10:54.002709 273992 quantize_finetune_llama.py:159] layer 0 gpu 0
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 03:12:17.927133 273992 quantize_finetune_llama.py:186] computed original embedding for layer 0 in 83.7491626739502s
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
I0315 03:12:44.510035 273992 quantize_finetune_llama.py:159] layer 1 gpu 1
I0315 03:12:46.513624 276902 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 03:12:46.513736 276902 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 03:12:46.513796 276902 utils.py:162] NumExpr defaulting to 16 threads.
I0315 03:12:46.690395 276902 config.py:58] PyTorch version 2.4.0 available.
I0315 03:12:48.792685 276902 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 03:12:49.138315 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:06,  2.16s/it]  6%|▋         | 2/32 [00:02<00:32,  1.08s/it]  9%|▉         | 3/32 [00:02<00:21,  1.36it/s] 12%|█▎        | 4/32 [00:03<00:15,  1.76it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.09it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.36it/s] 22%|██▏       | 7/32 [00:04<00:09,  2.55it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.71it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:05<00:07,  2.84it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.92it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.97it/s] 41%|████      | 13/32 [00:06<00:06,  3.00it/s] 44%|████▍     | 14/32 [00:06<00:05,  3.00it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.01it/s] 50%|█████     | 16/32 [00:07<00:05,  3.04it/s] 53%|█████▎    | 17/32 [00:07<00:04,  3.08it/s] 56%|█████▋    | 18/32 [00:07<00:04,  3.06it/s] 59%|█████▉    | 19/32 [00:08<00:04,  3.03it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.04it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.07it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.08it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.09it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.09it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.09it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.08it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.03it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.03it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.07it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.08it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.09it/s]100%|██████████| 32/32 [00:12<00:00,  3.09it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]
W0315 03:13:05.360000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.361000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.361000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.361000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.361000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.361000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.361000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.387000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.387000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.387000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.387000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.387000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.404000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.404000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.404000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.404000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.404000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.717000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.717000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.717000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.717000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:13:05.717000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.618000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.619000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.619000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.619000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.619000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.619000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.619000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.637000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.637000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.637000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.638000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.638000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.888000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.888000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.888000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.888000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:13:06.889000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.036000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.036000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.036000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.036000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.036000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.036000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.036000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.053000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.053000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.053000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.053000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.054000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.946000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.946000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.946000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.946000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:13:08.946000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 03:13:16.244908 276902 finetune.py:45] layer 0_v initial loss 1.149316801729583e-07
W0315 03:13:16.245342 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:13:51.758648 276902 finetune.py:68] layer 0_v @ epoch 0 new loss 9.445766835369795e-08 old loss 1.149316801729583e-07 BETTER
I0315 03:13:54.211933 273992 quantize_finetune_llama.py:186] computed original embedding for layer 1 in 69.46491146087646s
I0315 03:14:04.131590 273992 quantize_finetune_llama.py:159] layer 2 gpu 2
I0315 03:14:06.109354 278811 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 03:14:06.109459 278811 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 03:14:06.109516 278811 utils.py:162] NumExpr defaulting to 16 threads.
I0315 03:14:06.308474 278811 config.py:58] PyTorch version 2.4.0 available.
I0315 03:14:08.532205 278811 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 03:14:08.901872 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:14,  2.41s/it]  6%|▋         | 2/32 [00:02<00:35,  1.19s/it]  9%|▉         | 3/32 [00:03<00:23,  1.22it/s] 12%|█▎        | 4/32 [00:03<00:17,  1.58it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.88it/s] 19%|█▉        | 6/32 [00:04<00:12,  2.14it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.34it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:05<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.80it/s] 41%|████      | 13/32 [00:06<00:06,  2.85it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:07<00:05,  2.88it/s] 50%|█████     | 16/32 [00:07<00:05,  2.91it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:08<00:04,  2.92it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.87it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.89it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.89it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.93it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.88it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.90it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.95it/s]100%|██████████| 32/32 [00:13<00:00,  2.97it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
W0315 03:14:26.584000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.584000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.584000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.584000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.584000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.584000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.584000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.611000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.611000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.612000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.612000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.612000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.629000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.629000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.629000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.629000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.629000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.972000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.972000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.972000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.972000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:14:26.972000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.893000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.893000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.893000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.893000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.894000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.894000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.894000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.912000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.912000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.912000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.912000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:14:27.912000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:14:28.163000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:14:28.163000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:14:28.163000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:14:28.163000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:14:28.163000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
I0315 03:14:28.538422 276902 finetune.py:68] layer 0_v @ epoch 1 new loss 8.84559128166984e-08 old loss 9.445766835369795e-08 BETTER
W0315 03:14:29.322000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.322000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.322000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.322000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.322000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.322000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.322000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.340000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.340000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.340000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.341000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:14:29.341000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:14:30.271000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:14:30.271000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:14:30.271000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:14:30.271000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:14:30.271000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 03:14:37.006885 278811 finetune.py:45] layer 1_v initial loss 1.2942858802489354e-06
W0315 03:14:37.007150 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:15:05.644492 276902 finetune.py:68] layer 0_v @ epoch 2 new loss 8.560758146813896e-08 old loss 8.84559128166984e-08 BETTER
I0315 03:15:11.059632 278811 finetune.py:68] layer 1_v @ epoch 0 new loss 5.795416200271575e-07 old loss 1.2942858802489354e-06 BETTER
I0315 03:15:12.691490 273992 quantize_finetune_llama.py:186] computed original embedding for layer 2 in 68.37262988090515s
I0315 03:15:21.975609 273992 quantize_finetune_llama.py:159] layer 3 gpu 3
I0315 03:15:24.058236 280702 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 03:15:24.058361 280702 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 03:15:24.058421 280702 utils.py:162] NumExpr defaulting to 16 threads.
I0315 03:15:24.289587 280702 config.py:58] PyTorch version 2.4.0 available.
I0315 03:15:26.816307 280702 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 03:15:27.424854 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:18,  2.53s/it]  6%|▋         | 2/32 [00:02<00:37,  1.25s/it]  9%|▉         | 3/32 [00:03<00:24,  1.20it/s] 12%|█▎        | 4/32 [00:03<00:18,  1.55it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.86it/s] 19%|█▉        | 6/32 [00:04<00:12,  2.12it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.28it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:05<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:06<00:07,  2.71it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.75it/s] 41%|████      | 13/32 [00:06<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:07<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.80it/s] 50%|█████     | 16/32 [00:07<00:05,  2.81it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:08<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.84it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.83it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.85it/s]100%|██████████| 32/32 [00:13<00:00,  2.86it/s]100%|██████████| 32/32 [00:13<00:00,  2.39it/s]
I0315 03:15:43.043462 276902 finetune.py:68] layer 0_v @ epoch 3 new loss 8.398507134188549e-08 old loss 8.560758146813896e-08 BETTER
W0315 03:15:45.315000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.315000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.315000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.315000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.315000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.315000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.316000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.343000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.343000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.343000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.343000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.343000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.360000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.360000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.360000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.360000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.360000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.702000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.702000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.702000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.702000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:15:45.702000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
I0315 03:15:46.405249 278811 finetune.py:68] layer 1_v @ epoch 1 new loss 3.873172147450532e-07 old loss 5.795416200271575e-07 BETTER
W0315 03:15:46.639000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.639000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.639000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.640000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.640000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.640000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.640000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.658000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.658000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.658000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.658000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.658000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.906000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.906000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.906000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.906000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:15:46.906000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.126000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.126000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.126000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.126000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.126000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.126000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.126000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.144000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.145000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.145000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.145000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:15:48.145000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:15:49.095000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:15:49.095000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:15:49.095000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:15:49.096000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:15:49.096000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 03:15:56.160644 280702 finetune.py:45] layer 2_v initial loss 7.663920769118704e-06
W0315 03:15:56.161057 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:16:20.838642 276902 finetune.py:68] layer 0_v @ epoch 4 new loss 8.286195196660628e-08 old loss 8.398507134188549e-08 BETTER
I0315 03:16:22.388711 278811 finetune.py:68] layer 1_v @ epoch 2 new loss 3.1847210379964963e-07 old loss 3.873172147450532e-07 BETTER
W0315 03:16:22.618604 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_v proxy err 0.004657589364796877 tr(WHW.T) 1.3175290822982788
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:56,  1.82s/it]  6%|▋         | 2/32 [00:02<00:29,  1.03it/s]  9%|▉         | 3/32 [00:02<00:20,  1.42it/s] 12%|█▎        | 4/32 [00:02<00:16,  1.74it/s] 16%|█▌        | 5/32 [00:03<00:13,  1.99it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.18it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.32it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 41%|████      | 13/32 [00:06<00:07,  2.62it/s]I0315 03:16:30.291162 273992 quantize_finetune_llama.py:186] computed original embedding for layer 3 in 68.13068842887878s
 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s]I0315 03:16:30.773185 280702 finetune.py:68] layer 2_v @ epoch 0 new loss 1.4761393458684324e-06 old loss 7.663920769118704e-06 BETTER
 47%|████▋     | 15/32 [00:07<00:06,  2.60it/s] 50%|█████     | 16/32 [00:07<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.63it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.66it/s]100%|██████████| 32/32 [00:13<00:00,  2.66it/s]100%|██████████| 32/32 [00:13<00:00,  2.37it/s]
I0315 03:16:38.804717 273992 quantize_finetune_llama.py:159] layer 4 gpu 0
I0315 03:16:40.850676 282632 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 03:16:40.850817 282632 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 03:16:40.850882 282632 utils.py:162] NumExpr defaulting to 16 threads.
I0315 03:16:41.041141 282632 config.py:58] PyTorch version 2.4.0 available.
I0315 03:16:43.219957 282632 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 03:16:43.615000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.616000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.616000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.616000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.616000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.616000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.616000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.644000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.645000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.645000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.645000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.645000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.660059 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0315 03:16:43.660000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.661000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.661000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.661000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.661000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.825000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.825000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.825000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.825000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:16:43.825000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.065000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.065000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.065000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.065000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.065000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.065000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.065000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.086000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.086000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.086000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.086000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.086000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.153000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.153000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.153000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.153000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:16:44.154000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/32 [00:00<?, ?it/s]W0315 03:16:45.036000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.351000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.351000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.351000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.352000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.352000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.352000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.352000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.372000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.372000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.372000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.372000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.373000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.632000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.632000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.632000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.632000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.632000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:16:45.894000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:02<01:14,  2.40s/it]  6%|▋         | 2/32 [00:02<00:35,  1.19s/it]  9%|▉         | 3/32 [00:03<00:23,  1.24it/s] 12%|█▎        | 4/32 [00:03<00:17,  1.60it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.90it/s] 19%|█▉        | 6/32 [00:04<00:12,  2.13it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.32it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:05<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.74it/s] 41%|████      | 13/32 [00:06<00:06,  2.76it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.78it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.79it/s] 50%|█████     | 16/32 [00:07<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:08<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s]I0315 03:16:53.531726 276902 finetune.py:45] layer 0_q initial loss 8.559923969642114e-08
W0315 03:16:53.531984 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:09<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.82it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.83it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.83it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:13<00:00,  2.83it/s]100%|██████████| 32/32 [00:13<00:00,  2.40it/s]
I0315 03:16:58.661385 278811 finetune.py:68] layer 1_v @ epoch 3 new loss 2.885676622099709e-07 old loss 3.1847210379964963e-07 BETTER
W0315 03:17:01.290000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.290000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.290000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.290000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.291000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.291000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.291000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.318000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.318000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.318000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.318000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.318000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.335000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.335000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.335000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.335000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.336000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.680000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.680000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.680000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.680000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:01.680000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.619000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.619000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.619000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.619000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.619000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.619000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.619000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.638000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.638000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.638000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.638000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.638000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.894000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.894000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.894000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.894000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:02.894000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.123000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.124000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.124000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.124000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.124000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.124000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.124000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.142000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.142000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.142000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.142000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:04.142000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:05.100000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:05.101000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:05.101000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:05.101000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:05.101000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 03:17:06.893363 280702 finetune.py:68] layer 2_v @ epoch 1 new loss 7.28838699615153e-07 old loss 1.4761393458684324e-06 BETTER
I0315 03:17:11.957413 282632 finetune.py:45] layer 3_v initial loss 1.7054742784239352e-05
W0315 03:17:11.957750 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:17:29.731804 276902 finetune.py:68] layer 0_q @ epoch 0 new loss 8.327959477583136e-08 old loss 8.559923969642114e-08 BETTER
I0315 03:17:34.428219 278811 finetune.py:68] layer 1_v @ epoch 4 new loss 2.722872523008846e-07 old loss 2.885676622099709e-07 BETTER
W0315 03:17:36.194826 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_v proxy err 0.00461159273982048 tr(WHW.T) 5.699800491333008
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:56,  1.82s/it]  6%|▋         | 2/32 [00:02<00:29,  1.03it/s]  9%|▉         | 3/32 [00:02<00:20,  1.43it/s] 12%|█▎        | 4/32 [00:02<00:16,  1.74it/s] 16%|█▌        | 5/32 [00:03<00:13,  1.98it/s] 19%|█▉        | 6/32 [00:03<00:12,  2.16it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.29it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.39it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.45it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.49it/s]I0315 03:17:42.985361 280702 finetune.py:68] layer 2_v @ epoch 2 new loss 5.818257591272413e-07 old loss 7.28838699615153e-07 BETTER
 34%|███▍      | 11/32 [00:05<00:08,  2.51it/s] 38%|███▊      | 12/32 [00:06<00:07,  2.55it/s] 41%|████      | 13/32 [00:06<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.61it/s] 50%|█████     | 16/32 [00:07<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s]I0315 03:17:45.610697 282632 finetune.py:68] layer 3_v @ epoch 0 new loss 3.1575161756336456e-06 old loss 1.7054742784239352e-05 BETTER
 56%|█████▋    | 18/32 [00:08<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.63it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.61it/s]100%|██████████| 32/32 [00:13<00:00,  2.61it/s]100%|██████████| 32/32 [00:13<00:00,  2.34it/s]
W0315 03:17:57.437000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.437000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.438000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.438000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.438000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.438000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.438000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.469000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.469000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.470000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.470000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.470000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.485000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.485000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.486000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.486000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.486000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.649000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.649000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.650000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.650000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.650000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.884000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.885000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.885000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.885000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.885000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.885000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.885000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.908000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.908000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.909000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.909000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.909000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.976000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.976000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.976000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.976000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:57.976000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:58.874000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.196000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.196000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.197000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.197000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.197000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.197000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.197000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.220000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.220000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.220000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.220000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.220000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.485000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.485000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.485000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.485000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.486000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:17:59.762000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 03:18:07.272464 276902 finetune.py:68] layer 0_q @ epoch 1 new loss 8.234818693608759e-08 old loss 8.327959477583136e-08 BETTER
I0315 03:18:07.745261 278811 finetune.py:45] layer 1_q initial loss 2.972342088014557e-07
W0315 03:18:07.745615 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:18:19.146451 280702 finetune.py:68] layer 2_v @ epoch 3 new loss 5.278011485643219e-07 old loss 5.818257591272413e-07 BETTER
I0315 03:18:20.583487 282632 finetune.py:68] layer 3_v @ epoch 1 new loss 1.796703827494639e-06 old loss 3.1575161756336456e-06 BETTER
I0315 03:18:42.886888 278811 finetune.py:68] layer 1_q @ epoch 0 new loss 2.8425446885194106e-07 old loss 2.972342088014557e-07 BETTER
I0315 03:18:45.022369 276902 finetune.py:68] layer 0_q @ epoch 2 new loss 8.160773035115199e-08 old loss 8.234818693608759e-08 BETTER
I0315 03:18:55.658897 280702 finetune.py:68] layer 2_v @ epoch 4 new loss 4.978624588147795e-07 old loss 5.278011485643219e-07 BETTER
I0315 03:18:56.394509 282632 finetune.py:68] layer 3_v @ epoch 2 new loss 1.4845988971501356e-06 old loss 1.796703827494639e-06 BETTER
W0315 03:18:57.314368 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_v proxy err 0.0066235000267624855 tr(WHW.T) 26.839576721191406
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it]  6%|▋         | 2/32 [00:02<00:29,  1.00it/s]  9%|▉         | 3/32 [00:02<00:20,  1.39it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.69it/s] 16%|█▌        | 5/32 [00:03<00:14,  1.93it/s] 19%|█▉        | 6/32 [00:03<00:12,  2.10it/s] 22%|██▏       | 7/32 [00:04<00:11,  2.21it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.31it/s] 28%|██▊       | 9/32 [00:05<00:09,  2.37it/s] 31%|███▏      | 10/32 [00:05<00:09,  2.42it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.46it/s] 38%|███▊      | 12/32 [00:06<00:08,  2.50it/s] 41%|████      | 13/32 [00:06<00:07,  2.52it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.55it/s] 50%|█████     | 16/32 [00:07<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.54it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:10<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.57it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:11<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:12<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.57it/s] 94%|█████████▍| 30/32 [00:13<00:00,  2.56it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.29it/s]
W0315 03:19:18.813000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.813000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.813000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.813000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.814000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.814000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.814000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.844000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.844000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.845000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.845000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.845000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.861000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.861000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.861000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.861000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:19:18.861000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
I0315 03:19:18.993514 278811 finetune.py:68] layer 1_q @ epoch 1 new loss 2.7567895699576184e-07 old loss 2.8425446885194106e-07 BETTER
W0315 03:19:19.023000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.023000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.023000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.024000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.024000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.257000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.257000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.257000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.257000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.257000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.257000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.257000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.280000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.280000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.280000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.280000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.280000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.347000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.347000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.347000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.347000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:19:19.348000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.250000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.581000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.581000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.581000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.581000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.581000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.582000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.582000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.604000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.604000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.605000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.605000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.605000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.873000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.873000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.873000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.873000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:19:20.873000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:19:21.146000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 03:19:22.578507 276902 finetune.py:68] layer 0_q @ epoch 3 new loss 8.097748604996013e-08 old loss 8.160773035115199e-08 BETTER
I0315 03:19:28.668961 280702 finetune.py:45] layer 2_q initial loss 8.59072372350056e-07
W0315 03:19:28.669349 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:19:31.843029 282632 finetune.py:68] layer 3_v @ epoch 3 new loss 1.3512133136828197e-06 old loss 1.4845988971501356e-06 BETTER
I0315 03:19:54.864341 278811 finetune.py:68] layer 1_q @ epoch 2 new loss 2.692944178761536e-07 old loss 2.7567895699576184e-07 BETTER
I0315 03:20:00.233944 276902 finetune.py:68] layer 0_q @ epoch 4 new loss 8.043970467497275e-08 old loss 8.097748604996013e-08 BETTER
W0315 03:20:02.138221 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_q proxy err 9.710122685646638e-05 tr(WHW.T) 6233.9150390625
  0%|          | 0/32 [00:00<?, ?it/s]I0315 03:20:03.636657 280702 finetune.py:68] layer 2_q @ epoch 0 new loss 8.166481393345748e-07 old loss 8.59072372350056e-07 BETTER
  3%|▎         | 1/32 [00:01<00:36,  1.16s/it]  6%|▋         | 2/32 [00:01<00:20,  1.45it/s]  9%|▉         | 3/32 [00:01<00:15,  1.85it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.12it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s]I0315 03:20:07.394010 282632 finetune.py:68] layer 3_v @ epoch 4 new loss 1.2755600664604572e-06 old loss 1.3512133136828197e-06 BETTER
 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.71it/s] 41%|████      | 13/32 [00:05<00:06,  2.72it/s]W0315 03:20:08.966062 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.74it/s]3_v proxy err 0.008299178443849087 tr(WHW.T) 40.46714782714844
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.74it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.73it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.72it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s]  3%|▎         | 1/32 [00:01<00:56,  1.82s/it] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s]  6%|▋         | 2/32 [00:02<00:29,  1.02it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.74it/s]  9%|▉         | 3/32 [00:02<00:20,  1.41it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 12%|█▎        | 4/32 [00:02<00:16,  1.72it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.74it/s] 16%|█▌        | 5/32 [00:03<00:13,  1.95it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.75it/s] 19%|█▉        | 6/32 [00:03<00:12,  2.13it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 22%|██▏       | 7/32 [00:04<00:11,  2.27it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.75it/s] 25%|██▌       | 8/32 [00:04<00:10,  2.36it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.75it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.43it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.75it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.48it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.74it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.51it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
 38%|███▊      | 12/32 [00:06<00:07,  2.54it/s] 41%|████      | 13/32 [00:06<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:07<00:06,  2.55it/s] 50%|█████     | 16/32 [00:07<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:08<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:09<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:10<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:11<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:11<00:02,  2.55it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:12<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.57it/s]I0315 03:20:23.069876 276902 finetune.py:45] layer 0_k initial loss 8.40083629327637e-08
W0315 03:20:23.070197 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 94%|█████████▍| 30/32 [00:13<00:00,  2.58it/s] 97%|█████████▋| 31/32 [00:13<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.31it/s]
W0315 03:20:30.297000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.298000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.298000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.298000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.298000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.298000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.298000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.329000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.329000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.329000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.329000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.329000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.345000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.345000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.345000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.345000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.345000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.510000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.510000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.510000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.510000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.510000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
I0315 03:20:30.723827 278811 finetune.py:68] layer 1_q @ epoch 3 new loss 2.6435535005475685e-07 old loss 2.692944178761536e-07 BETTER
W0315 03:20:30.751000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.751000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.751000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.752000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.752000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.752000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.752000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.774000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.774000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.774000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.774000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.774000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.841000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.841000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.841000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.841000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:20:30.841000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:20:31.737000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.054000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.054000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.054000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.055000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.055000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.055000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.055000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.078000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.078000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.078000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.078000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.078000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.343000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.344000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.344000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.344000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.344000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:20:32.618000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 03:20:39.763169 280702 finetune.py:68] layer 2_q @ epoch 1 new loss 7.93704373336368e-07 old loss 8.166481393345748e-07 BETTER
I0315 03:20:40.327923 282632 finetune.py:45] layer 3_q initial loss 2.1442040178953903e-06
W0315 03:20:40.328301 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:20:59.543146 276902 finetune.py:68] layer 0_k @ epoch 0 new loss 8.284282415615962e-08 old loss 8.40083629327637e-08 BETTER
I0315 03:21:06.639781 278811 finetune.py:68] layer 1_q @ epoch 4 new loss 2.600311859168869e-07 old loss 2.6435535005475685e-07 BETTER
W0315 03:21:08.319703 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_q proxy err 7.96117092249915e-05 tr(WHW.T) 7565.47802734375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:36,  1.17s/it]  6%|▋         | 2/32 [00:01<00:21,  1.42it/s]  9%|▉         | 3/32 [00:01<00:16,  1.80it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.06it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.24it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s]I0315 03:21:14.640650 282632 finetune.py:68] layer 3_q @ epoch 0 new loss 2.0491363557084696e-06 old loss 2.1442040178953903e-06 BETTER
 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.64it/s]I0315 03:21:15.948338 280702 finetune.py:68] layer 2_q @ epoch 2 new loss 7.775893209327478e-07 old loss 7.93704373336368e-07 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0315 03:21:29.315713 278811 finetune.py:45] layer 1_k initial loss 2.9526930234169413e-07
W0315 03:21:29.316158 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:21:37.719217 276902 finetune.py:68] layer 0_k @ epoch 1 new loss 8.230572490219856e-08 old loss 8.284282415615962e-08 BETTER
I0315 03:21:50.536264 282632 finetune.py:68] layer 3_q @ epoch 1 new loss 1.9942062863265164e-06 old loss 2.0491363557084696e-06 BETTER
I0315 03:21:52.788428 280702 finetune.py:68] layer 2_q @ epoch 3 new loss 7.658587719561183e-07 old loss 7.775893209327478e-07 BETTER
I0315 03:22:04.828778 278811 finetune.py:68] layer 1_k @ epoch 0 new loss 2.860600716303452e-07 old loss 2.9526930234169413e-07 BETTER
I0315 03:22:15.655154 276902 finetune.py:68] layer 0_k @ epoch 2 new loss 8.181137900464819e-08 old loss 8.230572490219856e-08 BETTER
I0315 03:22:26.501647 282632 finetune.py:68] layer 3_q @ epoch 2 new loss 1.9561946373869432e-06 old loss 1.9942062863265164e-06 BETTER
I0315 03:22:29.341681 280702 finetune.py:68] layer 2_q @ epoch 4 new loss 7.554497756245837e-07 old loss 7.658587719561183e-07 BETTER
W0315 03:22:31.004240 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_q proxy err 0.0007222917047329247 tr(WHW.T) 7138.6318359375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:36,  1.18s/it]  6%|▋         | 2/32 [00:01<00:21,  1.42it/s]  9%|▉         | 3/32 [00:01<00:15,  1.82it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s]I0315 03:22:41.233221 278811 finetune.py:68] layer 1_k @ epoch 1 new loss 2.822175986239017e-07 old loss 2.860600716303452e-07 BETTER
 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0315 03:22:52.006620 280702 finetune.py:45] layer 2_k initial loss 1.0862387398447026e-06
W0315 03:22:52.006952 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:22:53.693484 276902 finetune.py:68] layer 0_k @ epoch 3 new loss 8.136420603932493e-08 old loss 8.181137900464819e-08 BETTER
I0315 03:23:02.392005 282632 finetune.py:68] layer 3_q @ epoch 3 new loss 1.9246449483034667e-06 old loss 1.9561946373869432e-06 BETTER
I0315 03:23:17.698076 278811 finetune.py:68] layer 1_k @ epoch 2 new loss 2.7916362910218595e-07 old loss 2.822175986239017e-07 BETTER
I0315 03:23:27.575168 280702 finetune.py:68] layer 2_k @ epoch 0 new loss 1.0114412134498707e-06 old loss 1.0862387398447026e-06 BETTER
I0315 03:23:31.783199 276902 finetune.py:68] layer 0_k @ epoch 4 new loss 8.096932191392625e-08 old loss 8.136420603932493e-08 BETTER
W0315 03:23:33.518475 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

0_k proxy err 6.385759479599074e-05 tr(WHW.T) 2167.74658203125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.10it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.61it/s]I0315 03:23:38.406177 282632 finetune.py:68] layer 3_q @ epoch 4 new loss 1.8991692058989429e-06 old loss 1.9246449483034667e-06 BETTER
 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s]W0315 03:23:39.982187 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.68it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s]3_q proxy err 0.0010417314479127526 tr(WHW.T) 6654.5380859375
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s]  3%|▎         | 1/32 [00:01<00:35,  1.14s/it] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s]  6%|▋         | 2/32 [00:01<00:20,  1.45it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s]  9%|▉         | 3/32 [00:01<00:15,  1.84it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.10it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.73it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.73it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0315 03:23:54.235683 278811 finetune.py:68] layer 1_k @ epoch 3 new loss 2.7655460144160315e-07 old loss 2.7916362910218595e-07 BETTER
I0315 03:23:54.759388 276902 finetune.py:45] layer 0_o initial loss 1.6295207672101242e-07
W0315 03:23:54.759797 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:24:01.114515 282632 finetune.py:45] layer 3_k initial loss 2.9423702017083997e-06
W0315 03:24:01.115138 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:24:04.650142 280702 finetune.py:68] layer 2_k @ epoch 1 new loss 1.000149381980009e-06 old loss 1.0114412134498707e-06 BETTER
I0315 03:24:30.756703 278811 finetune.py:68] layer 1_k @ epoch 4 new loss 2.7446745320958144e-07 old loss 2.7655460144160315e-07 BETTER
I0315 03:24:32.054222 276902 finetune.py:68] layer 0_o @ epoch 0 new loss 1.6070016783942265e-07 old loss 1.6295207672101242e-07 BETTER
W0315 03:24:32.545167 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

1_k proxy err 7.850670954212546e-05 tr(WHW.T) 3946.2490234375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.80it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s]I0315 03:24:35.993949 282632 finetune.py:68] layer 3_k @ epoch 0 new loss 2.6403379251860315e-06 old loss 2.9423702017083997e-06 BETTER
 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s]I0315 03:24:41.259611 280702 finetune.py:68] layer 2_k @ epoch 2 new loss 9.912815812640474e-07 old loss 1.000149381980009e-06 BETTER
 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
I0315 03:24:53.816583 278811 finetune.py:45] layer 1_o initial loss 7.4778762382266e-07
W0315 03:24:53.816937 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:25:09.937319 276902 finetune.py:68] layer 0_o @ epoch 1 new loss 1.5970975653090136e-07 old loss 1.6070016783942265e-07 BETTER
I0315 03:25:11.646840 282632 finetune.py:68] layer 3_k @ epoch 1 new loss 2.6113443709618878e-06 old loss 2.6403379251860315e-06 BETTER
I0315 03:25:17.417085 280702 finetune.py:68] layer 2_k @ epoch 3 new loss 9.83719360192481e-07 old loss 9.912815812640474e-07 BETTER
I0315 03:25:29.283370 278811 finetune.py:68] layer 1_o @ epoch 0 new loss 7.310711680474924e-07 old loss 7.4778762382266e-07 BETTER
I0315 03:25:47.439454 282632 finetune.py:68] layer 3_k @ epoch 2 new loss 2.58961017607362e-06 old loss 2.6113443709618878e-06 BETTER
I0315 03:25:48.111593 276902 finetune.py:68] layer 0_o @ epoch 2 new loss 1.5895219007688866e-07 old loss 1.5970975653090136e-07 BETTER
I0315 03:25:53.984382 280702 finetune.py:68] layer 2_k @ epoch 4 new loss 9.77953391156916e-07 old loss 9.83719360192481e-07 BETTER
W0315 03:25:55.666983 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_k proxy err 0.0007822552579455078 tr(WHW.T) 3891.08251953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s]I0315 03:26:05.638210 278811 finetune.py:68] layer 1_o @ epoch 1 new loss 7.22193931324e-07 old loss 7.310711680474924e-07 BETTER
 69%|██████▉   | 22/32 [00:08<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0315 03:26:17.406985 280702 finetune.py:45] layer 2_o initial loss 1.8445117575538461e-06
W0315 03:26:17.407377 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:26:23.754928 282632 finetune.py:68] layer 3_k @ epoch 3 new loss 2.5695005660963943e-06 old loss 2.58961017607362e-06 BETTER
I0315 03:26:26.447639 276902 finetune.py:68] layer 0_o @ epoch 3 new loss 1.5824943488951249e-07 old loss 1.5895219007688866e-07 BETTER
I0315 03:26:42.094182 278811 finetune.py:68] layer 1_o @ epoch 2 new loss 7.158347443692037e-07 old loss 7.22193931324e-07 BETTER
I0315 03:26:52.967347 280702 finetune.py:68] layer 2_o @ epoch 0 new loss 1.7654938346822746e-06 old loss 1.8445117575538461e-06 BETTER
I0315 03:26:59.155946 282632 finetune.py:68] layer 3_k @ epoch 4 new loss 2.5532031031616498e-06 old loss 2.5695005660963943e-06 BETTER
W0315 03:27:01.005572 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_k proxy err 0.0011096170637756586 tr(WHW.T) 3658.36083984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s]I0315 03:27:04.596496 276902 finetune.py:68] layer 0_o @ epoch 4 new loss 1.5768910088809207e-07 old loss 1.5824943488951249e-07 BETTER
 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s]W0315 03:27:06.137829 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 28%|██▊       | 9/32 [00:03<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s]0_o proxy err 0.0010145107517018914 tr(WHW.T) 0.23015311360359192
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s]  3%|▎         | 1/32 [00:02<01:02,  2.01s/it] 53%|█████▎    | 17/32 [00:06<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it]I0315 03:27:18.191559 278811 finetune.py:68] layer 1_o @ epoch 3 new loss 7.109387070158846e-07 old loss 7.158347443692037e-07 BETTER
 22%|██▏       | 7/32 [00:10<00:37,  1.50s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.49s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it]I0315 03:27:22.500321 282632 finetune.py:45] layer 3_o initial loss 4.540348527370952e-06
W0315 03:27:22.500809 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.48s/it]I0315 03:27:28.866338 280702 finetune.py:68] layer 2_o @ epoch 1 new loss 1.7327733985439409e-06 old loss 1.7654938346822746e-06 BETTER
 47%|████▋     | 15/32 [00:22<00:24,  1.47s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.47s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.47s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.47s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it] 75%|███████▌  | 24/32 [00:35<00:11,  1.47s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it]I0315 03:27:54.377321 278811 finetune.py:68] layer 1_o @ epoch 4 new loss 7.070604510772682e-07 old loss 7.109387070158846e-07 BETTER
100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
W0315 03:27:56.026852 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 03:27:57.232737 282632 finetune.py:68] layer 3_o @ epoch 0 new loss 4.3285936044412665e-06 old loss 4.540348527370952e-06 BETTER
1_o proxy err 0.0039080483838915825 tr(WHW.T) 0.3141918182373047
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.03s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it]I0315 03:28:03.557338 276902 finetune.py:45] layer 0_up initial loss 4.958763497597829e-07
W0315 03:28:03.557694 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it]I0315 03:28:05.245560 280702 finetune.py:68] layer 2_o @ epoch 2 new loss 1.714193217594584e-06 old loss 1.7327733985439409e-06 BETTER
 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:37,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.50s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it]I0315 03:28:32.477521 282632 finetune.py:68] layer 3_o @ epoch 1 new loss 4.269584223948186e-06 old loss 4.3285936044412665e-06 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it]I0315 03:28:39.323409 276902 finetune.py:68] layer 0_up @ epoch 0 new loss 4.893675509265449e-07 old loss 4.958763497597829e-07 BETTER
 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it]I0315 03:28:41.298563 280702 finetune.py:68] layer 2_o @ epoch 3 new loss 1.7018950302372104e-06 old loss 1.714193217594584e-06 BETTER
 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0315 03:28:54.508111 278811 finetune.py:45] layer 1_up initial loss 1.7530301192891784e-06
W0315 03:28:54.508465 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:29:07.842805 282632 finetune.py:68] layer 3_o @ epoch 2 new loss 4.236827408021782e-06 old loss 4.269584223948186e-06 BETTER
I0315 03:29:15.575597 276902 finetune.py:68] layer 0_up @ epoch 1 new loss 4.882571715825179e-07 old loss 4.893675509265449e-07 BETTER
I0315 03:29:17.470637 280702 finetune.py:68] layer 2_o @ epoch 4 new loss 1.6922421082199435e-06 old loss 1.7018950302372104e-06 BETTER
W0315 03:29:19.119591 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_o proxy err 0.003782151499763131 tr(WHW.T) 0.558995246887207
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.06s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.61s/it]I0315 03:29:28.231094 278811 finetune.py:68] layer 1_up @ epoch 0 new loss 1.4920555031494587e-06 old loss 1.7530301192891784e-06 BETTER
 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.53s/it]I0315 03:29:43.383353 282632 finetune.py:68] layer 3_o @ epoch 3 new loss 4.21074082623818e-06 old loss 4.236827408021782e-06 BETTER
 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:25<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it]I0315 03:29:51.931074 276902 finetune.py:68] layer 0_up @ epoch 2 new loss 4.875354306932422e-07 old loss 4.882571715825179e-07 BETTER
 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.54s/it]I0315 03:30:03.003244 278811 finetune.py:68] layer 1_up @ epoch 1 new loss 1.4882606365063111e-06 old loss 1.4920555031494587e-06 BETTER
 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0315 03:30:18.351993 280702 finetune.py:45] layer 2_up initial loss 3.6152762277197326e-06
W0315 03:30:18.352290 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:30:19.040186 282632 finetune.py:68] layer 3_o @ epoch 4 new loss 4.188282673567301e-06 old loss 4.21074082623818e-06 BETTER
W0315 03:30:20.600523 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_o proxy err 0.006046306807547808 tr(WHW.T) 0.9369207620620728
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it]  6%|▋         | 2/32 [00:03<00:52,  1.74s/it]  9%|▉         | 3/32 [00:05<00:47,  1.65s/it]I0315 03:30:28.282982 276902 finetune.py:68] layer 0_up @ epoch 3 new loss 4.869673944085662e-07 old loss 4.875354306932422e-07 BETTER
 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it]I0315 03:30:37.155868 278811 finetune.py:68] layer 1_up @ epoch 2 new loss 1.4881543393130414e-06 old loss 1.4882606365063111e-06 BETTER
 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it]I0315 03:30:52.136905 280702 finetune.py:68] layer 2_up @ epoch 0 new loss 3.591723952922621e-06 old loss 3.6152762277197326e-06 BETTER
 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it]I0315 03:31:04.425975 276902 finetune.py:68] layer 0_up @ epoch 4 new loss 4.864539278059965e-07 old loss 4.869673944085662e-07 BETTER
 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it]W0315 03:31:05.814688 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it]0_up proxy err 0.008555307984352112 tr(WHW.T) 101.63928985595703
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0315 03:31:11.432476 278811 finetune.py:68] layer 1_up @ epoch 3 new loss 1.4834052990408964e-06 old loss 1.4881543393130414e-06 BETTER
  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it]I0315 03:31:19.244635 282632 finetune.py:45] layer 3_up initial loss 8.443254955636803e-06
W0315 03:31:19.245047 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it]I0315 03:31:26.660910 280702 finetune.py:68] layer 2_up @ epoch 1 new loss 3.5815435239783255e-06 old loss 3.591723952922621e-06 BETTER
 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.47s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it]I0315 03:31:45.803894 278811 finetune.py:68] layer 1_up @ epoch 4 new loss 1.4813691677773022e-06 old loss 1.4834052990408964e-06 BETTER
 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it]W0315 03:31:47.168541 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it]1_up proxy err 0.01010379008948803 tr(WHW.T) 159.81480407714844
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]I0315 03:31:52.158018 282632 finetune.py:68] layer 3_up @ epoch 0 new loss 8.39638687466504e-06 old loss 8.443254955636803e-06 BETTER
 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it]  9%|▉         | 3/32 [00:04<00:46,  1.61s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.60s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it]I0315 03:32:01.112264 280702 finetune.py:68] layer 2_up @ epoch 2 new loss 3.5741227293328848e-06 old loss 3.5815435239783255e-06 BETTER
 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it]I0315 03:32:03.763891 276902 finetune.py:45] layer 0_gate initial loss 7.499520506826229e-07
W0315 03:32:03.764226 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.49s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.49s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it]I0315 03:32:25.903697 282632 finetune.py:68] layer 3_up @ epoch 1 new loss 8.373444870812818e-06 old loss 8.39638687466504e-06 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.50s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.50s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.50s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]I0315 03:32:35.896596 280702 finetune.py:68] layer 2_up @ epoch 3 new loss 3.567654630387551e-06 old loss 3.5741227293328848e-06 BETTER
100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0315 03:32:38.203179 276902 finetune.py:68] layer 0_gate @ epoch 0 new loss 7.401371249216027e-07 old loss 7.499520506826229e-07 BETTER
I0315 03:32:45.227135 278811 finetune.py:45] layer 1_gate initial loss 2.3669017537031323e-06
W0315 03:32:45.227576 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:33:00.029830 282632 finetune.py:68] layer 3_up @ epoch 2 new loss 8.351553333341144e-06 old loss 8.373444870812818e-06 BETTER
I0315 03:33:10.852555 280702 finetune.py:68] layer 2_up @ epoch 4 new loss 3.5606033179647056e-06 old loss 3.567654630387551e-06 BETTER
W0315 03:33:12.408719 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 03:33:13.122853 276902 finetune.py:68] layer 0_gate @ epoch 1 new loss 7.388502467620128e-07 old loss 7.401371249216027e-07 BETTER
2_up proxy err 0.011788571253418922 tr(WHW.T) 225.92709350585938
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.97s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]I0315 03:33:17.736773 278811 finetune.py:68] layer 1_gate @ epoch 0 new loss 2.117326175721246e-06 old loss 2.3669017537031323e-06 BETTER
  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it]I0315 03:33:33.965834 282632 finetune.py:68] layer 3_up @ epoch 3 new loss 8.33185731607955e-06 old loss 8.351553333341144e-06 BETTER
 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.53s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it]I0315 03:33:48.002766 276902 finetune.py:68] layer 0_gate @ epoch 2 new loss 7.381352133961627e-07 old loss 7.388502467620128e-07 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it]I0315 03:33:51.133397 278811 finetune.py:68] layer 1_gate @ epoch 1 new loss 2.1152627596165985e-06 old loss 2.117326175721246e-06 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0315 03:34:07.943759 282632 finetune.py:68] layer 3_up @ epoch 4 new loss 8.312617865158245e-06 old loss 8.33185731607955e-06 BETTER
W0315 03:34:09.526497 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

3_up proxy err 0.011352545581758022 tr(WHW.T) 316.0091552734375
  0%|          | 0/32 [00:00<?, ?it/s]I0315 03:34:11.717939 280702 finetune.py:45] layer 2_gate initial loss 5.100241196487332e-06
W0315 03:34:11.718306 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it]I0315 03:34:22.818293 276902 finetune.py:68] layer 0_gate @ epoch 3 new loss 7.375283530564047e-07 old loss 7.381352133961627e-07 BETTER
 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it]I0315 03:34:24.403368 278811 finetune.py:68] layer 1_gate @ epoch 2 new loss 2.113855316565605e-06 old loss 2.1152627596165985e-06 BETTER
 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it]I0315 03:34:44.558474 280702 finetune.py:68] layer 2_gate @ epoch 0 new loss 5.071808118373156e-06 old loss 5.100241196487332e-06 BETTER
 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it]I0315 03:34:57.906708 278811 finetune.py:68] layer 1_gate @ epoch 3 new loss 2.1122837097209413e-06 old loss 2.113855316565605e-06 BETTER
I0315 03:34:57.931943 276902 finetune.py:68] layer 0_gate @ epoch 4 new loss 7.370391585936886e-07 old loss 7.375283530564047e-07 BETTER
 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]W0315 03:34:59.124245 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
0_gate proxy err 0.005899496842175722 tr(WHW.T) 179.696533203125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s]  2%|▏         | 2/112 [00:01<01:03,  1.72it/s]  3%|▎         | 3/112 [00:01<00:53,  2.05it/s]  4%|▎         | 4/112 [00:01<00:47,  2.26it/s]  4%|▍         | 5/112 [00:02<00:45,  2.38it/s]  5%|▌         | 6/112 [00:02<00:42,  2.47it/s]  6%|▋         | 7/112 [00:03<00:41,  2.53it/s]  7%|▋         | 8/112 [00:03<00:40,  2.59it/s]  8%|▊         | 9/112 [00:03<00:39,  2.62it/s]  9%|▉         | 10/112 [00:04<00:38,  2.65it/s] 10%|▉         | 11/112 [00:04<00:37,  2.67it/s] 11%|█         | 12/112 [00:04<00:37,  2.68it/s] 12%|█▏        | 13/112 [00:05<00:36,  2.68it/s] 12%|█▎        | 14/112 [00:05<00:36,  2.69it/s]I0315 03:35:08.098133 282632 finetune.py:45] layer 3_gate initial loss 1.1271072253293823e-05
W0315 03:35:08.098491 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 13%|█▎        | 15/112 [00:06<00:36,  2.68it/s] 14%|█▍        | 16/112 [00:06<00:35,  2.68it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.68it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.66it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.67it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.68it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.69it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.71it/s] 21%|██        | 23/112 [00:09<00:32,  2.72it/s] 21%|██▏       | 24/112 [00:09<00:32,  2.72it/s] 22%|██▏       | 25/112 [00:09<00:31,  2.72it/s] 23%|██▎       | 26/112 [00:10<00:31,  2.72it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.71it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.70it/s] 26%|██▌       | 29/112 [00:11<00:30,  2.69it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.67it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.68it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.68it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.68it/s] 30%|███       | 34/112 [00:13<00:29,  2.69it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.69it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.70it/s] 33%|███▎      | 37/112 [00:14<00:27,  2.70it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.70it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.69it/s] 36%|███▌      | 40/112 [00:15<00:26,  2.68it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.68it/s]I0315 03:35:18.012756 280702 finetune.py:68] layer 2_gate @ epoch 1 new loss 5.064933702669805e-06 old loss 5.071808118373156e-06 BETTER
 38%|███▊      | 42/112 [00:16<00:26,  2.69it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.66it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.67it/s] 40%|████      | 45/112 [00:17<00:25,  2.68it/s] 41%|████      | 46/112 [00:17<00:24,  2.69it/s] 42%|████▏     | 47/112 [00:17<00:24,  2.69it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.70it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.69it/s] 45%|████▍     | 50/112 [00:19<00:22,  2.70it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.70it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.70it/s] 47%|████▋     | 53/112 [00:20<00:21,  2.70it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.69it/s] 49%|████▉     | 55/112 [00:20<00:21,  2.69it/s] 50%|█████     | 56/112 [00:21<00:21,  2.66it/s] 51%|█████     | 57/112 [00:21<00:20,  2.67it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.68it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.69it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.70it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.71it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.72it/s] 56%|█████▋    | 63/112 [00:23<00:18,  2.72it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.72it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.71it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.70it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.69it/s] 61%|██████    | 68/112 [00:25<00:16,  2.66it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.68it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.69it/s] 63%|██████▎   | 71/112 [00:26<00:15,  2.69it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.70it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.70it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.70it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.70it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.70it/s] 69%|██████▉   | 77/112 [00:29<00:12,  2.70it/s]I0315 03:35:31.399063 278811 finetune.py:68] layer 1_gate @ epoch 4 new loss 2.111067715304671e-06 old loss 2.1122837097209413e-06 BETTER
 70%|██████▉   | 78/112 [00:29<00:12,  2.70it/s] 71%|███████   | 79/112 [00:29<00:12,  2.70it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.72it/s]W0315 03:35:32.607915 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 81/112 [00:30<00:11,  2.69it/s] 73%|███████▎  | 82/112 [00:30<00:11,  2.69it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.71it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.70it/s] 76%|███████▌  | 85/112 [00:32<00:09,  2.70it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.70it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.72it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.72it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.72it/s]1_gate proxy err 0.007216199301183224 tr(WHW.T) 270.9404602050781
  0%|          | 0/112 [00:00<?, ?it/s] 80%|████████  | 90/112 [00:33<00:08,  2.72it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.71it/s]  1%|          | 1/112 [00:00<01:37,  1.13it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.70it/s]  2%|▏         | 2/112 [00:01<01:05,  1.69it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.70it/s]  3%|▎         | 3/112 [00:01<00:54,  2.00it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.67it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.68it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.69it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.68it/s]  6%|▋         | 7/112 [00:03<00:42,  2.45it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.69it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.70it/s]  7%|▋         | 8/112 [00:03<00:41,  2.49it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.70it/s]  8%|▊         | 9/112 [00:03<00:40,  2.52it/s] 90%|█████████ | 101/112 [00:37<00:04,  2.70it/s]  9%|▉         | 10/112 [00:04<00:39,  2.55it/s]I0315 03:35:40.204333 282632 finetune.py:68] layer 3_gate @ epoch 0 new loss 1.1222389730392024e-05 old loss 1.1271072253293823e-05 BETTER
 91%|█████████ | 102/112 [00:38<00:03,  2.71it/s] 10%|▉         | 11/112 [00:04<00:39,  2.58it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.71it/s] 11%|█         | 12/112 [00:05<00:38,  2.59it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.71it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.59it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.70it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.61it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.71it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.62it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.69it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.62it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.70it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.61it/s] 97%|█████████▋| 109/112 [00:40<00:01,  2.71it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.58it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.71it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.59it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.72it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.60it/s]100%|██████████| 112/112 [00:42<00:00,  2.73it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]
 19%|█▉        | 21/112 [00:08<00:34,  2.60it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.60it/s] 21%|██        | 23/112 [00:09<00:34,  2.60it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.61it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.62it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.61it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.60it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.60it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.60it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.57it/s] 29%|██▊       | 32/112 [00:12<00:31,  2.58it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s] 30%|███       | 34/112 [00:13<00:29,  2.61it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.63it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.64it/s] 34%|███▍      | 38/112 [00:15<00:27,  2.65it/s]I0315 03:35:51.144118 280702 finetune.py:68] layer 2_gate @ epoch 2 new loss 5.058891929365927e-06 old loss 5.064933702669805e-06 BETTER
 35%|███▍      | 39/112 [00:15<00:27,  2.65it/s]W0315 03:35:51.547000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.548000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.548000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.548000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.548000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.548000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.548000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.591000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.591000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.591000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.591000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.592000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.608000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.608000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.608000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.608000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.608000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 36%|███▌      | 40/112 [00:15<00:27,  2.66it/s]W0315 03:35:51.782000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.782000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.782000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.782000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:51.782000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 37%|███▋      | 41/112 [00:16<00:26,  2.65it/s]W0315 03:35:52.134000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.134000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.134000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.134000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.134000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.134000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.135000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.171000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.171000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.171000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.171000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.171000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.244000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.244000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.244000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.244000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:52.244000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 42/112 [00:16<00:26,  2.65it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.62it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.64it/s]W0315 03:35:53.453000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.467000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.475000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.475000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 40%|████      | 45/112 [00:17<00:25,  2.64it/s] 41%|████      | 46/112 [00:18<00:25,  2.64it/s]W0315 03:35:53.926000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.927000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.927000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.927000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.927000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.927000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.927000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.956000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.956000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.956000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.957000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:53.957000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 42%|████▏     | 47/112 [00:18<00:24,  2.64it/s]W0315 03:35:54.308000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.308000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.308000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.308000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.308000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.308000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.308000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.309000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.614000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.614000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.614000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.614000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.614000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 43%|████▎     | 48/112 [00:18<00:24,  2.63it/s]W0315 03:35:54.957000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:35:54.962000 140649111349056 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 49/112 [00:19<00:23,  2.63it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.63it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.64it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.65it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.65it/s] 48%|████▊     | 54/112 [00:21<00:21,  2.65it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.61it/s] 50%|█████     | 56/112 [00:21<00:21,  2.61it/s] 51%|█████     | 57/112 [00:22<00:21,  2.61it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.60it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.61it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.61it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.62it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.62it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.63it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.63it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.62it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.61it/s] 61%|██████    | 68/112 [00:26<00:17,  2.58it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.58it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s]I0315 03:36:03.312466 276902 finetune.py:45] layer 0_down initial loss 1.494535467827518e-06
W0315 03:36:03.312857 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 63%|██████▎   | 71/112 [00:27<00:15,  2.60it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.61it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.61it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.62it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.62it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.63it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.62it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.62it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.58it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.60it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.61it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.60it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.61it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.61it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.61it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.61it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.61it/s] 80%|████████  | 90/112 [00:34<00:08,  2.60it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.60it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.60it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.57it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.58it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.59it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s]I0315 03:36:13.090874 282632 finetune.py:68] layer 3_gate @ epoch 1 new loss 1.1203957001271192e-05 old loss 1.1222389730392024e-05 BETTER
 87%|████████▋ | 97/112 [00:37<00:05,  2.61it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.61it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.61it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.61it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.61it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.60it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.60it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.55it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.57it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.58it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.59it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.59it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.60it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.60it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]
I0315 03:36:24.198621 280702 finetune.py:68] layer 2_gate @ epoch 3 new loss 5.053879704064457e-06 old loss 5.058891929365927e-06 BETTER
W0315 03:36:26.725000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.725000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.725000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.725000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.726000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.726000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.726000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.770000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.770000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.770000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.770000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.770000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.786000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.786000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.787000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.787000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.787000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.957000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.958000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.958000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.958000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:26.958000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.289000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.290000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.290000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.290000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.290000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.290000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.290000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.324000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.325000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.325000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.325000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.325000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.394000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.395000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.395000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.395000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:27.395000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:28.602000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:28.608000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:28.614000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:36:28.614000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.071000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.071000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.072000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.072000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.072000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.072000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.072000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.103000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.104000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.104000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.104000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.104000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.458000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.459000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.459000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.459000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.459000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.459000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.459000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.459000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.768000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.768000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.768000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.768000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:36:29.768000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:36:30.119000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:36:30.125000 140474921563968 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 03:36:35.929325 276902 finetune.py:68] layer 0_down @ epoch 0 new loss 1.493725221735076e-06 old loss 1.494535467827518e-06 BETTER
I0315 03:36:38.350591 278811 finetune.py:45] layer 1_down initial loss 3.5653088161780033e-06
W0315 03:36:38.351156 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:36:45.410440 282632 finetune.py:68] layer 3_gate @ epoch 2 new loss 1.1188933058292605e-05 old loss 1.1203957001271192e-05 BETTER
I0315 03:36:57.171946 280702 finetune.py:68] layer 2_gate @ epoch 4 new loss 5.048455477663083e-06 old loss 5.053879704064457e-06 BETTER
W0315 03:36:58.367017 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

2_gate proxy err 0.007662304677069187 tr(WHW.T) 449.1044921875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.67it/s]  3%|▎         | 3/112 [00:01<00:55,  1.97it/s]  4%|▎         | 4/112 [00:02<00:49,  2.17it/s]  4%|▍         | 5/112 [00:02<00:46,  2.29it/s]  5%|▌         | 6/112 [00:02<00:44,  2.37it/s]  6%|▋         | 7/112 [00:03<00:43,  2.42it/s]  7%|▋         | 8/112 [00:03<00:42,  2.47it/s]  8%|▊         | 9/112 [00:04<00:41,  2.50it/s]  9%|▉         | 10/112 [00:04<00:40,  2.52it/s] 10%|▉         | 11/112 [00:04<00:39,  2.54it/s] 11%|█         | 12/112 [00:05<00:39,  2.56it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.57it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.57it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.56it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.52it/s] 16%|█▌        | 18/112 [00:07<00:37,  2.53it/s]I0315 03:37:09.342916 278811 finetune.py:76] layer 1_down @ epoch 0 new loss 3.565854967746418e-06 old loss 3.5653088161780033e-06 WORSE
I0315 03:37:09.413792 276902 finetune.py:68] layer 0_down @ epoch 1 new loss 1.4935540093574673e-06 old loss 1.493725221735076e-06 BETTER
 17%|█▋        | 19/112 [00:07<00:36,  2.54it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.55it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.55it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.56it/s] 21%|██        | 23/112 [00:09<00:34,  2.57it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.57it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.57it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.57it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.57it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.58it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.58it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.54it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.55it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.56it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.56it/s] 30%|███       | 34/112 [00:13<00:30,  2.56it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.57it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.58it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.58it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.58it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.57it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.56it/s]I0315 03:37:17.941819 282632 finetune.py:68] layer 3_gate @ epoch 3 new loss 1.1171888218086679e-05 old loss 1.1188933058292605e-05 BETTER
 37%|███▋      | 41/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.54it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.56it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.55it/s] 40%|████      | 45/112 [00:18<00:26,  2.56it/s] 41%|████      | 46/112 [00:18<00:25,  2.57it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.57it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.57it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.57it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.57it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.57it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.57it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.54it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.55it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.56it/s] 50%|█████     | 56/112 [00:22<00:21,  2.55it/s] 51%|█████     | 57/112 [00:22<00:21,  2.56it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.56it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.56it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.57it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.57it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.56it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.56it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.55it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.52it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.54it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.55it/s] 61%|██████    | 68/112 [00:27<00:17,  2.55it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.56it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.56it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.55it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.56it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.56it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.55it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.55it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.55it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.53it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.54it/s] 71%|███████   | 79/112 [00:31<00:12,  2.55it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.55it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.55it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.55it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.55it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.56it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.56it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.56it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.56it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.56it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.55it/s] 80%|████████  | 90/112 [00:35<00:08,  2.56it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.52it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.53it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.54it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.54it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.55it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.55it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.55it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.56it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.57it/s]I0315 03:37:40.763842 278811 finetune.py:68] layer 1_down @ epoch 1 new loss 3.5643065530166496e-06 old loss 3.5653088161780033e-06 BETTER
 89%|████████▉ | 100/112 [00:39<00:04,  2.57it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.57it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.56it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.56it/s]I0315 03:37:42.558714 276902 finetune.py:68] layer 0_down @ epoch 2 new loss 1.49353490996873e-06 old loss 1.4935540093574673e-06 BETTER
 93%|█████████▎| 104/112 [00:41<00:03,  2.56it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.56it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.53it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.54it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.54it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.55it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.55it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.57it/s]100%|██████████| 112/112 [00:44<00:00,  2.57it/s]100%|██████████| 112/112 [00:44<00:00,  2.53it/s]
I0315 03:37:50.529309 282632 finetune.py:68] layer 3_gate @ epoch 4 new loss 1.1156766959175002e-05 old loss 1.1171888218086679e-05 BETTER
W0315 03:37:51.755318 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0315 03:37:53.443000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.443000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.443000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.443000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.443000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.443000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.444000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.490000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.491000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.491000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.491000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.491000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.507000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.507000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.507000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.507000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.508000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.683000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.683000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.684000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.684000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:37:53.684000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.022000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.022000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.022000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.023000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.023000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.023000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.023000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.058000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.058000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.058000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.058000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.058000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.133000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.133000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.133000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.133000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:54.133000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
3_gate proxy err 0.006207296624779701 tr(WHW.T) 874.8200073242188
  0%|          | 0/112 [00:00<?, ?it/s]W0315 03:37:55.372000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.389000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.398000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.398000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.862000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.863000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.863000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.863000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.863000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.863000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.863000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
  1%|          | 1/112 [00:00<01:36,  1.15it/s]W0315 03:37:55.898000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.898000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.898000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.898000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:55.898000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.254000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.255000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.255000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.255000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.255000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.255000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.256000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.256000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
  2%|▏         | 2/112 [00:01<01:04,  1.72it/s]W0315 03:37:56.565000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.565000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.565000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.565000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.565000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 3/112 [00:01<00:53,  2.03it/s]W0315 03:37:56.915000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:37:56.920000 139990311352128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
  4%|▎         | 4/112 [00:02<00:48,  2.21it/s]  4%|▍         | 5/112 [00:02<00:45,  2.33it/s]  5%|▌         | 6/112 [00:02<00:43,  2.41it/s]  6%|▋         | 7/112 [00:03<00:42,  2.47it/s]  7%|▋         | 8/112 [00:03<00:41,  2.50it/s]  8%|▊         | 9/112 [00:03<00:40,  2.52it/s]  9%|▉         | 10/112 [00:04<00:40,  2.55it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 11%|█         | 12/112 [00:05<00:39,  2.56it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.57it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.58it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.58it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.59it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.58it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.54it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.55it/s] 21%|██        | 23/112 [00:09<00:34,  2.56it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.57it/s]I0315 03:38:05.119664 280702 finetune.py:45] layer 2_down initial loss 8.116368007904384e-06
W0315 03:38:05.120294 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 25/112 [00:10<00:33,  2.58it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.58it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.58it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.58it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.59it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.58it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 29%|██▊       | 32/112 [00:12<00:31,  2.57it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.54it/s] 30%|███       | 34/112 [00:13<00:30,  2.56it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.56it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.57it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.57it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.58it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.58it/s] 36%|███▌      | 40/112 [00:16<00:27,  2.58it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.58it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.58it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.57it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.56it/s]I0315 03:38:12.611813 278811 finetune.py:68] layer 1_down @ epoch 2 new loss 3.563309519449831e-06 old loss 3.5643065530166496e-06 BETTER
 40%|████      | 45/112 [00:17<00:26,  2.54it/s] 41%|████      | 46/112 [00:18<00:25,  2.55it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.56it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.57it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.57it/s] 45%|████▍     | 50/112 [00:19<00:24,  2.58it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.58it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.58it/s]I0315 03:38:15.767769 276902 finetune.py:68] layer 0_down @ epoch 3 new loss 1.4933843885955866e-06 old loss 1.49353490996873e-06 BETTER
 47%|████▋     | 53/112 [00:21<00:22,  2.57it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.57it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.57it/s] 50%|█████     | 56/112 [00:22<00:21,  2.58it/s] 51%|█████     | 57/112 [00:22<00:21,  2.56it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.57it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.57it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.57it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.58it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.58it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.58it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.58it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.58it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.57it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.57it/s] 61%|██████    | 68/112 [00:26<00:17,  2.57it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.55it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.56it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.57it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.57it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.58it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.59it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.59it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.59it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.59it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.58it/s] 71%|███████   | 79/112 [00:31<00:12,  2.60it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.59it/s] 72%|███████▏  | 81/112 [00:31<00:12,  2.55it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.57it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.57it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.58it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.58it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.59it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.59it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.60it/s] 80%|████████  | 90/112 [00:35<00:08,  2.58it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.58it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.58it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.53it/s] 84%|████████▍ | 94/112 [00:36<00:07,  2.55it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.56it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.57it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.57it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.58it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.58it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.58it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.58it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.58it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.58it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.58it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.55it/s]I0315 03:38:36.300410 280702 finetune.py:68] layer 2_down @ epoch 0 new loss 8.113447620416991e-06 old loss 8.116368007904384e-06 BETTER
 95%|█████████▍| 106/112 [00:41<00:02,  2.56it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.57it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.57it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.57it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.58it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.55it/s]
I0315 03:38:44.433067 278811 finetune.py:68] layer 1_down @ epoch 3 new loss 3.562759502528934e-06 old loss 3.563309519449831e-06 BETTER
W0315 03:38:46.672000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.672000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.673000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.673000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.673000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.673000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.673000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.720000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.720000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.720000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.720000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.721000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.737000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.737000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.737000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.737000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.737000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.913000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.913000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.913000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.913000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:46.913000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.250000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.250000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.250000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.251000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.251000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.251000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.251000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.287000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.287000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.288000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.288000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.288000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.362000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.362000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.362000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.362000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:47.362000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:48.606000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:48.612000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:48.619000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:48.619000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.088000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.088000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.088000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.088000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.088000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.088000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.088000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.122000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.122000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.122000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.122000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.122000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
I0315 03:38:49.226718 276902 finetune.py:68] layer 0_down @ epoch 4 new loss 1.4932786598365055e-06 old loss 1.4933843885955866e-06 BETTER
W0315 03:38:49.484000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.485000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.485000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.485000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.485000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.485000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.485000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.485000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.794000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.794000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.795000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.795000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:38:49.795000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:38:50.044503 276902 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0315 03:38:50.144000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:38:50.162000 140163048179520 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
0_down proxy err 0.006477679591625929 tr(WHW.T) 0.4815376400947571
I0315 03:38:58.869108 282632 finetune.py:45] layer 3_down initial loss 1.7232519894605502e-05
W0315 03:38:58.869558 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:39:08.279747 280702 finetune.py:68] layer 2_down @ epoch 1 new loss 8.112665454973467e-06 old loss 8.113447620416991e-06 BETTER
I0315 03:39:16.574640 278811 finetune.py:68] layer 1_down @ epoch 4 new loss 3.5617958928924054e-06 old loss 3.562759502528934e-06 BETTER
W0315 03:39:17.400649 278811 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

1_down proxy err 0.00019138729840051383 tr(WHW.T) 67.1446304321289
I0315 03:39:29.528670 282632 finetune.py:68] layer 3_down @ epoch 0 new loss 1.7228694559889846e-05 old loss 1.7232519894605502e-05 BETTER
I0315 03:39:39.953011 280702 finetune.py:68] layer 2_down @ epoch 2 new loss 8.111304850899614e-06 old loss 8.112665454973467e-06 BETTER
I0315 03:40:00.629481 282632 finetune.py:68] layer 3_down @ epoch 1 new loss 1.7226329873665236e-05 old loss 1.7228694559889846e-05 BETTER
I0315 03:40:11.451686 280702 finetune.py:68] layer 2_down @ epoch 3 new loss 8.110839189612307e-06 old loss 8.111304850899614e-06 BETTER
I0315 03:40:27.224466 273992 quantize_finetune_llama.py:186] computed original embedding for layer 4 in 64.22269892692566s
I0315 03:40:27.621844 273992 quantize_finetune_llama.py:159] layer 5 gpu 1
I0315 03:40:29.694285 308145 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 03:40:29.694414 308145 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 03:40:29.694477 308145 utils.py:162] NumExpr defaulting to 16 threads.
I0315 03:40:29.899485 308145 config.py:58] PyTorch version 2.4.0 available.
I0315 03:40:31.984351 282632 finetune.py:68] layer 3_down @ epoch 2 new loss 1.722471097309608e-05 old loss 1.7226329873665236e-05 BETTER
I0315 03:40:32.303102 308145 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 03:40:32.751532 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:45,  1.47s/it]  6%|▋         | 2/32 [00:01<00:23,  1.26it/s]  9%|▉         | 3/32 [00:02<00:16,  1.75it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 19%|█▉        | 6/32 [00:03<00:09,  2.66it/s] 22%|██▏       | 7/32 [00:03<00:08,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.95it/s] 28%|██▊       | 9/32 [00:03<00:07,  3.04it/s] 31%|███▏      | 10/32 [00:04<00:07,  3.07it/s] 34%|███▍      | 11/32 [00:04<00:06,  3.11it/s] 38%|███▊      | 12/32 [00:04<00:06,  3.14it/s] 41%|████      | 13/32 [00:05<00:05,  3.19it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.23it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.25it/s] 50%|█████     | 16/32 [00:06<00:04,  3.28it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.29it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.30it/s] 59%|█████▉    | 19/32 [00:07<00:03,  3.29it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.31it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.32it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.33it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.32it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.32it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.32it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.32it/s]I0315 03:40:43.063709 280702 finetune.py:68] layer 2_down @ epoch 4 new loss 8.109646842058282e-06 old loss 8.110839189612307e-06 BETTER
 84%|████████▍ | 27/32 [00:09<00:01,  3.31it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.29it/s]W0315 03:40:43.808064 280702 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

 91%|█████████ | 29/32 [00:10<00:00,  3.31it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.33it/s]2_down proxy err 0.010908398777246475 tr(WHW.T) 1.2016922235488892
 97%|█████████▋| 31/32 [00:10<00:00,  3.33it/s]100%|██████████| 32/32 [00:10<00:00,  3.32it/s]100%|██████████| 32/32 [00:10<00:00,  2.93it/s]
W0315 03:40:47.717000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.717000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.718000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.718000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.718000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.718000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.718000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.744000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.744000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.744000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.744000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.744000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.761000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.761000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.761000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.761000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:40:47.762000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.086000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.086000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.086000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.086000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.086000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.968000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.968000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.968000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.968000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.968000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.969000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.969000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.986000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.986000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.986000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.986000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:40:48.987000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:40:49.230000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:40:49.230000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:40:49.230000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:40:49.230000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:40:49.230000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.379000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.379000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.379000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.379000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.379000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.379000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.379000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.397000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.397000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.397000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.398000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:40:50.398000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:40:51.295000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:40:51.295000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:40:51.295000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:40:51.295000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:40:51.295000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 03:40:57.334220 308145 finetune.py:45] layer 4_v initial loss 1.3759284229308832e-05
W0315 03:40:57.336346 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:41:03.153581 282632 finetune.py:68] layer 3_down @ epoch 3 new loss 1.7223694158019498e-05 old loss 1.722471097309608e-05 BETTER
I0315 03:41:33.239849 308145 finetune.py:68] layer 4_v @ epoch 0 new loss 3.116516609225073e-06 old loss 1.3759284229308832e-05 BETTER
I0315 03:41:33.833562 282632 finetune.py:68] layer 3_down @ epoch 4 new loss 1.7223519535036758e-05 old loss 1.7223694158019498e-05 BETTER
W0315 03:41:34.572921 282632 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

3_down proxy err 0.012106670066714287 tr(WHW.T) 2.1205697059631348
I0315 03:41:50.491518 273992 quantize_finetune_llama.py:186] computed original embedding for layer 5 in 61.633169651031494s
I0315 03:41:50.859305 273992 quantize_finetune_llama.py:159] layer 6 gpu 2
I0315 03:41:52.970311 310007 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 03:41:52.970426 310007 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 03:41:52.970484 310007 utils.py:162] NumExpr defaulting to 16 threads.
I0315 03:41:53.149892 310007 config.py:58] PyTorch version 2.4.0 available.
I0315 03:41:55.313893 310007 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 03:41:55.666484 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:48,  1.57s/it]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.93it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.18it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.53it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.75it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.85it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.86it/s] 50%|█████     | 16/32 [00:06<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.85it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.87it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.87it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0315 03:42:10.770836 308145 finetune.py:68] layer 4_v @ epoch 1 new loss 2.2136537154437974e-06 old loss 3.116516609225073e-06 BETTER
W0315 03:42:12.167000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.167000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.167000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.167000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.167000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.167000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.168000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.194000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.194000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.194000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.194000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.194000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.211000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.211000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.211000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.211000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.211000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.544000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.544000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.544000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.544000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:42:12.544000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.449000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.449000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.449000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.449000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.449000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.449000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.449000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.468000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.468000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.468000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.468000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.468000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.722000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.722000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.723000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.723000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:42:13.723000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.920000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.921000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.921000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.921000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.921000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.921000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.921000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.940000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.940000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.940000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.940000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:42:14.940000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:42:15.867000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:42:15.867000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:42:15.867000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:42:15.868000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:42:15.868000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 03:42:21.877401 310007 finetune.py:45] layer 5_v initial loss 1.0546260455157608e-05
W0315 03:42:21.877606 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:42:48.688169 308145 finetune.py:68] layer 4_v @ epoch 2 new loss 1.949661054823082e-06 old loss 2.2136537154437974e-06 BETTER
I0315 03:42:52.381942 273992 quantize_finetune_llama.py:186] computed original embedding for layer 6 in 61.10861563682556s
I0315 03:42:52.742084 273992 quantize_finetune_llama.py:159] layer 7 gpu 3
I0315 03:42:55.141448 311534 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 03:42:55.141592 311534 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 03:42:55.141658 311534 utils.py:162] NumExpr defaulting to 16 threads.
I0315 03:42:55.367575 311534 config.py:58] PyTorch version 2.4.0 available.
I0315 03:42:56.461271 310007 finetune.py:68] layer 5_v @ epoch 0 new loss 2.733662313403329e-06 old loss 1.0546260455157608e-05 BETTER
I0315 03:42:57.635005 311534 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 03:42:58.050836 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:51,  1.65s/it]  6%|▋         | 2/32 [00:01<00:26,  1.13it/s]  9%|▉         | 3/32 [00:02<00:18,  1.56it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.14it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.77it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.83it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.80it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.83it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0315 03:43:14.873000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.873000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.874000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.874000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.874000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.874000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.874000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.901000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.901000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.901000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.901000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.901000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.918000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.918000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.918000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.918000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:43:14.918000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:43:15.245000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:43:15.245000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:43:15.245000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:43:15.245000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:43:15.246000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.176000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.177000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.177000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.177000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.177000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.177000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.177000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.195000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.196000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.196000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.196000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.196000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.450000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.450000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.450000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.450000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:43:16.450000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.675000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.676000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.676000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.676000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.676000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.676000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.676000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.694000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.695000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.695000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.695000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:43:17.695000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:43:18.647000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:43:18.647000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:43:18.647000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:43:18.647000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:43:18.647000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 03:43:25.030954 311534 finetune.py:45] layer 6_v initial loss 1.0269816812069621e-05
W0315 03:43:25.031276 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:43:26.800651 308145 finetune.py:68] layer 4_v @ epoch 3 new loss 1.8263924630446127e-06 old loss 1.949661054823082e-06 BETTER
I0315 03:43:32.226076 310007 finetune.py:68] layer 5_v @ epoch 1 new loss 2.130469056282891e-06 old loss 2.733662313403329e-06 BETTER
I0315 03:43:54.200623 273992 quantize_finetune_llama.py:186] computed original embedding for layer 7 in 60.97100615501404s
I0315 03:43:54.613155 273992 quantize_finetune_llama.py:159] layer 8 gpu 0
I0315 03:43:56.688987 313060 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 03:43:56.689129 313060 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 03:43:56.689193 313060 utils.py:162] NumExpr defaulting to 16 threads.
I0315 03:43:56.899516 313060 config.py:58] PyTorch version 2.4.0 available.
I0315 03:43:59.189345 313060 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 03:43:59.529663 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 03:43:59.703228 311534 finetune.py:68] layer 6_v @ epoch 0 new loss 3.586518687370699e-06 old loss 1.0269816812069621e-05 BETTER
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.59s/it]  6%|▋         | 2/32 [00:01<00:25,  1.16it/s]  9%|▉         | 3/32 [00:02<00:18,  1.58it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.91it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.56it/s]I0315 03:44:04.791149 308145 finetune.py:68] layer 4_v @ epoch 4 new loss 1.753243054736231e-06 old loss 1.8263924630446127e-06 BETTER
 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.71it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.74it/s] 41%|████      | 13/32 [00:05<00:06,  2.76it/s]W0315 03:44:06.667514 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:06<00:06,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.78it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.79it/s]4_v proxy err 0.008432278409600258 tr(WHW.T) 38.379119873046875
  0%|          | 0/32 [00:00<?, ?it/s]I0315 03:44:08.109703 310007 finetune.py:68] layer 5_v @ epoch 2 new loss 1.928368874359876e-06 old loss 2.130469056282891e-06 BETTER
 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.81it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.82it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.83it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.82it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.84it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s]W0315 03:44:16.241000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.241000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.241000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.241000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.241000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.241000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.241000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.268000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.268000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.269000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.269000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.269000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.286000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.286000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.286000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.286000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.286000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s]W0315 03:44:16.621000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.621000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.621000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.621000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:16.621000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s]W0315 03:44:17.537000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.537000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.537000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.537000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.538000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.538000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.538000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.557000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.557000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.557000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.557000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.557000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s]W0315 03:44:17.803000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.804000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.804000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.804000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:17.804000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 25/32 [00:10<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s]W0315 03:44:19.017000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.017000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.017000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.017000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.018000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.018000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.018000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.036000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.036000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.036000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.037000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.037000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s]W0315 03:44:19.970000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.970000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.970000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.971000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:19.971000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0315 03:44:26.294373 313060 finetune.py:45] layer 7_v initial loss 8.570148565922864e-06
W0315 03:44:26.294680 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0315 03:44:27.046000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.046000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.046000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.046000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.046000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.046000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.046000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.076000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.076000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.077000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.077000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.077000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.092000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.093000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.093000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.093000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.093000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.255000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.255000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.255000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.255000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.255000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.492000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.492000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.492000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.492000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.492000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.493000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.493000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.514000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.514000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.515000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.515000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.515000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.581000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.581000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.582000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.582000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:27.582000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.473000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.791000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.791000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.791000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.791000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.792000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.792000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.792000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.815000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.815000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.815000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.815000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:28.815000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:29.077000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:44:29.077000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:44:29.077000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:44:29.077000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:44:29.077000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:44:29.346000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 03:44:35.267684 311534 finetune.py:68] layer 6_v @ epoch 1 new loss 3.070565981033724e-06 old loss 3.586518687370699e-06 BETTER
I0315 03:44:36.186573 308145 finetune.py:45] layer 4_q initial loss 3.0128576327115297e-06
W0315 03:44:36.186980 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:44:44.272839 310007 finetune.py:68] layer 5_v @ epoch 3 new loss 1.832473571994342e-06 old loss 1.928368874359876e-06 BETTER
I0315 03:45:00.336046 313060 finetune.py:68] layer 7_v @ epoch 0 new loss 4.069897386216326e-06 old loss 8.570148565922864e-06 BETTER
I0315 03:45:11.869393 311534 finetune.py:68] layer 6_v @ epoch 2 new loss 2.881016825995175e-06 old loss 3.070565981033724e-06 BETTER
I0315 03:45:12.679747 308145 finetune.py:68] layer 4_q @ epoch 0 new loss 2.8899446533614537e-06 old loss 3.0128576327115297e-06 BETTER
I0315 03:45:20.824464 310007 finetune.py:68] layer 5_v @ epoch 4 new loss 1.7868787836050615e-06 old loss 1.832473571994342e-06 BETTER
W0315 03:45:22.470235 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_v proxy err 0.007989304140210152 tr(WHW.T) 37.700050354003906
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.00it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.53it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.52it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.54it/s] 50%|█████     | 16/32 [00:06<00:06,  2.55it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.55it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.56it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.57it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s]I0315 03:45:35.574785 313060 finetune.py:68] layer 7_v @ epoch 1 new loss 3.693232429213822e-06 old loss 4.069897386216326e-06 BETTER
 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
W0315 03:45:43.002000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.002000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.002000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.003000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.003000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.003000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.003000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.034000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.034000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.034000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.034000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.034000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.051000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.051000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.051000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.051000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.051000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.215000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.215000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.215000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.215000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.215000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.448000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.448000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.448000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.448000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.448000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.448000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.449000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.470000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.470000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.471000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.471000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.471000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.540000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.540000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.540000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.540000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:45:43.540000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.445000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.774000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.774000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.774000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.775000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.775000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.775000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.775000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.796000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.796000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.796000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.796000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:45:44.797000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:45:45.067000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:45:45.067000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:45:45.068000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:45:45.068000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:45:45.068000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:45:45.346000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 03:45:48.274220 311534 finetune.py:68] layer 6_v @ epoch 3 new loss 2.7787530143541517e-06 old loss 2.881016825995175e-06 BETTER
I0315 03:45:50.034789 308145 finetune.py:68] layer 4_q @ epoch 1 new loss 2.8166668926132843e-06 old loss 2.8899446533614537e-06 BETTER
I0315 03:45:52.189713 310007 finetune.py:45] layer 5_q initial loss 3.6514752537186723e-06
W0315 03:45:52.190134 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:46:10.887206 313060 finetune.py:68] layer 7_v @ epoch 2 new loss 3.5515365652827313e-06 old loss 3.693232429213822e-06 BETTER
I0315 03:46:24.896704 311534 finetune.py:68] layer 6_v @ epoch 4 new loss 2.7433297873358242e-06 old loss 2.7787530143541517e-06 BETTER
W0315 03:46:26.641165 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 03:46:27.312476 310007 finetune.py:68] layer 5_q @ epoch 0 new loss 3.492435325824772e-06 old loss 3.6514752537186723e-06 BETTER
I0315 03:46:27.415771 308145 finetune.py:68] layer 4_q @ epoch 2 new loss 2.7630508157017175e-06 old loss 2.8166668926132843e-06 BETTER
6_v proxy err 0.008387389592826366 tr(WHW.T) 42.837894439697266
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:31,  1.02s/it]  6%|▋         | 2/32 [00:01<00:19,  1.53it/s]  9%|▉         | 3/32 [00:01<00:15,  1.89it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.11it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
I0315 03:46:46.316986 313060 finetune.py:68] layer 7_v @ epoch 3 new loss 3.4768509067362174e-06 old loss 3.5515365652827313e-06 BETTER
W0315 03:46:47.079000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.080000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.080000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.080000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.080000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.080000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.080000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.111000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.111000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.111000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.112000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.112000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.127000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.128000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.128000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.128000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.128000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.290000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.291000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.291000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.291000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.291000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.519000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.519000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.519000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.519000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.519000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.519000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.519000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.539000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.539000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.539000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.539000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.540000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.606000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.606000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.606000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.606000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:46:47.607000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.497000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.814000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.814000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.814000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.815000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.815000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.815000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.815000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.835000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.835000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.835000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.835000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:46:48.835000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:46:49.093000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:46:49.093000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:46:49.093000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:46:49.093000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:46:49.093000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:46:49.363000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 03:46:56.034133 311534 finetune.py:45] layer 6_q initial loss 5.2678328756883275e-06
W0315 03:46:56.034483 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:47:03.403601 310007 finetune.py:68] layer 5_q @ epoch 1 new loss 3.41410850523971e-06 old loss 3.492435325824772e-06 BETTER
I0315 03:47:04.833097 308145 finetune.py:68] layer 4_q @ epoch 3 new loss 2.7197522740607383e-06 old loss 2.7630508157017175e-06 BETTER
I0315 03:47:21.932193 313060 finetune.py:76] layer 7_v @ epoch 4 new loss 3.5651330563268857e-06 old loss 3.4768509067362174e-06 WORSE
W0315 03:47:22.994587 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_v proxy err 0.006995062809437513 tr(WHW.T) 53.14668655395508
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:15,  1.90it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.10it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.33it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.39it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.44it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.53it/s]I0315 03:47:31.208536 311534 finetune.py:68] layer 6_q @ epoch 0 new loss 5.0571288738865405e-06 old loss 5.2678328756883275e-06 BETTER
 53%|█████▎    | 17/32 [00:07<00:05,  2.54it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.55it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.54it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.54it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:13<00:00,  2.56it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
I0315 03:47:39.541259 310007 finetune.py:68] layer 5_q @ epoch 2 new loss 3.355472017574357e-06 old loss 3.41410850523971e-06 BETTER
I0315 03:47:42.028405 308145 finetune.py:68] layer 4_q @ epoch 4 new loss 2.6831751256395364e-06 old loss 2.7197522740607383e-06 BETTER
W0315 03:47:43.640000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.640000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.640000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.640000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.641000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.641000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.641000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.671000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.671000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.671000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.671000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.671000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.682664 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0315 03:47:43.687000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.688000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.688000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.688000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.688000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.855000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.855000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.855000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.855000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 03:47:43.855000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.092000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.092000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.092000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.092000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.092000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.092000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.092000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.113000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.113000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.113000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.113000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.113000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.187000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.187000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.187000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.187000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 03:47:44.187000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
4_q proxy err 0.0008328338735736907 tr(WHW.T) 6740.5771484375
  0%|          | 0/32 [00:00<?, ?it/s]W0315 03:47:45.100000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.420000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.420000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.421000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.421000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.421000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.421000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.421000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.444000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.444000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.444000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.444000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.444000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]W0315 03:47:45.709000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.709000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.710000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.710000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.710000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 03:47:45.982000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:11,  2.21it/s] 25%|██▌       | 8/32 [00:03<00:10,  2.36it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.46it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s]I0315 03:47:52.784899 313060 finetune.py:45] layer 7_q initial loss 6.952719104447169e-06
W0315 03:47:52.785243 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:08<00:04,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.72it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.72it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.73it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.73it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.68it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.70it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.71it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
I0315 03:48:04.350103 308145 finetune.py:45] layer 4_k initial loss 3.903182005160488e-06
W0315 03:48:04.350660 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:48:07.164247 311534 finetune.py:68] layer 6_q @ epoch 1 new loss 4.95282210977166e-06 old loss 5.0571288738865405e-06 BETTER
I0315 03:48:15.534558 310007 finetune.py:68] layer 5_q @ epoch 3 new loss 3.3071701182052493e-06 old loss 3.355472017574357e-06 BETTER
I0315 03:48:27.235492 313060 finetune.py:68] layer 7_q @ epoch 0 new loss 6.753245543222874e-06 old loss 6.952719104447169e-06 BETTER
I0315 03:48:41.019337 308145 finetune.py:68] layer 4_k @ epoch 0 new loss 3.563439349818509e-06 old loss 3.903182005160488e-06 BETTER
I0315 03:48:43.279577 311534 finetune.py:68] layer 6_q @ epoch 2 new loss 4.872006684308872e-06 old loss 4.95282210977166e-06 BETTER
I0315 03:48:51.770935 310007 finetune.py:68] layer 5_q @ epoch 4 new loss 3.266897238063393e-06 old loss 3.3071701182052493e-06 BETTER
W0315 03:48:53.513407 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_q proxy err 0.0011584130115807056 tr(WHW.T) 6499.0439453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.07it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:14,  1.99it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s]I0315 03:49:02.615567 313060 finetune.py:68] layer 7_q @ epoch 1 new loss 6.663856765953824e-06 old loss 6.753245543222874e-06 BETTER
 62%|██████▎   | 20/32 [00:08<00:04,  2.63it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0315 03:49:14.552102 310007 finetune.py:45] layer 5_k initial loss 4.736530627269531e-06
W0315 03:49:14.552623 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:49:18.542003 308145 finetune.py:68] layer 4_k @ epoch 1 new loss 3.5246707739133853e-06 old loss 3.563439349818509e-06 BETTER
I0315 03:49:19.410833 311534 finetune.py:68] layer 6_q @ epoch 3 new loss 4.807887762581231e-06 old loss 4.872006684308872e-06 BETTER
I0315 03:49:38.256069 313060 finetune.py:68] layer 7_q @ epoch 2 new loss 6.502329142676899e-06 old loss 6.663856765953824e-06 BETTER
I0315 03:49:49.496012 310007 finetune.py:68] layer 5_k @ epoch 0 new loss 4.318100764066912e-06 old loss 4.736530627269531e-06 BETTER
I0315 03:49:55.646669 311534 finetune.py:68] layer 6_q @ epoch 4 new loss 4.7622293095628265e-06 old loss 4.807887762581231e-06 BETTER
I0315 03:49:56.275840 308145 finetune.py:68] layer 4_k @ epoch 2 new loss 3.487757794573554e-06 old loss 3.5246707739133853e-06 BETTER
W0315 03:49:57.405703 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_q proxy err 0.0013068676926195621 tr(WHW.T) 6022.56640625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:14,  1.94it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0315 03:50:13.434864 313060 finetune.py:76] layer 7_q @ epoch 3 new loss 6.510755611088825e-06 old loss 6.502329142676899e-06 WORSE
I0315 03:50:18.483712 311534 finetune.py:45] layer 6_k initial loss 6.901785582158482e-06
W0315 03:50:18.484121 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:50:25.666117 310007 finetune.py:68] layer 5_k @ epoch 1 new loss 4.263513346813852e-06 old loss 4.318100764066912e-06 BETTER
I0315 03:50:34.216337 308145 finetune.py:68] layer 4_k @ epoch 3 new loss 3.4610945931490278e-06 old loss 3.487757794573554e-06 BETTER
I0315 03:50:48.347500 313060 finetune.py:68] layer 7_q @ epoch 4 new loss 6.476418093370739e-06 old loss 6.502329142676899e-06 BETTER
W0315 03:50:50.193929 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_q proxy err 0.001261916826479137 tr(WHW.T) 6037.814453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:31,  1.01s/it]  6%|▋         | 2/32 [00:01<00:19,  1.54it/s]  9%|▉         | 3/32 [00:01<00:15,  1.88it/s]I0315 03:50:53.705529 311534 finetune.py:68] layer 6_k @ epoch 0 new loss 6.217314876266755e-06 old loss 6.901785582158482e-06 BETTER
 12%|█▎        | 4/32 [00:02<00:13,  2.10it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.25it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.52it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s]I0315 03:51:01.645117 310007 finetune.py:68] layer 5_k @ epoch 2 new loss 4.22478660766501e-06 old loss 4.263513346813852e-06 BETTER
 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
I0315 03:51:11.598582 313060 finetune.py:45] layer 7_k initial loss 9.040955774253234e-06
W0315 03:51:11.598940 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:51:12.090816 308145 finetune.py:68] layer 4_k @ epoch 4 new loss 3.435854523559101e-06 old loss 3.4610945931490278e-06 BETTER
W0315 03:51:13.798524 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_k proxy err 0.0008285869844257832 tr(WHW.T) 3938.07568359375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.10it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0315 03:51:30.263546 311534 finetune.py:68] layer 6_k @ epoch 1 new loss 6.140561254142085e-06 old loss 6.217314876266755e-06 BETTER
I0315 03:51:35.349481 308145 finetune.py:45] layer 4_o initial loss 5.933316970185842e-06
W0315 03:51:35.349925 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:51:38.025735 310007 finetune.py:68] layer 5_k @ epoch 3 new loss 4.193401309748879e-06 old loss 4.22478660766501e-06 BETTER
I0315 03:51:45.953414 313060 finetune.py:68] layer 7_k @ epoch 0 new loss 8.418443940172438e-06 old loss 9.040955774253234e-06 BETTER
I0315 03:52:06.636740 311534 finetune.py:68] layer 6_k @ epoch 2 new loss 6.083915195631562e-06 old loss 6.140561254142085e-06 BETTER
I0315 03:52:11.941007 308145 finetune.py:68] layer 4_o @ epoch 0 new loss 5.687498742190655e-06 old loss 5.933316970185842e-06 BETTER
I0315 03:52:14.107321 310007 finetune.py:68] layer 5_k @ epoch 4 new loss 4.165272002865095e-06 old loss 4.193401309748879e-06 BETTER
W0315 03:52:15.746169 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

5_k proxy err 0.0010759929427877069 tr(WHW.T) 4148.6337890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s]I0315 03:52:21.033531 313060 finetune.py:68] layer 7_k @ epoch 1 new loss 8.37412244436564e-06 old loss 8.418443940172438e-06 BETTER
 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0315 03:52:36.938225 310007 finetune.py:45] layer 5_o initial loss 6.902263521624263e-06
W0315 03:52:36.938470 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:52:43.078821 311534 finetune.py:68] layer 6_k @ epoch 3 new loss 6.0453130572568625e-06 old loss 6.083915195631562e-06 BETTER
I0315 03:52:49.790458 308145 finetune.py:68] layer 4_o @ epoch 1 new loss 5.62220793653978e-06 old loss 5.687498742190655e-06 BETTER
I0315 03:52:56.343115 313060 finetune.py:68] layer 7_k @ epoch 2 new loss 8.249675374827348e-06 old loss 8.37412244436564e-06 BETTER
I0315 03:53:12.096001 310007 finetune.py:68] layer 5_o @ epoch 0 new loss 6.706962267344352e-06 old loss 6.902263521624263e-06 BETTER
I0315 03:53:19.286690 311534 finetune.py:68] layer 6_k @ epoch 4 new loss 6.009541266394081e-06 old loss 6.0453130572568625e-06 BETTER
W0315 03:53:20.996901 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_k proxy err 0.0010125457774847746 tr(WHW.T) 4415.07275390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s]I0315 03:53:27.295093 308145 finetune.py:68] layer 4_o @ epoch 2 new loss 5.578596301347716e-06 old loss 5.62220793653978e-06 BETTER
 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s]I0315 03:53:31.609916 313060 finetune.py:68] layer 7_k @ epoch 3 new loss 8.204331606975757e-06 old loss 8.249675374827348e-06 BETTER
 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0315 03:53:42.311349 311534 finetune.py:45] layer 6_o initial loss 1.0662993190635461e-05
W0315 03:53:42.311775 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:53:48.049728 310007 finetune.py:68] layer 5_o @ epoch 1 new loss 6.639326329604955e-06 old loss 6.706962267344352e-06 BETTER
I0315 03:54:04.713891 308145 finetune.py:68] layer 4_o @ epoch 3 new loss 5.540315669350093e-06 old loss 5.578596301347716e-06 BETTER
I0315 03:54:07.709692 313060 finetune.py:76] layer 7_k @ epoch 4 new loss 8.233815606217831e-06 old loss 8.204331606975757e-06 WORSE
W0315 03:54:08.839256 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_k proxy err 0.0010425745276734233 tr(WHW.T) 4607.21044921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.54it/s] 50%|█████     | 16/32 [00:06<00:06,  2.51it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.52it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.52it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.53it/s]I0315 03:54:18.217136 311534 finetune.py:68] layer 6_o @ epoch 0 new loss 1.0390415809524711e-05 old loss 1.0662993190635461e-05 BETTER
 62%|██████▎   | 20/32 [00:08<00:04,  2.55it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.55it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.54it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.54it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0315 03:54:24.988332 310007 finetune.py:68] layer 5_o @ epoch 2 new loss 6.590699285879964e-06 old loss 6.639326329604955e-06 BETTER
I0315 03:54:30.438363 313060 finetune.py:45] layer 7_o initial loss 1.441193944629049e-05
W0315 03:54:30.438733 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:54:42.287841 308145 finetune.py:68] layer 4_o @ epoch 4 new loss 5.509262337000109e-06 old loss 5.540315669350093e-06 BETTER
W0315 03:54:43.887198 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_o proxy err 0.006985518615692854 tr(WHW.T) 1.3722667694091797
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it]  9%|▉         | 3/32 [00:04<00:46,  1.61s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it]I0315 03:54:54.475099 311534 finetune.py:68] layer 6_o @ epoch 1 new loss 1.0282063158228993e-05 old loss 1.0390415809524711e-05 BETTER
 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it]I0315 03:55:01.452161 310007 finetune.py:68] layer 5_o @ epoch 3 new loss 6.552199465659214e-06 old loss 6.590699285879964e-06 BETTER
 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it]I0315 03:55:05.170276 313060 finetune.py:68] layer 7_o @ epoch 0 new loss 1.4093192476138938e-05 old loss 1.441193944629049e-05 BETTER
 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.50s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.50s/it]I0315 03:55:31.279786 311534 finetune.py:68] layer 6_o @ epoch 2 new loss 1.0203792953689117e-05 old loss 1.0282063158228993e-05 BETTER
 97%|█████████▋| 31/32 [00:46<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
I0315 03:55:38.266716 310007 finetune.py:68] layer 5_o @ epoch 4 new loss 6.517796919069951e-06 old loss 6.552199465659214e-06 BETTER
W0315 03:55:40.058286 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 03:55:40.754385 313060 finetune.py:68] layer 7_o @ epoch 1 new loss 1.3937450603407342e-05 old loss 1.4093192476138938e-05 BETTER
5_o proxy err 0.006712468806654215 tr(WHW.T) 1.8210787773132324
  0%|          | 0/32 [00:00<?, ?it/s]I0315 03:55:42.487339 308145 finetune.py:45] layer 4_up initial loss 1.3088745618006214e-05
W0315 03:55:42.487874 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:02<01:04,  2.08s/it]  6%|▋         | 2/32 [00:03<00:53,  1.77s/it]  9%|▉         | 3/32 [00:05<00:48,  1.67s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.59s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.58s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.57s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it] 41%|████      | 13/32 [00:20<00:29,  1.56s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:25,  1.56s/it]I0315 03:56:07.338636 311534 finetune.py:68] layer 6_o @ epoch 3 new loss 1.0138112884305883e-05 old loss 1.0203792953689117e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it]I0315 03:56:16.363299 313060 finetune.py:68] layer 7_o @ epoch 2 new loss 1.3818891602568328e-05 old loss 1.3937450603407342e-05 BETTER
 72%|███████▏  | 23/32 [00:36<00:13,  1.55s/it]I0315 03:56:17.853214 308145 finetune.py:68] layer 4_up @ epoch 0 new loss 1.2975083336641546e-05 old loss 1.3088745618006214e-05 BETTER
 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.56s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.55s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]100%|██████████| 32/32 [00:50<00:00,  1.54s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0315 03:56:39.907048 310007 finetune.py:45] layer 5_up initial loss 1.7664526239968836e-05
W0315 03:56:39.907419 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:56:43.635544 311534 finetune.py:68] layer 6_o @ epoch 4 new loss 1.0078075320052449e-05 old loss 1.0138112884305883e-05 BETTER
W0315 03:56:45.303884 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_o proxy err 0.008793761022388935 tr(WHW.T) 2.608874797821045
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it]I0315 03:56:51.751877 313060 finetune.py:68] layer 7_o @ epoch 3 new loss 1.3720386959903408e-05 old loss 1.3818891602568328e-05 BETTER
  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.61s/it]I0315 03:56:53.749039 308145 finetune.py:68] layer 4_up @ epoch 1 new loss 1.2909964425489306e-05 old loss 1.2975083336641546e-05 BETTER
 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.55s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:19<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.54s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 50%|█████     | 16/32 [00:25<00:24,  1.54s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it]I0315 03:57:13.919754 310007 finetune.py:68] layer 5_up @ epoch 0 new loss 1.7470072634750977e-05 old loss 1.7664526239968836e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.54s/it]I0315 03:57:27.090316 313060 finetune.py:68] layer 7_o @ epoch 4 new loss 1.3634850802191067e-05 old loss 1.3720386959903408e-05 BETTER
 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it]W0315 03:57:28.945395 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:42<00:08,  1.62s/it]I0315 03:57:30.150779 308145 finetune.py:68] layer 4_up @ epoch 2 new loss 1.2853621228714474e-05 old loss 1.2909964425489306e-05 BETTER
7_o proxy err 0.00827507022768259 tr(WHW.T) 3.801004409790039
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:43<00:06,  1.60s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.58s/it]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.57s/it]  6%|▋         | 2/32 [00:03<00:52,  1.76s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.56s/it]  9%|▉         | 3/32 [00:05<00:48,  1.68s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.61s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.60s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.59s/it] 25%|██▌       | 8/32 [00:13<00:38,  1.59s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it]I0315 03:57:44.840977 311534 finetune.py:45] layer 6_up initial loss 2.4275490432046354e-05
W0315 03:57:44.841331 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:16<00:34,  1.58s/it] 34%|███▍      | 11/32 [00:17<00:33,  1.58s/it]I0315 03:57:48.798444 310007 finetune.py:68] layer 5_up @ epoch 1 new loss 1.7349597328575328e-05 old loss 1.7470072634750977e-05 BETTER
 38%|███▊      | 12/32 [00:19<00:31,  1.58s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.57s/it] 47%|████▋     | 15/32 [00:24<00:26,  1.57s/it] 50%|█████     | 16/32 [00:25<00:25,  1.57s/it] 53%|█████▎    | 17/32 [00:27<00:23,  1.58s/it] 56%|█████▋    | 18/32 [00:28<00:22,  1.57s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.57s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.57s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.57s/it] 69%|██████▉   | 22/32 [00:35<00:15,  1.57s/it]I0315 03:58:06.170385 308145 finetune.py:68] layer 4_up @ epoch 3 new loss 1.279681600863114e-05 old loss 1.2853621228714474e-05 BETTER
 72%|███████▏  | 23/32 [00:36<00:14,  1.57s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.57s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.56s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.56s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.55s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.55s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it]I0315 03:58:19.488729 311534 finetune.py:68] layer 6_up @ epoch 0 new loss 2.3983779101399705e-05 old loss 2.4275490432046354e-05 BETTER
100%|██████████| 32/32 [00:50<00:00,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]
I0315 03:58:24.042891 310007 finetune.py:68] layer 5_up @ epoch 2 new loss 1.724392313917633e-05 old loss 1.7349597328575328e-05 BETTER
I0315 03:58:29.511556 313060 finetune.py:45] layer 7_up initial loss 2.9489539883797988e-05
W0315 03:58:29.511985 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 03:58:42.201592 308145 finetune.py:68] layer 4_up @ epoch 4 new loss 1.2745700587402098e-05 old loss 1.279681600863114e-05 BETTER
W0315 03:58:43.828079 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_up proxy err 0.011065896600484848 tr(WHW.T) 400.2276611328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it]I0315 03:58:54.758679 311534 finetune.py:68] layer 6_up @ epoch 1 new loss 2.3786815290804952e-05 old loss 2.3983779101399705e-05 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it]I0315 03:58:59.237454 310007 finetune.py:68] layer 5_up @ epoch 3 new loss 1.714147765596863e-05 old loss 1.724392313917633e-05 BETTER
 28%|██▊       | 9/32 [00:14<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.50s/it]I0315 03:59:02.521550 313060 finetune.py:68] layer 7_up @ epoch 0 new loss 2.9136725061107427e-05 old loss 2.9489539883797988e-05 BETTER
 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it] 41%|████      | 13/32 [00:20<00:28,  1.50s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.50s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it]I0315 03:59:29.617044 311534 finetune.py:68] layer 6_up @ epoch 2 new loss 2.3612019504071213e-05 old loss 2.3786815290804952e-05 BETTER
 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0315 03:59:33.923776 310007 finetune.py:68] layer 5_up @ epoch 4 new loss 1.7049829693860374e-05 old loss 1.714147765596863e-05 BETTER
W0315 03:59:35.544327 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 03:59:36.277694 313060 finetune.py:68] layer 7_up @ epoch 1 new loss 2.8887548978673294e-05 old loss 2.9136725061107427e-05 BETTER
5_up proxy err 0.010805248282849789 tr(WHW.T) 497.26190185546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.97s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it]I0315 03:59:42.207098 308145 finetune.py:45] layer 4_gate initial loss 1.7436988855479285e-05
W0315 03:59:42.207542 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it]I0315 04:00:04.165862 311534 finetune.py:68] layer 6_up @ epoch 3 new loss 2.344519270991441e-05 old loss 2.3612019504071213e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it]I0315 04:00:09.871035 313060 finetune.py:68] layer 7_up @ epoch 2 new loss 2.866309841920156e-05 old loss 2.8887548978673294e-05 BETTER
 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it]I0315 04:00:15.889201 308145 finetune.py:68] layer 4_gate @ epoch 0 new loss 1.731932752591092e-05 old loss 1.7436988855479285e-05 BETTER
 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0315 04:00:34.803572 310007 finetune.py:45] layer 5_gate initial loss 2.3789247279637493e-05
W0315 04:00:34.803980 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:00:38.659468 311534 finetune.py:68] layer 6_up @ epoch 4 new loss 2.3294920538319275e-05 old loss 2.344519270991441e-05 BETTER
W0315 04:00:40.282922 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

6_up proxy err 0.010211625136435032 tr(WHW.T) 571.6026611328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  2.00s/it]I0315 04:00:43.666110 313060 finetune.py:68] layer 7_up @ epoch 3 new loss 2.8467544325394556e-05 old loss 2.866309841920156e-05 BETTER
  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it]I0315 04:00:50.310711 308145 finetune.py:68] layer 4_gate @ epoch 1 new loss 1.7266964277951047e-05 old loss 1.731932752591092e-05 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it]I0315 04:01:07.065143 310007 finetune.py:68] layer 5_gate @ epoch 0 new loss 2.3610364223713987e-05 old loss 2.3789247279637493e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it]I0315 04:01:17.582215 313060 finetune.py:68] layer 7_up @ epoch 4 new loss 2.8279522666707635e-05 old loss 2.8467544325394556e-05 BETTER
 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it]W0315 04:01:19.017986 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

7_up proxy err 0.009518045000731945 tr(WHW.T) 651.335205078125
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]I0315 04:01:24.785935 308145 finetune.py:68] layer 4_gate @ epoch 2 new loss 1.7218562788912095e-05 old loss 1.7266964277951047e-05 BETTER
 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it]  9%|▉         | 3/32 [00:05<00:47,  1.65s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.61s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.59s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.58s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.57s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it]I0315 04:01:39.270684 311534 finetune.py:45] layer 6_gate initial loss 3.1449970265384763e-05
W0315 04:01:39.271157 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it]I0315 04:01:40.405877 310007 finetune.py:68] layer 5_gate @ epoch 1 new loss 2.3511680410592817e-05 old loss 2.3610364223713987e-05 BETTER
 41%|████      | 13/32 [00:20<00:29,  1.56s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:24,  1.56s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.56s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.56s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it]I0315 04:01:59.030523 308145 finetune.py:68] layer 4_gate @ epoch 3 new loss 1.7172953448607586e-05 old loss 1.7218562788912095e-05 BETTER
 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.56s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.55s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0315 04:02:11.819056 311534 finetune.py:68] layer 6_gate @ epoch 0 new loss 3.1198251235764474e-05 old loss 3.1449970265384763e-05 BETTER
I0315 04:02:13.411835 310007 finetune.py:68] layer 5_gate @ epoch 2 new loss 2.3421835066983476e-05 old loss 2.3511680410592817e-05 BETTER
I0315 04:02:18.679018 313060 finetune.py:45] layer 7_gate initial loss 3.789072798099369e-05
W0315 04:02:18.679327 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:02:33.207590 308145 finetune.py:68] layer 4_gate @ epoch 4 new loss 1.712894118099939e-05 old loss 1.7172953448607586e-05 BETTER
W0315 04:02:34.503478 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

4_gate proxy err 0.004986640531569719 tr(WHW.T) 1577.7347412109375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:38,  1.12it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s]  3%|▎         | 3/112 [00:01<00:53,  2.03it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s]  5%|▌         | 6/112 [00:02<00:43,  2.44it/s]  6%|▋         | 7/112 [00:03<00:41,  2.50it/s]  7%|▋         | 8/112 [00:03<00:40,  2.55it/s]  8%|▊         | 9/112 [00:03<00:40,  2.57it/s]  9%|▉         | 10/112 [00:04<00:39,  2.60it/s] 10%|▉         | 11/112 [00:04<00:38,  2.62it/s] 11%|█         | 12/112 [00:05<00:38,  2.63it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.64it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.63it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.64it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.59it/s]I0315 04:02:44.948968 311534 finetune.py:68] layer 6_gate @ epoch 1 new loss 3.104386996710673e-05 old loss 3.1198251235764474e-05 BETTER
 16%|█▌        | 18/112 [00:07<00:35,  2.61it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.62it/s] 18%|█▊        | 20/112 [00:08<00:40,  2.30it/s] 19%|█▉        | 21/112 [00:08<00:37,  2.40it/s]I0315 04:02:46.528169 310007 finetune.py:68] layer 5_gate @ epoch 3 new loss 2.3338503524428234e-05 old loss 2.3421835066983476e-05 BETTER
 20%|█▉        | 22/112 [00:09<00:36,  2.47it/s] 21%|██        | 23/112 [00:09<00:35,  2.52it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.56it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.59it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.61it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.62it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.61it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.63it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s]I0315 04:02:50.615968 313060 finetune.py:68] layer 7_gate @ epoch 0 new loss 3.7607176636811346e-05 old loss 3.789072798099369e-05 BETTER
 29%|██▉       | 33/112 [00:13<00:29,  2.65it/s] 30%|███       | 34/112 [00:13<00:29,  2.65it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.65it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.65it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.66it/s] 34%|███▍      | 38/112 [00:15<00:27,  2.66it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.65it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.61it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.63it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.63it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.64it/s] 40%|████      | 45/112 [00:17<00:25,  2.65it/s] 41%|████      | 46/112 [00:18<00:24,  2.65it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.65it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.66it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.67it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.67it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.66it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.67it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.67it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.63it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.63it/s] 50%|█████     | 56/112 [00:21<00:21,  2.63it/s] 51%|█████     | 57/112 [00:22<00:20,  2.65it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.64it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.63it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.64it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.64it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.62it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.61it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.62it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.61it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.61it/s] 61%|██████    | 68/112 [00:26<00:16,  2.63it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.65it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.66it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.66it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.66it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.66it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.68it/s] 67%|██████▋   | 75/112 [00:29<00:13,  2.67it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.65it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.65it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.64it/s] 71%|███████   | 79/112 [00:30<00:12,  2.61it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.62it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.63it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.63it/s] 74%|███████▍  | 83/112 [00:32<00:10,  2.64it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.64it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.65it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.65it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.65it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.64it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.63it/s] 80%|████████  | 90/112 [00:34<00:08,  2.62it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.60it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.62it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.62it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.63it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.64it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.64it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.65it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.65it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.65it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.65it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.64it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.64it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.64it/s]I0315 04:03:17.680629 311534 finetune.py:68] layer 6_gate @ epoch 2 new loss 3.090574318775907e-05 old loss 3.104386996710673e-05 BETTER
 93%|█████████▎| 104/112 [00:40<00:03,  2.62it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.62it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.63it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.65it/s]I0315 04:03:19.181936 310007 finetune.py:68] layer 5_gate @ epoch 4 new loss 2.325749483134132e-05 old loss 2.3338503524428234e-05 BETTER
 96%|█████████▋| 108/112 [00:41<00:01,  2.66it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.67it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.67it/s]W0315 04:03:20.342747 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 99%|█████████▉| 111/112 [00:42<00:00,  2.67it/s]100%|██████████| 112/112 [00:43<00:00,  2.68it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]
I0315 04:03:22.831461 313060 finetune.py:68] layer 7_gate @ epoch 1 new loss 3.7428599171107635e-05 old loss 3.7607176636811346e-05 BETTER
5_gate proxy err 0.004779677838087082 tr(WHW.T) 1971.638671875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s]  2%|▏         | 2/112 [00:01<01:04,  1.71it/s]  3%|▎         | 3/112 [00:01<00:53,  2.04it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s]  5%|▌         | 6/112 [00:02<00:43,  2.46it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s]  7%|▋         | 8/112 [00:03<00:41,  2.53it/s]  8%|▊         | 9/112 [00:03<00:40,  2.54it/s]  9%|▉         | 10/112 [00:04<00:39,  2.56it/s]W0315 04:03:28.314000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.314000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.314000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.314000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.314000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.315000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.315000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.356000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.356000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.357000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.357000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.357000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.373000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.373000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.373000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.373000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.373000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 10%|▉         | 11/112 [00:04<00:39,  2.57it/s]W0315 04:03:28.539000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.539000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.539000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.539000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.539000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 11%|█         | 12/112 [00:05<00:38,  2.57it/s]W0315 04:03:28.876000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.876000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.876000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.876000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.876000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.877000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.877000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.911000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.911000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.911000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.911000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.911000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.985000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.985000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.985000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.985000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:28.985000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
 12%|█▏        | 13/112 [00:05<00:38,  2.56it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.57it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.58it/s]W0315 04:03:30.209000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.223000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.231000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.232000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s]W0315 04:03:30.695000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.695000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.696000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.696000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.696000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.696000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.696000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.729000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.729000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.729000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.729000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:30.730000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s]W0315 04:03:31.083000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.083000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.083000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.083000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.083000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.083000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.083000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.083000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s]W0315 04:03:31.394000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.394000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.395000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.395000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.395000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 17%|█▋        | 19/112 [00:07<00:35,  2.61it/s]W0315 04:03:31.741000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:03:31.747000 139824637314880 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 18%|█▊        | 20/112 [00:08<00:35,  2.60it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.60it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.59it/s] 21%|██        | 23/112 [00:09<00:34,  2.59it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.60it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.57it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.57it/s] 24%|██▍       | 27/112 [00:10<00:33,  2.57it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.58it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.58it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.58it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.58it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s] 30%|███       | 34/112 [00:13<00:30,  2.58it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.58it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.57it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.55it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.56it/s]I0315 04:03:38.981656 308145 finetune.py:45] layer 4_down initial loss 2.7211204724153504e-05
W0315 04:03:38.982078 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 35%|███▍      | 39/112 [00:15<00:28,  2.57it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.57it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.58it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.58it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.58it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 41%|████      | 46/112 [00:18<00:25,  2.58it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.58it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.55it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.56it/s] 45%|████▍     | 50/112 [00:19<00:24,  2.57it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.57it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.58it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.58it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.59it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.59it/s] 50%|█████     | 56/112 [00:22<00:21,  2.59it/s] 51%|█████     | 57/112 [00:22<00:21,  2.59it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.59it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.59it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.56it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.57it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.58it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.59it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.60it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.60it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.60it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.60it/s] 61%|██████    | 68/112 [00:26<00:16,  2.61it/s]I0315 04:03:50.640611 311534 finetune.py:68] layer 6_gate @ epoch 3 new loss 3.0776296625845134e-05 old loss 3.090574318775907e-05 BETTER
 62%|██████▏   | 69/112 [00:27<00:16,  2.61it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.60it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.59it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.57it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.58it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.59it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.59it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.60it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.60it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.60it/s] 71%|███████   | 79/112 [00:31<00:12,  2.60it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.61it/s]I0315 04:03:55.314785 313060 finetune.py:68] layer 7_gate @ epoch 2 new loss 3.725705755641684e-05 old loss 3.7428599171107635e-05 BETTER
 72%|███████▏  | 81/112 [00:31<00:11,  2.61it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.60it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.59it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.56it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.57it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.58it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.59it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.60it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.61it/s] 80%|████████  | 90/112 [00:35<00:08,  2.61it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.61it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.61it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.61it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.61it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.60it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.56it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.58it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.58it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.59it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.60it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.60it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.60it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.58it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.59it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.59it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.60it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.60it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.57it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.58it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.56it/s]
I0315 04:04:10.720441 308145 finetune.py:68] layer 4_down @ epoch 0 new loss 2.7202841010876e-05 old loss 2.7211204724153504e-05 BETTER
W0315 04:04:14.877000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.877000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.877000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.877000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.878000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.878000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.878000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.922000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.922000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.922000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.922000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.922000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.938000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.938000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.939000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.939000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:14.939000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.115000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.115000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.115000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.115000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.115000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.465000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.466000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.466000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.466000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.466000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.466000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.466000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.502000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.502000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.503000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.503000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.503000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.577000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.577000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.577000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.577000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:15.577000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:16.814000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:16.821000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:16.827000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:04:16.827000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.298000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.299000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.299000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.299000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.299000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.299000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.299000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.329000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.329000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.329000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.329000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.329000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.689000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.689000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.689000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.689000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.689000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.690000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.690000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.690000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.997000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.997000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.997000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.997000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:04:17.997000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:04:18.347000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:04:18.352000 139708419016512 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 04:04:23.865178 311534 finetune.py:68] layer 6_gate @ epoch 4 new loss 3.065043347305618e-05 old loss 3.0776296625845134e-05 BETTER
W0315 04:04:25.194852 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 04:04:25.563211 310007 finetune.py:45] layer 5_down initial loss 3.741097680176608e-05
W0315 04:04:25.563626 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:04:27.594573 313060 finetune.py:68] layer 7_gate @ epoch 3 new loss 3.709894008352421e-05 old loss 3.725705755641684e-05 BETTER
6_gate proxy err 0.004021652042865753 tr(WHW.T) 2574.89794921875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s]  3%|▎         | 3/112 [00:01<00:54,  2.00it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s]  4%|▍         | 5/112 [00:02<00:46,  2.33it/s]  5%|▌         | 6/112 [00:02<00:43,  2.41it/s]  6%|▋         | 7/112 [00:03<00:42,  2.47it/s]  7%|▋         | 8/112 [00:03<00:41,  2.51it/s]  8%|▊         | 9/112 [00:03<00:40,  2.53it/s]  9%|▉         | 10/112 [00:04<00:40,  2.55it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 11%|█         | 12/112 [00:05<00:39,  2.55it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.56it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.58it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.59it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.59it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.59it/s] 21%|██        | 23/112 [00:09<00:34,  2.56it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.57it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.58it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.59it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.59it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.60it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.60it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.60it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.60it/s] 30%|███       | 34/112 [00:13<00:30,  2.59it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.57it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.58it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.59it/s]I0315 04:04:43.347429 308145 finetune.py:68] layer 4_down @ epoch 1 new loss 2.719963413255755e-05 old loss 2.7202841010876e-05 BETTER
 34%|███▍      | 38/112 [00:15<00:28,  2.59it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.59it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.60it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.60it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.60it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.60it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.54it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.56it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.58it/s] 45%|████▍     | 50/112 [00:19<00:24,  2.58it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.59it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.60it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.60it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.59it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.59it/s] 50%|█████     | 56/112 [00:22<00:21,  2.59it/s] 51%|█████     | 57/112 [00:22<00:21,  2.59it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.60it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.56it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.57it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.58it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.58it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.59it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.59it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.60it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.60it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.60it/s] 61%|██████    | 68/112 [00:26<00:17,  2.58it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.57it/s] 63%|██████▎   | 71/112 [00:27<00:16,  2.53it/s]I0315 04:04:56.387984 310007 finetune.py:68] layer 5_down @ epoch 0 new loss 3.7398738641059026e-05 old loss 3.741097680176608e-05 BETTER
 64%|██████▍   | 72/112 [00:28<00:15,  2.54it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.55it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.56it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.57it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.57it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.58it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.58it/s] 71%|███████   | 79/112 [00:31<00:12,  2.58it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.58it/s] 72%|███████▏  | 81/112 [00:31<00:12,  2.57it/s]I0315 04:05:00.278394 313060 finetune.py:68] layer 7_gate @ epoch 4 new loss 3.695286432048306e-05 old loss 3.709894008352421e-05 BETTER
 73%|███████▎  | 82/112 [00:32<00:11,  2.57it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.54it/s]W0315 04:05:01.391963 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 84/112 [00:33<00:10,  2.55it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.56it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.56it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.56it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.57it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.57it/s] 80%|████████  | 90/112 [00:35<00:08,  2.57it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.57it/s]7_gate proxy err 0.003915996756404638 tr(WHW.T) 2631.195556640625
  0%|          | 0/112 [00:00<?, ?it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.57it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.57it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s] 84%|████████▍ | 94/112 [00:36<00:07,  2.54it/s]  2%|▏         | 2/112 [00:01<01:04,  1.70it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.56it/s]  3%|▎         | 3/112 [00:01<00:54,  2.01it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.57it/s]  4%|▎         | 4/112 [00:02<00:49,  2.19it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.57it/s]  4%|▍         | 5/112 [00:02<00:46,  2.30it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.57it/s]  5%|▌         | 6/112 [00:02<00:44,  2.38it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.58it/s]  6%|▋         | 7/112 [00:03<00:43,  2.44it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.59it/s]  7%|▋         | 8/112 [00:03<00:42,  2.47it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.60it/s]  8%|▊         | 9/112 [00:04<00:41,  2.49it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.60it/s]  9%|▉         | 10/112 [00:04<00:40,  2.49it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.60it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.59it/s] 10%|▉         | 11/112 [00:04<00:40,  2.50it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.58it/s] 11%|█         | 12/112 [00:05<00:39,  2.51it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.52it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.55it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.56it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.52it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.56it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.53it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.57it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.54it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.57it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.55it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.57it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.55it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.55it/s]
 17%|█▋        | 19/112 [00:07<00:36,  2.55it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.55it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.54it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.51it/s] 21%|██        | 23/112 [00:09<00:35,  2.53it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.53it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.54it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.54it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.54it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.55it/s]I0315 04:05:16.133316 308145 finetune.py:68] layer 4_down @ epoch 2 new loss 2.7198355383006856e-05 old loss 2.719963413255755e-05 BETTER
 26%|██▌       | 29/112 [00:11<00:32,  2.55it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.55it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.54it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.54it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.51it/s] 30%|███       | 34/112 [00:13<00:30,  2.52it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.53it/s] 32%|███▏      | 36/112 [00:14<00:30,  2.53it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.53it/s]W0315 04:05:19.561000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.561000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.561000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.561000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.562000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.562000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.562000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.607000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.607000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.607000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.608000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.608000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.624000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.624000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.624000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.624000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.624000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.798000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.798000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.799000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.799000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:19.799000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 38/112 [00:15<00:29,  2.53it/s]W0315 04:05:20.146000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.146000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.147000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.147000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.147000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.147000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.147000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.181000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.181000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.181000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.181000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.181000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 35%|███▍      | 39/112 [00:15<00:28,  2.53it/s]W0315 04:05:20.254000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.254000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.254000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.254000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:20.254000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 36%|███▌      | 40/112 [00:16<00:28,  2.54it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.54it/s] 38%|███▊      | 42/112 [00:17<00:27,  2.53it/s]W0315 04:05:21.474000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.486000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.494000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.494000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 43/112 [00:17<00:27,  2.53it/s]W0315 04:05:21.949000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.949000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.950000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.950000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.950000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.950000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.950000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.981000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.981000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.982000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.982000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:21.982000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
 39%|███▉      | 44/112 [00:17<00:26,  2.53it/s]W0315 04:05:22.337000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.337000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.337000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.337000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.338000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.338000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.338000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.338000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 40%|████      | 45/112 [00:18<00:26,  2.51it/s]W0315 04:05:22.638000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.639000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.639000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.639000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.639000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.981000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:22.986000 139773119948608 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 41%|████      | 46/112 [00:18<00:26,  2.52it/s] 42%|████▏     | 47/112 [00:19<00:25,  2.52it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.53it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.53it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.54it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.53it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.53it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.53it/s] 48%|████▊     | 54/112 [00:21<00:23,  2.52it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.51it/s] 50%|█████     | 56/112 [00:22<00:22,  2.46it/s] 51%|█████     | 57/112 [00:23<00:22,  2.48it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.49it/s]I0315 04:05:27.986184 310007 finetune.py:68] layer 5_down @ epoch 1 new loss 3.7393521779449657e-05 old loss 3.7398738641059026e-05 BETTER
 53%|█████▎    | 59/112 [00:23<00:21,  2.49it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.50it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.51it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.51it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.52it/s] 57%|█████▋    | 64/112 [00:25<00:19,  2.52it/s]I0315 04:05:30.226974 311534 finetune.py:45] layer 6_down initial loss 4.778776201419532e-05
W0315 04:05:30.227343 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 58%|█████▊    | 65/112 [00:26<00:18,  2.52it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.51it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.51it/s] 61%|██████    | 68/112 [00:27<00:17,  2.48it/s] 62%|██████▏   | 69/112 [00:27<00:17,  2.49it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.50it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.50it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.50it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.51it/s] 66%|██████▌   | 74/112 [00:29<00:15,  2.51it/s] 67%|██████▋   | 75/112 [00:30<00:14,  2.52it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.52it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.51it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.51it/s] 71%|███████   | 79/112 [00:31<00:13,  2.51it/s] 71%|███████▏  | 80/112 [00:32<00:12,  2.49it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.50it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.51it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.51it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.51it/s] 76%|███████▌  | 85/112 [00:34<00:10,  2.52it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.53it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.53it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.52it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.53it/s] 80%|████████  | 90/112 [00:36<00:08,  2.52it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.49it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.51it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.51it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.51it/s] 85%|████████▍ | 95/112 [00:38<00:06,  2.52it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.51it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.52it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.53it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.52it/s] 89%|████████▉ | 100/112 [00:40<00:04,  2.52it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.52it/s] 91%|█████████ | 102/112 [00:40<00:04,  2.49it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.50it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.51it/s] 94%|█████████▍| 105/112 [00:42<00:02,  2.51it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.52it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.52it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.53it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.53it/s] 98%|█████████▊| 110/112 [00:44<00:00,  2.53it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.53it/s]I0315 04:05:48.896821 308145 finetune.py:68] layer 4_down @ epoch 3 new loss 2.7195263101020828e-05 old loss 2.7198355383006856e-05 BETTER
100%|██████████| 112/112 [00:44<00:00,  2.52it/s]100%|██████████| 112/112 [00:44<00:00,  2.50it/s]
W0315 04:05:56.581000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.581000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.581000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.581000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.581000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.582000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.582000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.627000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.627000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.628000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.628000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.628000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.644000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.644000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.644000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.644000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.644000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.820000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.820000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.820000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.820000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:56.821000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.156000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.156000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.156000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.157000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.157000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.157000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.157000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.189000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.189000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.189000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.189000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.190000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.263000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.263000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.263000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.263000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:57.263000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.493000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.508000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.516000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.517000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.986000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.986000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.986000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.986000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.986000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.987000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:58.987000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.020000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.020000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.020000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.020000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.020000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.384000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.384000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.384000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.384000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.384000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.385000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.385000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.385000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.710000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.710000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.710000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.710000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:05:59.710000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:06:00.060000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:06:00.066000 139808533792576 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 04:06:00.076818 310007 finetune.py:68] layer 5_down @ epoch 2 new loss 3.739132807822898e-05 old loss 3.7393521779449657e-05 BETTER
I0315 04:06:01.431001 311534 finetune.py:68] layer 6_down @ epoch 0 new loss 4.7770721721462905e-05 old loss 4.778776201419532e-05 BETTER
I0315 04:06:07.132696 313060 finetune.py:45] layer 7_down initial loss 5.621639866149053e-05
W0315 04:06:07.133158 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:06:21.744875 308145 finetune.py:68] layer 4_down @ epoch 4 new loss 2.7192614652449265e-05 old loss 2.7195263101020828e-05 BETTER
W0315 04:06:22.605077 308145 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

4_down proxy err 0.01250614132732153 tr(WHW.T) 3.4265825748443604
I0315 04:06:32.627134 310007 finetune.py:68] layer 5_down @ epoch 3 new loss 3.738799205166288e-05 old loss 3.739132807822898e-05 BETTER
I0315 04:06:33.843405 311534 finetune.py:68] layer 6_down @ epoch 1 new loss 4.776442074216902e-05 old loss 4.7770721721462905e-05 BETTER
I0315 04:06:37.934123 313060 finetune.py:68] layer 7_down @ epoch 0 new loss 5.619594958261587e-05 old loss 5.621639866149053e-05 BETTER
I0315 04:07:04.954861 310007 finetune.py:68] layer 5_down @ epoch 4 new loss 3.73848233721219e-05 old loss 3.738799205166288e-05 BETTER
W0315 04:07:05.784365 310007 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

I0315 04:07:06.165533 311534 finetune.py:76] layer 6_down @ epoch 2 new loss 4.7765381168574095e-05 old loss 4.776442074216902e-05 WORSE
5_down proxy err 0.012275062501430511 tr(WHW.T) 4.920976161956787
I0315 04:07:09.428797 313060 finetune.py:68] layer 7_down @ epoch 1 new loss 5.619122748612426e-05 old loss 5.619594958261587e-05 BETTER
I0315 04:07:37.579233 311534 finetune.py:68] layer 6_down @ epoch 3 new loss 4.775972047355026e-05 old loss 4.776442074216902e-05 BETTER
I0315 04:07:40.745412 313060 finetune.py:68] layer 7_down @ epoch 2 new loss 5.618435170617886e-05 old loss 5.619122748612426e-05 BETTER
I0315 04:08:09.414165 311534 finetune.py:68] layer 6_down @ epoch 4 new loss 4.775519118993543e-05 old loss 4.775972047355026e-05 BETTER
W0315 04:08:10.164596 311534 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

6_down proxy err 0.011758239008486271 tr(WHW.T) 6.245090484619141
I0315 04:08:11.886313 313060 finetune.py:68] layer 7_down @ epoch 3 new loss 5.6180291721830145e-05 old loss 5.618435170617886e-05 BETTER
I0315 04:08:16.479240 273992 quantize_finetune_llama.py:186] computed original embedding for layer 8 in 66.47678065299988s
I0315 04:08:16.896946 273992 quantize_finetune_llama.py:159] layer 9 gpu 1
I0315 04:08:18.898225 331922 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 04:08:18.898351 331922 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 04:08:18.898415 331922 utils.py:162] NumExpr defaulting to 16 threads.
I0315 04:08:19.089130 331922 config.py:58] PyTorch version 2.4.0 available.
I0315 04:08:21.328450 331922 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 04:08:21.665497 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:44,  1.45s/it]  6%|▋         | 2/32 [00:01<00:23,  1.26it/s]  9%|▉         | 3/32 [00:02<00:17,  1.69it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.04it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.81it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.83it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.86it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.90it/s] 41%|████      | 13/32 [00:05<00:06,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.95it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.92it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.93it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.96it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.98it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.96it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.96it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.97it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.97it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.97it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.97it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
W0315 04:08:37.726000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.726000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.726000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.726000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.727000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.727000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.727000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.754000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.755000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.755000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.755000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.755000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.772000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.772000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.772000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.773000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:08:37.773000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:08:38.115000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:08:38.115000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:08:38.115000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:08:38.115000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:08:38.115000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.052000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.052000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.052000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.052000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.052000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.052000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.052000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.071000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.071000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.071000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.071000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.071000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.326000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.327000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.327000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.327000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:08:39.327000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.562000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.562000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.562000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.562000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.562000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.563000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.563000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.581000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.581000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.581000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.582000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:08:40.582000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:08:41.538000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:08:41.538000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:08:41.538000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:08:41.538000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:08:41.539000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 04:08:42.938774 313060 finetune.py:76] layer 7_down @ epoch 4 new loss 5.6180728279286996e-05 old loss 5.6180291721830145e-05 WORSE
W0315 04:08:43.442074 313060 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

7_down proxy err 0.01211092434823513 tr(WHW.T) 6.822657585144043
I0315 04:08:47.974268 331922 finetune.py:45] layer 8_v initial loss 8.226274985645432e-06
W0315 04:08:47.974625 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:09:19.450353 273992 quantize_finetune_llama.py:186] computed original embedding for layer 9 in 62.1065137386322s
I0315 04:09:19.846182 273992 quantize_finetune_llama.py:159] layer 10 gpu 2
I0315 04:09:21.923785 332492 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 04:09:21.923907 332492 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 04:09:21.923988 332492 utils.py:162] NumExpr defaulting to 16 threads.
I0315 04:09:22.115328 332492 config.py:58] PyTorch version 2.4.0 available.
I0315 04:09:24.226282 331922 finetune.py:68] layer 8_v @ epoch 0 new loss 4.889470801572315e-06 old loss 8.226274985645432e-06 BETTER
I0315 04:09:24.316659 332492 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 04:09:24.726736 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:47,  1.54s/it]  6%|▋         | 2/32 [00:01<00:24,  1.20it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.25it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.78it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.84it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.91it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.91it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.91it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.93it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.93it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.93it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.95it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.91it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.95it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.98it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.00it/s] 91%|█████████ | 29/32 [00:11<00:00,  3.02it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.02it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.94it/s]100%|██████████| 32/32 [00:12<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
W0315 04:09:40.955000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.955000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.956000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.956000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.956000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.956000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.956000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.982000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.982000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.982000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.983000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.983000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.999000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.999000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.999000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.999000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:09:40.999000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:09:41.328000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:09:41.328000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:09:41.328000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:09:41.328000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:09:41.329000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.244000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.244000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.244000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.244000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.245000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.245000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.245000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.263000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.263000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.263000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.264000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.264000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.519000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.519000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.519000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.519000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:09:42.519000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.735000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.735000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.735000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.735000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.735000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.735000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.736000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.754000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.754000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.754000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.754000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:09:43.754000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:09:44.675000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:09:44.675000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:09:44.675000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:09:44.675000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:09:44.675000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 04:09:51.302371 332492 finetune.py:45] layer 9_v initial loss 1.3663674508279655e-05
W0315 04:09:51.302625 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:10:01.458811 331922 finetune.py:68] layer 8_v @ epoch 1 new loss 4.540209374681581e-06 old loss 4.889470801572315e-06 BETTER
I0315 04:10:21.728987 273992 quantize_finetune_llama.py:186] computed original embedding for layer 10 in 61.44151329994202s
I0315 04:10:22.128049 273992 quantize_finetune_llama.py:159] layer 11 gpu 3
I0315 04:10:24.228044 333064 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 04:10:24.228246 333064 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 04:10:24.228312 333064 utils.py:162] NumExpr defaulting to 16 threads.
I0315 04:10:24.430186 333064 config.py:58] PyTorch version 2.4.0 available.
I0315 04:10:25.876450 332492 finetune.py:68] layer 9_v @ epoch 0 new loss 7.049552095850231e-06 old loss 1.3663674508279655e-05 BETTER
I0315 04:10:26.656606 333064 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 04:10:27.023812 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:50,  1.63s/it]  6%|▋         | 2/32 [00:01<00:26,  1.13it/s]  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.88it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.13it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.75it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.77it/s] 50%|█████     | 16/32 [00:07<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.79it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.81it/s]I0315 04:10:38.729066 331922 finetune.py:68] layer 8_v @ epoch 2 new loss 4.397739758132957e-06 old loss 4.540209374681581e-06 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.82it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.83it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.83it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
W0315 04:10:44.142000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.142000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.142000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.142000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.142000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.142000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.142000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.170000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.170000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.170000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.170000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.170000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.187000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.187000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.187000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.187000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.187000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.527000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.528000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.528000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.528000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:10:44.528000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.452000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.452000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.452000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.452000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.452000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.452000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.452000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.471000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.472000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.472000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.472000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.472000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.728000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.728000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.728000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.728000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:10:45.729000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.943000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.943000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.943000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.943000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.943000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.943000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.944000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.962000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.962000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.962000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.962000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:10:46.962000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:10:47.935000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:10:47.936000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:10:47.936000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:10:47.936000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:10:47.936000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 04:10:54.044358 333064 finetune.py:45] layer 10_v initial loss 1.156343932962045e-05
W0315 04:10:54.044940 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:11:01.533863 332492 finetune.py:68] layer 9_v @ epoch 1 new loss 6.400450274668401e-06 old loss 7.049552095850231e-06 BETTER
I0315 04:11:16.547262 331922 finetune.py:68] layer 8_v @ epoch 3 new loss 4.303434707253473e-06 old loss 4.397739758132957e-06 BETTER
I0315 04:11:24.159153 273992 quantize_finetune_llama.py:186] computed original embedding for layer 11 in 61.54555797576904s
I0315 04:11:24.602848 273992 quantize_finetune_llama.py:159] layer 12 gpu 0
I0315 04:11:26.709326 333636 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 04:11:26.709464 333636 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 04:11:26.709534 333636 utils.py:162] NumExpr defaulting to 16 threads.
I0315 04:11:26.921223 333636 config.py:58] PyTorch version 2.4.0 available.
I0315 04:11:28.889933 333064 finetune.py:68] layer 10_v @ epoch 0 new loss 6.6372431319905445e-06 old loss 1.156343932962045e-05 BETTER
I0315 04:11:29.279111 333636 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 04:11:29.689449 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:57,  1.87s/it]  6%|▋         | 2/32 [00:02<00:29,  1.02it/s]  9%|▉         | 3/32 [00:02<00:20,  1.45it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.79it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.07it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.28it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.65it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 41%|████      | 13/32 [00:06<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.83it/s]I0315 04:11:37.424873 332492 finetune.py:68] layer 9_v @ epoch 2 new loss 6.216957444848958e-06 old loss 6.400450274668401e-06 BETTER
 47%|████▋     | 15/32 [00:06<00:05,  2.85it/s] 50%|█████     | 16/32 [00:07<00:05,  2.86it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.86it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.86it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.86it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.87it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
W0315 04:11:46.649000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.649000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.650000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.650000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.650000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.650000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.650000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.677000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.677000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.677000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.677000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.677000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.694000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.695000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.695000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.695000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:11:46.695000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.028000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.028000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.028000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.028000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.028000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.946000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.946000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.947000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.947000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.947000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.947000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.947000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.965000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.966000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.966000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.966000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:11:47.966000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:11:48.216000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:11:48.216000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:11:48.216000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:11:48.216000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:11:48.216000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.427000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.427000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.427000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.427000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.427000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.427000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.427000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.445000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.445000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.446000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.446000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:11:49.446000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:11:50.387000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:11:50.387000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:11:50.387000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:11:50.387000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:11:50.387000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 04:11:54.356247 331922 finetune.py:68] layer 8_v @ epoch 4 new loss 4.287416231818497e-06 old loss 4.303434707253473e-06 BETTER
W0315 04:11:56.215185 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 04:11:56.771565 333636 finetune.py:45] layer 11_v initial loss 9.09021127881715e-06
W0315 04:11:56.771964 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

8_v proxy err 0.009063364937901497 tr(WHW.T) 53.42280197143555
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:35,  1.13s/it]  6%|▋         | 2/32 [00:01<00:20,  1.45it/s]  9%|▉         | 3/32 [00:01<00:15,  1.82it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s]I0315 04:12:04.621116 333064 finetune.py:68] layer 10_v @ epoch 1 new loss 6.165455488371663e-06 old loss 6.6372431319905445e-06 BETTER
 53%|█████▎    | 17/32 [00:07<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0315 04:12:13.702960 332492 finetune.py:68] layer 9_v @ epoch 3 new loss 6.096672677813331e-06 old loss 6.216957444848958e-06 BETTER
W0315 04:12:16.836000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.836000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.836000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.836000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.836000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.836000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.837000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.868000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.868000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.868000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.869000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.869000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.885000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.885000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.885000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.885000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:12:16.885000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.051000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.051000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.051000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.051000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.051000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.288000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.288000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.288000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.288000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.289000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.289000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.289000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.313000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.313000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.313000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.313000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.313000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.384000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.384000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.384000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.384000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:12:17.384000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.299000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.629000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.629000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.629000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.629000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.629000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.629000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.629000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.650000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.650000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.650000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.650000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.650000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.923000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.924000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.924000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.924000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:12:18.924000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:12:19.202000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 04:12:26.209719 331922 finetune.py:45] layer 8_q initial loss 8.265350516012404e-06
W0315 04:12:26.210107 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:12:30.588384 333636 finetune.py:68] layer 11_v @ epoch 0 new loss 5.73630723010865e-06 old loss 9.09021127881715e-06 BETTER
I0315 04:12:40.641836 333064 finetune.py:68] layer 10_v @ epoch 2 new loss 5.949118985881796e-06 old loss 6.165455488371663e-06 BETTER
I0315 04:12:50.168210 332492 finetune.py:76] layer 9_v @ epoch 4 new loss 6.157925781735685e-06 old loss 6.096672677813331e-06 WORSE
W0315 04:12:51.314872 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_v proxy err 0.009173627011477947 tr(WHW.T) 72.69827270507812
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:15,  1.91it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s]I0315 04:13:02.939153 331922 finetune.py:68] layer 8_q @ epoch 0 new loss 7.98974451754475e-06 old loss 8.265350516012404e-06 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0315 04:13:05.780771 333636 finetune.py:68] layer 11_v @ epoch 1 new loss 5.37151981916395e-06 old loss 5.73630723010865e-06 BETTER
W0315 04:13:11.701000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.701000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.702000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.702000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.702000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.702000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.702000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.732000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.733000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.733000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.733000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.733000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.749000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.749000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.749000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.749000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.749000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.912000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.912000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.912000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.912000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:13:11.912000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.146000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.146000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.146000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.147000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.147000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.147000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.147000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.169000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.169000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.170000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.170000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.170000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.236000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.236000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.236000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.237000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:13:12.237000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.128000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.451000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.451000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.451000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.451000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.451000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.452000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.452000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.473000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.473000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.473000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.473000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.473000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.737000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.737000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.737000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.737000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:13:13.737000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:13:14.007000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 04:13:16.815499 333064 finetune.py:68] layer 10_v @ epoch 3 new loss 5.870111181138782e-06 old loss 5.949118985881796e-06 BETTER
I0315 04:13:20.871672 332492 finetune.py:45] layer 9_q initial loss 1.0493213267182e-05
W0315 04:13:20.872048 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:13:40.515496 331922 finetune.py:68] layer 8_q @ epoch 1 new loss 7.796375939506106e-06 old loss 7.98974451754475e-06 BETTER
I0315 04:13:41.343351 333636 finetune.py:68] layer 11_v @ epoch 2 new loss 5.2276636779424734e-06 old loss 5.37151981916395e-06 BETTER
I0315 04:13:53.236456 333064 finetune.py:68] layer 10_v @ epoch 4 new loss 5.7832171478366945e-06 old loss 5.870111181138782e-06 BETTER
W0315 04:13:55.038884 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 04:13:55.967202 332492 finetune.py:68] layer 9_q @ epoch 0 new loss 1.0052568541141227e-05 old loss 1.0493213267182e-05 BETTER
10_v proxy err 0.00860156212002039 tr(WHW.T) 60.08286666870117
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.03s/it]  6%|▋         | 2/32 [00:01<00:19,  1.52it/s]  9%|▉         | 3/32 [00:01<00:15,  1.86it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.39it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.54it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.55it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.54it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.55it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.55it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.54it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.55it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.54it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.55it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:13<00:00,  2.56it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
W0315 04:14:15.800000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.800000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.800000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.801000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.801000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.801000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.801000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.832000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.832000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.832000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.832000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.832000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.848000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.848000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.848000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.848000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:14:15.848000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.016000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.016000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.016000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.016000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.016000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.258000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.258000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.258000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.258000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.258000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.259000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.259000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.280000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.280000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.280000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.280000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.280000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.349000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.349000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.349000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.349000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:14:16.349000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
I0315 04:14:16.704838 333636 finetune.py:76] layer 11_v @ epoch 3 new loss 5.548287390411133e-06 old loss 5.2276636779424734e-06 WORSE
W0315 04:14:17.276000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.607000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.607000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.607000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.607000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.607000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.607000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.607000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.631000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.631000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.631000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.631000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.631000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.902000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.903000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.903000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.903000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:14:17.903000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:14:18.180000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 04:14:18.376667 331922 finetune.py:68] layer 8_q @ epoch 2 new loss 7.683487638132647e-06 old loss 7.796375939506106e-06 BETTER
I0315 04:14:24.868754 333064 finetune.py:45] layer 10_q initial loss 1.0857966117328033e-05
W0315 04:14:24.869121 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:14:31.883669 332492 finetune.py:68] layer 9_q @ epoch 1 new loss 9.815298653848004e-06 old loss 1.0052568541141227e-05 BETTER
I0315 04:14:51.373037 333636 finetune.py:76] layer 11_v @ epoch 4 new loss 5.4307452046487015e-06 old loss 5.2276636779424734e-06 WORSE
W0315 04:14:52.574199 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_v proxy err 0.00707711698487401 tr(WHW.T) 74.2698974609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:31,  1.00s/it]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:15,  1.91it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s]I0315 04:14:56.066541 331922 finetune.py:68] layer 8_q @ epoch 3 new loss 7.582251782878302e-06 old loss 7.683487638132647e-06 BETTER
 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.60it/s]I0315 04:14:59.940257 333064 finetune.py:68] layer 10_q @ epoch 0 new loss 1.0410115464765113e-05 old loss 1.0857966117328033e-05 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.57it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.57it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0315 04:15:07.851011 332492 finetune.py:68] layer 9_q @ epoch 2 new loss 9.731496902531944e-06 old loss 9.815298653848004e-06 BETTER
W0315 04:15:13.205000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.205000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.206000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.206000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.206000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.206000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.206000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.238000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.238000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.238000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.238000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.238000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.254000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.254000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.255000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.255000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.255000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.418000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.419000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.419000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.419000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.419000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.660000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.660000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.660000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.660000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.660000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.660000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.661000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.685000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.685000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.686000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.686000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.686000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.754000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.754000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.755000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.755000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:15:13.755000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:15:14.684000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.009000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.009000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.010000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.010000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.010000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.010000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.010000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.031000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.032000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.032000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.032000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.032000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.300000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.300000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.300000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.300000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.300000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:15:15.574000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 04:15:22.179837 333636 finetune.py:45] layer 11_q initial loss 1.065744163497584e-05
W0315 04:15:22.180368 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:15:34.509960 331922 finetune.py:68] layer 8_q @ epoch 4 new loss 7.50125491322251e-06 old loss 7.582251782878302e-06 BETTER
W0315 04:15:36.326128 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 04:15:36.501797 333064 finetune.py:68] layer 10_q @ epoch 1 new loss 1.0175514034926891e-05 old loss 1.0410115464765113e-05 BETTER
8_q proxy err 0.0016410760581493378 tr(WHW.T) 5516.1103515625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:17,  1.67it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s]I0315 04:15:44.057570 332492 finetune.py:68] layer 9_q @ epoch 3 new loss 9.578322533343453e-06 old loss 9.731496902531944e-06 BETTER
 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.71it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
I0315 04:15:56.936388 333636 finetune.py:68] layer 11_q @ epoch 0 new loss 1.0291093531122897e-05 old loss 1.065744163497584e-05 BETTER
I0315 04:15:56.943351 331922 finetune.py:45] layer 8_k initial loss 9.974686690839007e-06
W0315 04:15:56.943943 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:16:13.071561 333064 finetune.py:68] layer 10_q @ epoch 2 new loss 1.0027252756117377e-05 old loss 1.0175514034926891e-05 BETTER
I0315 04:16:20.859668 332492 finetune.py:76] layer 9_q @ epoch 4 new loss 9.59948738454841e-06 old loss 9.578322533343453e-06 WORSE
W0315 04:16:22.022478 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_q proxy err 0.0017365084495395422 tr(WHW.T) 5307.87451171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]  6%|▋         | 2/32 [00:01<00:18,  1.63it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s]I0315 04:16:32.889964 333636 finetune.py:68] layer 11_q @ epoch 1 new loss 1.0107467460329644e-05 old loss 1.0291093531122897e-05 BETTER
 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s]I0315 04:16:33.937809 331922 finetune.py:68] layer 8_k @ epoch 0 new loss 9.459511602472048e-06 old loss 9.974686690839007e-06 BETTER
 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.66it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0315 04:16:43.449167 332492 finetune.py:45] layer 9_k initial loss 1.2827558748540469e-05
W0315 04:16:43.449668 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:16:49.542475 333064 finetune.py:68] layer 10_q @ epoch 3 new loss 9.93724006548291e-06 old loss 1.0027252756117377e-05 BETTER
I0315 04:17:08.782232 333636 finetune.py:68] layer 11_q @ epoch 2 new loss 1.0059660780825652e-05 old loss 1.0107467460329644e-05 BETTER
I0315 04:17:11.933407 331922 finetune.py:68] layer 8_k @ epoch 1 new loss 9.361260708828922e-06 old loss 9.459511602472048e-06 BETTER
I0315 04:17:19.210437 332492 finetune.py:68] layer 9_k @ epoch 0 new loss 1.1796119906648528e-05 old loss 1.2827558748540469e-05 BETTER
I0315 04:17:26.109920 333064 finetune.py:68] layer 10_q @ epoch 4 new loss 9.78779553406639e-06 old loss 9.93724006548291e-06 BETTER
W0315 04:17:27.797233 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_q proxy err 0.0018490044167265296 tr(WHW.T) 5569.5126953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.05it/s]  6%|▋         | 2/32 [00:01<00:18,  1.61it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0315 04:17:44.836279 333636 finetune.py:68] layer 11_q @ epoch 3 new loss 9.94614492810797e-06 old loss 1.0059660780825652e-05 BETTER
I0315 04:17:49.009646 333064 finetune.py:45] layer 10_k initial loss 1.2379330655676313e-05
W0315 04:17:49.010065 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:17:49.804848 331922 finetune.py:68] layer 8_k @ epoch 2 new loss 9.315737770521082e-06 old loss 9.361260708828922e-06 BETTER
I0315 04:17:55.882769 332492 finetune.py:68] layer 9_k @ epoch 1 new loss 1.166571382782422e-05 old loss 1.1796119906648528e-05 BETTER
I0315 04:18:20.645878 333636 finetune.py:68] layer 11_q @ epoch 4 new loss 9.8391510618967e-06 old loss 9.94614492810797e-06 BETTER
W0315 04:18:22.480125 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_q proxy err 0.0018606679514050484 tr(WHW.T) 5158.9169921875
  0%|          | 0/32 [00:00<?, ?it/s]I0315 04:18:24.716839 333064 finetune.py:68] layer 10_k @ epoch 0 new loss 1.2002983567072079e-05 old loss 1.2379330655676313e-05 BETTER
  3%|▎         | 1/32 [00:00<00:29,  1.04it/s]  6%|▋         | 2/32 [00:01<00:18,  1.62it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s]I0315 04:18:27.904878 331922 finetune.py:76] layer 8_k @ epoch 3 new loss 9.325843166152481e-06 old loss 9.315737770521082e-06 WORSE
 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.63it/s]I0315 04:18:32.340901 332492 finetune.py:68] layer 9_k @ epoch 2 new loss 1.1576667930057738e-05 old loss 1.166571382782422e-05 BETTER
 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.37it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.45it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0315 04:18:43.989005 333636 finetune.py:45] layer 11_k initial loss 1.2622644135262817e-05
W0315 04:18:43.989399 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:19:01.721969 333064 finetune.py:68] layer 10_k @ epoch 1 new loss 1.1953099601669237e-05 old loss 1.2002983567072079e-05 BETTER
I0315 04:19:05.634754 331922 finetune.py:68] layer 8_k @ epoch 4 new loss 9.265633707400411e-06 old loss 9.315737770521082e-06 BETTER
W0315 04:19:07.506820 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_k proxy err 0.0011943624122068286 tr(WHW.T) 4663.8251953125
  0%|          | 0/32 [00:00<?, ?it/s]I0315 04:19:09.317320 332492 finetune.py:68] layer 9_k @ epoch 3 new loss 1.1492745215946343e-05 old loss 1.1576667930057738e-05 BETTER
  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s]I0315 04:19:18.735734 333636 finetune.py:68] layer 11_k @ epoch 0 new loss 1.2052391866745893e-05 old loss 1.2622644135262817e-05 BETTER
 78%|███████▊  | 25/32 [00:09<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0315 04:19:29.179171 331922 finetune.py:45] layer 8_o initial loss 1.7059970559785143e-05
W0315 04:19:29.179557 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:19:38.305956 333064 finetune.py:68] layer 10_k @ epoch 2 new loss 1.1899938726855908e-05 old loss 1.1953099601669237e-05 BETTER
I0315 04:19:45.936890 332492 finetune.py:68] layer 9_k @ epoch 4 new loss 1.1477748557808809e-05 old loss 1.1492745215946343e-05 BETTER
W0315 04:19:47.620300 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

9_k proxy err 0.001308408216573298 tr(WHW.T) 4327.7998046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:13,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s]I0315 04:19:54.132917 333636 finetune.py:76] layer 11_k @ epoch 1 new loss 1.210965547215892e-05 old loss 1.2052391866745893e-05 WORSE
 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0315 04:20:06.222060 331922 finetune.py:68] layer 8_o @ epoch 0 new loss 1.665174204390496e-05 old loss 1.7059970559785143e-05 BETTER
I0315 04:20:09.133763 332492 finetune.py:45] layer 9_o initial loss 2.0858591597061604e-05
W0315 04:20:09.134140 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:20:14.903284 333064 finetune.py:68] layer 10_k @ epoch 3 new loss 1.1843238098663278e-05 old loss 1.1899938726855908e-05 BETTER
I0315 04:20:29.192199 333636 finetune.py:68] layer 11_k @ epoch 2 new loss 1.1973318578384351e-05 old loss 1.2052391866745893e-05 BETTER
I0315 04:20:44.446825 331922 finetune.py:68] layer 8_o @ epoch 1 new loss 1.647147291805595e-05 old loss 1.665174204390496e-05 BETTER
I0315 04:20:44.821789 332492 finetune.py:68] layer 9_o @ epoch 0 new loss 2.0282324840081856e-05 old loss 2.0858591597061604e-05 BETTER
I0315 04:20:51.505795 333064 finetune.py:68] layer 10_k @ epoch 4 new loss 1.1821982297988143e-05 old loss 1.1843238098663278e-05 BETTER
W0315 04:20:53.141848 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_k proxy err 0.0013198459055274725 tr(WHW.T) 4705.7119140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s]I0315 04:21:05.282087 333636 finetune.py:68] layer 11_k @ epoch 3 new loss 1.1819471183116548e-05 old loss 1.1973318578384351e-05 BETTER
 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0315 04:21:14.597549 333064 finetune.py:45] layer 10_o initial loss 2.0886647689621896e-05
W0315 04:21:14.597976 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:21:21.249597 332492 finetune.py:68] layer 9_o @ epoch 1 new loss 2.0051365936524235e-05 old loss 2.0282324840081856e-05 BETTER
I0315 04:21:21.996554 331922 finetune.py:68] layer 8_o @ epoch 2 new loss 1.6329819118254818e-05 old loss 1.647147291805595e-05 BETTER
I0315 04:21:40.859055 333636 finetune.py:76] layer 11_k @ epoch 4 new loss 1.186339977721218e-05 old loss 1.1819471183116548e-05 WORSE
W0315 04:21:42.042157 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

11_k proxy err 0.0015012832591310143 tr(WHW.T) 4183.07275390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:11,  2.22it/s] 25%|██▌       | 8/32 [00:03<00:11,  2.16it/s] 28%|██▊       | 9/32 [00:04<00:10,  2.25it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.35it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.41it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.46it/s] 41%|████      | 13/32 [00:05<00:07,  2.50it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.53it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s]I0315 04:21:50.398118 333064 finetune.py:68] layer 10_o @ epoch 0 new loss 2.0326742742327042e-05 old loss 2.0886647689621896e-05 BETTER
 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.55it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.55it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.55it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.52it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.50it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.49it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.47it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.47it/s]100%|██████████| 32/32 [00:13<00:00,  2.47it/s]100%|██████████| 32/32 [00:13<00:00,  2.42it/s]
I0315 04:21:58.486068 332492 finetune.py:68] layer 9_o @ epoch 2 new loss 1.986494316952303e-05 old loss 2.0051365936524235e-05 BETTER
I0315 04:21:59.963499 331922 finetune.py:68] layer 8_o @ epoch 3 new loss 1.6211541151278652e-05 old loss 1.6329819118254818e-05 BETTER
I0315 04:22:04.202635 333636 finetune.py:45] layer 11_o initial loss 2.134376518370118e-05
W0315 04:22:04.203265 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:22:27.340173 333064 finetune.py:68] layer 10_o @ epoch 1 new loss 2.009651143453084e-05 old loss 2.0326742742327042e-05 BETTER
I0315 04:22:35.852257 332492 finetune.py:68] layer 9_o @ epoch 3 new loss 1.971353958651889e-05 old loss 1.986494316952303e-05 BETTER
I0315 04:22:38.026610 331922 finetune.py:68] layer 8_o @ epoch 4 new loss 1.6109261196106672e-05 old loss 1.6211541151278652e-05 BETTER
I0315 04:22:39.600739 333636 finetune.py:68] layer 11_o @ epoch 0 new loss 2.084983862005174e-05 old loss 2.134376518370118e-05 BETTER
W0315 04:22:39.737969 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_o proxy err 0.010818978771567345 tr(WHW.T) 3.750805377960205
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  2.00s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.48s/it] 41%|████      | 13/32 [00:19<00:28,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it]I0315 04:23:03.608269 333064 finetune.py:68] layer 10_o @ epoch 2 new loss 1.9909110051230527e-05 old loss 2.009651143453084e-05 BETTER
 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 50%|█████     | 16/32 [00:24<00:23,  1.47s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it]I0315 04:23:12.816153 332492 finetune.py:68] layer 9_o @ epoch 4 new loss 1.9579456420615315e-05 old loss 1.971353958651889e-05 BETTER
 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it]W0315 04:23:14.439922 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 04:23:15.067054 333636 finetune.py:68] layer 11_o @ epoch 1 new loss 2.0603876691893674e-05 old loss 2.084983862005174e-05 BETTER
 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it]9_o proxy err 0.011002810671925545 tr(WHW.T) 4.459726810455322
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it]  3%|▎         | 1/32 [00:02<01:03,  2.03s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.47s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.48s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it]I0315 04:23:36.957744 331922 finetune.py:45] layer 8_up initial loss 3.353911961312406e-05
W0315 04:23:36.958100 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it]I0315 04:23:39.861104 333064 finetune.py:68] layer 10_o @ epoch 3 new loss 1.975667146325577e-05 old loss 1.9909110051230527e-05 BETTER
 50%|█████     | 16/32 [00:24<00:24,  1.54s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it]I0315 04:23:50.326796 333636 finetune.py:68] layer 11_o @ epoch 2 new loss 2.0418547137524e-05 old loss 2.0603876691893674e-05 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.57s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.57s/it] 84%|████████▍ | 27/32 [00:42<00:08,  1.65s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.65s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.66s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.66s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.66s/it]100%|██████████| 32/32 [00:50<00:00,  1.66s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]
I0315 04:24:12.751782 331922 finetune.py:68] layer 8_up @ epoch 0 new loss 3.311978070996702e-05 old loss 3.353911961312406e-05 BETTER
I0315 04:24:14.937647 332492 finetune.py:45] layer 9_up initial loss 3.8975475035840645e-05
W0315 04:24:14.938160 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:24:16.249603 333064 finetune.py:68] layer 10_o @ epoch 4 new loss 1.9627086658147164e-05 old loss 1.975667146325577e-05 BETTER
W0315 04:24:17.868916 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_o proxy err 0.010828427970409393 tr(WHW.T) 4.213674545288086
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:05,  2.10s/it]  6%|▋         | 2/32 [00:03<00:54,  1.81s/it]  9%|▉         | 3/32 [00:05<00:49,  1.70s/it] 12%|█▎        | 4/32 [00:06<00:46,  1.67s/it]I0315 04:24:26.371272 333636 finetune.py:68] layer 11_o @ epoch 3 new loss 2.025738467636984e-05 old loss 2.0418547137524e-05 BETTER
 16%|█▌        | 5/32 [00:08<00:44,  1.65s/it] 19%|█▉        | 6/32 [00:10<00:42,  1.62s/it] 22%|██▏       | 7/32 [00:11<00:40,  1.60s/it] 25%|██▌       | 8/32 [00:13<00:38,  1.59s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.58s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.57s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it] 41%|████      | 13/32 [00:21<00:30,  1.58s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.59s/it] 47%|████▋     | 15/32 [00:24<00:26,  1.58s/it] 50%|█████     | 16/32 [00:25<00:25,  1.58s/it] 53%|█████▎    | 17/32 [00:27<00:23,  1.57s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.57s/it]I0315 04:24:49.019621 332492 finetune.py:68] layer 9_up @ epoch 0 new loss 3.84844170184806e-05 old loss 3.8975475035840645e-05 BETTER
I0315 04:24:49.294867 331922 finetune.py:68] layer 8_up @ epoch 1 new loss 3.2833315344760194e-05 old loss 3.311978070996702e-05 BETTER
 59%|█████▉    | 19/32 [00:30<00:20,  1.57s/it] 62%|██████▎   | 20/32 [00:32<00:18,  1.57s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.58s/it] 69%|██████▉   | 22/32 [00:35<00:15,  1.59s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.60s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.61s/it] 78%|███████▊  | 25/32 [00:40<00:11,  1.61s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.59s/it]I0315 04:25:01.893287 333636 finetune.py:68] layer 11_o @ epoch 4 new loss 2.011620563280303e-05 old loss 2.025738467636984e-05 BETTER
 84%|████████▍ | 27/32 [00:43<00:07,  1.58s/it]W0315 04:25:03.432128 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:44<00:06,  1.57s/it]11_o proxy err 0.01121758483350277 tr(WHW.T) 4.479811668395996
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:46<00:04,  1.56s/it]  3%|▎         | 1/32 [00:02<01:02,  2.03s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.56s/it]  6%|▋         | 2/32 [00:03<00:52,  1.74s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.55s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it]100%|██████████| 32/32 [00:50<00:00,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.59s/it]
 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it]I0315 04:25:18.597387 333064 finetune.py:45] layer 10_up initial loss 4.00646313210018e-05
W0315 04:25:18.597935 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.57s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.57s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it]I0315 04:25:24.040186 332492 finetune.py:68] layer 9_up @ epoch 1 new loss 3.8131431210786104e-05 old loss 3.84844170184806e-05 BETTER
 41%|████      | 13/32 [00:20<00:29,  1.56s/it]I0315 04:25:25.910007 331922 finetune.py:68] layer 8_up @ epoch 2 new loss 3.2580668630544096e-05 old loss 3.2833315344760194e-05 BETTER
 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:25,  1.56s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.58s/it] 56%|█████▋    | 18/32 [00:28<00:22,  1.59s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.59s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.57s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.56s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.58s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.56s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.56s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.56s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.59s/it]I0315 04:25:52.850363 333064 finetune.py:68] layer 10_up @ epoch 0 new loss 3.955257125198841e-05 old loss 4.00646313210018e-05 BETTER
 97%|█████████▋| 31/32 [00:48<00:01,  1.58s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0315 04:25:58.810265 332492 finetune.py:68] layer 9_up @ epoch 2 new loss 3.782642670557834e-05 old loss 3.8131431210786104e-05 BETTER
I0315 04:26:02.446596 331922 finetune.py:68] layer 8_up @ epoch 3 new loss 3.2345975341740996e-05 old loss 3.2580668630544096e-05 BETTER
I0315 04:26:03.386626 333636 finetune.py:45] layer 11_up initial loss 4.209157850709744e-05
W0315 04:26:03.387155 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:26:27.990125 333064 finetune.py:68] layer 10_up @ epoch 1 new loss 3.918887159670703e-05 old loss 3.955257125198841e-05 BETTER
I0315 04:26:33.869099 332492 finetune.py:68] layer 9_up @ epoch 3 new loss 3.755045690922998e-05 old loss 3.782642670557834e-05 BETTER
I0315 04:26:37.083480 333636 finetune.py:68] layer 11_up @ epoch 0 new loss 4.151992470724508e-05 old loss 4.209157850709744e-05 BETTER
I0315 04:26:39.094731 331922 finetune.py:68] layer 8_up @ epoch 4 new loss 3.212827141396701e-05 old loss 3.2345975341740996e-05 BETTER
W0315 04:26:40.508636 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_up proxy err 0.009694555774331093 tr(WHW.T) 668.2127685546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.54s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 22%|██▏       | 7/32 [00:10<00:38,  1.52s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:19<00:28,  1.51s/it]I0315 04:27:02.942718 333064 finetune.py:68] layer 10_up @ epoch 2 new loss 3.8878453779034317e-05 old loss 3.918887159670703e-05 BETTER
 44%|████▍     | 14/32 [00:21<00:27,  1.50s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.50s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it]I0315 04:27:08.767998 332492 finetune.py:68] layer 9_up @ epoch 4 new loss 3.7295922084013e-05 old loss 3.755045690922998e-05 BETTER
 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it]W0315 04:27:10.238949 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it]9_up proxy err 0.009325692430138588 tr(WHW.T) 723.3861083984375
  0%|          | 0/32 [00:00<?, ?it/s]I0315 04:27:11.711540 333636 finetune.py:68] layer 11_up @ epoch 1 new loss 4.111685120733455e-05 old loss 4.151992470724508e-05 BETTER
 62%|██████▎   | 20/32 [00:30<00:17,  1.50s/it]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.50s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it]100%|██████████| 32/32 [00:48<00:00,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]
 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it]I0315 04:27:37.641810 333064 finetune.py:68] layer 10_up @ epoch 3 new loss 3.858363925246522e-05 old loss 3.8878453779034317e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it]I0315 04:27:38.549791 331922 finetune.py:45] layer 8_gate initial loss 4.263941082172096e-05
W0315 04:27:38.550166 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it]I0315 04:27:45.708523 333636 finetune.py:68] layer 11_up @ epoch 2 new loss 4.0757156966719776e-05 old loss 4.111685120733455e-05 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
I0315 04:28:08.810098 332492 finetune.py:45] layer 9_gate initial loss 4.893412551609799e-05
W0315 04:28:08.810470 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:28:12.546633 333064 finetune.py:68] layer 10_up @ epoch 4 new loss 3.832768197753467e-05 old loss 3.858363925246522e-05 BETTER
I0315 04:28:12.894010 331922 finetune.py:68] layer 8_gate @ epoch 0 new loss 4.230608101352118e-05 old loss 4.263941082172096e-05 BETTER
W0315 04:28:14.016964 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

10_up proxy err 0.009451107122004032 tr(WHW.T) 748.6864624023438
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.01s/it]  6%|▋         | 2/32 [00:03<00:52,  1.74s/it]I0315 04:28:19.603196 333636 finetune.py:68] layer 11_up @ epoch 3 new loss 4.043209628434852e-05 old loss 4.0757156966719776e-05 BETTER
  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.57s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.57s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:25,  1.56s/it]I0315 04:28:41.501648 332492 finetune.py:68] layer 9_gate @ epoch 0 new loss 4.85375894641038e-05 old loss 4.893412551609799e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.57s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it]I0315 04:28:47.967233 331922 finetune.py:68] layer 8_gate @ epoch 1 new loss 4.209533290122636e-05 old loss 4.230608101352118e-05 BETTER
 66%|██████▌   | 21/32 [00:33<00:17,  1.57s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.56s/it]I0315 04:28:53.255781 333636 finetune.py:68] layer 11_up @ epoch 4 new loss 4.013910802314058e-05 old loss 4.043209628434852e-05 BETTER
 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it]W0315 04:28:54.632117 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 78%|███████▊  | 25/32 [00:39<00:10,  1.56s/it]11_up proxy err 0.009400549344718456 tr(WHW.T) 788.8045654296875
  0%|          | 0/32 [00:00<?, ?it/s] 81%|████████▏ | 26/32 [00:41<00:09,  1.55s/it]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.56s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.56s/it]  9%|▉         | 3/32 [00:04<00:46,  1.61s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.55s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.55s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.54s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.58s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.57s/it]I0315 04:29:14.058512 333064 finetune.py:45] layer 10_gate initial loss 5.0501519581303e-05
W0315 04:29:14.058873 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 38%|███▊      | 12/32 [00:18<00:30,  1.55s/it]I0315 04:29:14.720673 332492 finetune.py:68] layer 9_gate @ epoch 1 new loss 4.829369572689757e-05 old loss 4.85375894641038e-05 BETTER
 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it]I0315 04:29:22.797687 331922 finetune.py:68] layer 8_gate @ epoch 2 new loss 4.189787796349265e-05 old loss 4.209533290122636e-05 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.52s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.52s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0315 04:29:46.619748 333064 finetune.py:68] layer 10_gate @ epoch 0 new loss 5.0079383072443306e-05 old loss 5.0501519581303e-05 BETTER
I0315 04:29:47.593485 332492 finetune.py:68] layer 9_gate @ epoch 2 new loss 4.8063819122035056e-05 old loss 4.829369572689757e-05 BETTER
I0315 04:29:52.969648 333636 finetune.py:45] layer 11_gate initial loss 5.2909846999682486e-05
W0315 04:29:52.970011 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:29:57.721062 331922 finetune.py:68] layer 8_gate @ epoch 3 new loss 4.171715772827156e-05 old loss 4.189787796349265e-05 BETTER
I0315 04:30:19.998772 333064 finetune.py:68] layer 10_gate @ epoch 1 new loss 4.9826303438749164e-05 old loss 5.0079383072443306e-05 BETTER
I0315 04:30:20.864423 332492 finetune.py:68] layer 9_gate @ epoch 3 new loss 4.7848890972090885e-05 old loss 4.8063819122035056e-05 BETTER
I0315 04:30:24.956158 333636 finetune.py:68] layer 11_gate @ epoch 0 new loss 5.2513325499603525e-05 old loss 5.2909846999682486e-05 BETTER
I0315 04:30:32.943331 331922 finetune.py:68] layer 8_gate @ epoch 4 new loss 4.1547791624907404e-05 old loss 4.171715772827156e-05 BETTER
W0315 04:30:34.159185 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

8_gate proxy err 0.0037522846832871437 tr(WHW.T) 2923.6318359375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:41,  1.09it/s]  2%|▏         | 2/112 [00:01<01:07,  1.64it/s]  3%|▎         | 3/112 [00:01<00:54,  1.98it/s]  4%|▎         | 4/112 [00:02<00:49,  2.19it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s]  5%|▌         | 6/112 [00:02<00:43,  2.41it/s]  6%|▋         | 7/112 [00:03<00:42,  2.46it/s]  7%|▋         | 8/112 [00:03<00:41,  2.50it/s]  8%|▊         | 9/112 [00:03<00:40,  2.54it/s]  9%|▉         | 10/112 [00:04<00:39,  2.57it/s] 10%|▉         | 11/112 [00:04<00:38,  2.60it/s] 11%|█         | 12/112 [00:05<00:38,  2.62it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.62it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.62it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.62it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.62it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.62it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.62it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.60it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 21%|██        | 23/112 [00:09<00:34,  2.62it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.63it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.63it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.63it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.64it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.65it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.65it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.63it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.62it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.60it/s] 30%|███       | 34/112 [00:13<00:29,  2.61it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.63it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.63it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.63it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.63it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.63it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.64it/s]I0315 04:30:53.728364 333064 finetune.py:68] layer 10_gate @ epoch 2 new loss 4.958961289958097e-05 old loss 4.9826303438749164e-05 BETTER
 38%|███▊      | 42/112 [00:16<00:26,  2.63it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.64it/s]I0315 04:30:54.513139 332492 finetune.py:68] layer 9_gate @ epoch 4 new loss 4.765001722262241e-05 old loss 4.7848890972090885e-05 BETTER
 39%|███▉      | 44/112 [00:17<00:26,  2.61it/s] 40%|████      | 45/112 [00:17<00:25,  2.62it/s] 41%|████      | 46/112 [00:18<00:25,  2.64it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.64it/s]W0315 04:30:55.828171 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 43%|████▎     | 48/112 [00:18<00:24,  2.64it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.65it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.65it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.65it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.65it/s]I0315 04:30:57.802339 333636 finetune.py:68] layer 11_gate @ epoch 1 new loss 5.2229319408070296e-05 old loss 5.2513325499603525e-05 BETTER
 47%|████▋     | 53/112 [00:20<00:22,  2.65it/s] 48%|████▊     | 54/112 [00:21<00:21,  2.65it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.65it/s]9_gate proxy err 0.0036231297999620438 tr(WHW.T) 3172.0
  0%|          | 0/112 [00:00<?, ?it/s] 50%|█████     | 56/112 [00:21<00:21,  2.62it/s] 51%|█████     | 57/112 [00:22<00:20,  2.63it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.63it/s]  1%|          | 1/112 [00:00<01:37,  1.13it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.63it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.63it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.63it/s]  4%|▎         | 4/112 [00:02<00:49,  2.17it/s] 55%|█████▌    | 62/112 [00:24<00:18,  2.64it/s]  4%|▍         | 5/112 [00:02<00:46,  2.29it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.64it/s]  5%|▌         | 6/112 [00:02<00:44,  2.37it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.64it/s]  6%|▋         | 7/112 [00:03<00:43,  2.44it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.64it/s]  7%|▋         | 8/112 [00:03<00:42,  2.47it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.63it/s]  8%|▊         | 9/112 [00:04<00:41,  2.49it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.63it/s]  9%|▉         | 10/112 [00:04<00:40,  2.52it/s] 61%|██████    | 68/112 [00:26<00:16,  2.63it/s] 10%|▉         | 11/112 [00:04<00:39,  2.53it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.60it/s] 11%|█         | 12/112 [00:05<00:39,  2.54it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.62it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.55it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.63it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.65it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.57it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.65it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.57it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.66it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.57it/s] 67%|██████▋   | 75/112 [00:29<00:13,  2.66it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.55it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.66it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.58it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.67it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.66it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.61it/s] 71%|███████   | 79/112 [00:30<00:12,  2.67it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.61it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.67it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.65it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.61it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.65it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.61it/s] 74%|███████▍  | 83/112 [00:32<00:10,  2.65it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.66it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.66it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.60it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.66it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.60it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.67it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.57it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.67it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.67it/s] 80%|████████  | 90/112 [00:34<00:08,  2.65it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.59it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.65it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.66it/s] 30%|███       | 34/112 [00:13<00:29,  2.60it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.64it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.62it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.64it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.62it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.64it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.61it/s] 86%|████████▌ | 96/112 [00:36<00:06,  2.65it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.61it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.65it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.60it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.66it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.61it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.66it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.60it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.66it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.57it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.67it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.58it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.66it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.58it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.66it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 93%|█████████▎| 104/112 [00:39<00:03,  2.65it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.62it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.60it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.64it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.61it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.65it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.62it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.65it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.62it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.66it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.60it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.66it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.59it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.65it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.59it/s]100%|██████████| 112/112 [00:42<00:00,  2.65it/s]100%|██████████| 112/112 [00:42<00:00,  2.61it/s]
 48%|████▊     | 54/112 [00:21<00:22,  2.56it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.57it/s] 50%|█████     | 56/112 [00:22<00:21,  2.57it/s] 51%|█████     | 57/112 [00:22<00:21,  2.57it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.57it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.58it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.58it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.59it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.59it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.59it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.58it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.58it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.55it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.56it/s] 61%|██████    | 68/112 [00:26<00:17,  2.56it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.57it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.58it/s]I0315 04:31:27.077103 333064 finetune.py:68] layer 10_gate @ epoch 3 new loss 4.937346966471523e-05 old loss 4.958961289958097e-05 BETTER
 64%|██████▍   | 72/112 [00:28<00:15,  2.58it/s]W0315 04:31:27.458000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.458000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.459000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.459000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.459000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.459000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.459000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.503000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.504000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.504000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.504000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.504000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.520000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.520000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.520000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.520000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.520000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.695000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.695000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.695000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.695000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:27.695000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 65%|██████▌   | 73/112 [00:28<00:15,  2.58it/s]W0315 04:31:28.037000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.037000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.037000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.038000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.038000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.038000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.038000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.070000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.070000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.070000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.070000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.070000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.146000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.146000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.146000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.146000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:28.146000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
 66%|██████▌   | 74/112 [00:29<00:14,  2.58it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.59it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.58it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.55it/s]W0315 04:31:29.386000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.401000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.410000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.411000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 70%|██████▉   | 78/112 [00:30<00:13,  2.55it/s]W0315 04:31:29.886000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.886000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.886000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.886000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.886000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.886000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.887000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.920000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.920000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.920000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.920000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:29.920000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 71%|███████   | 79/112 [00:31<00:12,  2.56it/s]W0315 04:31:30.284000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.284000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.284000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.285000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.285000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.285000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.285000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.285000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 71%|███████▏  | 80/112 [00:31<00:12,  2.57it/s]W0315 04:31:30.593000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.593000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.593000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.593000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.593000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
I0315 04:31:30.747493 333636 finetune.py:68] layer 11_gate @ epoch 2 new loss 5.1972347137052566e-05 old loss 5.2229319408070296e-05 BETTER
 72%|███████▏  | 81/112 [00:31<00:12,  2.57it/s]W0315 04:31:30.948000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:30.953000 140675818874688 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 73%|███████▎  | 82/112 [00:32<00:11,  2.58it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.58it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.58it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.58it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.57it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.57it/s] 79%|███████▉  | 89/112 [00:34<00:09,  2.55it/s] 80%|████████  | 90/112 [00:35<00:08,  2.56it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.57it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.57it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.57it/s] 84%|████████▍ | 94/112 [00:36<00:07,  2.57it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.57it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.57it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.58it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.58it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.57it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.57it/s]I0315 04:31:38.308994 331922 finetune.py:45] layer 8_down initial loss 6.223258969839662e-05
W0315 04:31:38.309376 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 90%|█████████ | 101/112 [00:39<00:04,  2.54it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.56it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.57it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.57it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.57it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.57it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.58it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.59it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.58it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.58it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.55it/s]100%|██████████| 112/112 [00:43<00:00,  2.55it/s]
W0315 04:31:50.447000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.448000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.448000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.448000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.448000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.448000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.448000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.493000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.493000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.493000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.493000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.493000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.509000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.509000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.510000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.510000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.510000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.681000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.681000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.682000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.682000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:50.682000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.013000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.014000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.014000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.014000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.014000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.014000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.014000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.045000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.045000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.045000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.045000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.046000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.120000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.120000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.120000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.120000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:51.120000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.343000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.350000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.356000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.356000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.842000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.842000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.842000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.843000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.843000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.843000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.843000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.876000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.877000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.877000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.877000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:52.877000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.231000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.231000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.232000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.232000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.232000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.232000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.232000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.232000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.538000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.538000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.538000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.538000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.538000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.889000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:31:53.894000 140650563221312 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 04:32:00.256678 333064 finetune.py:68] layer 10_gate @ epoch 4 new loss 4.916945181321353e-05 old loss 4.937346966471523e-05 BETTER
I0315 04:32:01.371722 332492 finetune.py:45] layer 9_down initial loss 7.030744745861739e-05
W0315 04:32:01.372142 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0315 04:32:01.646751 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 04:32:03.410097 333636 finetune.py:68] layer 11_gate @ epoch 3 new loss 5.17341322847642e-05 old loss 5.1972347137052566e-05 BETTER
10_gate proxy err 0.0036623103078454733 tr(WHW.T) 3037.440185546875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:41,  1.09it/s]  2%|▏         | 2/112 [00:01<01:07,  1.63it/s]  3%|▎         | 3/112 [00:01<00:56,  1.94it/s]  4%|▎         | 4/112 [00:02<00:50,  2.13it/s]  4%|▍         | 5/112 [00:02<00:47,  2.23it/s]  5%|▌         | 6/112 [00:02<00:45,  2.32it/s]  6%|▋         | 7/112 [00:03<00:44,  2.38it/s]  7%|▋         | 8/112 [00:03<00:43,  2.42it/s]  8%|▊         | 9/112 [00:04<00:42,  2.45it/s]  9%|▉         | 10/112 [00:04<00:41,  2.47it/s] 10%|▉         | 11/112 [00:04<00:40,  2.48it/s] 11%|█         | 12/112 [00:05<00:40,  2.49it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.50it/s]I0315 04:32:10.828013 331922 finetune.py:68] layer 8_down @ epoch 0 new loss 6.22056977590546e-05 old loss 6.223258969839662e-05 BETTER
 12%|█▎        | 14/112 [00:06<00:39,  2.50it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.50it/s] 14%|█▍        | 16/112 [00:06<00:38,  2.48it/s] 15%|█▌        | 17/112 [00:07<00:38,  2.49it/s] 16%|█▌        | 18/112 [00:07<00:37,  2.49it/s] 17%|█▋        | 19/112 [00:08<00:37,  2.50it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.50it/s] 19%|█▉        | 21/112 [00:08<00:36,  2.51it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.51it/s] 21%|██        | 23/112 [00:09<00:35,  2.51it/s] 21%|██▏       | 24/112 [00:10<00:35,  2.51it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.50it/s] 23%|██▎       | 26/112 [00:10<00:34,  2.50it/s] 24%|██▍       | 27/112 [00:11<00:34,  2.48it/s] 25%|██▌       | 28/112 [00:11<00:33,  2.49it/s] 26%|██▌       | 29/112 [00:12<00:33,  2.50it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.50it/s] 28%|██▊       | 31/112 [00:12<00:32,  2.51it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.52it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.52it/s] 30%|███       | 34/112 [00:14<00:30,  2.52it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.52it/s] 32%|███▏      | 36/112 [00:14<00:30,  2.51it/s] 33%|███▎      | 37/112 [00:15<00:30,  2.50it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.50it/s] 35%|███▍      | 39/112 [00:16<00:29,  2.47it/s] 36%|███▌      | 40/112 [00:16<00:29,  2.48it/s] 37%|███▋      | 41/112 [00:16<00:28,  2.49it/s] 38%|███▊      | 42/112 [00:17<00:28,  2.49it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.50it/s] 39%|███▉      | 44/112 [00:18<00:27,  2.51it/s] 40%|████      | 45/112 [00:18<00:26,  2.51it/s] 41%|████      | 46/112 [00:18<00:26,  2.51it/s] 42%|████▏     | 47/112 [00:19<00:25,  2.51it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.51it/s] 44%|████▍     | 49/112 [00:20<00:25,  2.51it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.51it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.48it/s] 46%|████▋     | 52/112 [00:21<00:24,  2.49it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.50it/s] 48%|████▊     | 54/112 [00:22<00:23,  2.51it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.51it/s] 50%|█████     | 56/112 [00:22<00:22,  2.52it/s] 51%|█████     | 57/112 [00:23<00:21,  2.52it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.53it/s] 53%|█████▎    | 59/112 [00:24<00:20,  2.53it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.51it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.51it/s] 55%|█████▌    | 62/112 [00:25<00:20,  2.48it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.49it/s] 57%|█████▋    | 64/112 [00:26<00:19,  2.50it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.51it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.52it/s] 60%|█████▉    | 67/112 [00:27<00:17,  2.52it/s]I0315 04:32:32.452773 332492 finetune.py:68] layer 9_down @ epoch 0 new loss 7.027720857877284e-05 old loss 7.030744745861739e-05 BETTER
 61%|██████    | 68/112 [00:27<00:17,  2.52it/s] 62%|██████▏   | 69/112 [00:28<00:17,  2.52it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.52it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.51it/s] 64%|██████▍   | 72/112 [00:29<00:15,  2.51it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.52it/s] 66%|██████▌   | 74/112 [00:30<00:15,  2.48it/s] 67%|██████▋   | 75/112 [00:30<00:14,  2.50it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.50it/s]I0315 04:32:36.146140 333636 finetune.py:68] layer 11_gate @ epoch 4 new loss 5.150649303686805e-05 old loss 5.17341322847642e-05 BETTER
 69%|██████▉   | 77/112 [00:31<00:13,  2.51it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.52it/s] 71%|███████   | 79/112 [00:32<00:13,  2.52it/s]W0315 04:32:37.304608 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 71%|███████▏  | 80/112 [00:32<00:12,  2.52it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.52it/s] 73%|███████▎  | 82/112 [00:33<00:11,  2.52it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.51it/s] 75%|███████▌  | 84/112 [00:34<00:11,  2.51it/s] 76%|███████▌  | 85/112 [00:34<00:10,  2.48it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.49it/s] 78%|███████▊  | 87/112 [00:35<00:10,  2.50it/s]11_gate proxy err 0.0035531031899154186 tr(WHW.T) 3156.551513671875
  0%|          | 0/112 [00:00<?, ?it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.50it/s] 79%|███████▉  | 89/112 [00:36<00:09,  2.51it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s] 80%|████████  | 90/112 [00:36<00:08,  2.51it/s]  2%|▏         | 2/112 [00:01<01:04,  1.71it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.51it/s]  3%|▎         | 3/112 [00:01<00:53,  2.02it/s] 82%|████████▏ | 92/112 [00:37<00:07,  2.51it/s]  4%|▎         | 4/112 [00:02<00:48,  2.21it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.51it/s]  4%|▍         | 5/112 [00:02<00:45,  2.33it/s] 84%|████████▍ | 94/112 [00:38<00:07,  2.51it/s]  5%|▌         | 6/112 [00:02<00:43,  2.41it/s] 85%|████████▍ | 95/112 [00:38<00:06,  2.51it/s]  6%|▋         | 7/112 [00:03<00:42,  2.47it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.51it/s]  7%|▋         | 8/112 [00:03<00:41,  2.50it/s]I0315 04:32:44.195712 331922 finetune.py:68] layer 8_down @ epoch 1 new loss 6.219993520062417e-05 old loss 6.22056977590546e-05 BETTER
 87%|████████▋ | 97/112 [00:39<00:06,  2.48it/s]  8%|▊         | 9/112 [00:03<00:40,  2.54it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.51it/s]  9%|▉         | 10/112 [00:04<00:39,  2.56it/s] 88%|████████▊ | 99/112 [00:40<00:05,  2.51it/s] 10%|▉         | 11/112 [00:04<00:39,  2.58it/s] 89%|████████▉ | 100/112 [00:40<00:04,  2.52it/s] 11%|█         | 12/112 [00:05<00:38,  2.59it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.53it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.61it/s] 91%|█████████ | 102/112 [00:41<00:03,  2.53it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.61it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.53it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.62it/s] 93%|█████████▎| 104/112 [00:42<00:03,  2.54it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.63it/s] 94%|█████████▍| 105/112 [00:42<00:02,  2.54it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.61it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.55it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.61it/s] 96%|█████████▌| 107/112 [00:43<00:01,  2.55it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.61it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.52it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.60it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.53it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 98%|█████████▊| 110/112 [00:44<00:00,  2.53it/s] 21%|██        | 23/112 [00:09<00:34,  2.60it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.53it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.60it/s]100%|██████████| 112/112 [00:45<00:00,  2.53it/s]100%|██████████| 112/112 [00:45<00:00,  2.48it/s]
 22%|██▏       | 25/112 [00:10<00:33,  2.60it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.59it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.60it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.59it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.59it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.59it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.58it/s] 30%|███       | 34/112 [00:13<00:30,  2.59it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.60it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.61it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.63it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.62it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.63it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.63it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.63it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.63it/s]W0315 04:32:57.368000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.368000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.368000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.368000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.368000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.369000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.369000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.413000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.413000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.413000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.413000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.413000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.430000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.430000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.430000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.430000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.430000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.605000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.605000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.605000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.605000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.606000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s]W0315 04:32:57.951000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.951000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.951000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.951000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.952000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.952000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.952000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.984000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.984000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.984000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.985000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:57.985000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:32:58.058000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:32:58.058000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:32:58.058000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:32:58.058000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:58.058000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 40%|████      | 45/112 [00:17<00:25,  2.60it/s] 41%|████      | 46/112 [00:18<00:25,  2.61it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.62it/s]W0315 04:32:59.261000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.275000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 43%|████▎     | 48/112 [00:18<00:24,  2.62it/s]W0315 04:32:59.283000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.283000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 49/112 [00:19<00:24,  2.62it/s]W0315 04:32:59.737000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.738000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.738000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.738000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.738000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.738000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.738000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.771000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.771000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.771000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.771000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:32:59.771000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 45%|████▍     | 50/112 [00:19<00:23,  2.62it/s]W0315 04:33:00.128000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.128000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.128000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.128000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.128000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.128000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.128000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.128000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 46%|████▌     | 51/112 [00:20<00:23,  2.63it/s]W0315 04:33:00.438000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.438000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.438000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.438000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.438000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.784000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:00.789000 140104111724352 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 46%|████▋     | 52/112 [00:20<00:22,  2.63it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.63it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.63it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.62it/s] 50%|█████     | 56/112 [00:21<00:21,  2.62it/s] 51%|█████     | 57/112 [00:22<00:21,  2.59it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.60it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.60it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.60it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.61it/s]I0315 04:33:04.292152 332492 finetune.py:68] layer 9_down @ epoch 1 new loss 7.027160609140992e-05 old loss 7.027720857877284e-05 BETTER
 55%|█████▌    | 62/112 [00:24<00:19,  2.62it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.63it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.62it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.62it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.62it/s] 61%|██████    | 68/112 [00:26<00:16,  2.61it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.58it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s]I0315 04:33:08.064023 333064 finetune.py:45] layer 10_down initial loss 7.320861186599359e-05
W0315 04:33:08.064511 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 63%|██████▎   | 71/112 [00:27<00:15,  2.60it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.60it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.61it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.61it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.61it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.62it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.61it/s] 71%|███████   | 79/112 [00:30<00:12,  2.62it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.62it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.60it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.60it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.61it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.61it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.61it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.61it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.62it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.61it/s] 80%|████████  | 90/112 [00:34<00:08,  2.61it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.60it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.52it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.55it/s] 84%|████████▍ | 94/112 [00:36<00:07,  2.57it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.58it/s]I0315 04:33:17.367372 331922 finetune.py:68] layer 8_down @ epoch 2 new loss 6.219220085768029e-05 old loss 6.219993520062417e-05 BETTER
 86%|████████▌ | 96/112 [00:37<00:06,  2.60it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.60it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.60it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.60it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.60it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.60it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.60it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.60it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.61it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.61it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.60it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.61it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.59it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.60it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.61it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.61it/s]100%|██████████| 112/112 [00:43<00:00,  2.62it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]
W0315 04:33:31.125000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.125000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.126000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.126000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.126000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.126000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.126000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.169000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.170000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.170000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.170000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.170000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.186000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.186000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.186000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.186000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.186000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.359000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.359000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.359000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.359000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.360000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.687000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.687000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.687000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.687000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.687000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.688000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.688000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.723000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.724000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.724000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.724000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.724000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.796000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.796000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.796000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.796000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:33:31.796000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.027000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.042000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.050000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.050000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.510000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.510000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.510000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.510000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.511000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.511000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.511000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.544000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.544000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.544000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.544000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.544000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.895000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.895000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.895000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.895000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.895000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.896000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.896000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:33.896000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:33:34.199000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:33:34.199000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:33:34.200000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:33:34.200000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:33:34.200000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:34.545000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:33:34.551000 140320157112128 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 04:33:36.236681 332492 finetune.py:68] layer 9_down @ epoch 2 new loss 7.026594539638609e-05 old loss 7.027160609140992e-05 BETTER
I0315 04:33:39.345298 333064 finetune.py:68] layer 10_down @ epoch 0 new loss 7.317576091736555e-05 old loss 7.320861186599359e-05 BETTER
I0315 04:33:41.740284 333636 finetune.py:45] layer 11_down initial loss 7.6723110396415e-05
W0315 04:33:41.740723 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:33:50.906912 331922 finetune.py:76] layer 8_down @ epoch 3 new loss 6.219278293428943e-05 old loss 6.219220085768029e-05 WORSE
I0315 04:34:08.164163 332492 finetune.py:68] layer 9_down @ epoch 3 new loss 7.025844388408586e-05 old loss 7.026594539638609e-05 BETTER
I0315 04:34:11.441632 333064 finetune.py:68] layer 10_down @ epoch 1 new loss 7.317196286749095e-05 old loss 7.317576091736555e-05 BETTER
I0315 04:34:12.420649 333636 finetune.py:68] layer 11_down @ epoch 0 new loss 7.668595935683697e-05 old loss 7.6723110396415e-05 BETTER
I0315 04:34:24.011293 331922 finetune.py:68] layer 8_down @ epoch 4 new loss 6.218547787284479e-05 old loss 6.219220085768029e-05 BETTER
W0315 04:34:24.885097 331922 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

8_down proxy err 0.012210306711494923 tr(WHW.T) 7.277468681335449
I0315 04:34:40.301425 332492 finetune.py:68] layer 9_down @ epoch 4 new loss 7.025316153885797e-05 old loss 7.025844388408586e-05 BETTER
W0315 04:34:41.309842 332492 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

9_down proxy err 0.01216804701834917 tr(WHW.T) 8.026899337768555
I0315 04:34:43.684062 333064 finetune.py:68] layer 10_down @ epoch 2 new loss 7.316881965380162e-05 old loss 7.317196286749095e-05 BETTER
I0315 04:34:43.865390 333636 finetune.py:68] layer 11_down @ epoch 1 new loss 7.6684431405738e-05 old loss 7.668595935683697e-05 BETTER
I0315 04:35:14.905321 333636 finetune.py:68] layer 11_down @ epoch 2 new loss 7.667783938813955e-05 old loss 7.6684431405738e-05 BETTER
I0315 04:35:15.336227 333064 finetune.py:68] layer 10_down @ epoch 3 new loss 7.31673208065331e-05 old loss 7.316881965380162e-05 BETTER
I0315 04:35:46.128396 333636 finetune.py:68] layer 11_down @ epoch 3 new loss 7.667238241992891e-05 old loss 7.667783938813955e-05 BETTER
I0315 04:35:46.675425 333064 finetune.py:68] layer 10_down @ epoch 4 new loss 7.316019764402881e-05 old loss 7.31673208065331e-05 BETTER
W0315 04:35:47.548858 333064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

10_down proxy err 0.012166007421910763 tr(WHW.T) 8.442428588867188
I0315 04:35:52.726842 273992 quantize_finetune_llama.py:186] computed original embedding for layer 12 in 66.88855528831482s
I0315 04:35:53.154480 273992 quantize_finetune_llama.py:159] layer 13 gpu 1
I0315 04:35:55.276421 335849 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 04:35:55.276552 335849 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 04:35:55.276615 335849 utils.py:162] NumExpr defaulting to 16 threads.
I0315 04:35:55.554214 335849 config.py:58] PyTorch version 2.4.0 available.
I0315 04:35:58.022164 335849 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 04:35:58.646782 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:48,  1.58s/it]  6%|▋         | 2/32 [00:01<00:25,  1.18it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.25it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.76it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.75it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.82it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.82it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.84it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.83it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.86it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
W0315 04:36:15.838000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.839000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.839000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.839000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.839000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.839000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.839000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.868000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.868000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.868000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.868000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.868000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.887000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.887000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.887000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.887000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:36:15.887000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:36:16.239000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:36:16.239000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:36:16.239000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:36:16.239000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:36:16.239000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.192000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.192000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.192000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.192000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.192000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.192000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.193000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.226000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.226000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.226000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.227000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.227000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
I0315 04:36:17.274363 333636 finetune.py:68] layer 11_down @ epoch 4 new loss 7.666812598472461e-05 old loss 7.667238241992891e-05 BETTER
W0315 04:36:17.477000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.477000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.477000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.477000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:36:17.477000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.086355 333636 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

11_down proxy err 0.011790398508310318 tr(WHW.T) 9.095731735229492
W0315 04:36:18.655000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.655000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.655000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.655000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.655000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.655000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.655000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.672000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.672000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.672000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.672000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:36:18.672000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:36:19.576000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:36:19.577000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:36:19.577000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:36:19.577000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:36:19.577000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 04:36:26.731182 335849 finetune.py:45] layer 12_v initial loss 1.435620833944995e-05
W0315 04:36:26.731539 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:36:56.081401 273992 quantize_finetune_llama.py:186] computed original embedding for layer 13 in 62.198928356170654s
I0315 04:36:56.507851 273992 quantize_finetune_llama.py:159] layer 14 gpu 2
I0315 04:36:58.670224 336419 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 04:36:58.670343 336419 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 04:36:58.670406 336419 utils.py:162] NumExpr defaulting to 16 threads.
I0315 04:36:58.866220 336419 config.py:58] PyTorch version 2.4.0 available.
I0315 04:37:01.061977 336419 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 04:37:01.561854 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]I0315 04:37:03.207509 335849 finetune.py:68] layer 12_v @ epoch 0 new loss 8.99093174666632e-06 old loss 1.435620833944995e-05 BETTER
  3%|▎         | 1/32 [00:01<00:59,  1.91s/it]  6%|▋         | 2/32 [00:02<00:30,  1.01s/it]  9%|▉         | 3/32 [00:02<00:20,  1.40it/s] 12%|█▎        | 4/32 [00:03<00:16,  1.75it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.02it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.25it/s] 22%|██▏       | 7/32 [00:04<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.68it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.70it/s] 41%|████      | 13/32 [00:06<00:06,  2.74it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.77it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.79it/s] 50%|█████     | 16/32 [00:07<00:05,  2.75it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:08<00:04,  2.80it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.85it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.86it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
W0315 04:37:18.556000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.556000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.556000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.556000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.556000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.556000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.556000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.582000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.582000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.582000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.582000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.582000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.599000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.599000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.599000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.599000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.599000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.928000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.928000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.929000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.929000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:37:18.929000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.847000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.847000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.847000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.847000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.847000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.847000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.848000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.866000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.866000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.866000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.866000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:37:19.866000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:37:20.114000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:37:20.114000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:37:20.115000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:37:20.115000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:37:20.115000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.309000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.309000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.309000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.309000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.309000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.309000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.309000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.327000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.327000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.328000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.328000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:37:21.328000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:37:22.265000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:37:22.265000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:37:22.265000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:37:22.265000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:37:22.266000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 04:37:28.862405 336419 finetune.py:45] layer 13_v initial loss 1.1791328688559588e-05
W0315 04:37:28.862845 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:37:40.927083 335849 finetune.py:68] layer 12_v @ epoch 1 new loss 8.42348254082026e-06 old loss 8.99093174666632e-06 BETTER
I0315 04:37:58.195377 273992 quantize_finetune_llama.py:186] computed original embedding for layer 14 in 61.26549673080444s
I0315 04:37:58.620293 273992 quantize_finetune_llama.py:159] layer 15 gpu 3
I0315 04:38:00.843080 336988 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 04:38:00.843226 336988 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 04:38:00.843310 336988 utils.py:162] NumExpr defaulting to 16 threads.
I0315 04:38:01.049222 336988 config.py:58] PyTorch version 2.4.0 available.
I0315 04:38:03.363367 336419 finetune.py:68] layer 13_v @ epoch 0 new loss 8.030571734707337e-06 old loss 1.1791328688559588e-05 BETTER
I0315 04:38:03.393157 336988 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 04:38:03.754049 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:51,  1.67s/it]  6%|▋         | 2/32 [00:02<00:26,  1.12it/s]  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.90it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.17it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.36it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.68it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.73it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.81it/s] 41%|████      | 13/32 [00:05<00:06,  2.84it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.86it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.85it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.85it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.87it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.88it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.88it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.89it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0315 04:38:18.817808 335849 finetune.py:68] layer 12_v @ epoch 2 new loss 8.390922630496789e-06 old loss 8.42348254082026e-06 BETTER
W0315 04:38:20.363000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.363000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.363000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.363000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.364000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.364000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.364000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.390000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.391000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.391000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.391000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.391000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.407000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.408000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.408000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.408000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.408000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.741000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.741000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.741000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.741000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:38:20.741000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.672000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.672000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.673000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.673000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.673000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.673000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.673000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.691000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.691000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.691000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.691000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.691000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.941000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.941000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.941000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.941000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:38:21.941000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.148000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.148000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.148000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.148000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.148000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.148000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.148000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.166000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.166000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.166000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.166000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:38:23.166000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:38:24.115000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:38:24.116000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:38:24.116000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:38:24.116000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:38:24.116000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 04:38:30.278735 336988 finetune.py:45] layer 14_v initial loss 1.2183162652945612e-05
W0315 04:38:30.279064 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:38:38.921606 336419 finetune.py:68] layer 13_v @ epoch 1 new loss 7.581371846754337e-06 old loss 8.030571734707337e-06 BETTER
I0315 04:38:56.533650 335849 finetune.py:68] layer 12_v @ epoch 3 new loss 8.184828402590938e-06 old loss 8.390922630496789e-06 BETTER
I0315 04:38:59.588291 273992 quantize_finetune_llama.py:186] computed original embedding for layer 15 in 60.46211051940918s
I0315 04:38:59.965606 273992 quantize_finetune_llama.py:159] layer 16 gpu 0
I0315 04:39:02.054127 337557 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 04:39:02.054275 337557 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 04:39:02.054340 337557 utils.py:162] NumExpr defaulting to 16 threads.
I0315 04:39:02.261359 337557 config.py:58] PyTorch version 2.4.0 available.
I0315 04:39:04.501569 337557 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0315 04:39:04.884643 336988 finetune.py:68] layer 14_v @ epoch 0 new loss 8.519221410097089e-06 old loss 1.2183162652945612e-05 BETTER
W0315 04:39:04.937665 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:52,  1.68s/it]  6%|▋         | 2/32 [00:02<00:27,  1.11it/s]  9%|▉         | 3/32 [00:02<00:18,  1.53it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.87it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.12it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.73it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.75it/s] 41%|████      | 13/32 [00:05<00:06,  2.77it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 50%|█████     | 16/32 [00:07<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.82it/s]I0315 04:39:14.860735 336419 finetune.py:68] layer 13_v @ epoch 2 new loss 7.497808837797493e-06 old loss 7.581371846754337e-06 BETTER
 69%|██████▉   | 22/32 [00:09<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.82it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.83it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.83it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.82it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.81it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.46it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0315 04:39:21.717000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.717000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.718000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.718000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.718000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.718000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.718000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.743000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.743000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.744000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.744000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.744000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.760000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.760000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.760000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.760000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:21.761000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:22.096000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:22.096000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:22.097000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:22.097000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:22.097000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.013000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.013000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.013000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.013000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.013000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.013000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.013000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.031000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.032000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.032000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.032000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.032000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.282000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.282000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.282000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.283000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:23.283000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.490000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.490000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.490000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.490000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.490000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.490000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.490000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.509000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.509000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.509000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.509000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:24.509000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:25.451000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:25.451000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:25.452000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:25.452000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:25.452000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 04:39:31.380122 337557 finetune.py:45] layer 15_v initial loss 2.2163383619044907e-05
W0315 04:39:31.380564 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:39:34.244818 335849 finetune.py:76] layer 12_v @ epoch 4 new loss 9.10376911633648e-06 old loss 8.184828402590938e-06 WORSE
W0315 04:39:35.376640 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_v proxy err 0.009097253903746605 tr(WHW.T) 68.45219421386719
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:15,  1.91it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.27it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.36it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s]I0315 04:39:40.653353 336988 finetune.py:68] layer 14_v @ epoch 1 new loss 8.05911076895427e-06 old loss 8.519221410097089e-06 BETTER
 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.56it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.57it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.56it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.56it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]100%|██████████| 32/32 [00:13<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
I0315 04:39:50.708157 336419 finetune.py:68] layer 13_v @ epoch 3 new loss 7.224786259030225e-06 old loss 7.497808837797493e-06 BETTER
W0315 04:39:55.834000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.834000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.834000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.834000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.834000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.834000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.834000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.867000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.867000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.867000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.868000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.868000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.884000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.884000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.884000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.884000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:55.884000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.048000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.048000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.048000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.048000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.048000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.285000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.285000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.285000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.285000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.285000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.286000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.286000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.306000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.307000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.307000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.307000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.307000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.379000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.380000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.380000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.380000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:56.380000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.289000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.615000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.616000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.616000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.616000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.616000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.616000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.616000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.637000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.637000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.637000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.637000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.637000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.906000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.906000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.906000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.906000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:39:57.906000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:39:58.175000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 04:40:04.972599 337557 finetune.py:68] layer 15_v @ epoch 0 new loss 1.1605185136431828e-05 old loss 2.2163383619044907e-05 BETTER
I0315 04:40:04.987129 335849 finetune.py:45] layer 12_q initial loss 1.2651981705857906e-05
W0315 04:40:04.987612 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:40:16.487043 336988 finetune.py:68] layer 14_v @ epoch 2 new loss 7.821784492989536e-06 old loss 8.05911076895427e-06 BETTER
I0315 04:40:26.723762 336419 finetune.py:76] layer 13_v @ epoch 4 new loss 7.802864274708554e-06 old loss 7.224786259030225e-06 WORSE
W0315 04:40:27.783033 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_v proxy err 0.009518183767795563 tr(WHW.T) 71.04966735839844
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:15,  1.90it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.11it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.25it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.53it/s] 41%|████      | 13/32 [00:05<00:07,  2.53it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.53it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.52it/s] 50%|█████     | 16/32 [00:06<00:06,  2.51it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.52it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.53it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.54it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.54it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.54it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.54it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.54it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.54it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.51it/s]I0315 04:40:39.705667 337557 finetune.py:68] layer 15_v @ epoch 1 new loss 1.0669942639651708e-05 old loss 1.1605185136431828e-05 BETTER
 84%|████████▍ | 27/32 [00:11<00:01,  2.52it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.53it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.53it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
I0315 04:40:42.054917 335849 finetune.py:68] layer 12_q @ epoch 0 new loss 1.2114399396523368e-05 old loss 1.2651981705857906e-05 BETTER
W0315 04:40:48.315000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.315000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.316000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.316000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.316000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.316000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.316000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.347000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.347000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.347000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.347000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.347000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.363000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.363000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.363000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.363000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.363000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.533000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.534000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.534000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.534000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.534000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.780000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.781000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.781000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.781000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.781000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.781000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.781000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.802000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.802000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.802000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.802000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.802000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.871000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.872000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.872000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.872000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:40:48.872000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:40:49.788000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.117000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.117000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.118000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.118000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.118000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.118000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.118000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.142000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.142000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.142000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.142000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.142000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.411000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.412000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.412000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.412000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.412000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:40:50.688000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 04:40:52.714324 336988 finetune.py:76] layer 14_v @ epoch 3 new loss 7.964273208926897e-06 old loss 7.821784492989536e-06 WORSE
I0315 04:40:57.360709 336419 finetune.py:45] layer 13_q initial loss 1.2507887731771916e-05
W0315 04:40:57.361004 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:41:14.943985 337557 finetune.py:68] layer 15_v @ epoch 2 new loss 1.0234945875708945e-05 old loss 1.0669942639651708e-05 BETTER
I0315 04:41:19.723209 335849 finetune.py:68] layer 12_q @ epoch 1 new loss 1.1848793292301707e-05 old loss 1.2114399396523368e-05 BETTER
I0315 04:41:28.807692 336988 finetune.py:68] layer 14_v @ epoch 4 new loss 7.573262337245978e-06 old loss 7.821784492989536e-06 BETTER
W0315 04:41:30.572192 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_v proxy err 0.008708442561328411 tr(WHW.T) 74.802978515625
  0%|          | 0/32 [00:00<?, ?it/s]I0315 04:41:32.737119 336419 finetune.py:68] layer 13_q @ epoch 0 new loss 1.2102428627258632e-05 old loss 1.2507887731771916e-05 BETTER
  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]  6%|▋         | 2/32 [00:01<00:18,  1.58it/s]  9%|▉         | 3/32 [00:01<00:14,  1.94it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.60it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.63it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0315 04:41:50.830741 337557 finetune.py:68] layer 15_v @ epoch 3 new loss 9.993701496568974e-06 old loss 1.0234945875708945e-05 BETTER
W0315 04:41:50.919000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.919000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.920000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.920000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.920000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.920000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.920000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.949000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.949000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.949000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.949000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.949000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.965000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.965000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.965000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.965000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:41:50.965000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.126000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.126000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.126000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.127000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.127000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.359000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.360000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.360000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.360000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.360000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.360000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.360000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.380000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.380000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.380000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.380000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.380000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.447000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.448000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.448000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.448000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:41:51.448000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.339000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.653000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.653000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.653000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.653000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.653000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.653000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.653000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.674000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.674000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.675000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.675000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.675000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.940000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.940000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.940000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.940000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:41:52.940000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:41:53.212000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 04:41:57.565955 335849 finetune.py:68] layer 12_q @ epoch 2 new loss 1.1644097867247183e-05 old loss 1.1848793292301707e-05 BETTER
I0315 04:42:00.044319 336988 finetune.py:45] layer 14_q initial loss 1.4546474631060846e-05
W0315 04:42:00.044891 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:42:08.993655 336419 finetune.py:68] layer 13_q @ epoch 1 new loss 1.1909670320164878e-05 old loss 1.2102428627258632e-05 BETTER
I0315 04:42:26.562263 337557 finetune.py:68] layer 15_v @ epoch 4 new loss 9.93344838207122e-06 old loss 9.993701496568974e-06 BETTER
W0315 04:42:28.311342 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_v proxy err 0.011341140605509281 tr(WHW.T) 67.86465454101562
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.00it/s]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:15,  1.89it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.11it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.25it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.49it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.49it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.50it/s] 41%|████      | 13/32 [00:05<00:07,  2.50it/s]I0315 04:42:35.536506 336988 finetune.py:68] layer 14_q @ epoch 0 new loss 1.3968614439363591e-05 old loss 1.4546474631060846e-05 BETTER
 44%|████▍     | 14/32 [00:06<00:07,  2.52it/s]I0315 04:42:35.721484 335849 finetune.py:68] layer 12_q @ epoch 3 new loss 1.146439080912387e-05 old loss 1.1644097867247183e-05 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.54it/s] 50%|█████     | 16/32 [00:06<00:06,  2.55it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.54it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.54it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.54it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.52it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.52it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.51it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.50it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.51it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.53it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.52it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.52it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.53it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.53it/s]100%|██████████| 32/32 [00:13<00:00,  2.42it/s]
I0315 04:42:45.454770 336419 finetune.py:68] layer 13_q @ epoch 2 new loss 1.1833826647489332e-05 old loss 1.1909670320164878e-05 BETTER
W0315 04:42:49.236000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.237000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.237000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.237000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.237000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.237000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.237000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.267000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.267000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.268000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.268000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.268000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.284000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.284000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.284000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.284000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.284000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.447000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.448000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.448000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.448000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.448000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.677000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.677000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.677000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.677000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.677000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.677000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.677000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.699000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.699000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.699000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.700000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.700000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.767000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.768000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.768000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.768000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:42:49.768000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.651000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.976000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.976000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.976000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.976000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.976000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.977000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.977000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.998000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.998000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.998000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.998000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:42:50.999000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:42:51.265000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:42:51.265000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:42:51.265000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:42:51.265000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:42:51.265000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:42:51.537000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 04:42:58.391839 337557 finetune.py:45] layer 15_q initial loss 1.5112892469915096e-05
W0315 04:42:58.392273 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:43:11.768274 336988 finetune.py:68] layer 14_q @ epoch 1 new loss 1.3658434909302741e-05 old loss 1.3968614439363591e-05 BETTER
I0315 04:43:13.818010 335849 finetune.py:68] layer 12_q @ epoch 4 new loss 1.131111457652878e-05 old loss 1.146439080912387e-05 BETTER
W0315 04:43:15.640872 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_q proxy err 0.0013028077082708478 tr(WHW.T) 6421.748046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.33it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s]I0315 04:43:21.796862 336419 finetune.py:68] layer 13_q @ epoch 3 new loss 1.1791055840149056e-05 old loss 1.1833826647489332e-05 BETTER
 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.64it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.66it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.64it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0315 04:43:32.821479 337557 finetune.py:68] layer 15_q @ epoch 0 new loss 1.4695907339046244e-05 old loss 1.5112892469915096e-05 BETTER
I0315 04:43:37.001802 335849 finetune.py:45] layer 12_k initial loss 1.570795757288579e-05
W0315 04:43:37.002238 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:43:48.184428 336988 finetune.py:68] layer 14_q @ epoch 2 new loss 1.3440118891594466e-05 old loss 1.3658434909302741e-05 BETTER
I0315 04:43:58.211169 336419 finetune.py:76] layer 13_q @ epoch 4 new loss 1.1944099242100492e-05 old loss 1.1791055840149056e-05 WORSE
W0315 04:43:59.407727 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_q proxy err 0.0020543416030704975 tr(WHW.T) 5300.26806640625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]  6%|▋         | 2/32 [00:01<00:18,  1.63it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s]I0315 04:44:08.282603 337557 finetune.py:68] layer 15_q @ epoch 1 new loss 1.4292665582615882e-05 old loss 1.4695907339046244e-05 BETTER
 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0315 04:44:13.955084 335849 finetune.py:68] layer 12_k @ epoch 0 new loss 1.4549089428328443e-05 old loss 1.570795757288579e-05 BETTER
I0315 04:44:20.406774 336419 finetune.py:45] layer 13_k initial loss 1.4876850400469266e-05
W0315 04:44:20.407166 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:44:24.294806 336988 finetune.py:68] layer 14_q @ epoch 3 new loss 1.3272159776533954e-05 old loss 1.3440118891594466e-05 BETTER
I0315 04:44:43.779353 337557 finetune.py:68] layer 15_q @ epoch 2 new loss 1.4181123333401047e-05 old loss 1.4292665582615882e-05 BETTER
I0315 04:44:51.952789 335849 finetune.py:68] layer 12_k @ epoch 1 new loss 1.4366764844453428e-05 old loss 1.4549089428328443e-05 BETTER
I0315 04:44:56.049453 336419 finetune.py:68] layer 13_k @ epoch 0 new loss 1.4351414392876904e-05 old loss 1.4876850400469266e-05 BETTER
I0315 04:45:00.888800 336988 finetune.py:68] layer 14_q @ epoch 4 new loss 1.3146600394975394e-05 old loss 1.3272159776533954e-05 BETTER
W0315 04:45:02.535269 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_q proxy err 0.0018484843894839287 tr(WHW.T) 5553.27783203125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.09it/s]  6%|▋         | 2/32 [00:01<00:17,  1.68it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.67it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.71it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.71it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.70it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.70it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0315 04:45:19.164089 337557 finetune.py:68] layer 15_q @ epoch 3 new loss 1.3946655599283986e-05 old loss 1.4181123333401047e-05 BETTER
I0315 04:45:23.321700 336988 finetune.py:45] layer 14_k initial loss 1.8509188521420583e-05
W0315 04:45:23.322082 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:45:29.739303 335849 finetune.py:68] layer 12_k @ epoch 2 new loss 1.425616028427612e-05 old loss 1.4366764844453428e-05 BETTER
I0315 04:45:32.235459 336419 finetune.py:68] layer 13_k @ epoch 1 new loss 1.419670479663182e-05 old loss 1.4351414392876904e-05 BETTER
I0315 04:45:54.746900 337557 finetune.py:68] layer 15_q @ epoch 4 new loss 1.385613541060593e-05 old loss 1.3946655599283986e-05 BETTER
W0315 04:45:56.552240 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_q proxy err 0.0018613138236105442 tr(WHW.T) 6711.75244140625
  0%|          | 0/32 [00:00<?, ?it/s]I0315 04:45:58.642624 336988 finetune.py:68] layer 14_k @ epoch 0 new loss 1.7673593902145512e-05 old loss 1.8509188521420583e-05 BETTER
  3%|▎         | 1/32 [00:01<00:33,  1.08s/it]  6%|▋         | 2/32 [00:01<00:20,  1.48it/s]  9%|▉         | 3/32 [00:01<00:15,  1.84it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.52it/s] 41%|████      | 13/32 [00:05<00:07,  2.52it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.56it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.56it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s]I0315 04:46:07.779839 335849 finetune.py:68] layer 12_k @ epoch 3 new loss 1.41657610583934e-05 old loss 1.425616028427612e-05 BETTER
 75%|███████▌  | 24/32 [00:10<00:03,  2.55it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s]I0315 04:46:08.781065 336419 finetune.py:68] layer 13_k @ epoch 2 new loss 1.4076262232265435e-05 old loss 1.419670479663182e-05 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.55it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.56it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.57it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.57it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]100%|██████████| 32/32 [00:13<00:00,  2.57it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
I0315 04:46:18.069750 337557 finetune.py:45] layer 15_k initial loss 1.7728520106174983e-05
W0315 04:46:18.070062 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:46:34.693426 336988 finetune.py:68] layer 14_k @ epoch 1 new loss 1.7478603695053607e-05 old loss 1.7673593902145512e-05 BETTER
I0315 04:46:45.077482 336419 finetune.py:76] layer 13_k @ epoch 3 new loss 1.419064574292861e-05 old loss 1.4076262232265435e-05 WORSE
I0315 04:46:45.927339 335849 finetune.py:76] layer 12_k @ epoch 4 new loss 1.4240562450140715e-05 old loss 1.41657610583934e-05 WORSE
W0315 04:46:47.109556 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_k proxy err 0.0011716664303094149 tr(WHW.T) 4333.2451171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.16it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s]I0315 04:46:52.438938 337557 finetune.py:68] layer 15_k @ epoch 0 new loss 1.7143738659797236e-05 old loss 1.7728520106174983e-05 BETTER
 28%|██▊       | 9/32 [00:03<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.56it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.53it/s] 50%|█████     | 16/32 [00:06<00:06,  2.53it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.54it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.54it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.56it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.55it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.54it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.54it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.55it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.46it/s]
I0315 04:47:09.182945 335849 finetune.py:45] layer 12_o initial loss 2.5292807549703866e-05
W0315 04:47:09.183485 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:47:11.247601 336988 finetune.py:68] layer 14_k @ epoch 2 new loss 1.732888813421596e-05 old loss 1.7478603695053607e-05 BETTER
I0315 04:47:21.067663 336419 finetune.py:68] layer 13_k @ epoch 4 new loss 1.404749127686955e-05 old loss 1.4076262232265435e-05 BETTER
W0315 04:47:22.780975 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_k proxy err 0.0015425110468640924 tr(WHW.T) 4510.34521484375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.19it/s]  6%|▋         | 2/32 [00:01<00:17,  1.72it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s]I0315 04:47:27.717405 337557 finetune.py:68] layer 15_k @ epoch 1 new loss 1.6998606952256523e-05 old loss 1.7143738659797236e-05 BETTER
 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.49it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.49it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.50it/s] 41%|████      | 13/32 [00:05<00:07,  2.51it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.51it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.49it/s] 50%|█████     | 16/32 [00:06<00:06,  2.50it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.51it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.52it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.53it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.53it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.53it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.53it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.51it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.50it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.51it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.51it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.51it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.43it/s]
I0315 04:47:45.033148 336419 finetune.py:45] layer 13_o initial loss 2.760562529147137e-05
W0315 04:47:45.033562 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:47:46.334002 335849 finetune.py:68] layer 12_o @ epoch 0 new loss 2.4680179194547236e-05 old loss 2.5292807549703866e-05 BETTER
I0315 04:47:47.866141 336988 finetune.py:68] layer 14_k @ epoch 3 new loss 1.7205142285092734e-05 old loss 1.732888813421596e-05 BETTER
I0315 04:48:03.182846 337557 finetune.py:68] layer 15_k @ epoch 2 new loss 1.6883584976312704e-05 old loss 1.6998606952256523e-05 BETTER
I0315 04:48:20.369931 336419 finetune.py:68] layer 13_o @ epoch 0 new loss 2.680924444575794e-05 old loss 2.760562529147137e-05 BETTER
I0315 04:48:24.416778 335849 finetune.py:68] layer 12_o @ epoch 1 new loss 2.4362396288779564e-05 old loss 2.4680179194547236e-05 BETTER
I0315 04:48:24.558724 336988 finetune.py:76] layer 14_k @ epoch 4 new loss 1.7283296983805485e-05 old loss 1.7205142285092734e-05 WORSE
W0315 04:48:25.863168 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_k proxy err 0.0013592925388365984 tr(WHW.T) 4942.00537109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.28it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.50it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.65it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.64it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s]I0315 04:48:39.008702 337557 finetune.py:68] layer 15_k @ epoch 3 new loss 1.6805350242066197e-05 old loss 1.6883584976312704e-05 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0315 04:48:47.107542 336988 finetune.py:45] layer 14_o initial loss 3.121445843135007e-05
W0315 04:48:47.107903 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:48:56.678636 336419 finetune.py:68] layer 13_o @ epoch 1 new loss 2.6451914891367778e-05 old loss 2.680924444575794e-05 BETTER
I0315 04:49:02.888821 335849 finetune.py:68] layer 12_o @ epoch 2 new loss 2.410911474726163e-05 old loss 2.4362396288779564e-05 BETTER
I0315 04:49:14.991588 337557 finetune.py:68] layer 15_k @ epoch 4 new loss 1.6712776414351538e-05 old loss 1.6805350242066197e-05 BETTER
W0315 04:49:16.694137 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

15_k proxy err 0.0014407031703740358 tr(WHW.T) 4503.8759765625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.19it/s]  6%|▋         | 2/32 [00:01<00:17,  1.74it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.46it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.48it/s]I0315 04:49:22.726008 336988 finetune.py:68] layer 14_o @ epoch 0 new loss 3.0393555789487436e-05 old loss 3.121445843135007e-05 BETTER
 34%|███▍      | 11/32 [00:04<00:08,  2.49it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.51it/s] 41%|████      | 13/32 [00:05<00:07,  2.52it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.53it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.53it/s] 50%|█████     | 16/32 [00:06<00:06,  2.53it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.53it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.53it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.53it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.51it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.51it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.52it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.52it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.53it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.53it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.53it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.53it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.52it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.51it/s]100%|██████████| 32/32 [00:13<00:00,  2.50it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
I0315 04:49:32.862110 336419 finetune.py:68] layer 13_o @ epoch 2 new loss 2.6182548026554286e-05 old loss 2.6451914891367778e-05 BETTER
I0315 04:49:38.459244 337557 finetune.py:45] layer 15_o initial loss 3.166951137245633e-05
W0315 04:49:38.459644 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:49:40.559361 335849 finetune.py:68] layer 12_o @ epoch 3 new loss 2.3889815565780737e-05 old loss 2.410911474726163e-05 BETTER
I0315 04:49:58.923703 336988 finetune.py:68] layer 14_o @ epoch 1 new loss 2.9930544769740663e-05 old loss 3.0393555789487436e-05 BETTER
I0315 04:50:08.807833 336419 finetune.py:68] layer 13_o @ epoch 3 new loss 2.595879414002411e-05 old loss 2.6182548026554286e-05 BETTER
I0315 04:50:13.051183 337557 finetune.py:68] layer 15_o @ epoch 0 new loss 3.0740957299713045e-05 old loss 3.166951137245633e-05 BETTER
I0315 04:50:18.131581 335849 finetune.py:68] layer 12_o @ epoch 4 new loss 2.3701413738308474e-05 old loss 2.3889815565780737e-05 BETTER
W0315 04:50:19.796966 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_o proxy err 0.01056727860122919 tr(WHW.T) 5.644436836242676
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.06s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it]I0315 04:50:34.851895 336988 finetune.py:68] layer 14_o @ epoch 2 new loss 2.9597831598948687e-05 old loss 2.9930544769740663e-05 BETTER
 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it]I0315 04:50:44.827708 336419 finetune.py:68] layer 13_o @ epoch 4 new loss 2.577007580839563e-05 old loss 2.595879414002411e-05 BETTER
 50%|█████     | 16/32 [00:25<00:24,  1.53s/it]W0315 04:50:46.434658 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

13_o proxy err 0.01056668721139431 tr(WHW.T) 6.771387100219727
  0%|          | 0/32 [00:00<?, ?it/s] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it]I0315 04:50:48.096755 337557 finetune.py:68] layer 15_o @ epoch 1 new loss 3.0285509637906216e-05 old loss 3.0740957299713045e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it]  3%|▎         | 1/32 [00:02<01:03,  2.06s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.54s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.59s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.56s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.57s/it]I0315 04:51:10.698208 336988 finetune.py:68] layer 14_o @ epoch 3 new loss 2.931180461018812e-05 old loss 2.9597831598948687e-05 BETTER
100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
 47%|████▋     | 15/32 [00:23<00:26,  1.57s/it] 50%|█████     | 16/32 [00:25<00:25,  1.57s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.57s/it] 56%|█████▋    | 18/32 [00:28<00:22,  1.57s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.57s/it]I0315 04:51:19.359449 335849 finetune.py:45] layer 12_up initial loss 4.7507121053058654e-05
W0315 04:51:19.359886 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 62%|██████▎   | 20/32 [00:31<00:18,  1.57s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.58s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.58s/it]I0315 04:51:23.127347 337557 finetune.py:68] layer 15_o @ epoch 2 new loss 2.9961100153741427e-05 old loss 3.0285509637906216e-05 BETTER
 72%|███████▏  | 23/32 [00:36<00:14,  1.57s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.58s/it] 78%|███████▊  | 25/32 [00:39<00:11,  1.57s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.57s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.57s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.57s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.57s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.57s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.57s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]
I0315 04:51:46.347994 336419 finetune.py:45] layer 13_up initial loss 5.296536983223632e-05
W0315 04:51:46.348351 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:51:46.817975 336988 finetune.py:68] layer 14_o @ epoch 4 new loss 2.9073547921143472e-05 old loss 2.931180461018812e-05 BETTER
W0315 04:51:48.403978 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_o proxy err 0.011030455119907856 tr(WHW.T) 6.878230094909668
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.03s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it]I0315 04:51:54.789577 335849 finetune.py:68] layer 12_up @ epoch 0 new loss 4.679030826082453e-05 old loss 4.7507121053058654e-05 BETTER
  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it]I0315 04:51:58.146839 337557 finetune.py:68] layer 15_o @ epoch 3 new loss 2.9688260838156566e-05 old loss 2.9961100153741427e-05 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.51s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it]I0315 04:52:20.541344 336419 finetune.py:68] layer 13_up @ epoch 0 new loss 5.214030898059718e-05 old loss 5.296536983223632e-05 BETTER
 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.50s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.50s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it]I0315 04:52:31.335129 335849 finetune.py:68] layer 12_up @ epoch 1 new loss 4.627308589988388e-05 old loss 4.679030826082453e-05 BETTER
 88%|████████▊ | 28/32 [00:42<00:06,  1.50s/it]I0315 04:52:33.624034 337557 finetune.py:68] layer 15_o @ epoch 4 new loss 2.9465616535162553e-05 old loss 2.9688260838156566e-05 BETTER
 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it]W0315 04:52:35.163709 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 94%|█████████▍| 30/32 [00:45<00:02,  1.50s/it]15_o proxy err 0.011774173937737942 tr(WHW.T) 6.784595489501953
  0%|          | 0/32 [00:00<?, ?it/s] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
  3%|▎         | 1/32 [00:02<01:03,  2.05s/it]  6%|▋         | 2/32 [00:03<00:53,  1.77s/it]  9%|▉         | 3/32 [00:05<00:49,  1.69s/it] 12%|█▎        | 4/32 [00:06<00:46,  1.65s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.62s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.61s/it]I0315 04:52:46.549942 336988 finetune.py:45] layer 14_up initial loss 6.169924017740414e-05
W0315 04:52:46.550262 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 22%|██▏       | 7/32 [00:11<00:39,  1.60s/it] 25%|██▌       | 8/32 [00:13<00:38,  1.59s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.59s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.58s/it] 34%|███▍      | 11/32 [00:17<00:33,  1.58s/it]I0315 04:52:55.614149 336419 finetune.py:68] layer 13_up @ epoch 1 new loss 5.154933751327917e-05 old loss 5.214030898059718e-05 BETTER
 38%|███▊      | 12/32 [00:19<00:31,  1.58s/it] 41%|████      | 13/32 [00:20<00:29,  1.58s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.58s/it] 47%|████▋     | 15/32 [00:24<00:26,  1.58s/it] 50%|█████     | 16/32 [00:25<00:25,  1.58s/it] 53%|█████▎    | 17/32 [00:27<00:23,  1.58s/it] 56%|█████▋    | 18/32 [00:28<00:22,  1.58s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.57s/it]I0315 04:53:07.964608 335849 finetune.py:68] layer 12_up @ epoch 2 new loss 4.582443943945691e-05 old loss 4.627308589988388e-05 BETTER
 62%|██████▎   | 20/32 [00:32<00:18,  1.58s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.57s/it] 69%|██████▉   | 22/32 [00:35<00:15,  1.57s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.57s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.57s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.57s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.57s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.57s/it]I0315 04:53:20.763089 336988 finetune.py:68] layer 14_up @ epoch 0 new loss 6.061765816411935e-05 old loss 6.169924017740414e-05 BETTER
 88%|████████▊ | 28/32 [00:44<00:06,  1.56s/it] 91%|█████████ | 29/32 [00:46<00:04,  1.56s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.56s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.59s/it]
I0315 04:53:30.210675 336419 finetune.py:68] layer 13_up @ epoch 2 new loss 5.10380050400272e-05 old loss 5.154933751327917e-05 BETTER
I0315 04:53:35.221947 337557 finetune.py:45] layer 15_up initial loss 6.786033191019669e-05
W0315 04:53:35.222421 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:53:44.472700 335849 finetune.py:68] layer 12_up @ epoch 3 new loss 4.541849557426758e-05 old loss 4.582443943945691e-05 BETTER
I0315 04:53:55.808368 336988 finetune.py:68] layer 14_up @ epoch 1 new loss 5.98437873122748e-05 old loss 6.061765816411935e-05 BETTER
I0315 04:54:05.232477 336419 finetune.py:68] layer 13_up @ epoch 3 new loss 5.057624730397947e-05 old loss 5.10380050400272e-05 BETTER
I0315 04:54:08.988911 337557 finetune.py:68] layer 15_up @ epoch 0 new loss 6.646302790613845e-05 old loss 6.786033191019669e-05 BETTER
I0315 04:54:21.374258 335849 finetune.py:68] layer 12_up @ epoch 4 new loss 4.5048636820865795e-05 old loss 4.541849557426758e-05 BETTER
W0315 04:54:22.876026 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

12_up proxy err 0.00878096092492342 tr(WHW.T) 854.7149658203125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it]I0315 04:54:30.936490 336988 finetune.py:68] layer 14_up @ epoch 2 new loss 5.917573071201332e-05 old loss 5.98437873122748e-05 BETTER
 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it]I0315 04:54:40.229792 336419 finetune.py:68] layer 13_up @ epoch 4 new loss 5.015909482608549e-05 old loss 5.057624730397947e-05 BETTER
W0315 04:54:41.648219 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it]13_up proxy err 0.008652926422655582 tr(WHW.T) 915.7598876953125
  0%|          | 0/32 [00:00<?, ?it/s] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it]I0315 04:54:43.679254 337557 finetune.py:68] layer 15_up @ epoch 1 new loss 6.54871400911361e-05 old loss 6.646302790613845e-05 BETTER
 41%|████      | 13/32 [00:20<00:29,  1.53s/it]  3%|▎         | 1/32 [00:01<01:00,  1.95s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.55s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.55s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it]I0315 04:55:05.723911 336988 finetune.py:68] layer 14_up @ epoch 3 new loss 5.8584122598404065e-05 old loss 5.917573071201332e-05 BETTER
 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 50%|█████     | 16/32 [00:25<00:24,  1.54s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.55s/it]I0315 04:55:17.797271 337557 finetune.py:68] layer 15_up @ epoch 2 new loss 6.466421473305672e-05 old loss 6.54871400911361e-05 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.55s/it]I0315 04:55:22.410812 335849 finetune.py:45] layer 12_gate initial loss 5.93066361034289e-05
W0315 04:55:22.411259 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 81%|████████▏ | 26/32 [00:40<00:09,  1.60s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.58s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.58s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.57s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.56s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]
I0315 04:55:40.974806 336988 finetune.py:68] layer 14_up @ epoch 4 new loss 5.8048361097462475e-05 old loss 5.8584122598404065e-05 BETTER
I0315 04:55:41.452639 336419 finetune.py:45] layer 13_gate initial loss 6.590352131752297e-05
W0315 04:55:41.452898 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0315 04:55:42.455359 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_up proxy err 0.00931088998913765 tr(WHW.T) 931.4024047851562
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]  9%|▉         | 3/32 [00:04<00:46,  1.61s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it]I0315 04:55:51.857948 337557 finetune.py:68] layer 15_up @ epoch 3 new loss 6.392280920408666e-05 old loss 6.466421473305672e-05 BETTER
 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it]I0315 04:55:57.059763 335849 finetune.py:68] layer 12_gate @ epoch 0 new loss 5.87838985666167e-05 old loss 5.93066361034289e-05 BETTER
 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it]I0315 04:56:14.265605 336419 finetune.py:68] layer 13_gate @ epoch 0 new loss 6.530323298648e-05 old loss 6.590352131752297e-05 BETTER
 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it]I0315 04:56:26.260480 337557 finetune.py:68] layer 15_up @ epoch 4 new loss 6.326209404505789e-05 old loss 6.392280920408666e-05 BETTER
 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it]W0315 04:56:27.706631 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it]15_up proxy err 0.009353064931929111 tr(WHW.T) 988.3074340820312
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it]  3%|▎         | 1/32 [00:02<01:02,  2.00s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]I0315 04:56:32.170592 335849 finetune.py:68] layer 12_gate @ epoch 1 new loss 5.842702375957742e-05 old loss 5.87838985666167e-05 BETTER
  6%|▋         | 2/32 [00:03<00:52,  1.74s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.63s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.58s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it]I0315 04:56:40.789761 336988 finetune.py:45] layer 14_gate initial loss 7.587381696794182e-05
W0315 04:56:40.790177 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.57s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.57s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.57s/it]I0315 04:56:47.802764 336419 finetune.py:68] layer 13_gate @ epoch 1 new loss 6.48884306428954e-05 old loss 6.530323298648e-05 BETTER
 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it] 41%|████      | 13/32 [00:20<00:29,  1.57s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it] 50%|█████     | 16/32 [00:25<00:24,  1.56s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.55s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.56s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it]I0315 04:57:07.320334 335849 finetune.py:68] layer 12_gate @ epoch 2 new loss 5.8106415963266045e-05 old loss 5.842702375957742e-05 BETTER
 78%|███████▊  | 25/32 [00:39<00:10,  1.56s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.56s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.56s/it]I0315 04:57:13.709877 336988 finetune.py:68] layer 14_gate @ epoch 0 new loss 7.508022099500522e-05 old loss 7.587381696794182e-05 BETTER
 91%|█████████ | 29/32 [00:45<00:04,  1.55s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.55s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0315 04:57:20.963673 336419 finetune.py:68] layer 13_gate @ epoch 2 new loss 6.452079833252355e-05 old loss 6.48884306428954e-05 BETTER
I0315 04:57:27.443348 337557 finetune.py:45] layer 15_gate initial loss 8.375682955374941e-05
W0315 04:57:27.443731 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 04:57:42.330591 335849 finetune.py:68] layer 12_gate @ epoch 3 new loss 5.78048056922853e-05 old loss 5.8106415963266045e-05 BETTER
I0315 04:57:47.028467 336988 finetune.py:68] layer 14_gate @ epoch 1 new loss 7.456822640961036e-05 old loss 7.508022099500522e-05 BETTER
I0315 04:57:53.753325 336419 finetune.py:68] layer 13_gate @ epoch 3 new loss 6.418170232791454e-05 old loss 6.452079833252355e-05 BETTER
I0315 04:57:59.401454 337557 finetune.py:68] layer 15_gate @ epoch 0 new loss 8.269769750768319e-05 old loss 8.375682955374941e-05 BETTER
I0315 04:58:17.483239 335849 finetune.py:68] layer 12_gate @ epoch 4 new loss 5.7527522585587576e-05 old loss 5.78048056922853e-05 BETTER
W0315 04:58:18.987512 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 04:58:20.599080 336988 finetune.py:68] layer 14_gate @ epoch 2 new loss 7.40909090382047e-05 old loss 7.456822640961036e-05 BETTER
12_gate proxy err 0.0033597962465137243 tr(WHW.T) 3175.857421875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.11it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s]  3%|▎         | 3/112 [00:01<00:54,  1.99it/s]  4%|▎         | 4/112 [00:02<00:49,  2.19it/s]  4%|▍         | 5/112 [00:02<00:46,  2.30it/s]  5%|▌         | 6/112 [00:02<00:44,  2.39it/s]  6%|▋         | 7/112 [00:03<00:42,  2.44it/s]  7%|▋         | 8/112 [00:03<00:42,  2.47it/s]  8%|▊         | 9/112 [00:04<00:41,  2.50it/s]  9%|▉         | 10/112 [00:04<00:40,  2.51it/s]I0315 04:58:26.906509 336419 finetune.py:68] layer 13_gate @ epoch 4 new loss 6.385936285369098e-05 old loss 6.418170232791454e-05 BETTER
 10%|▉         | 11/112 [00:04<00:39,  2.53it/s] 11%|█         | 12/112 [00:05<00:39,  2.53it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.54it/s]W0315 04:58:28.175857 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 12%|█▎        | 14/112 [00:05<00:38,  2.55it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.57it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.57it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.58it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.58it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.58it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.58it/s]13_gate proxy err 0.003166835755109787 tr(WHW.T) 3552.197998046875
  0%|          | 0/112 [00:00<?, ?it/s] 21%|██        | 23/112 [00:09<00:34,  2.57it/s]I0315 04:58:31.894622 337557 finetune.py:68] layer 15_gate @ epoch 1 new loss 8.202985918615013e-05 old loss 8.269769750768319e-05 BETTER
 21%|██▏       | 24/112 [00:09<00:34,  2.55it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.55it/s]  2%|▏         | 2/112 [00:01<01:06,  1.66it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.55it/s]  3%|▎         | 3/112 [00:01<00:55,  1.96it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.56it/s]  4%|▎         | 4/112 [00:02<00:50,  2.15it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.57it/s]  4%|▍         | 5/112 [00:02<00:47,  2.27it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.57it/s]  5%|▌         | 6/112 [00:02<00:45,  2.34it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.57it/s]  6%|▋         | 7/112 [00:03<00:43,  2.39it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s]  7%|▋         | 8/112 [00:03<00:42,  2.43it/s] 29%|██▊       | 32/112 [00:12<00:31,  2.56it/s]  8%|▊         | 9/112 [00:04<00:41,  2.45it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.56it/s]  9%|▉         | 10/112 [00:04<00:41,  2.47it/s] 30%|███       | 34/112 [00:13<00:30,  2.55it/s] 10%|▉         | 11/112 [00:04<00:40,  2.48it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.56it/s] 11%|█         | 12/112 [00:05<00:40,  2.48it/s] 32%|███▏      | 36/112 [00:14<00:30,  2.53it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.48it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.53it/s] 12%|█▎        | 14/112 [00:06<00:39,  2.49it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.54it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.49it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.54it/s] 14%|█▍        | 16/112 [00:06<00:38,  2.51it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.56it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.52it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.57it/s] 16%|█▌        | 18/112 [00:07<00:37,  2.51it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.56it/s] 17%|█▋        | 19/112 [00:08<00:36,  2.52it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.57it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.52it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.57it/s] 19%|█▉        | 21/112 [00:08<00:36,  2.52it/s] 40%|████      | 45/112 [00:18<00:26,  2.57it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.52it/s] 41%|████      | 46/112 [00:18<00:25,  2.57it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.57it/s] 21%|██        | 23/112 [00:09<00:35,  2.49it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.53it/s] 21%|██▏       | 24/112 [00:10<00:35,  2.50it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.55it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.51it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.56it/s] 23%|██▎       | 26/112 [00:10<00:34,  2.51it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.56it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.51it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.57it/s] 25%|██▌       | 28/112 [00:11<00:33,  2.51it/s] 47%|████▋     | 53/112 [00:21<00:22,  2.57it/s] 26%|██▌       | 29/112 [00:12<00:32,  2.52it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.57it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.52it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.58it/s] 28%|██▊       | 31/112 [00:12<00:32,  2.52it/s] 50%|█████     | 56/112 [00:22<00:21,  2.57it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.51it/s] 51%|█████     | 57/112 [00:22<00:21,  2.57it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.51it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.57it/s] 30%|███       | 34/112 [00:14<00:31,  2.51it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.57it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.49it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.53it/s] 32%|███▏      | 36/112 [00:14<00:30,  2.49it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.54it/s] 33%|███▎      | 37/112 [00:15<00:30,  2.49it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.55it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.50it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.56it/s] 35%|███▍      | 39/112 [00:16<00:29,  2.51it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.56it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.51it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.56it/s] 37%|███▋      | 41/112 [00:16<00:28,  2.51it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.56it/s] 38%|███▊      | 42/112 [00:17<00:27,  2.51it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.57it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.51it/s] 61%|██████    | 68/112 [00:27<00:17,  2.58it/s] 39%|███▉      | 44/112 [00:18<00:27,  2.50it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s] 40%|████      | 45/112 [00:18<00:26,  2.50it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.56it/s] 41%|████      | 46/112 [00:18<00:26,  2.47it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.56it/s] 42%|████▏     | 47/112 [00:19<00:26,  2.49it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.54it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.49it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.55it/s] 44%|████▍     | 49/112 [00:20<00:25,  2.50it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.56it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.51it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.57it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.51it/s] 68%|██████▊   | 76/112 [00:30<00:13,  2.57it/s] 46%|████▋     | 52/112 [00:21<00:23,  2.52it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.58it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.52it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.58it/s] 48%|████▊     | 54/112 [00:22<00:23,  2.51it/s] 71%|███████   | 79/112 [00:31<00:12,  2.58it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.51it/s]I0315 04:58:54.066547 336988 finetune.py:68] layer 14_gate @ epoch 3 new loss 7.366257341345772e-05 old loss 7.40909090382047e-05 BETTER
 71%|███████▏  | 80/112 [00:31<00:12,  2.59it/s] 50%|█████     | 56/112 [00:22<00:22,  2.52it/s] 72%|███████▏  | 81/112 [00:32<00:11,  2.59it/s] 51%|█████     | 57/112 [00:23<00:22,  2.49it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.58it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.49it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.57it/s] 53%|█████▎    | 59/112 [00:24<00:21,  2.49it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.56it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.50it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.53it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.51it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.55it/s] 55%|█████▌    | 62/112 [00:25<00:19,  2.51it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.56it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.51it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.56it/s] 57%|█████▋    | 64/112 [00:26<00:19,  2.51it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.57it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.51it/s] 80%|████████  | 90/112 [00:35<00:08,  2.56it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.56it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.50it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.57it/s] 60%|█████▉    | 67/112 [00:27<00:17,  2.50it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.57it/s] 61%|██████    | 68/112 [00:27<00:17,  2.51it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.57it/s] 62%|██████▏   | 69/112 [00:28<00:17,  2.49it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.57it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.50it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.57it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.51it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.57it/s] 64%|██████▍   | 72/112 [00:29<00:15,  2.51it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.54it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.52it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.55it/s] 66%|██████▌   | 74/112 [00:30<00:15,  2.52it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.56it/s] 67%|██████▋   | 75/112 [00:30<00:14,  2.52it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.57it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.52it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.58it/s] 69%|██████▉   | 77/112 [00:31<00:13,  2.52it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.58it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.51it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.58it/s] 71%|███████   | 79/112 [00:32<00:13,  2.51it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.59it/s] 71%|███████▏  | 80/112 [00:32<00:12,  2.49it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.58it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.49it/s]I0315 04:59:04.539143 337557 finetune.py:68] layer 15_gate @ epoch 2 new loss 8.144219464156777e-05 old loss 8.202985918615013e-05 BETTER
 96%|█████████▌| 107/112 [00:42<00:01,  2.59it/s] 73%|███████▎  | 82/112 [00:33<00:11,  2.52it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.59it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.54it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.57it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.54it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.54it/s] 76%|███████▌  | 85/112 [00:34<00:10,  2.55it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.55it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.56it/s]100%|██████████| 112/112 [00:44<00:00,  2.56it/s]100%|██████████| 112/112 [00:44<00:00,  2.53it/s]
 78%|███████▊  | 87/112 [00:35<00:09,  2.56it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.56it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.54it/s] 80%|████████  | 90/112 [00:36<00:08,  2.53it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.53it/s] 82%|████████▏ | 92/112 [00:37<00:08,  2.50it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.51it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.53it/s] 85%|████████▍ | 95/112 [00:38<00:06,  2.53it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.54it/s] 87%|████████▋ | 97/112 [00:39<00:05,  2.54it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.54it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.54it/s] 89%|████████▉ | 100/112 [00:40<00:04,  2.54it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.53it/s] 91%|█████████ | 102/112 [00:41<00:03,  2.52it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.49it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.50it/s] 94%|█████████▍| 105/112 [00:42<00:02,  2.51it/s]W0315 04:59:13.959000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:13.960000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:13.960000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:13.960000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:13.960000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:13.960000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:59:13.960000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.004000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.004000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.004000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.004000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.004000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.020000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.020000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.020000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.020000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.020000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.196000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.196000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.196000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.196000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.196000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
 95%|█████████▍| 106/112 [00:42<00:02,  2.51it/s]W0315 04:59:14.528000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.528000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.528000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.528000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.528000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.529000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.529000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.562000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.562000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.562000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.562000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.562000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.637000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.637000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.638000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.638000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:14.638000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 96%|█████████▌| 107/112 [00:43<00:01,  2.52it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.52it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.53it/s] 98%|█████████▊| 110/112 [00:44<00:00,  2.53it/s]W0315 04:59:15.867000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:15.882000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:15.890000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:15.890000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 99%|█████████▉| 111/112 [00:44<00:00,  2.53it/s]W0315 04:59:16.351000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.351000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.351000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.351000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.351000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.351000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.351000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.382000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.382000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.382000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.383000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.383000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
100%|██████████| 112/112 [00:45<00:00,  2.54it/s]100%|██████████| 112/112 [00:45<00:00,  2.49it/s]
W0315 04:59:16.735000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.735000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.735000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.735000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.735000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.735000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.735000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:16.735000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:17.039000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:17.039000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:17.039000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:17.039000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:17.039000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:17.390000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:17.396000 139731980457792 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.440000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.441000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.441000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.441000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.441000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.441000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.441000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.485000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.485000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.485000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.485000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.485000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.502000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.502000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.502000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.502000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.502000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.684000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.684000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.685000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.685000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:24.685000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
I0315 04:59:24.802448 335849 finetune.py:45] layer 12_down initial loss 8.450022141914815e-05
W0315 04:59:24.802851 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0315 04:59:25.028000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.028000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.029000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.029000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.029000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.029000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.029000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.062000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.063000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.063000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.063000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.063000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.137000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.137000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.137000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.137000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:25.138000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.395000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.408000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.417000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.417000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.895000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.895000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.895000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.896000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.896000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.896000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.896000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.929000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.929000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.929000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.929000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:26.929000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
I0315 04:59:27.299124 336988 finetune.py:68] layer 14_gate @ epoch 4 new loss 7.326588820433244e-05 old loss 7.366257341345772e-05 BETTER
W0315 04:59:27.300000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.300000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.300000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.301000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.301000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.301000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.301000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.301000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.631000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.631000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.631000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.632000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.632000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.982000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 04:59:27.987000 139764559828800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0315 04:59:28.442435 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

14_gate proxy err 0.0030760825611650944 tr(WHW.T) 4239.72412109375
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:39,  1.11it/s]  2%|▏         | 2/112 [00:01<01:05,  1.69it/s]  3%|▎         | 3/112 [00:01<00:54,  2.02it/s]  4%|▎         | 4/112 [00:02<00:48,  2.23it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s]  5%|▌         | 6/112 [00:02<00:43,  2.45it/s]  6%|▋         | 7/112 [00:03<00:41,  2.52it/s]I0315 04:59:35.249797 336419 finetune.py:45] layer 13_down initial loss 9.600351040717214e-05
W0315 04:59:35.250163 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  7%|▋         | 8/112 [00:03<00:40,  2.57it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s]  9%|▉         | 10/112 [00:04<00:38,  2.62it/s] 10%|▉         | 11/112 [00:04<00:38,  2.62it/s] 11%|█         | 12/112 [00:05<00:38,  2.62it/s]I0315 04:59:37.125495 337557 finetune.py:68] layer 15_gate @ epoch 3 new loss 8.091014751698822e-05 old loss 8.144219464156777e-05 BETTER
 12%|█▏        | 13/112 [00:05<00:38,  2.59it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.61it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.61it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.63it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.63it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.63it/s] 18%|█▊        | 20/112 [00:08<00:34,  2.64it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.65it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.64it/s] 21%|██        | 23/112 [00:09<00:33,  2.64it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.65it/s] 22%|██▏       | 25/112 [00:09<00:33,  2.62it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.63it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.64it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.64it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.64it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.64it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.63it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.63it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.63it/s] 30%|███       | 34/112 [00:13<00:29,  2.63it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.63it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.63it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.58it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.59it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.60it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.61it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.61it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.61it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.62it/s] 40%|████      | 45/112 [00:17<00:25,  2.62it/s] 41%|████      | 46/112 [00:18<00:25,  2.62it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.62it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.61it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.61it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.59it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.60it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.61it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.61it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.62it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.62it/s] 50%|█████     | 56/112 [00:21<00:21,  2.62it/s] 51%|█████     | 57/112 [00:22<00:20,  2.63it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.63it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.63it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.62it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.61it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.58it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.59it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.60it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.60it/s]I0315 04:59:57.052740 335849 finetune.py:68] layer 12_down @ epoch 0 new loss 8.44571550260298e-05 old loss 8.450022141914815e-05 BETTER
 59%|█████▉    | 66/112 [00:25<00:17,  2.62it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.62it/s] 61%|██████    | 68/112 [00:26<00:16,  2.63it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.63it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.63it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.63it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.62it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.63it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.63it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.61it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.61it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.62it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.62it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.63it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.63it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.62it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.62it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.63it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.59it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.60it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.61it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.61it/s]I0315 05:00:06.534609 336419 finetune.py:68] layer 13_down @ epoch 0 new loss 9.596525342203677e-05 old loss 9.600351040717214e-05 BETTER
 80%|████████  | 90/112 [00:34<00:08,  2.62it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.62it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.62it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.63it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.63it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.63it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.62it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.62it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.63it/s]I0315 05:00:09.693153 337557 finetune.py:68] layer 15_gate @ epoch 4 new loss 8.041984983719885e-05 old loss 8.091014751698822e-05 BETTER
 88%|████████▊ | 99/112 [00:38<00:04,  2.60it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.61it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.61it/s]W0315 05:00:10.932907 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 102/112 [00:39<00:03,  2.62it/s] 92%|█████████▏| 103/112 [00:40<00:04,  2.21it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.32it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.40it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.47it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.51it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.54it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.57it/s]15_gate proxy err 0.002932025818154216 tr(WHW.T) 5067.3544921875
  0%|          | 0/112 [00:00<?, ?it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.59it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]
  1%|          | 1/112 [00:00<01:38,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.67it/s]  3%|▎         | 3/112 [00:01<00:55,  1.97it/s]  4%|▎         | 4/112 [00:02<00:50,  2.15it/s]  4%|▍         | 5/112 [00:02<00:46,  2.28it/s]  5%|▌         | 6/112 [00:02<00:45,  2.35it/s]  6%|▋         | 7/112 [00:03<00:43,  2.40it/s]  7%|▋         | 8/112 [00:03<00:42,  2.44it/s]  8%|▊         | 9/112 [00:04<00:41,  2.47it/s]  9%|▉         | 10/112 [00:04<00:40,  2.50it/s] 10%|▉         | 11/112 [00:04<00:39,  2.53it/s] 11%|█         | 12/112 [00:05<00:39,  2.55it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.56it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.55it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.55it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.55it/s] 16%|█▌        | 18/112 [00:07<00:37,  2.52it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.52it/s]W0315 05:00:22.520000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.521000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.521000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.521000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.521000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.521000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.521000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.566000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.566000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.566000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.566000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.566000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.582000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.582000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.583000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.583000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.583000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.757000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.757000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.757000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.757000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:22.757000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 18%|█▊        | 20/112 [00:08<00:36,  2.53it/s]W0315 05:00:23.094000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.094000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.094000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.094000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.094000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.094000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.094000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.126000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.126000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.126000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.126000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.126000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 19%|█▉        | 21/112 [00:08<00:35,  2.54it/s]W0315 05:00:23.203000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.204000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.204000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.204000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:23.204000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 20%|█▉        | 22/112 [00:09<00:35,  2.54it/s] 21%|██        | 23/112 [00:09<00:34,  2.54it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.54it/s]W0315 05:00:24.447000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.453000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.460000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.460000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 22%|██▏       | 25/112 [00:10<00:34,  2.55it/s]W0315 05:00:24.923000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.923000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.923000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.923000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.923000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.923000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.923000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.952000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.952000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.953000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.953000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:24.953000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 23%|██▎       | 26/112 [00:10<00:33,  2.54it/s]W0315 05:00:25.313000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.313000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.313000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.313000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.313000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.313000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.314000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.314000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 24%|██▍       | 27/112 [00:11<00:33,  2.54it/s]W0315 05:00:25.624000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.625000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.625000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.625000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.625000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 25%|██▌       | 28/112 [00:11<00:33,  2.54it/s]W0315 05:00:25.973000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:00:25.979000 140111734368064 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 26%|██▌       | 29/112 [00:11<00:33,  2.51it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.52it/s] 28%|██▊       | 31/112 [00:12<00:32,  2.53it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.53it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.54it/s] 30%|███       | 34/112 [00:13<00:30,  2.55it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.55it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.55it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.56it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.56it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.55it/s]I0315 05:00:30.572403 335849 finetune.py:68] layer 12_down @ epoch 1 new loss 8.44535170472227e-05 old loss 8.44571550260298e-05 BETTER
 36%|███▌      | 40/112 [00:16<00:28,  2.55it/s] 37%|███▋      | 41/112 [00:16<00:28,  2.50it/s] 38%|███▊      | 42/112 [00:17<00:27,  2.52it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.53it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.55it/s] 40%|████      | 45/112 [00:18<00:26,  2.55it/s] 41%|████      | 46/112 [00:18<00:25,  2.55it/s]I0315 05:00:33.264426 336988 finetune.py:45] layer 14_down initial loss 0.00011127022298751399
W0315 05:00:33.264949 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 42%|████▏     | 47/112 [00:18<00:25,  2.56it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.56it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.56it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.56it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.56it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.55it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.55it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.53it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.53it/s] 50%|█████     | 56/112 [00:22<00:22,  2.54it/s] 51%|█████     | 57/112 [00:22<00:21,  2.55it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.56it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.56it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.56it/s]I0315 05:00:38.594502 336419 finetune.py:68] layer 13_down @ epoch 1 new loss 9.595537994755432e-05 old loss 9.596525342203677e-05 BETTER
 54%|█████▍    | 61/112 [00:24<00:19,  2.56it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.56it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.55it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.54it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.51it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.52it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.53it/s] 61%|██████    | 68/112 [00:27<00:17,  2.53it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.54it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.55it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.55it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.56it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.56it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.55it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.54it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.54it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.52it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.53it/s] 71%|███████   | 79/112 [00:31<00:13,  2.53it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.54it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.54it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.54it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.55it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.55it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.55it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.55it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.55it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.50it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.51it/s] 80%|████████  | 90/112 [00:35<00:08,  2.52it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.53it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.54it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.55it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.55it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.56it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.57it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.57it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.56it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.56it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.56it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.54it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.54it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.55it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.55it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.56it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.56it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.56it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.56it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.56it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.56it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.55it/s]100%|██████████| 112/112 [00:44<00:00,  2.51it/s]100%|██████████| 112/112 [00:44<00:00,  2.51it/s]
I0315 05:01:03.819762 335849 finetune.py:68] layer 12_down @ epoch 2 new loss 8.444506966043264e-05 old loss 8.44535170472227e-05 BETTER
I0315 05:01:04.207167 336988 finetune.py:68] layer 14_down @ epoch 0 new loss 0.00011120890121674165 old loss 0.00011127022298751399 BETTER
W0315 05:01:06.625000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.625000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.626000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.626000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.626000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.626000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.626000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.671000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.671000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.671000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.671000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.671000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.688000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.688000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.688000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.688000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.688000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.861000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.861000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.861000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.861000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:06.862000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.197000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.198000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.198000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.198000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.198000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.198000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.198000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.230000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.230000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.230000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.230000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.230000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.303000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.303000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.303000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.303000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:07.303000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:08.555000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:08.561000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:08.568000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:08.568000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.033000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.033000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.033000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.033000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.033000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.034000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.034000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.068000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.068000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.068000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.068000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.069000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.426000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.426000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.426000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.426000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.426000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.426000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.427000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.427000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.735000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.735000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.735000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.735000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:01:09.735000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:01:10.085000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:01:10.090000 140010476771136 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 05:01:10.194878 336419 finetune.py:68] layer 13_down @ epoch 2 new loss 9.59463432081975e-05 old loss 9.595537994755432e-05 BETTER
I0315 05:01:17.035133 337557 finetune.py:45] layer 15_down initial loss 0.0001284987956751138
W0315 05:01:17.035421 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:01:35.816557 336988 finetune.py:68] layer 14_down @ epoch 1 new loss 0.00011119848932139575 old loss 0.00011120890121674165 BETTER
I0315 05:01:37.041284 335849 finetune.py:68] layer 12_down @ epoch 3 new loss 8.443897968390957e-05 old loss 8.444506966043264e-05 BETTER
I0315 05:01:41.732008 336419 finetune.py:68] layer 13_down @ epoch 3 new loss 9.594159200787544e-05 old loss 9.59463432081975e-05 BETTER
I0315 05:01:47.083564 337557 finetune.py:68] layer 15_down @ epoch 0 new loss 0.00012843443255405873 old loss 0.0001284987956751138 BETTER
I0315 05:02:07.485767 336988 finetune.py:68] layer 14_down @ epoch 2 new loss 0.00011119374539703131 old loss 0.00011119848932139575 BETTER
I0315 05:02:10.363455 335849 finetune.py:68] layer 12_down @ epoch 4 new loss 8.443283149972558e-05 old loss 8.443897968390957e-05 BETTER
W0315 05:02:11.321947 335849 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

12_down proxy err 0.011391784995794296 tr(WHW.T) 10.052689552307129
I0315 05:02:13.585698 336419 finetune.py:68] layer 13_down @ epoch 4 new loss 9.593576396582648e-05 old loss 9.594159200787544e-05 BETTER
W0315 05:02:14.395186 336419 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

13_down proxy err 0.01166565902531147 tr(WHW.T) 11.732809066772461
I0315 05:02:18.602173 337557 finetune.py:68] layer 15_down @ epoch 1 new loss 0.00012842242722399533 old loss 0.00012843443255405873 BETTER
I0315 05:02:38.885388 336988 finetune.py:68] layer 14_down @ epoch 3 new loss 0.00011118587281089276 old loss 0.00011119374539703131 BETTER
I0315 05:02:49.714546 337557 finetune.py:68] layer 15_down @ epoch 2 new loss 0.00012841397256124765 old loss 0.00012842242722399533 BETTER
I0315 05:03:10.343459 336988 finetune.py:68] layer 14_down @ epoch 4 new loss 0.00011117400572402403 old loss 0.00011118587281089276 BETTER
W0315 05:03:11.154158 336988 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

14_down proxy err 0.012122162617743015 tr(WHW.T) 13.382335662841797
I0315 05:03:20.887825 337557 finetune.py:68] layer 15_down @ epoch 3 new loss 0.0001284067693632096 old loss 0.00012841397256124765 BETTER
I0315 05:03:25.165200 273992 quantize_finetune_llama.py:186] computed original embedding for layer 16 in 66.32721424102783s
I0315 05:03:25.590279 273992 quantize_finetune_llama.py:159] layer 17 gpu 1
I0315 05:03:27.662028 339725 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 05:03:27.662143 339725 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 05:03:27.662201 339725 utils.py:162] NumExpr defaulting to 16 threads.
I0315 05:03:27.870561 339725 config.py:58] PyTorch version 2.4.0 available.
I0315 05:03:30.029231 339725 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 05:03:30.421536 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.42s/it]  6%|▋         | 2/32 [00:01<00:23,  1.28it/s]  9%|▉         | 3/32 [00:02<00:16,  1.73it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.76it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.94it/s] 41%|████      | 13/32 [00:05<00:06,  2.96it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.96it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.96it/s] 50%|█████     | 16/32 [00:06<00:05,  2.98it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.02it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.98it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.99it/s] 62%|██████▎   | 20/32 [00:07<00:04,  3.00it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.99it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.00it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.00it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.02it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.97it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.98it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.99it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.99it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.99it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.01it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  3.02it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]
W0315 05:03:46.857000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.857000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.858000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.858000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.858000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.858000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.858000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.885000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.885000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.885000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.886000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.886000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.902000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.903000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.903000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.903000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:03:46.903000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:03:47.239000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:03:47.239000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:03:47.240000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:03:47.240000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:03:47.240000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.177000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.177000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.177000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.177000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.177000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.177000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.177000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.195000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.196000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.196000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.196000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.196000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.453000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.454000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.454000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.454000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:03:48.455000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.688000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.688000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.689000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.689000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.689000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.689000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.689000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.707000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.708000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.708000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.708000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:03:49.708000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:03:50.660000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:03:50.660000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:03:50.660000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:03:50.660000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:03:50.660000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 05:03:52.070896 337557 finetune.py:68] layer 15_down @ epoch 4 new loss 0.00012839847477152944 old loss 0.0001284067693632096 BETTER
W0315 05:03:52.856570 337557 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

15_down proxy err 0.012137847021222115 tr(WHW.T) 16.94580841064453
I0315 05:03:57.206219 339725 finetune.py:45] layer 16_v initial loss 2.148472412955016e-05
W0315 05:03:57.206610 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:04:27.457968 273992 quantize_finetune_llama.py:186] computed original embedding for layer 17 in 61.40785264968872s
I0315 05:04:27.915899 273992 quantize_finetune_llama.py:159] layer 18 gpu 2
I0315 05:04:29.974238 340292 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 05:04:29.974359 340292 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 05:04:29.974435 340292 utils.py:162] NumExpr defaulting to 16 threads.
I0315 05:04:30.163579 340292 config.py:58] PyTorch version 2.4.0 available.
I0315 05:04:32.366025 340292 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 05:04:32.736327 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 05:04:33.621895 339725 finetune.py:68] layer 16_v @ epoch 0 new loss 1.0843153177120257e-05 old loss 2.148472412955016e-05 BETTER
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:46,  1.49s/it]  6%|▋         | 2/32 [00:01<00:24,  1.22it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.63it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.70it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.82it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.85it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.85it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.88it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.81it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.86it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.88it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.92it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.89it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.91it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:12<00:00,  2.91it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
W0315 05:04:49.105000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.105000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.105000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.106000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.106000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.106000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.106000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.133000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.133000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.134000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.134000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.134000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.151000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.151000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.151000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.151000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.151000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.487000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.487000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.487000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.488000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:04:49.488000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.395000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.395000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.395000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.395000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.395000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.395000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.395000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.413000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.413000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.413000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.413000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.413000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.653000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.653000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.653000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.653000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:04:50.653000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.848000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.848000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.848000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.848000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.848000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.848000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.848000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.866000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.867000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.867000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.867000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:04:51.867000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:04:52.787000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:04:52.787000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:04:52.787000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:04:52.787000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:04:52.787000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 05:04:59.059731 340292 finetune.py:45] layer 17_v initial loss 2.92936856567394e-05
W0315 05:04:59.060070 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:05:11.101329 339725 finetune.py:68] layer 16_v @ epoch 1 new loss 9.936211426975206e-06 old loss 1.0843153177120257e-05 BETTER
I0315 05:05:29.982806 273992 quantize_finetune_llama.py:186] computed original embedding for layer 18 in 61.62173509597778s
I0315 05:05:30.361610 273992 quantize_finetune_llama.py:159] layer 19 gpu 3
I0315 05:05:32.424816 340861 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 05:05:32.425031 340861 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 05:05:32.425137 340861 utils.py:162] NumExpr defaulting to 16 threads.
I0315 05:05:32.630857 340861 config.py:58] PyTorch version 2.4.0 available.
I0315 05:05:33.691033 340292 finetune.py:68] layer 17_v @ epoch 0 new loss 1.2715892808046192e-05 old loss 2.92936856567394e-05 BETTER
I0315 05:05:34.847982 340861 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 05:05:35.263983 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.59s/it]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s]  9%|▉         | 3/32 [00:02<00:18,  1.60it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.67it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.69it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.74it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.74it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.73it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.73it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.75it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.75it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.76it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.77it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.79it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.79it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.77it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.80it/s]I0315 05:05:48.804928 339725 finetune.py:68] layer 16_v @ epoch 2 new loss 9.481224878982175e-06 old loss 9.936211426975206e-06 BETTER
100%|██████████| 32/32 [00:12<00:00,  2.84it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
W0315 05:05:52.240000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.241000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.241000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.241000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.241000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.241000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.241000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.268000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.268000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.268000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.268000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.268000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.285000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.285000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.285000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.285000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.285000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.614000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.614000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.614000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.614000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:05:52.614000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.550000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.550000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.550000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.550000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.550000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.550000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.550000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.568000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.569000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.569000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.569000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.569000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.818000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.818000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.818000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.818000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:05:53.818000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:05:54.998000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:05:54.999000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:05:54.999000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:05:54.999000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:05:54.999000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:05:54.999000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:05:54.999000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:05:55.016000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:05:55.016000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:05:55.017000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:05:55.017000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:05:55.017000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:05:55.966000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:05:55.966000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:05:55.966000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:05:55.966000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:05:55.966000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 05:06:01.928432 340861 finetune.py:45] layer 18_v initial loss 2.9520231692004018e-05
W0315 05:06:01.928722 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:06:09.290168 340292 finetune.py:68] layer 17_v @ epoch 1 new loss 1.1438959518272895e-05 old loss 1.2715892808046192e-05 BETTER
I0315 05:06:26.913159 339725 finetune.py:68] layer 16_v @ epoch 3 new loss 9.185495400743093e-06 old loss 9.481224878982175e-06 BETTER
I0315 05:06:31.319288 273992 quantize_finetune_llama.py:186] computed original embedding for layer 19 in 60.46276044845581s
I0315 05:06:31.716063 273992 quantize_finetune_llama.py:159] layer 20 gpu 0
I0315 05:06:33.803514 341427 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 05:06:33.803662 341427 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 05:06:33.803754 341427 utils.py:162] NumExpr defaulting to 16 threads.
I0315 05:06:34.011994 341427 config.py:58] PyTorch version 2.4.0 available.
I0315 05:06:36.264068 341427 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0315 05:06:36.650022 340861 finetune.py:68] layer 18_v @ epoch 0 new loss 8.637684004497714e-06 old loss 2.9520231692004018e-05 BETTER
W0315 05:06:36.743844 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:55,  1.78s/it]  6%|▋         | 2/32 [00:02<00:28,  1.07it/s]  9%|▉         | 3/32 [00:02<00:19,  1.50it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.84it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.11it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.71it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 50%|█████     | 16/32 [00:07<00:05,  2.83it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.83it/s]I0315 05:06:45.403774 340292 finetune.py:68] layer 17_v @ epoch 2 new loss 1.084986979549285e-05 old loss 1.1438959518272895e-05 BETTER
 56%|█████▋    | 18/32 [00:07<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.86it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.85it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.85it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.86it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.87it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.95it/s]100%|██████████| 32/32 [00:12<00:00,  2.97it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
W0315 05:06:54.067000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.067000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.067000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.067000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.067000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.068000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.068000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.095000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.095000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.095000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.095000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.096000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.113000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.113000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.113000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.113000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.113000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.454000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.455000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.455000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.455000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:06:54.455000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.391000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.392000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.392000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.392000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.392000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.392000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.392000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.411000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.411000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.411000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.412000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.412000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.666000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.667000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.667000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.667000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:06:55.667000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.907000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.907000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.907000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.907000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.908000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.908000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.908000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.927000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.927000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.927000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.927000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:06:56.927000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:06:57.884000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:06:57.884000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:06:57.884000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:06:57.884000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:06:57.884000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 05:07:04.423931 341427 finetune.py:45] layer 19_v initial loss 4.250956772011705e-05
W0315 05:07:04.424287 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:07:05.125124 339725 finetune.py:76] layer 16_v @ epoch 4 new loss 9.437750122742727e-06 old loss 9.185495400743093e-06 WORSE
W0315 05:07:06.256265 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_v proxy err 0.00923982448875904 tr(WHW.T) 68.60269165039062
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.64it/s]I0315 05:07:12.606180 340861 finetune.py:68] layer 18_v @ epoch 1 new loss 7.5897437454841565e-06 old loss 8.637684004497714e-06 BETTER
 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.66it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0315 05:07:21.701277 340292 finetune.py:68] layer 17_v @ epoch 3 new loss 1.050155424309196e-05 old loss 1.084986979549285e-05 BETTER
W0315 05:07:26.521000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.522000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.522000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.522000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.522000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.522000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.522000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.554000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.554000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.554000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.554000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.554000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.570000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.570000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.571000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.571000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.571000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.732000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.733000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.733000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.733000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.733000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.971000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.971000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.971000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.971000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.972000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.972000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.972000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.992000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.992000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.992000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.992000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:07:26.992000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:07:27.059000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:07:27.059000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:07:27.059000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:07:27.060000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:07:27.060000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:07:27.956000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.279000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.280000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.280000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.280000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.280000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.280000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.280000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.303000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.303000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.303000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.303000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.303000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.569000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.569000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.569000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.569000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.570000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:07:28.840000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 05:07:35.971557 339725 finetune.py:45] layer 16_q initial loss 1.4761145394004416e-05
W0315 05:07:35.971995 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:07:38.713168 341427 finetune.py:68] layer 19_v @ epoch 0 new loss 1.054137828759849e-05 old loss 4.250956772011705e-05 BETTER
I0315 05:07:49.057379 340861 finetune.py:68] layer 18_v @ epoch 2 new loss 7.160952918638941e-06 old loss 7.5897437454841565e-06 BETTER
I0315 05:07:58.270751 340292 finetune.py:68] layer 17_v @ epoch 4 new loss 1.0261335773975588e-05 old loss 1.050155424309196e-05 BETTER
W0315 05:08:00.022229 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_v proxy err 0.012142300605773926 tr(WHW.T) 69.1786880493164
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.00it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:15,  1.91it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.57it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.55it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.54it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.56it/s]I0315 05:08:12.629666 339725 finetune.py:68] layer 16_q @ epoch 0 new loss 1.4207480489858426e-05 old loss 1.4761145394004416e-05 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s]I0315 05:08:13.861801 341427 finetune.py:68] layer 19_v @ epoch 1 new loss 9.048918400367256e-06 old loss 1.054137828759849e-05 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.59it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
W0315 05:08:20.718000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.719000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.719000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.719000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.719000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.719000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.719000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.749000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.749000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.750000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.750000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.750000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.767000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.767000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.767000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.767000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.767000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.937000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.938000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.938000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.938000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:08:20.938000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.175000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.176000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.176000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.176000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.176000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.176000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.176000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.197000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.197000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.197000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.197000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.197000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.267000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.267000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.268000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.268000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:08:21.268000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.178000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.504000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.504000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.504000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.505000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.505000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.505000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.505000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.528000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.528000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.528000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.528000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.528000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.797000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.797000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.797000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.797000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:08:22.797000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:08:23.071000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 05:08:25.231607 340861 finetune.py:68] layer 18_v @ epoch 3 new loss 6.923942237335723e-06 old loss 7.160952918638941e-06 BETTER
I0315 05:08:29.992640 340292 finetune.py:45] layer 17_q initial loss 1.5277602869900875e-05
W0315 05:08:29.993024 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:08:49.478832 341427 finetune.py:68] layer 19_v @ epoch 2 new loss 8.486906153848395e-06 old loss 9.048918400367256e-06 BETTER
I0315 05:08:50.319585 339725 finetune.py:68] layer 16_q @ epoch 1 new loss 1.3858310012437869e-05 old loss 1.4207480489858426e-05 BETTER
I0315 05:09:02.028490 340861 finetune.py:68] layer 18_v @ epoch 4 new loss 6.738573119946523e-06 old loss 6.923942237335723e-06 BETTER
W0315 05:09:03.981918 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_v proxy err 0.010070223361253738 tr(WHW.T) 73.61924743652344
  0%|          | 0/32 [00:00<?, ?it/s]I0315 05:09:05.528457 340292 finetune.py:68] layer 17_q @ epoch 0 new loss 1.4823977835476398e-05 old loss 1.5277602869900875e-05 BETTER
  3%|▎         | 1/32 [00:00<00:30,  1.01it/s]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:15,  1.91it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.57it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
W0315 05:09:24.447000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.448000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.448000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.448000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.448000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.448000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.448000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.479000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.479000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.479000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.479000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.479000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.495000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.495000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.495000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.496000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.496000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.658000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.658000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.659000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.659000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.659000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.894000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.894000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.894000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.894000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.894000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.895000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.895000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.915000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.916000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.916000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.916000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.916000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
I0315 05:09:24.949028 341427 finetune.py:68] layer 19_v @ epoch 3 new loss 8.180737495422363e-06 old loss 8.486906153848395e-06 BETTER
W0315 05:09:24.983000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.983000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.983000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.984000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:09:24.984000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:09:25.871000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.189000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.189000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.189000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.189000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.189000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.189000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.189000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.211000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.211000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.212000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.212000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.212000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.473000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.473000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.473000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.473000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.473000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:09:26.742000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 05:09:28.009565 339725 finetune.py:68] layer 16_q @ epoch 2 new loss 1.3570243027061224e-05 old loss 1.3858310012437869e-05 BETTER
I0315 05:09:33.863768 340861 finetune.py:45] layer 18_q initial loss 1.146935665019555e-05
W0315 05:09:33.864177 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:09:41.436327 340292 finetune.py:68] layer 17_q @ epoch 1 new loss 1.454136690881569e-05 old loss 1.4823977835476398e-05 BETTER
I0315 05:10:00.615021 341427 finetune.py:68] layer 19_v @ epoch 4 new loss 7.966232260514516e-06 old loss 8.180737495422363e-06 BETTER
W0315 05:10:02.447118 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_v proxy err 0.009779744781553745 tr(WHW.T) 87.24554443359375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:31,  1.01s/it]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:15,  1.90it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s]I0315 05:10:05.924941 339725 finetune.py:68] layer 16_q @ epoch 3 new loss 1.3346059859031811e-05 old loss 1.3570243027061224e-05 BETTER
 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.38it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.45it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s]I0315 05:10:09.083659 340861 finetune.py:68] layer 18_q @ epoch 0 new loss 1.1086305676144548e-05 old loss 1.146935665019555e-05 BETTER
 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:03,  2.28it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.37it/s] 84%|████████▍ | 27/32 [00:11<00:02,  2.44it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.49it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.52it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.56it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
I0315 05:10:17.842540 340292 finetune.py:68] layer 17_q @ epoch 2 new loss 1.430933662049938e-05 old loss 1.454136690881569e-05 BETTER
W0315 05:10:23.160000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.161000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.161000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.161000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.161000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.161000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.161000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.192000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.192000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.192000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.192000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.192000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.208000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.208000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.208000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.208000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.208000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.371000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.371000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.371000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.371000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.371000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.604000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.604000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.605000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.605000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.605000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.605000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.605000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.627000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.627000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.627000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.628000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.628000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.695000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.695000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.695000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.695000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:10:23.695000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.594000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.919000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.919000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.919000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.920000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.920000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.920000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.920000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.943000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.943000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.943000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.943000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:10:24.943000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:10:25.210000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:10:25.210000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:10:25.210000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:10:25.210000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:10:25.210000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:10:25.492000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 05:10:32.261546 341427 finetune.py:45] layer 19_q initial loss 1.2079559382982552e-05
W0315 05:10:32.261929 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:10:43.871787 339725 finetune.py:68] layer 16_q @ epoch 4 new loss 1.3168121768103447e-05 old loss 1.3346059859031811e-05 BETTER
I0315 05:10:45.293136 340861 finetune.py:68] layer 18_q @ epoch 1 new loss 1.0858962014026474e-05 old loss 1.1086305676144548e-05 BETTER
W0315 05:10:45.605368 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_q proxy err 0.001781544298864901 tr(WHW.T) 6125.36279296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.39it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.70it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.71it/s] 41%|████      | 13/32 [00:05<00:07,  2.70it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s]I0315 05:10:54.296940 340292 finetune.py:68] layer 17_q @ epoch 3 new loss 1.4147536603559274e-05 old loss 1.430933662049938e-05 BETTER
 59%|█████▉    | 19/32 [00:07<00:04,  2.72it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.75it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.76it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.76it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.74it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.72it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.73it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
I0315 05:11:06.321618 339725 finetune.py:45] layer 16_k initial loss 1.721210719551891e-05
W0315 05:11:06.321919 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:11:06.756564 341427 finetune.py:68] layer 19_q @ epoch 0 new loss 1.1660303243843373e-05 old loss 1.2079559382982552e-05 BETTER
I0315 05:11:21.356849 340861 finetune.py:68] layer 18_q @ epoch 2 new loss 1.068499477696605e-05 old loss 1.0858962014026474e-05 BETTER
I0315 05:11:30.569082 340292 finetune.py:68] layer 17_q @ epoch 4 new loss 1.4027691577211954e-05 old loss 1.4147536603559274e-05 BETTER
W0315 05:11:32.245788 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_q proxy err 0.0018122277688235044 tr(WHW.T) 6717.3701171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.07it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.62it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.66it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s]I0315 05:11:41.822766 341427 finetune.py:68] layer 19_q @ epoch 1 new loss 1.1409139005991165e-05 old loss 1.1660303243843373e-05 BETTER
 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s]I0315 05:11:42.986165 339725 finetune.py:68] layer 16_k @ epoch 0 new loss 1.6417041479144245e-05 old loss 1.721210719551891e-05 BETTER
 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.64it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0315 05:11:52.909726 340292 finetune.py:45] layer 17_k initial loss 1.7754509826772846e-05
W0315 05:11:52.910170 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:11:57.463728 340861 finetune.py:68] layer 18_q @ epoch 3 new loss 1.0545069926592987e-05 old loss 1.068499477696605e-05 BETTER
I0315 05:12:16.830852 341427 finetune.py:68] layer 19_q @ epoch 2 new loss 1.1231384632992558e-05 old loss 1.1409139005991165e-05 BETTER
I0315 05:12:20.864402 339725 finetune.py:68] layer 16_k @ epoch 1 new loss 1.6232477719313465e-05 old loss 1.6417041479144245e-05 BETTER
I0315 05:12:27.811376 340292 finetune.py:68] layer 17_k @ epoch 0 new loss 1.714608515612781e-05 old loss 1.7754509826772846e-05 BETTER
I0315 05:12:33.360806 340861 finetune.py:68] layer 18_q @ epoch 4 new loss 1.0433072020532563e-05 old loss 1.0545069926592987e-05 BETTER
W0315 05:12:34.986142 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_q proxy err 0.002221530070528388 tr(WHW.T) 5734.1162109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.07it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.63it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0315 05:12:52.008088 341427 finetune.py:68] layer 19_q @ epoch 3 new loss 1.1082122910011094e-05 old loss 1.1231384632992558e-05 BETTER
I0315 05:12:55.906232 340861 finetune.py:45] layer 18_k initial loss 1.3464428775478154e-05
W0315 05:12:55.906492 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:12:59.201855 339725 finetune.py:68] layer 16_k @ epoch 2 new loss 1.6098023479571566e-05 old loss 1.6232477719313465e-05 BETTER
I0315 05:13:03.615081 340292 finetune.py:68] layer 17_k @ epoch 1 new loss 1.698024061624892e-05 old loss 1.714608515612781e-05 BETTER
I0315 05:13:27.415189 341427 finetune.py:68] layer 19_q @ epoch 4 new loss 1.0975242730637547e-05 old loss 1.1082122910011094e-05 BETTER
W0315 05:13:29.282079 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_q proxy err 0.002134541980922222 tr(WHW.T) 6152.9287109375
  0%|          | 0/32 [00:00<?, ?it/s]I0315 05:13:31.162883 340861 finetune.py:68] layer 18_k @ epoch 0 new loss 1.302975488215452e-05 old loss 1.3464428775478154e-05 BETTER
  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:17,  1.69it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.55it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.63it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.68it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s]I0315 05:13:37.498761 339725 finetune.py:68] layer 16_k @ epoch 3 new loss 1.5994563000276685e-05 old loss 1.6098023479571566e-05 BETTER
 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.70it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s]I0315 05:13:39.987988 340292 finetune.py:68] layer 17_k @ epoch 2 new loss 1.6860241885297e-05 old loss 1.698024061624892e-05 BETTER
 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0315 05:13:49.747085 341427 finetune.py:45] layer 19_k initial loss 1.4477178410743363e-05
W0315 05:13:49.747473 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:14:07.396869 340861 finetune.py:68] layer 18_k @ epoch 1 new loss 1.291557327931514e-05 old loss 1.302975488215452e-05 BETTER
I0315 05:14:15.890433 339725 finetune.py:68] layer 16_k @ epoch 4 new loss 1.5924893887131475e-05 old loss 1.5994563000276685e-05 BETTER
I0315 05:14:16.961883 340292 finetune.py:68] layer 17_k @ epoch 3 new loss 1.675946441537235e-05 old loss 1.6860241885297e-05 BETTER
W0315 05:14:17.694499 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_k proxy err 0.0012699783546850085 tr(WHW.T) 4871.78955078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s]  9%|▉         | 3/32 [00:01<00:13,  2.11it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.51it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.62it/s]I0315 05:14:24.193589 341427 finetune.py:68] layer 19_k @ epoch 0 new loss 1.378488104819553e-05 old loss 1.4477178410743363e-05 BETTER
 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.67it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.66it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.66it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.67it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]
I0315 05:14:39.233597 339725 finetune.py:45] layer 16_o initial loss 3.041359013877809e-05
W0315 05:14:39.234049 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:14:43.855144 340861 finetune.py:68] layer 18_k @ epoch 2 new loss 1.282892935705604e-05 old loss 1.291557327931514e-05 BETTER
I0315 05:14:53.870888 340292 finetune.py:68] layer 17_k @ epoch 4 new loss 1.6674721337039955e-05 old loss 1.675946441537235e-05 BETTER
W0315 05:14:55.499107 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

17_k proxy err 0.0016144434921443462 tr(WHW.T) 4240.025390625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s]I0315 05:14:59.863312 341427 finetune.py:68] layer 19_k @ epoch 1 new loss 1.3670297448697966e-05 old loss 1.378488104819553e-05 BETTER
 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.57it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.57it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.57it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0315 05:15:16.108586 339725 finetune.py:68] layer 16_o @ epoch 0 new loss 2.949569716292899e-05 old loss 3.041359013877809e-05 BETTER
I0315 05:15:17.157183 340292 finetune.py:45] layer 17_o initial loss 3.0322853490361013e-05
W0315 05:15:17.157621 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:15:20.446625 340861 finetune.py:68] layer 18_k @ epoch 3 new loss 1.2745519597956445e-05 old loss 1.282892935705604e-05 BETTER
I0315 05:15:35.699620 341427 finetune.py:68] layer 19_k @ epoch 2 new loss 1.3570360351877753e-05 old loss 1.3670297448697966e-05 BETTER
I0315 05:15:52.864986 340292 finetune.py:68] layer 17_o @ epoch 0 new loss 2.9347891540965065e-05 old loss 3.0322853490361013e-05 BETTER
I0315 05:15:54.007724 339725 finetune.py:68] layer 16_o @ epoch 1 new loss 2.90560183202615e-05 old loss 2.949569716292899e-05 BETTER
I0315 05:15:57.071128 340861 finetune.py:68] layer 18_k @ epoch 4 new loss 1.2692112250078935e-05 old loss 1.2745519597956445e-05 BETTER
W0315 05:15:58.863155 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_k proxy err 0.0017496593063697219 tr(WHW.T) 4448.1474609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.56it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.54it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.54it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.54it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.57it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.58it/s]I0315 05:16:11.392136 341427 finetune.py:68] layer 19_k @ epoch 3 new loss 1.3493298865796532e-05 old loss 1.3570360351877753e-05 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0315 05:16:20.473526 340861 finetune.py:45] layer 18_o initial loss 2.3339367544394918e-05
W0315 05:16:20.473932 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:16:29.237754 340292 finetune.py:68] layer 17_o @ epoch 1 new loss 2.895525722124148e-05 old loss 2.9347891540965065e-05 BETTER
I0315 05:16:31.966849 339725 finetune.py:68] layer 16_o @ epoch 2 new loss 2.8719565307255834e-05 old loss 2.90560183202615e-05 BETTER
I0315 05:16:47.354561 341427 finetune.py:68] layer 19_k @ epoch 4 new loss 1.342643463431159e-05 old loss 1.3493298865796532e-05 BETTER
W0315 05:16:48.980584 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

19_k proxy err 0.0018559526652097702 tr(WHW.T) 3975.261962890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.09it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s]I0315 05:16:55.919225 340861 finetune.py:68] layer 18_o @ epoch 0 new loss 2.243573726445902e-05 old loss 2.3339367544394918e-05 BETTER
 44%|████▍     | 14/32 [00:05<00:06,  2.61it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.61it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0315 05:17:05.786898 340292 finetune.py:68] layer 17_o @ epoch 2 new loss 2.8656997528742068e-05 old loss 2.895525722124148e-05 BETTER
I0315 05:17:09.796631 339725 finetune.py:68] layer 16_o @ epoch 3 new loss 2.844435402948875e-05 old loss 2.8719565307255834e-05 BETTER
I0315 05:17:10.548563 341427 finetune.py:45] layer 19_o initial loss 2.3539678295492195e-05
W0315 05:17:10.549039 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:17:32.307610 340861 finetune.py:68] layer 18_o @ epoch 1 new loss 2.2185426132637076e-05 old loss 2.243573726445902e-05 BETTER
I0315 05:17:42.379987 340292 finetune.py:68] layer 17_o @ epoch 3 new loss 2.8423770345398225e-05 old loss 2.8656997528742068e-05 BETTER
I0315 05:17:45.069048 341427 finetune.py:68] layer 19_o @ epoch 0 new loss 2.2571932277060114e-05 old loss 2.3539678295492195e-05 BETTER
I0315 05:17:47.547546 339725 finetune.py:68] layer 16_o @ epoch 4 new loss 2.8210455639055e-05 old loss 2.844435402948875e-05 BETTER
W0315 05:17:49.167703 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_o proxy err 0.00982110295444727 tr(WHW.T) 7.393532752990723
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.02s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it]  9%|▉         | 3/32 [00:04<00:46,  1.60s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.51s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.49s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.50s/it]I0315 05:18:08.577092 340861 finetune.py:68] layer 18_o @ epoch 2 new loss 2.199315895268228e-05 old loss 2.2185426132637076e-05 BETTER
 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it]I0315 05:18:18.419469 340292 finetune.py:68] layer 17_o @ epoch 4 new loss 2.8221389584359713e-05 old loss 2.8423770345398225e-05 BETTER
 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it]W0315 05:18:19.956967 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 05:18:20.650902 341427 finetune.py:68] layer 19_o @ epoch 1 new loss 2.2350312065100297e-05 old loss 2.2571932277060114e-05 BETTER
 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it]17_o proxy err 0.010228893719613552 tr(WHW.T) 6.412209510803223
  0%|          | 0/32 [00:00<?, ?it/s] 66%|██████▌   | 21/32 [00:31<00:16,  1.47s/it]  3%|▎         | 1/32 [00:02<01:03,  2.04s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.47s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.47s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.54s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it]I0315 05:18:44.988332 340861 finetune.py:68] layer 18_o @ epoch 3 new loss 2.185007178923115e-05 old loss 2.199315895268228e-05 BETTER
 50%|█████     | 16/32 [00:25<00:24,  1.54s/it]I0315 05:18:46.915322 339725 finetune.py:45] layer 16_up initial loss 6.896163540659472e-05
W0315 05:18:46.915883 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it]I0315 05:18:55.900043 341427 finetune.py:68] layer 19_o @ epoch 2 new loss 2.2192387405084446e-05 old loss 2.2350312065100297e-05 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]
I0315 05:19:19.490512 340292 finetune.py:45] layer 17_up initial loss 7.414192077703774e-05
W0315 05:19:19.490762 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:19:21.473761 340861 finetune.py:68] layer 18_o @ epoch 4 new loss 2.1726744307670742e-05 old loss 2.185007178923115e-05 BETTER
I0315 05:19:23.029559 339725 finetune.py:68] layer 16_up @ epoch 0 new loss 6.754440983058885e-05 old loss 6.896163540659472e-05 BETTER
W0315 05:19:23.198438 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_o proxy err 0.00984653364866972 tr(WHW.T) 4.740108489990234
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it]  6%|▋         | 2/32 [00:03<00:52,  1.76s/it]  9%|▉         | 3/32 [00:05<00:48,  1.67s/it]I0315 05:19:31.102636 341427 finetune.py:68] layer 19_o @ epoch 3 new loss 2.2062064090278e-05 old loss 2.2192387405084446e-05 BETTER
 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.58s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.55s/it] 41%|████      | 13/32 [00:20<00:29,  1.55s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.55s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it]I0315 05:19:53.538680 340292 finetune.py:68] layer 17_up @ epoch 0 new loss 7.256666867760941e-05 old loss 7.414192077703774e-05 BETTER
 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.55s/it]I0315 05:19:59.746735 339725 finetune.py:68] layer 16_up @ epoch 1 new loss 6.657567428192124e-05 old loss 6.754440983058885e-05 BETTER
 72%|███████▏  | 23/32 [00:36<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.55s/it]I0315 05:20:06.284776 341427 finetune.py:68] layer 19_o @ epoch 4 new loss 2.196095920226071e-05 old loss 2.2062064090278e-05 BETTER
 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it]W0315 05:20:07.908613 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 88%|████████▊ | 28/32 [00:43<00:06,  1.55s/it]19_o proxy err 0.010451243259012699 tr(WHW.T) 3.9443047046661377
  0%|          | 0/32 [00:00<?, ?it/s] 91%|█████████ | 29/32 [00:45<00:04,  1.55s/it]  3%|▎         | 1/32 [00:02<01:02,  2.02s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it]100%|██████████| 32/32 [00:50<00:00,  1.54s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it]I0315 05:20:22.840634 340861 finetune.py:45] layer 18_up initial loss 6.89188891556114e-05
W0315 05:20:22.841214 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it]I0315 05:20:28.250835 340292 finetune.py:68] layer 17_up @ epoch 1 new loss 7.148808072088286e-05 old loss 7.256666867760941e-05 BETTER
 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it]I0315 05:20:36.447777 339725 finetune.py:68] layer 16_up @ epoch 2 new loss 6.575949373655021e-05 old loss 6.657567428192124e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.54s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it]I0315 05:20:56.924269 340861 finetune.py:68] layer 18_up @ epoch 0 new loss 6.752713670721278e-05 old loss 6.89188891556114e-05 BETTER
 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.62s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]
I0315 05:21:03.006133 340292 finetune.py:68] layer 17_up @ epoch 2 new loss 7.058725168462843e-05 old loss 7.148808072088286e-05 BETTER
I0315 05:21:07.526513 341427 finetune.py:45] layer 19_up initial loss 7.265596650540829e-05
W0315 05:21:07.526981 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:21:13.315296 339725 finetune.py:68] layer 16_up @ epoch 3 new loss 6.502290489152074e-05 old loss 6.575949373655021e-05 BETTER
I0315 05:21:32.329828 340861 finetune.py:68] layer 18_up @ epoch 1 new loss 6.656745244981721e-05 old loss 6.752713670721278e-05 BETTER
I0315 05:21:37.620764 340292 finetune.py:68] layer 17_up @ epoch 3 new loss 6.97961077094078e-05 old loss 7.058725168462843e-05 BETTER
I0315 05:21:40.562710 341427 finetune.py:68] layer 19_up @ epoch 0 new loss 7.127005665097386e-05 old loss 7.265596650540829e-05 BETTER
I0315 05:21:49.953746 339725 finetune.py:68] layer 16_up @ epoch 4 new loss 6.434975512092933e-05 old loss 6.502290489152074e-05 BETTER
W0315 05:21:51.346129 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_up proxy err 0.010297371074557304 tr(WHW.T) 981.7088012695312
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:59,  1.93s/it]  6%|▋         | 2/32 [00:03<00:49,  1.67s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.55s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:35,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it]I0315 05:22:07.261873 340861 finetune.py:68] layer 18_up @ epoch 2 new loss 6.577833846677095e-05 old loss 6.656745244981721e-05 BETTER
 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it]I0315 05:22:12.421046 340292 finetune.py:68] layer 17_up @ epoch 4 new loss 6.91006425768137e-05 old loss 6.97961077094078e-05 BETTER
 41%|████      | 13/32 [00:19<00:28,  1.49s/it]W0315 05:22:13.779417 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it]I0315 05:22:14.416572 341427 finetune.py:68] layer 19_up @ epoch 1 new loss 7.03216137480922e-05 old loss 7.127005665097386e-05 BETTER
17_up proxy err 0.010092196986079216 tr(WHW.T) 1061.660400390625
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.49s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.47s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 81%|████████▏ | 26/32 [00:38<00:08,  1.46s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.46s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 88%|████████▊ | 28/32 [00:41<00:05,  1.47s/it] 41%|████      | 13/32 [00:20<00:28,  1.53s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.47s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 94%|█████████▍| 30/32 [00:44<00:02,  1.47s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it]100%|██████████| 32/32 [00:47<00:00,  1.47s/it]100%|██████████| 32/32 [00:47<00:00,  1.49s/it]
 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it]I0315 05:22:41.838237 340861 finetune.py:68] layer 18_up @ epoch 3 new loss 6.506365753011778e-05 old loss 6.577833846677095e-05 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it]I0315 05:22:47.895083 341427 finetune.py:68] layer 19_up @ epoch 2 new loss 6.95119088049978e-05 old loss 7.03216137480922e-05 BETTER
I0315 05:22:48.769329 339725 finetune.py:45] layer 16_gate initial loss 8.749932021601126e-05
W0315 05:22:48.769681 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:33<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0315 05:23:12.239659 340292 finetune.py:45] layer 17_gate initial loss 9.622224752092734e-05
W0315 05:23:12.240341 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:23:16.284659 340861 finetune.py:68] layer 18_up @ epoch 4 new loss 6.443919119192287e-05 old loss 6.506365753011778e-05 BETTER
W0315 05:23:17.738504 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

18_up proxy err 0.010980798862874508 tr(WHW.T) 1056.4208984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]I0315 05:23:21.337430 341427 finetune.py:68] layer 19_up @ epoch 3 new loss 6.88107538735494e-05 old loss 6.95119088049978e-05 BETTER
  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]I0315 05:23:22.938428 339725 finetune.py:68] layer 16_gate @ epoch 0 new loss 8.643470209790394e-05 old loss 8.749932021601126e-05 BETTER
  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.54s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 50%|█████     | 16/32 [00:25<00:24,  1.54s/it]I0315 05:23:44.543056 340292 finetune.py:68] layer 17_gate @ epoch 0 new loss 9.500095620751381e-05 old loss 9.622224752092734e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it]I0315 05:23:54.989183 341427 finetune.py:68] layer 19_up @ epoch 4 new loss 6.819266127422452e-05 old loss 6.88107538735494e-05 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it]W0315 05:23:56.350990 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it]19_up proxy err 0.011551703326404095 tr(WHW.T) 1067.7037353515625
  0%|          | 0/32 [00:00<?, ?it/s]I0315 05:23:57.741969 339725 finetune.py:68] layer 16_gate @ epoch 1 new loss 8.578169217798859e-05 old loss 8.643470209790394e-05 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.54s/it]  3%|▎         | 1/32 [00:01<01:00,  1.96s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it]  9%|▉         | 3/32 [00:04<00:46,  1.62s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.53s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.51s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it]I0315 05:24:16.932783 340861 finetune.py:45] layer 18_gate initial loss 9.396651148563251e-05
W0315 05:24:16.933218 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:20<00:28,  1.51s/it]I0315 05:24:17.812195 340292 finetune.py:68] layer 17_gate @ epoch 1 new loss 9.428213525097817e-05 old loss 9.500095620751381e-05 BETTER
 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it]I0315 05:24:33.017048 339725 finetune.py:68] layer 16_gate @ epoch 2 new loss 8.520216942997649e-05 old loss 8.578169217798859e-05 BETTER
 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.50s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]
I0315 05:24:50.178445 340861 finetune.py:68] layer 18_gate @ epoch 0 new loss 9.285627311328426e-05 old loss 9.396651148563251e-05 BETTER
I0315 05:24:51.478319 340292 finetune.py:68] layer 17_gate @ epoch 2 new loss 9.365780715597793e-05 old loss 9.428213525097817e-05 BETTER
I0315 05:24:54.724671 341427 finetune.py:45] layer 19_gate initial loss 0.00010090354771818966
W0315 05:24:54.725101 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:25:08.143526 339725 finetune.py:68] layer 16_gate @ epoch 3 new loss 8.467675070278347e-05 old loss 8.520216942997649e-05 BETTER
I0315 05:25:24.144622 340861 finetune.py:68] layer 18_gate @ epoch 1 new loss 9.221452637575567e-05 old loss 9.285627311328426e-05 BETTER
I0315 05:25:25.577110 340292 finetune.py:68] layer 17_gate @ epoch 3 new loss 9.309253073297441e-05 old loss 9.365780715597793e-05 BETTER
I0315 05:25:27.205295 341427 finetune.py:68] layer 19_gate @ epoch 0 new loss 9.978777961805463e-05 old loss 0.00010090354771818966 BETTER
I0315 05:25:43.030289 339725 finetune.py:68] layer 16_gate @ epoch 4 new loss 8.419874939136207e-05 old loss 8.467675070278347e-05 BETTER
W0315 05:25:44.144691 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

16_gate proxy err 0.0036142533645033836 tr(WHW.T) 4838.939453125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s]  2%|▏         | 2/112 [00:01<01:03,  1.72it/s]  3%|▎         | 3/112 [00:01<00:53,  2.05it/s]  4%|▎         | 4/112 [00:02<00:47,  2.25it/s]  4%|▍         | 5/112 [00:02<00:44,  2.39it/s]  5%|▌         | 6/112 [00:02<00:42,  2.48it/s]  6%|▋         | 7/112 [00:03<00:41,  2.53it/s]  7%|▋         | 8/112 [00:03<00:40,  2.57it/s]  8%|▊         | 9/112 [00:03<00:39,  2.61it/s]  9%|▉         | 10/112 [00:04<00:38,  2.62it/s] 10%|▉         | 11/112 [00:04<00:38,  2.63it/s] 11%|█         | 12/112 [00:05<00:37,  2.64it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.65it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.64it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.62it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.62it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.62it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.63it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.63it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.64it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.65it/s] 21%|██        | 23/112 [00:09<00:33,  2.66it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.66it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.65it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.65it/s]I0315 05:25:58.020026 340861 finetune.py:68] layer 18_gate @ epoch 2 new loss 9.165624214801937e-05 old loss 9.221452637575567e-05 BETTER
 24%|██▍       | 27/112 [00:10<00:32,  2.62it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.64it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.65it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.65it/s]I0315 05:25:59.294526 340292 finetune.py:68] layer 17_gate @ epoch 4 new loss 9.258392674382776e-05 old loss 9.309253073297441e-05 BETTER
 28%|██▊       | 31/112 [00:12<00:30,  2.66it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.66it/s]I0315 05:26:00.082721 341427 finetune.py:68] layer 19_gate @ epoch 1 new loss 9.915041300700977e-05 old loss 9.978777961805463e-05 BETTER
 29%|██▉       | 33/112 [00:12<00:29,  2.67it/s]W0315 05:26:00.446402 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 30%|███       | 34/112 [00:13<00:29,  2.68it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.67it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.68it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.67it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.66it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.63it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.65it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.66it/s]17_gate proxy err 0.00364237860776484 tr(WHW.T) 5234.73876953125
  0%|          | 0/112 [00:00<?, ?it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.66it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.66it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.66it/s]  2%|▏         | 2/112 [00:01<01:04,  1.69it/s] 40%|████      | 45/112 [00:17<00:25,  2.66it/s]  3%|▎         | 3/112 [00:01<00:54,  2.01it/s] 41%|████      | 46/112 [00:17<00:24,  2.67it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.66it/s]  4%|▍         | 5/112 [00:02<00:46,  2.32it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.66it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.66it/s]  6%|▋         | 7/112 [00:03<00:42,  2.44it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.65it/s]  7%|▋         | 8/112 [00:03<00:41,  2.48it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.62it/s]  8%|▊         | 9/112 [00:04<00:41,  2.50it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.63it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.64it/s]  9%|▉         | 10/112 [00:04<00:40,  2.50it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.65it/s] 10%|▉         | 11/112 [00:04<00:40,  2.51it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.65it/s] 11%|█         | 12/112 [00:05<00:39,  2.52it/s] 50%|█████     | 56/112 [00:21<00:21,  2.66it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.53it/s] 51%|█████     | 57/112 [00:21<00:20,  2.67it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.54it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.67it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.55it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.68it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.56it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.67it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.57it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.66it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.58it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.66it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.58it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.66it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.57it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.63it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.57it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.64it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.57it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.65it/s] 21%|██        | 23/112 [00:09<00:34,  2.57it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.66it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.57it/s] 61%|██████    | 68/112 [00:26<00:16,  2.67it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.54it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.67it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.55it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.67it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.55it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.68it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.56it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.68it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.56it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.68it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.58it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.67it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.59it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.67it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.59it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.64it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.58it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.65it/s] 30%|███       | 34/112 [00:13<00:30,  2.58it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.66it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.57it/s] 71%|███████   | 79/112 [00:30<00:12,  2.67it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.67it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.57it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.68it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.54it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.68it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.55it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.68it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.55it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.69it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.56it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.69it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.57it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.69it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.58it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.69it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.59it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.67it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.60it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.68it/s] 40%|████      | 45/112 [00:18<00:25,  2.60it/s] 80%|████████  | 90/112 [00:34<00:08,  2.68it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.68it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.60it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.70it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.58it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.69it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.58it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.70it/s] 45%|████▍     | 50/112 [00:19<00:24,  2.56it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.70it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.56it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.71it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.57it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.70it/s] 47%|████▋     | 53/112 [00:21<00:22,  2.57it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.70it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.58it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.69it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.58it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.66it/s] 50%|█████     | 56/112 [00:22<00:21,  2.59it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.68it/s] 51%|█████     | 57/112 [00:22<00:21,  2.61it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.69it/s] 52%|█████▏    | 58/112 [00:23<00:20,  2.60it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.69it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.69it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.59it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.69it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.58it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.69it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.57it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.69it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.53it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.70it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.54it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.70it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.55it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.69it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.55it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.69it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.55it/s]100%|██████████| 112/112 [00:42<00:00,  2.64it/s]100%|██████████| 112/112 [00:42<00:00,  2.63it/s]
 60%|█████▉    | 67/112 [00:26<00:17,  2.56it/s] 61%|██████    | 68/112 [00:26<00:17,  2.56it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s]I0315 05:26:31.155061 340861 finetune.py:68] layer 18_gate @ epoch 3 new loss 9.116795263253152e-05 old loss 9.165624214801937e-05 BETTER
 62%|██████▎   | 70/112 [00:27<00:16,  2.58it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.58it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.58it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.58it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.58it/s]I0315 05:26:32.766612 341427 finetune.py:68] layer 19_gate @ epoch 2 new loss 9.860003046924248e-05 old loss 9.915041300700977e-05 BETTER
 67%|██████▋   | 75/112 [00:29<00:14,  2.56it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.56it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.56it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.57it/s] 71%|███████   | 79/112 [00:31<00:12,  2.58it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.58it/s] 72%|███████▏  | 81/112 [00:32<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.59it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.60it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.61it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.61it/s]W0315 05:26:37.177000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.178000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.178000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.178000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.178000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.178000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.178000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.219000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.219000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.219000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.219000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.219000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.235000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.235000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.235000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.235000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.235000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 77%|███████▋  | 86/112 [00:33<00:09,  2.60it/s]W0315 05:26:37.410000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.410000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.410000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.410000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.410000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.737000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.738000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.738000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.738000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.738000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.738000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.738000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 78%|███████▊  | 87/112 [00:34<00:09,  2.60it/s]W0315 05:26:37.774000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.774000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.774000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.774000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.774000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.849000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.849000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.849000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.849000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:37.849000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 79%|███████▊  | 88/112 [00:34<00:09,  2.57it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.58it/s] 80%|████████  | 90/112 [00:35<00:08,  2.58it/s]W0315 05:26:39.072000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.084000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.093000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.093000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:35<00:08,  2.58it/s]W0315 05:26:39.560000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.560000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.560000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.561000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.561000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.561000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.561000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.594000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.594000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.594000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.594000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.595000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 82%|████████▏ | 92/112 [00:36<00:07,  2.59it/s]W0315 05:26:39.948000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.948000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.948000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.948000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.948000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.949000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.949000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:39.949000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 83%|████████▎ | 93/112 [00:36<00:07,  2.59it/s]W0315 05:26:40.248000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:40.248000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:40.248000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:40.248000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:40.248000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:37<00:06,  2.59it/s]W0315 05:26:40.588000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:40.593000 139655928440640 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 85%|████████▍ | 95/112 [00:37<00:06,  2.59it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.59it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.60it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.60it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.60it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.56it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.56it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.55it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.55it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.56it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.56it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.57it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.57it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.57it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.57it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.57it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.57it/s]100%|██████████| 112/112 [00:44<00:00,  2.57it/s]100%|██████████| 112/112 [00:44<00:00,  2.54it/s]
I0315 05:26:47.968940 339725 finetune.py:45] layer 16_down initial loss 0.0001360548922093585
W0315 05:26:47.969378 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0315 05:26:55.001000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.002000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.002000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.002000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.002000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.002000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.002000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.045000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.045000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.045000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.045000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.045000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.062000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.062000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.062000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.062000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.062000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.234000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.235000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.235000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.235000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.235000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.579000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.579000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.579000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.579000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.579000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.579000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.579000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.609000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.609000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.609000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.609000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.609000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.684000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.685000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.685000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.685000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:55.685000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:56.943000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:56.954000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:56.964000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:56.964000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.455000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.456000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.456000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.456000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.456000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.456000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.456000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.487000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.487000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.487000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.488000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.488000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.848000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.849000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.849000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.849000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.849000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.849000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.849000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:26:57.850000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:58.154000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:26:58.154000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:58.154000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:26:58.154000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:26:58.154000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:26:58.509000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:26:58.514000 140641617725248 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 05:27:04.543148 340861 finetune.py:68] layer 18_gate @ epoch 4 new loss 9.071974636754021e-05 old loss 9.116795263253152e-05 BETTER
I0315 05:27:05.587518 341427 finetune.py:68] layer 19_gate @ epoch 3 new loss 9.810656047193334e-05 old loss 9.860003046924248e-05 BETTER
W0315 05:27:05.783805 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 05:27:05.956168 340292 finetune.py:45] layer 17_down initial loss 0.00015572320262435824
W0315 05:27:05.956721 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

18_gate proxy err 0.004500864539295435 tr(WHW.T) 4654.62060546875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:38,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.67it/s]  3%|▎         | 3/112 [00:01<00:54,  1.98it/s]  4%|▎         | 4/112 [00:02<00:49,  2.17it/s]  4%|▍         | 5/112 [00:02<00:46,  2.30it/s]  5%|▌         | 6/112 [00:02<00:44,  2.39it/s]  6%|▋         | 7/112 [00:03<00:42,  2.45it/s]  7%|▋         | 8/112 [00:03<00:41,  2.48it/s]  8%|▊         | 9/112 [00:04<00:41,  2.51it/s]  9%|▉         | 10/112 [00:04<00:40,  2.52it/s] 10%|▉         | 11/112 [00:04<00:39,  2.53it/s] 11%|█         | 12/112 [00:05<00:39,  2.51it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.53it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.55it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.55it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.55it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.55it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.55it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.55it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.55it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.55it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.54it/s] 21%|██        | 23/112 [00:09<00:35,  2.50it/s] 21%|██▏       | 24/112 [00:09<00:35,  2.51it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.51it/s] 23%|██▎       | 26/112 [00:10<00:34,  2.52it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.52it/s]I0315 05:27:20.586488 339725 finetune.py:68] layer 16_down @ epoch 0 new loss 0.00013598268560599536 old loss 0.0001360548922093585 BETTER
 25%|██▌       | 28/112 [00:11<00:33,  2.53it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.53it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.53it/s] 28%|██▊       | 31/112 [00:12<00:32,  2.53it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.53it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.54it/s] 30%|███       | 34/112 [00:13<00:31,  2.51it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.52it/s] 32%|███▏      | 36/112 [00:14<00:30,  2.53it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.54it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.54it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.55it/s] 36%|███▌      | 40/112 [00:16<00:28,  2.54it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.54it/s] 38%|███▊      | 42/112 [00:17<00:27,  2.54it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.53it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.52it/s] 40%|████      | 45/112 [00:18<00:26,  2.52it/s] 41%|████      | 46/112 [00:18<00:26,  2.47it/s] 42%|████▏     | 47/112 [00:19<00:26,  2.49it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.50it/s] 44%|████▍     | 49/112 [00:19<00:25,  2.51it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.52it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.52it/s] 46%|████▋     | 52/112 [00:21<00:23,  2.52it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.53it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.53it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.54it/s] 50%|█████     | 56/112 [00:22<00:22,  2.54it/s] 51%|█████     | 57/112 [00:22<00:21,  2.53it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.50it/s] 53%|█████▎    | 59/112 [00:23<00:21,  2.51it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.51it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.51it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.52it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.52it/s] 57%|█████▋    | 64/112 [00:25<00:19,  2.52it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.53it/s] 59%|█████▉    | 66/112 [00:26<00:18,  2.53it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.53it/s] 61%|██████    | 68/112 [00:27<00:17,  2.53it/s] 62%|██████▏   | 69/112 [00:27<00:17,  2.52it/s]I0315 05:27:37.111651 340292 finetune.py:68] layer 17_down @ epoch 0 new loss 0.00015564437489956617 old loss 0.00015572320262435824 BETTER
 62%|██████▎   | 70/112 [00:28<00:16,  2.49it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.51it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.51it/s]I0315 05:27:38.207288 341427 finetune.py:68] layer 19_gate @ epoch 4 new loss 9.767059236764908e-05 old loss 9.810656047193334e-05 BETTER
 65%|██████▌   | 73/112 [00:29<00:15,  2.53it/s] 66%|██████▌   | 74/112 [00:29<00:15,  2.53it/s] 67%|██████▋   | 75/112 [00:30<00:14,  2.53it/s]W0315 05:27:39.313861 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 68%|██████▊   | 76/112 [00:30<00:14,  2.54it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.54it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.54it/s] 71%|███████   | 79/112 [00:31<00:13,  2.54it/s] 71%|███████▏  | 80/112 [00:32<00:12,  2.53it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.50it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.51it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.51it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.52it/s]19_gate proxy err 0.00497297802940011 tr(WHW.T) 4568.935546875
  0%|          | 0/112 [00:00<?, ?it/s] 76%|███████▌  | 85/112 [00:34<00:10,  2.52it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.52it/s]  1%|          | 1/112 [00:00<01:37,  1.14it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.52it/s]  2%|▏         | 2/112 [00:01<01:04,  1.71it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.52it/s]  3%|▎         | 3/112 [00:01<00:53,  2.02it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.53it/s]  4%|▎         | 4/112 [00:02<00:48,  2.22it/s] 80%|████████  | 90/112 [00:36<00:08,  2.53it/s]  4%|▍         | 5/112 [00:02<00:45,  2.36it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.52it/s]  5%|▌         | 6/112 [00:02<00:43,  2.44it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.53it/s]  6%|▋         | 7/112 [00:03<00:42,  2.49it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.50it/s]  7%|▋         | 8/112 [00:03<00:41,  2.53it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.51it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s] 85%|████████▍ | 95/112 [00:38<00:06,  2.53it/s]  9%|▉         | 10/112 [00:04<00:39,  2.56it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.53it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.55it/s] 11%|█         | 12/112 [00:05<00:38,  2.59it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.55it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.59it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.55it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.59it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.60it/s] 89%|████████▉ | 100/112 [00:40<00:04,  2.54it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.61it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.54it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.60it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.53it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.52it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.61it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.52it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.61it/s] 94%|█████████▍| 105/112 [00:42<00:02,  2.50it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.60it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.51it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.61it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.52it/s] 21%|██        | 23/112 [00:09<00:34,  2.57it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.53it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.58it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.54it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.60it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.55it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.55it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s]I0315 05:27:53.884033 339725 finetune.py:68] layer 16_down @ epoch 1 new loss 0.00013597545330412686 old loss 0.00013598268560599536 BETTER
100%|██████████| 112/112 [00:44<00:00,  2.56it/s]100%|██████████| 112/112 [00:44<00:00,  2.50it/s]
 25%|██▌       | 28/112 [00:11<00:32,  2.62it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.61it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.60it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.61it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.61it/s] 30%|███       | 34/112 [00:13<00:29,  2.61it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.58it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.60it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.61it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.62it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.62it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.63it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.63it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.63it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.63it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.61it/s] 40%|████      | 45/112 [00:17<00:25,  2.62it/s] 41%|████      | 46/112 [00:18<00:25,  2.64it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.64it/s]W0315 05:28:01.443000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.443000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.443000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.444000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.444000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.444000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.444000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.489000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.489000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.489000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.489000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.489000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.505000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.505000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.505000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.505000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.505000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.679000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.680000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.680000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.680000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:01.680000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
 43%|████▎     | 48/112 [00:18<00:24,  2.61it/s]W0315 05:28:02.005000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.006000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.006000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.006000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.006000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.006000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.006000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.046000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.046000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.046000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.046000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.046000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.121000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.121000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.121000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.121000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:02.121000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 49/112 [00:19<00:23,  2.63it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.63it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.64it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.64it/s]W0315 05:28:03.343000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.357000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.365000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.365000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 47%|████▋     | 53/112 [00:20<00:22,  2.64it/s]W0315 05:28:03.819000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.819000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.819000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.819000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.819000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.820000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.820000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.851000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.851000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.851000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.851000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:03.851000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 48%|████▊     | 54/112 [00:21<00:22,  2.63it/s]W0315 05:28:04.209000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.209000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.209000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.209000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.209000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.209000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.210000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.210000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
 49%|████▉     | 55/112 [00:21<00:21,  2.63it/s]W0315 05:28:04.523000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.523000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.523000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.523000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.523000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 50%|█████     | 56/112 [00:21<00:21,  2.63it/s]W0315 05:28:04.874000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:04.880000 140296172484416 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 51%|█████     | 57/112 [00:22<00:20,  2.62it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.61it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.58it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.59it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.60it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.59it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.60it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.60it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.59it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.60it/s]I0315 05:28:08.848112 340292 finetune.py:68] layer 17_down @ epoch 1 new loss 0.00015563030319754034 old loss 0.00015564437489956617 BETTER
 60%|█████▉    | 67/112 [00:26<00:17,  2.60it/s] 61%|██████    | 68/112 [00:26<00:16,  2.60it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.60it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.56it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.58it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.59it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.60it/s]I0315 05:28:12.158199 340861 finetune.py:45] layer 18_down initial loss 0.00015454395906999707
W0315 05:28:12.158573 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.61it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.62it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.62it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.62it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.61it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.60it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.58it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.59it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.60it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.60it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.60it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.60it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.61it/s] 80%|████████  | 90/112 [00:35<00:08,  2.61it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.61it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.60it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.60it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.59it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.57it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.58it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.59it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.59it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.60it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.60it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.61it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.61it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.61it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.60it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.59it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.58it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.56it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.58it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.59it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.59it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.58it/s]
I0315 05:28:27.054129 339725 finetune.py:68] layer 16_down @ epoch 2 new loss 0.0001359605957986787 old loss 0.00013597545330412686 BETTER
W0315 05:28:34.033000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.033000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.034000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.034000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.034000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.034000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.034000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.083000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.083000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.083000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.083000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.083000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.100000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.101000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.101000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.101000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.101000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.275000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.275000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.275000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.275000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.275000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.614000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.614000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.614000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.614000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.614000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.614000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.615000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.650000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.651000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.651000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.651000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.651000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.722000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.722000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.722000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.722000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:34.723000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:35.979000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:35.994000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.002000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.002000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.482000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.482000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.483000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.483000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.483000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.483000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.483000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.518000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.518000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.518000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.519000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.519000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.884000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.884000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.884000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.884000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.884000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.885000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.885000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:36.885000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:37.200000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:28:37.200000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:28:37.200000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:28:37.200000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:28:37.200000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:37.552000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:28:37.557000 140489804412736 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 05:28:41.081781 340292 finetune.py:68] layer 17_down @ epoch 2 new loss 0.00015561700274702162 old loss 0.00015563030319754034 BETTER
I0315 05:28:43.853589 340861 finetune.py:68] layer 18_down @ epoch 0 new loss 0.00015447968326043338 old loss 0.00015454395906999707 BETTER
I0315 05:28:44.952753 341427 finetune.py:45] layer 19_down initial loss 0.00016392003453802317
W0315 05:28:44.953384 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:29:00.383675 339725 finetune.py:68] layer 16_down @ epoch 3 new loss 0.00013595113705378026 old loss 0.0001359605957986787 BETTER
I0315 05:29:13.377829 340292 finetune.py:68] layer 17_down @ epoch 3 new loss 0.00015560758765786886 old loss 0.00015561700274702162 BETTER
I0315 05:29:15.298840 341427 finetune.py:68] layer 19_down @ epoch 0 new loss 0.00016386118659283966 old loss 0.00016392003453802317 BETTER
I0315 05:29:16.135110 340861 finetune.py:68] layer 18_down @ epoch 1 new loss 0.00015446965699084103 old loss 0.00015447968326043338 BETTER
I0315 05:29:33.341904 339725 finetune.py:68] layer 16_down @ epoch 4 new loss 0.00013593830226454884 old loss 0.00013595113705378026 BETTER
W0315 05:29:34.094923 339725 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

16_down proxy err 0.012377331033349037 tr(WHW.T) 17.9730224609375
I0315 05:29:45.705679 340292 finetune.py:68] layer 17_down @ epoch 4 new loss 0.0001556034985696897 old loss 0.00015560758765786886 BETTER
I0315 05:29:46.905075 341427 finetune.py:68] layer 19_down @ epoch 1 new loss 0.00016384579066652805 old loss 0.00016386118659283966 BETTER
W0315 05:29:46.942981 340292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

17_down proxy err 0.012600591406226158 tr(WHW.T) 21.36526870727539
I0315 05:29:48.385354 340861 finetune.py:68] layer 18_down @ epoch 2 new loss 0.00015445580356754363 old loss 0.00015446965699084103 BETTER
I0315 05:30:18.025862 341427 finetune.py:68] layer 19_down @ epoch 2 new loss 0.00016383419279009104 old loss 0.00016384579066652805 BETTER
I0315 05:30:20.017729 340861 finetune.py:68] layer 18_down @ epoch 3 new loss 0.00015444893506355584 old loss 0.00015445580356754363 BETTER
I0315 05:30:49.348081 341427 finetune.py:68] layer 19_down @ epoch 3 new loss 0.00016382533067371696 old loss 0.00016383419279009104 BETTER
I0315 05:30:51.502127 340861 finetune.py:68] layer 18_down @ epoch 4 new loss 0.0001544425613246858 old loss 0.00015444893506355584 BETTER
W0315 05:30:52.366828 340861 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

18_down proxy err 0.012891290709376335 tr(WHW.T) 21.162017822265625
I0315 05:30:57.918054 273992 quantize_finetune_llama.py:186] computed original embedding for layer 20 in 66.65778589248657s
I0315 05:30:58.350696 273992 quantize_finetune_llama.py:159] layer 21 gpu 1
I0315 05:31:00.404232 343568 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 05:31:00.404409 343568 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 05:31:00.404473 343568 utils.py:162] NumExpr defaulting to 16 threads.
I0315 05:31:00.602969 343568 config.py:58] PyTorch version 2.4.0 available.
I0315 05:31:03.082832 343568 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 05:31:03.531044 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:50,  1.63s/it]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.94it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.20it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:04<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.84it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.88it/s] 41%|████      | 13/32 [00:05<00:06,  2.90it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.93it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.92it/s] 50%|█████     | 16/32 [00:06<00:05,  2.88it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.89it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.92it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.97it/s] 62%|██████▎   | 20/32 [00:08<00:03,  3.02it/s] 66%|██████▌   | 21/32 [00:08<00:03,  3.02it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.07it/s] 72%|███████▏  | 23/32 [00:09<00:02,  3.11it/s] 75%|███████▌  | 24/32 [00:09<00:02,  3.15it/s] 78%|███████▊  | 25/32 [00:09<00:02,  3.06it/s] 81%|████████▏ | 26/32 [00:10<00:01,  3.08it/s] 84%|████████▍ | 27/32 [00:10<00:01,  3.11it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.12it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.14it/s] 94%|█████████▍| 30/32 [00:11<00:00,  3.14it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.14it/s]100%|██████████| 32/32 [00:11<00:00,  3.11it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
W0315 05:31:20.156000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.156000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.156000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.156000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.156000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.156000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.156000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.184000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.184000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.184000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.184000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.184000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.200000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.200000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.200000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.200000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.200000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
I0315 05:31:20.214534 341427 finetune.py:68] layer 19_down @ epoch 4 new loss 0.0001638128887861967 old loss 0.00016382533067371696 BETTER
W0315 05:31:20.510000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.510000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.510000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.510000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:31:20.510000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.256177 341427 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

W0315 05:31:21.390000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.390000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.390000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.390000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.390000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.390000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.390000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.407000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.407000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.407000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.407000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.407000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
19_down proxy err 0.012931093573570251 tr(WHW.T) 22.04034423828125
W0315 05:31:21.640000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.640000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.640000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.640000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:31:21.640000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.794000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.794000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.794000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.794000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.794000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.794000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.794000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.811000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.811000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.812000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.812000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:31:22.812000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:31:23.704000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:31:23.704000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:31:23.704000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:31:23.704000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:31:23.704000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 05:31:29.683153 343568 finetune.py:45] layer 20_v initial loss 3.6925899621564895e-05
W0315 05:31:29.683340 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:32:02.584715 273992 quantize_finetune_llama.py:186] computed original embedding for layer 21 in 63.798423051834106s
I0315 05:32:03.018736 273992 quantize_finetune_llama.py:159] layer 22 gpu 2
I0315 05:32:05.063939 344138 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 05:32:05.064055 344138 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 05:32:05.064123 344138 utils.py:162] NumExpr defaulting to 16 threads.
I0315 05:32:05.253240 344138 config.py:58] PyTorch version 2.4.0 available.
I0315 05:32:05.312498 343568 finetune.py:68] layer 20_v @ epoch 0 new loss 1.136101309384685e-05 old loss 3.6925899621564895e-05 BETTER
I0315 05:32:07.430249 344138 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 05:32:07.765164 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:48,  1.55s/it]  6%|▋         | 2/32 [00:01<00:25,  1.20it/s]  9%|▉         | 3/32 [00:02<00:17,  1.63it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.97it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.24it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.82it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.84it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.93it/s] 50%|█████     | 16/32 [00:06<00:05,  2.95it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.99it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.00it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.91it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.94it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.95it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.97it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.93it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.91it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.88it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.92it/s]100%|██████████| 32/32 [00:12<00:00,  2.90it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
W0315 05:32:23.943000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.943000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.943000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.943000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.944000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.944000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.944000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.969000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.969000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.969000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.969000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.969000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.985000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.985000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.985000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.985000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:32:23.985000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:32:24.334000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:32:24.334000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:32:24.334000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:32:24.334000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:32:24.334000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.226000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.226000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.226000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.226000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.226000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.226000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.227000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.245000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.245000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.245000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.245000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.245000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.478000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.478000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.478000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.478000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:32:25.478000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.655000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.655000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.655000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.655000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.655000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.655000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.655000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.674000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.674000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.674000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.674000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:32:26.674000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:32:27.589000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:32:27.589000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:32:27.589000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:32:27.589000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:32:27.589000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 05:32:34.025385 344138 finetune.py:45] layer 21_v initial loss 4.494667155086063e-05
W0315 05:32:34.025726 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:32:42.714901 343568 finetune.py:68] layer 20_v @ epoch 1 new loss 1.0102726264449302e-05 old loss 1.136101309384685e-05 BETTER
I0315 05:33:05.403374 273992 quantize_finetune_llama.py:186] computed original embedding for layer 22 in 61.95537495613098s
I0315 05:33:05.823588 273992 quantize_finetune_llama.py:159] layer 23 gpu 3
I0315 05:33:07.939397 344704 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 05:33:07.939600 344704 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 05:33:07.939692 344704 utils.py:162] NumExpr defaulting to 16 threads.
I0315 05:33:08.160047 344704 config.py:58] PyTorch version 2.4.0 available.
I0315 05:33:08.269779 344138 finetune.py:68] layer 21_v @ epoch 0 new loss 1.4694062883791048e-05 old loss 4.494667155086063e-05 BETTER
I0315 05:33:10.502290 344704 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 05:33:11.035342 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:51,  1.65s/it]  6%|▋         | 2/32 [00:02<00:26,  1.13it/s]  9%|▉         | 3/32 [00:02<00:18,  1.56it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.89it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.76it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.83it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.85it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s]I0315 05:33:20.314530 343568 finetune.py:68] layer 20_v @ epoch 2 new loss 9.602248610462993e-06 old loss 1.0102726264449302e-05 BETTER
 62%|██████▎   | 20/32 [00:08<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.87it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.85it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.88it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.88it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
W0315 05:33:27.862000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.862000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.862000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.862000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.862000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.862000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.862000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.891000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.891000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.891000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.891000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.891000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.909000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.909000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.909000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.909000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:33:27.909000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:33:28.249000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:33:28.249000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:33:28.249000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:33:28.249000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:33:28.249000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.183000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.183000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.183000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.183000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.183000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.183000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.183000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.202000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.202000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.202000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.202000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.202000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.455000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.455000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.455000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.455000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:33:29.455000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.668000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.668000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.668000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.668000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.668000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.668000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.668000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.688000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.688000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.688000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.688000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:33:30.688000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:33:31.639000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:33:31.639000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:33:31.639000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:33:31.639000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:33:31.639000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 05:33:37.832686 344704 finetune.py:45] layer 22_v initial loss 5.052929918747395e-05
W0315 05:33:37.833108 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:33:43.926787 344138 finetune.py:68] layer 21_v @ epoch 1 new loss 1.2929392141813878e-05 old loss 1.4694062883791048e-05 BETTER
I0315 05:33:57.736215 343568 finetune.py:68] layer 20_v @ epoch 3 new loss 9.313886948802974e-06 old loss 9.602248610462993e-06 BETTER
I0315 05:34:07.304448 273992 quantize_finetune_llama.py:186] computed original embedding for layer 23 in 60.98492741584778s
I0315 05:34:07.723219 273992 quantize_finetune_llama.py:159] layer 24 gpu 0
I0315 05:34:09.869397 345273 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 05:34:09.869523 345273 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 05:34:09.869589 345273 utils.py:162] NumExpr defaulting to 16 threads.
I0315 05:34:10.103562 345273 config.py:58] PyTorch version 2.4.0 available.
I0315 05:34:12.404759 344704 finetune.py:68] layer 22_v @ epoch 0 new loss 1.3112751730659511e-05 old loss 5.052929918747395e-05 BETTER
I0315 05:34:12.476074 345273 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 05:34:12.916879 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:48,  1.57s/it]  6%|▋         | 2/32 [00:01<00:25,  1.17it/s]  9%|▉         | 3/32 [00:02<00:18,  1.59it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.92it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.72it/s]I0315 05:34:19.404238 344138 finetune.py:68] layer 21_v @ epoch 2 new loss 1.2214634807605762e-05 old loss 1.2929392141813878e-05 BETTER
 38%|███▊      | 12/32 [00:05<00:07,  2.74it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.80it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.81it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.81it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
W0315 05:34:29.614000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.614000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.614000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.614000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.615000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.615000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.615000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.643000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.643000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.643000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.643000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.643000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.660000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.660000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.660000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.660000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.660000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.992000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.992000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.993000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.993000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:29.993000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.914000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.914000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.914000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.914000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.915000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.915000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.915000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.933000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.933000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.933000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.933000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:30.933000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:31.188000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:31.188000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:31.188000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:31.188000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:31.188000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.394000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.394000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.394000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.395000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.395000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.395000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.395000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.413000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.413000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.413000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.413000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:32.413000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:33.354000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:34:33.354000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:33.354000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:33.354000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:33.355000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 05:34:35.055591 343568 finetune.py:68] layer 20_v @ epoch 4 new loss 9.098406735574827e-06 old loss 9.313886948802974e-06 BETTER
W0315 05:34:36.901006 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_v proxy err 0.010641125030815601 tr(WHW.T) 90.61302185058594
  0%|          | 0/32 [00:00<?, ?it/s]I0315 05:34:39.157966 345273 finetune.py:45] layer 23_v initial loss 7.890422421041876e-05
W0315 05:34:39.158185 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  3%|▎         | 1/32 [00:01<00:31,  1.01s/it]  6%|▋         | 2/32 [00:01<00:19,  1.55it/s]  9%|▉         | 3/32 [00:01<00:15,  1.90it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.56it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.57it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.63it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s]I0315 05:34:47.887171 344704 finetune.py:68] layer 22_v @ epoch 1 new loss 1.132082525145961e-05 old loss 1.3112751730659511e-05 BETTER
 75%|███████▌  | 24/32 [00:09<00:03,  2.61it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.62it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0315 05:34:55.638289 344138 finetune.py:68] layer 21_v @ epoch 3 new loss 1.176342266262509e-05 old loss 1.2214634807605762e-05 BETTER
W0315 05:34:57.758000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.759000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.759000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.759000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.759000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.759000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.759000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.793000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.793000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.793000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.793000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.793000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.810000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.810000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.811000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.811000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.811000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.976000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.976000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.976000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.976000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:57.976000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.216000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.216000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.216000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.216000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.216000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.217000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.217000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.237000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.237000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.237000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.238000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.238000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.307000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.307000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.307000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.308000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:58.308000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.222000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.550000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.550000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.550000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.550000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.550000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.550000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.551000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.572000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.572000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.572000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.572000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.572000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.837000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.837000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.837000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.837000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:34:59.837000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:35:00.111000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 05:35:07.003918 343568 finetune.py:45] layer 20_q initial loss 1.3738318557443563e-05
W0315 05:35:07.004293 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:35:12.743794 345273 finetune.py:68] layer 23_v @ epoch 0 new loss 1.6768066416261718e-05 old loss 7.890422421041876e-05 BETTER
I0315 05:35:23.792026 344704 finetune.py:68] layer 22_v @ epoch 2 new loss 1.0674330042093061e-05 old loss 1.132082525145961e-05 BETTER
I0315 05:35:31.893157 344138 finetune.py:68] layer 21_v @ epoch 4 new loss 1.1446844837337267e-05 old loss 1.176342266262509e-05 BETTER
W0315 05:35:33.545785 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_v proxy err 0.010498197749257088 tr(WHW.T) 95.41011047363281
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.15it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.39it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.52it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s]I0315 05:35:43.203239 343568 finetune.py:68] layer 20_q @ epoch 0 new loss 1.3228346688265447e-05 old loss 1.3738318557443563e-05 BETTER
 66%|██████▌   | 21/32 [00:08<00:04,  2.57it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]I0315 05:35:47.572312 345273 finetune.py:68] layer 23_v @ epoch 1 new loss 1.3510517419490498e-05 old loss 1.6768066416261718e-05 BETTER
100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
W0315 05:35:53.929000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.930000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.930000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.930000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.930000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.930000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.930000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.962000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.962000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.962000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.962000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.962000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.978000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.979000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.979000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.979000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:35:53.979000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.143000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.143000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.144000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.144000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.144000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.375000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.375000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.375000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.375000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.375000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.375000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.375000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.396000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.396000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.396000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.396000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.396000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.466000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.466000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.466000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.466000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:35:54.466000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.359000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.684000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.684000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.684000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.684000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.685000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.685000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.685000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.708000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.708000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.708000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.708000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.708000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.978000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.978000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.978000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.979000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:35:55.979000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:35:56.258000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 05:35:59.642029 344704 finetune.py:68] layer 22_v @ epoch 3 new loss 1.0296577784174588e-05 old loss 1.0674330042093061e-05 BETTER
I0315 05:36:03.051005 344138 finetune.py:45] layer 21_q initial loss 1.8823189748218283e-05
W0315 05:36:03.051402 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:36:20.443609 343568 finetune.py:68] layer 20_q @ epoch 1 new loss 1.2949653864779975e-05 old loss 1.3228346688265447e-05 BETTER
I0315 05:36:23.018385 345273 finetune.py:68] layer 23_v @ epoch 2 new loss 1.2485445950005669e-05 old loss 1.3510517419490498e-05 BETTER
I0315 05:36:36.371226 344704 finetune.py:68] layer 22_v @ epoch 4 new loss 1.0053478945337702e-05 old loss 1.0296577784174588e-05 BETTER
I0315 05:36:38.170458 344138 finetune.py:68] layer 21_q @ epoch 0 new loss 1.807394983188715e-05 old loss 1.8823189748218283e-05 BETTER
W0315 05:36:38.218259 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_v proxy err 0.011455560103058815 tr(WHW.T) 101.29380798339844
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.00it/s]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:15,  1.90it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.12it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.44it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.53it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.55it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.55it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.55it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.53it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.54it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.55it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.57it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.56it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.57it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.55it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
I0315 05:36:57.867729 343568 finetune.py:68] layer 20_q @ epoch 2 new loss 1.2739255907945335e-05 old loss 1.2949653864779975e-05 BETTER
I0315 05:36:58.479521 345273 finetune.py:68] layer 23_v @ epoch 3 new loss 1.194869491882855e-05 old loss 1.2485445950005669e-05 BETTER
W0315 05:36:59.034000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.034000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.034000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.034000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.034000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.034000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.034000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.066000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.066000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.066000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.066000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.066000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.082000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.082000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.082000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.082000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.082000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.247000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.247000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.247000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.247000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.248000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.483000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.484000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.484000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.484000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.484000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.484000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.484000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.506000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.506000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.506000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.506000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.506000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.577000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.577000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.577000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.577000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:36:59.577000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.492000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.821000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.821000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.821000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.822000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.822000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.822000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.822000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.843000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.843000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.843000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.843000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:00.843000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:01.114000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:01.114000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:01.114000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:37:01.114000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:01.114000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:01.386000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 05:37:08.077006 344704 finetune.py:45] layer 22_q initial loss 1.5878988051554188e-05
W0315 05:37:08.077496 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:37:14.432631 344138 finetune.py:68] layer 21_q @ epoch 1 new loss 1.759474616847001e-05 old loss 1.807394983188715e-05 BETTER
I0315 05:37:34.846878 345273 finetune.py:68] layer 23_v @ epoch 4 new loss 1.1598621313169133e-05 old loss 1.194869491882855e-05 BETTER
I0315 05:37:35.456920 343568 finetune.py:68] layer 20_q @ epoch 3 new loss 1.2573678759508766e-05 old loss 1.2739255907945335e-05 BETTER
W0315 05:37:36.448088 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_v proxy err 0.011988220736384392 tr(WHW.T) 112.7859115600586
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:15,  1.92it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.13it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.26it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.59it/s]I0315 05:37:43.747235 344704 finetune.py:68] layer 22_q @ epoch 0 new loss 1.534393049951177e-05 old loss 1.5878988051554188e-05 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.59it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.58it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.58it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
I0315 05:37:50.734129 344138 finetune.py:68] layer 21_q @ epoch 2 new loss 1.7236558051081374e-05 old loss 1.759474616847001e-05 BETTER
W0315 05:37:56.680000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.680000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.681000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.681000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.681000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.681000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.681000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.710000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.711000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.711000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.711000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.711000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.726000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.726000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.726000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.726000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.727000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.893000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.893000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.893000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.893000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:37:56.893000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.130000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.130000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.131000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.131000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.131000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.131000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.131000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.151000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.151000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.151000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.151000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.152000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.218000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.218000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.218000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.218000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:57.218000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.105000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.422000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.422000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.422000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.422000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.422000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.422000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.422000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.444000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.444000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.444000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.445000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.445000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.705000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.705000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.705000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.705000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.706000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:37:58.975000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 05:38:05.643558 345273 finetune.py:45] layer 23_q initial loss 1.7047972505679354e-05
W0315 05:38:05.643920 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:38:12.668238 343568 finetune.py:68] layer 20_q @ epoch 4 new loss 1.2444057574612089e-05 old loss 1.2573678759508766e-05 BETTER
W0315 05:38:14.630494 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_q proxy err 0.0023133023642003536 tr(WHW.T) 5693.36083984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.05it/s]  6%|▋         | 2/32 [00:01<00:18,  1.62it/s]  9%|▉         | 3/32 [00:01<00:14,  1.98it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s]I0315 05:38:19.800898 344704 finetune.py:68] layer 22_q @ epoch 1 new loss 1.5027448171167634e-05 old loss 1.534393049951177e-05 BETTER
 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.60it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.62it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.63it/s] 50%|█████     | 16/32 [00:06<00:06,  2.63it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.63it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.63it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.63it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.62it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s]I0315 05:38:26.913289 344138 finetune.py:68] layer 21_q @ epoch 3 new loss 1.694162710919045e-05 old loss 1.7236558051081374e-05 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.64it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0315 05:38:36.286529 343568 finetune.py:45] layer 20_k initial loss 1.5866422472754493e-05
W0315 05:38:36.286971 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:38:40.117590 345273 finetune.py:68] layer 23_q @ epoch 0 new loss 1.6453946955152787e-05 old loss 1.7047972505679354e-05 BETTER
I0315 05:38:56.115111 344704 finetune.py:68] layer 22_q @ epoch 2 new loss 1.4799050404690206e-05 old loss 1.5027448171167634e-05 BETTER
I0315 05:39:03.511290 344138 finetune.py:68] layer 21_q @ epoch 4 new loss 1.671018617344089e-05 old loss 1.694162710919045e-05 BETTER
W0315 05:39:05.224861 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_q proxy err 0.0018419255502521992 tr(WHW.T) 6795.296875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.04it/s]  6%|▋         | 2/32 [00:01<00:18,  1.61it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.58it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s]I0315 05:39:12.696613 343568 finetune.py:68] layer 20_k @ epoch 0 new loss 1.5348317901953124e-05 old loss 1.5866422472754493e-05 BETTER
 50%|█████     | 16/32 [00:06<00:06,  2.65it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.65it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.65it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s]I0315 05:39:15.306975 345273 finetune.py:68] layer 23_q @ epoch 1 new loss 1.6086045434349217e-05 old loss 1.6453946955152787e-05 BETTER
 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.63it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.63it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.67it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0315 05:39:25.980590 344138 finetune.py:45] layer 21_k initial loss 2.2692900529364124e-05
W0315 05:39:25.981099 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:39:32.573206 344704 finetune.py:68] layer 22_q @ epoch 3 new loss 1.4618390196119435e-05 old loss 1.4799050404690206e-05 BETTER
I0315 05:39:49.969466 343568 finetune.py:68] layer 20_k @ epoch 1 new loss 1.522035108791897e-05 old loss 1.5348317901953124e-05 BETTER
I0315 05:39:50.756989 345273 finetune.py:68] layer 23_q @ epoch 2 new loss 1.5824192814761773e-05 old loss 1.6086045434349217e-05 BETTER
I0315 05:40:01.401292 344138 finetune.py:68] layer 21_k @ epoch 0 new loss 2.140627293556463e-05 old loss 2.2692900529364124e-05 BETTER
I0315 05:40:08.900498 344704 finetune.py:68] layer 22_q @ epoch 4 new loss 1.4463085790339392e-05 old loss 1.4618390196119435e-05 BETTER
W0315 05:40:10.668358 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_q proxy err 0.0020806228276342154 tr(WHW.T) 5951.06298828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:18,  1.64it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.31it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.60it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.63it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.64it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.64it/s] 50%|█████     | 16/32 [00:06<00:06,  2.64it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.64it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.63it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.65it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.63it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0315 05:40:26.336377 345273 finetune.py:68] layer 23_q @ epoch 3 new loss 1.5617857570759952e-05 old loss 1.5824192814761773e-05 BETTER
I0315 05:40:27.305824 343568 finetune.py:68] layer 20_k @ epoch 2 new loss 1.5109205378394108e-05 old loss 1.522035108791897e-05 BETTER
I0315 05:40:31.712066 344704 finetune.py:45] layer 22_k initial loss 1.8889633793151006e-05
W0315 05:40:31.712499 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:40:37.723538 344138 finetune.py:68] layer 21_k @ epoch 1 new loss 2.1157818991923705e-05 old loss 2.140627293556463e-05 BETTER
I0315 05:41:02.378886 345273 finetune.py:68] layer 23_q @ epoch 4 new loss 1.545019040349871e-05 old loss 1.5617857570759952e-05 BETTER
W0315 05:41:04.275489 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 05:41:04.615793 343568 finetune.py:68] layer 20_k @ epoch 3 new loss 1.5030237591417972e-05 old loss 1.5109205378394108e-05 BETTER
23_q proxy err 0.0020465035922825336 tr(WHW.T) 6413.02978515625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]I0315 05:41:07.010911 344704 finetune.py:68] layer 22_k @ epoch 0 new loss 1.8343831470701844e-05 old loss 1.8889633793151006e-05 BETTER
  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.58it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.62it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.65it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.65it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.64it/s]I0315 05:41:14.296500 344138 finetune.py:68] layer 21_k @ epoch 2 new loss 2.0968993339920416e-05 old loss 2.1157818991923705e-05 BETTER
 69%|██████▉   | 22/32 [00:08<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.64it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.64it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.65it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.65it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.64it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.65it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.65it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
I0315 05:41:25.575385 345273 finetune.py:45] layer 23_k initial loss 2.0953422790626064e-05
W0315 05:41:25.575832 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:41:42.382200 343568 finetune.py:68] layer 20_k @ epoch 4 new loss 1.496318873250857e-05 old loss 1.5030237591417972e-05 BETTER
I0315 05:41:43.742707 344704 finetune.py:68] layer 22_k @ epoch 1 new loss 1.8202748833573423e-05 old loss 1.8343831470701844e-05 BETTER
W0315 05:41:44.327811 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_k proxy err 0.0017464515985921025 tr(WHW.T) 4221.162109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:26,  1.19it/s]  6%|▋         | 2/32 [00:01<00:17,  1.75it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.46it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.58it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.60it/s]I0315 05:41:50.795668 344138 finetune.py:68] layer 21_k @ epoch 3 new loss 2.082138780679088e-05 old loss 2.0968993339920416e-05 BETTER
 38%|███▊      | 12/32 [00:05<00:07,  2.61it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.60it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.62it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0315 05:42:00.567575 345273 finetune.py:68] layer 23_k @ epoch 0 new loss 2.0033328837598674e-05 old loss 2.0953422790626064e-05 BETTER
I0315 05:42:06.037926 343568 finetune.py:45] layer 20_o initial loss 2.6293804694432765e-05
W0315 05:42:06.038283 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:42:20.279700 344704 finetune.py:68] layer 22_k @ epoch 2 new loss 1.809719469747506e-05 old loss 1.8202748833573423e-05 BETTER
I0315 05:42:27.132008 344138 finetune.py:68] layer 21_k @ epoch 4 new loss 2.068482899630908e-05 old loss 2.082138780679088e-05 BETTER
W0315 05:42:28.765757 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

21_k proxy err 0.0016481116181239486 tr(WHW.T) 4408.005859375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.21it/s]  6%|▋         | 2/32 [00:01<00:17,  1.76it/s]  9%|▉         | 3/32 [00:01<00:14,  2.06it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.52it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.57it/s]I0315 05:42:35.985220 345273 finetune.py:68] layer 23_k @ epoch 1 new loss 1.9858323867083527e-05 old loss 2.0033328837598674e-05 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.59it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.59it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.59it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.59it/s]I0315 05:42:42.874884 343568 finetune.py:68] layer 20_o @ epoch 0 new loss 2.530912206566427e-05 old loss 2.6293804694432765e-05 BETTER
100%|██████████| 32/32 [00:12<00:00,  2.59it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0315 05:42:50.250486 344138 finetune.py:45] layer 21_o initial loss 3.574935544747859e-05
W0315 05:42:50.251104 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:42:56.707784 344704 finetune.py:68] layer 22_k @ epoch 3 new loss 1.8002649085246958e-05 old loss 1.809719469747506e-05 BETTER
I0315 05:43:11.692044 345273 finetune.py:68] layer 23_k @ epoch 2 new loss 1.9741155483643524e-05 old loss 1.9858323867083527e-05 BETTER
I0315 05:43:20.438741 343568 finetune.py:68] layer 20_o @ epoch 1 new loss 2.5032255507539958e-05 old loss 2.530912206566427e-05 BETTER
I0315 05:43:25.610808 344138 finetune.py:68] layer 21_o @ epoch 0 new loss 3.435183680267073e-05 old loss 3.574935544747859e-05 BETTER
I0315 05:43:33.635668 344704 finetune.py:68] layer 22_k @ epoch 4 new loss 1.793611045286525e-05 old loss 1.8002649085246958e-05 BETTER
W0315 05:43:35.256069 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_k proxy err 0.0016893151914700866 tr(WHW.T) 4304.8935546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.53it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s]I0315 05:43:48.253590 345273 finetune.py:68] layer 23_k @ epoch 3 new loss 1.9640008758869953e-05 old loss 1.9741155483643524e-05 BETTER
 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0315 05:43:56.670815 344704 finetune.py:45] layer 22_o initial loss 3.348701284267008e-05
W0315 05:43:56.671185 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:43:57.859581 343568 finetune.py:68] layer 20_o @ epoch 2 new loss 2.4832208509906195e-05 old loss 2.5032255507539958e-05 BETTER
I0315 05:44:02.080382 344138 finetune.py:68] layer 21_o @ epoch 1 new loss 3.388149707461707e-05 old loss 3.435183680267073e-05 BETTER
I0315 05:44:24.122081 345273 finetune.py:68] layer 23_k @ epoch 4 new loss 1.9550199795048684e-05 old loss 1.9640008758869953e-05 BETTER
W0315 05:44:25.715491 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

23_k proxy err 0.0018021769355982542 tr(WHW.T) 4207.1435546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:01<00:16,  1.79it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.27it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.58it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s]I0315 05:44:31.929121 344704 finetune.py:68] layer 22_o @ epoch 0 new loss 3.230879156035371e-05 old loss 3.348701284267008e-05 BETTER
 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.61it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.61it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s]I0315 05:44:35.647916 343568 finetune.py:68] layer 20_o @ epoch 3 new loss 2.4673940060893074e-05 old loss 2.4832208509906195e-05 BETTER
 69%|██████▉   | 22/32 [00:08<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.59it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s]I0315 05:44:38.417805 344138 finetune.py:68] layer 21_o @ epoch 2 new loss 3.3554009860381484e-05 old loss 3.388149707461707e-05 BETTER
 94%|█████████▍| 30/32 [00:11<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0315 05:44:46.546885 345273 finetune.py:45] layer 23_o initial loss 3.6266312235966325e-05
W0315 05:44:46.547236 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:45:07.888167 344704 finetune.py:68] layer 22_o @ epoch 1 new loss 3.19838582072407e-05 old loss 3.230879156035371e-05 BETTER
I0315 05:45:13.161409 343568 finetune.py:68] layer 20_o @ epoch 4 new loss 2.4537450372008607e-05 old loss 2.4673940060893074e-05 BETTER
I0315 05:45:14.259379 344138 finetune.py:68] layer 21_o @ epoch 3 new loss 3.330765321152285e-05 old loss 3.3554009860381484e-05 BETTER
W0315 05:45:14.920550 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_o proxy err 0.01092433836311102 tr(WHW.T) 4.085928440093994
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.02s/it]  6%|▋         | 2/32 [00:03<00:51,  1.73s/it]I0315 05:45:20.870520 345273 finetune.py:68] layer 23_o @ epoch 0 new loss 3.467024362180382e-05 old loss 3.6266312235966325e-05 BETTER
  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it]I0315 05:45:43.733719 344704 finetune.py:68] layer 22_o @ epoch 2 new loss 3.1764771847520024e-05 old loss 3.19838582072407e-05 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it]I0315 05:45:50.201505 344138 finetune.py:68] layer 21_o @ epoch 4 new loss 3.308739178464748e-05 old loss 3.330765321152285e-05 BETTER
 69%|██████▉   | 22/32 [00:34<00:15,  1.52s/it]W0315 05:45:51.829914 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it]21_o proxy err 0.007899152114987373 tr(WHW.T) 7.604722499847412
  0%|          | 0/32 [00:00<?, ?it/s] 75%|███████▌  | 24/32 [00:37<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it]  3%|▎         | 1/32 [00:02<01:04,  2.07s/it]I0315 05:45:55.921634 345273 finetune.py:68] layer 23_o @ epoch 1 new loss 3.434651443967596e-05 old loss 3.467024362180382e-05 BETTER
 81%|████████▏ | 26/32 [00:40<00:09,  1.51s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.50s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.51s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it]100%|██████████| 32/32 [00:49<00:00,  1.51s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]
 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it]I0315 05:46:13.728231 343568 finetune.py:45] layer 20_up initial loss 7.945256948005408e-05
W0315 05:46:13.728631 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.53s/it] 50%|█████     | 16/32 [00:25<00:24,  1.53s/it]I0315 05:46:19.549219 344704 finetune.py:68] layer 22_o @ epoch 3 new loss 3.160353298881091e-05 old loss 3.1764771847520024e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it]I0315 05:46:31.106912 345273 finetune.py:68] layer 23_o @ epoch 2 new loss 3.412257501622662e-05 old loss 3.434651443967596e-05 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0315 05:46:49.010955 343568 finetune.py:68] layer 20_up @ epoch 0 new loss 7.797982834745198e-05 old loss 7.945256948005408e-05 BETTER
I0315 05:46:51.093780 344138 finetune.py:45] layer 21_up initial loss 9.59167955443263e-05
W0315 05:46:51.094221 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:46:55.602020 344704 finetune.py:68] layer 22_o @ epoch 4 new loss 3.14715871354565e-05 old loss 3.160353298881091e-05 BETTER
W0315 05:46:57.302112 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_o proxy err 0.011766994372010231 tr(WHW.T) 5.015883445739746
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:02,  2.03s/it]  6%|▋         | 2/32 [00:03<00:52,  1.73s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it]I0315 05:47:06.298182 345273 finetune.py:68] layer 23_o @ epoch 3 new loss 3.395047315279953e-05 old loss 3.412257501622662e-05 BETTER
 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:25<00:24,  1.53s/it]I0315 05:47:25.021427 343568 finetune.py:68] layer 20_up @ epoch 1 new loss 7.697912224102765e-05 old loss 7.797982834745198e-05 BETTER
 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it]I0315 05:47:25.671900 344138 finetune.py:68] layer 21_up @ epoch 0 new loss 9.414258238393813e-05 old loss 9.59167955443263e-05 BETTER
 56%|█████▋    | 18/32 [00:28<00:21,  1.54s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.54s/it]I0315 05:47:41.900461 345273 finetune.py:68] layer 23_o @ epoch 4 new loss 3.382572685950436e-05 old loss 3.395047315279953e-05 BETTER
 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it]W0315 05:47:43.458365 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it]23_o proxy err 0.010021132417023182 tr(WHW.T) 6.170232772827148
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]  3%|▎         | 1/32 [00:02<01:03,  2.03s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
  6%|▋         | 2/32 [00:03<00:52,  1.73s/it]  9%|▉         | 3/32 [00:05<00:47,  1.65s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.56s/it]I0315 05:47:56.469381 344704 finetune.py:45] layer 22_up initial loss 9.774358477443457e-05
W0315 05:47:56.469729 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it]I0315 05:48:00.532657 344138 finetune.py:68] layer 21_up @ epoch 1 new loss 9.292171307606623e-05 old loss 9.414258238393813e-05 BETTER
 31%|███▏      | 10/32 [00:15<00:34,  1.55s/it]I0315 05:48:00.991845 343568 finetune.py:68] layer 20_up @ epoch 2 new loss 7.614514470333233e-05 old loss 7.697912224102765e-05 BETTER
 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.54s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.54s/it] 50%|█████     | 16/32 [00:25<00:24,  1.54s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.54s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.54s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.53s/it]I0315 05:48:30.547757 344704 finetune.py:68] layer 22_up @ epoch 0 new loss 9.607725223759189e-05 old loss 9.774358477443457e-05 BETTER
 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0315 05:48:35.462877 344138 finetune.py:68] layer 21_up @ epoch 2 new loss 9.191723802359775e-05 old loss 9.292171307606623e-05 BETTER
I0315 05:48:36.860556 343568 finetune.py:68] layer 20_up @ epoch 3 new loss 7.541439117630944e-05 old loss 7.614514470333233e-05 BETTER
I0315 05:48:42.615763 345273 finetune.py:45] layer 23_up initial loss 0.00010637670493451878
W0315 05:48:42.616193 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:49:05.499358 344704 finetune.py:68] layer 22_up @ epoch 1 new loss 9.493943070992827e-05 old loss 9.607725223759189e-05 BETTER
I0315 05:49:10.602683 344138 finetune.py:68] layer 21_up @ epoch 3 new loss 9.104385389946401e-05 old loss 9.191723802359775e-05 BETTER
I0315 05:49:12.788851 343568 finetune.py:68] layer 20_up @ epoch 4 new loss 7.477152394130826e-05 old loss 7.541439117630944e-05 BETTER
W0315 05:49:14.331102 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

20_up proxy err 0.01181043405085802 tr(WHW.T) 1127.914794921875
  0%|          | 0/32 [00:00<?, ?it/s]I0315 05:49:16.094818 345273 finetune.py:68] layer 23_up @ epoch 0 new loss 0.00010470255801919848 old loss 0.00010637670493451878 BETTER
  3%|▎         | 1/32 [00:01<01:00,  1.96s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 41%|████      | 13/32 [00:20<00:28,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it]I0315 05:49:40.416068 344704 finetune.py:68] layer 22_up @ epoch 2 new loss 9.400822455063462e-05 old loss 9.493943070992827e-05 BETTER
 50%|█████     | 16/32 [00:24<00:24,  1.51s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.51s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it]I0315 05:49:45.441524 344138 finetune.py:68] layer 21_up @ epoch 4 new loss 9.028454223880544e-05 old loss 9.104385389946401e-05 BETTER
 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it]W0315 05:49:46.912095 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 66%|██████▌   | 21/32 [00:32<00:16,  1.51s/it]21_up proxy err 0.011401820927858353 tr(WHW.T) 1237.661376953125
  0%|          | 0/32 [00:00<?, ?it/s] 69%|██████▉   | 22/32 [00:33<00:15,  1.51s/it]I0315 05:49:50.099235 345273 finetune.py:68] layer 23_up @ epoch 1 new loss 0.00010362629109295085 old loss 0.00010470255801919848 BETTER
  3%|▎         | 1/32 [00:01<01:00,  1.96s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it]  9%|▉         | 3/32 [00:04<00:46,  1.62s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.54s/it]I0315 05:50:13.317614 343568 finetune.py:45] layer 20_gate initial loss 0.00011197674757568166
W0315 05:50:13.318064 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it]I0315 05:50:15.306485 344704 finetune.py:68] layer 22_up @ epoch 3 new loss 9.319520177086815e-05 old loss 9.400822455063462e-05 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.54s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it]I0315 05:50:23.975210 345273 finetune.py:68] layer 23_up @ epoch 2 new loss 0.00010272976942360401 old loss 0.00010362629109295085 BETTER
 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.54s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.54s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0315 05:50:46.065650 344138 finetune.py:45] layer 21_gate initial loss 0.0001331183302681893
W0315 05:50:46.066253 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:50:46.843483 343568 finetune.py:68] layer 20_gate @ epoch 0 new loss 0.00011080379044869915 old loss 0.00011197674757568166 BETTER
I0315 05:50:49.670321 344704 finetune.py:68] layer 22_up @ epoch 4 new loss 9.249329741578549e-05 old loss 9.319520177086815e-05 BETTER
W0315 05:50:51.051302 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

22_up proxy err 0.01192424539476633 tr(WHW.T) 1250.030517578125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]  6%|▋         | 2/32 [00:03<00:50,  1.69s/it]  9%|▉         | 3/32 [00:04<00:46,  1.62s/it]I0315 05:50:57.839066 345273 finetune.py:68] layer 23_up @ epoch 3 new loss 0.00010197466326644644 old loss 0.00010272976942360401 BETTER
 12%|█▎        | 4/32 [00:06<00:44,  1.57s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it]I0315 05:51:18.710389 344138 finetune.py:68] layer 21_gate @ epoch 0 new loss 0.00013173224579077214 old loss 0.0001331183302681893 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it]I0315 05:51:21.031253 343568 finetune.py:68] layer 20_gate @ epoch 1 new loss 0.00011011821334250271 old loss 0.00011080379044869915 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it]I0315 05:51:31.745176 345273 finetune.py:68] layer 23_up @ epoch 4 new loss 0.00010131138697033748 old loss 0.00010197466326644644 BETTER
 81%|████████▏ | 26/32 [00:39<00:09,  1.52s/it]W0315 05:51:33.073882 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it]23_up proxy err 0.012198313139379025 tr(WHW.T) 1298.55126953125
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:42<00:06,  1.51s/it]  3%|▎         | 1/32 [00:01<01:00,  1.97s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.52s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.61s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it]I0315 05:51:49.387940 344704 finetune.py:45] layer 22_gate initial loss 0.00013905811647418886
W0315 05:51:49.388332 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:15<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it]I0315 05:51:52.100399 344138 finetune.py:68] layer 21_gate @ epoch 1 new loss 0.0001309177023358643 old loss 0.00013173224579077214 BETTER
 38%|███▊      | 12/32 [00:19<00:31,  1.55s/it] 41%|████      | 13/32 [00:20<00:29,  1.55s/it]I0315 05:51:55.234047 343568 finetune.py:68] layer 20_gate @ epoch 2 new loss 0.00010953423043247312 old loss 0.00011011821334250271 BETTER
 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.55s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.54s/it] 66%|██████▌   | 21/32 [00:32<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.54s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.55s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.55s/it]I0315 05:52:21.857107 344704 finetune.py:68] layer 22_gate @ epoch 0 new loss 0.0001377375447191298 old loss 0.00013905811647418886 BETTER
 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.60s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0315 05:52:25.250727 344138 finetune.py:68] layer 21_gate @ epoch 2 new loss 0.00013022823259234428 old loss 0.0001309177023358643 BETTER
I0315 05:52:29.433693 343568 finetune.py:68] layer 20_gate @ epoch 3 new loss 0.00010902134818024933 old loss 0.00010953423043247312 BETTER
I0315 05:52:32.999695 345273 finetune.py:45] layer 23_gate initial loss 0.00015383074060082436
W0315 05:52:33.000197 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:52:55.253500 344704 finetune.py:68] layer 22_gate @ epoch 1 new loss 0.00013698317343369126 old loss 0.0001377375447191298 BETTER
I0315 05:52:58.641491 344138 finetune.py:68] layer 21_gate @ epoch 3 new loss 0.00012961402535438538 old loss 0.00013022823259234428 BETTER
I0315 05:53:03.699766 343568 finetune.py:68] layer 20_gate @ epoch 4 new loss 0.00010856091830646619 old loss 0.00010902134818024933 BETTER
W0315 05:53:05.058871 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 05:53:05.064543 345273 finetune.py:68] layer 23_gate @ epoch 0 new loss 0.00015256974438671023 old loss 0.00015383074060082436 BETTER
20_gate proxy err 0.005362591240555048 tr(WHW.T) 4544.3095703125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:40,  1.11it/s]  2%|▏         | 2/112 [00:01<01:06,  1.66it/s]  3%|▎         | 3/112 [00:01<00:55,  1.98it/s]  4%|▎         | 4/112 [00:02<00:49,  2.17it/s]  4%|▍         | 5/112 [00:02<00:46,  2.28it/s]  5%|▌         | 6/112 [00:02<00:44,  2.37it/s]  6%|▋         | 7/112 [00:03<00:43,  2.43it/s]  7%|▋         | 8/112 [00:03<00:41,  2.48it/s]  8%|▊         | 9/112 [00:04<00:40,  2.52it/s]  9%|▉         | 10/112 [00:04<00:40,  2.54it/s] 10%|▉         | 11/112 [00:04<00:39,  2.53it/s] 11%|█         | 12/112 [00:05<00:39,  2.55it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.56it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.52it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.52it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.53it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.53it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.54it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.55it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.56it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.57it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.58it/s] 21%|██        | 23/112 [00:09<00:34,  2.57it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.57it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.58it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.58it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.56it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.56it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.57it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.57it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.58it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.57it/s] 30%|███       | 34/112 [00:13<00:30,  2.59it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.59it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.60it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.61it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.60it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.60it/s] 36%|███▌      | 40/112 [00:16<00:27,  2.58it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.60it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.61it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.61it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.62it/s] 40%|████      | 45/112 [00:17<00:25,  2.63it/s] 41%|████      | 46/112 [00:18<00:25,  2.63it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.64it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.64it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.64it/s]I0315 05:53:28.258340 344704 finetune.py:68] layer 22_gate @ epoch 2 new loss 0.0001363524643238634 old loss 0.00013698317343369126 BETTER
 45%|████▍     | 50/112 [00:19<00:23,  2.63it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.62it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.62it/s] 47%|████▋     | 53/112 [00:21<00:22,  2.59it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.60it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.61it/s] 50%|█████     | 56/112 [00:22<00:21,  2.61it/s] 51%|█████     | 57/112 [00:22<00:21,  2.62it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.61it/s]I0315 05:53:31.752907 344138 finetune.py:68] layer 21_gate @ epoch 4 new loss 0.00012908903590869159 old loss 0.00012961402535438538 BETTER
 53%|█████▎    | 59/112 [00:23<00:20,  2.63it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.64it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.63it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.63it/s]W0315 05:53:33.093700 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.61it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.60it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.57it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.59it/s] 61%|██████    | 68/112 [00:26<00:16,  2.60it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.60it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.61it/s]21_gate proxy err 0.005196668207645416 tr(WHW.T) 5019.17529296875
  0%|          | 0/112 [00:00<?, ?it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.61it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.62it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.62it/s]  2%|▏         | 2/112 [00:01<01:05,  1.67it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.62it/s]I0315 05:53:37.742854 345273 finetune.py:68] layer 23_gate @ epoch 1 new loss 0.00015187829558271915 old loss 0.00015256974438671023 BETTER
  3%|▎         | 3/112 [00:01<00:54,  1.99it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.62it/s]  4%|▎         | 4/112 [00:02<00:49,  2.18it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.62it/s]  4%|▍         | 5/112 [00:02<00:46,  2.31it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.60it/s]  5%|▌         | 6/112 [00:02<00:44,  2.39it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.60it/s]  6%|▋         | 7/112 [00:03<00:42,  2.45it/s] 71%|███████   | 79/112 [00:31<00:12,  2.56it/s]  7%|▋         | 8/112 [00:03<00:41,  2.49it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.58it/s]  8%|▊         | 9/112 [00:04<00:41,  2.50it/s] 72%|███████▏  | 81/112 [00:31<00:12,  2.58it/s]  9%|▉         | 10/112 [00:04<00:40,  2.52it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.58it/s] 10%|▉         | 11/112 [00:04<00:40,  2.51it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.60it/s] 11%|█         | 12/112 [00:05<00:39,  2.54it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.61it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.55it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.61it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.56it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.62it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.62it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.62it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.63it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s] 80%|████████  | 90/112 [00:35<00:08,  2.61it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.58it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.60it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.59it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.57it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.59it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.58it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.60it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.58it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.61it/s] 21%|██        | 23/112 [00:09<00:34,  2.58it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.61it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.55it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.62it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.56it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.62it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.56it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.63it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.57it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.63it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.58it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.62it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.58it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.62it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.57it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.61it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.57it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.59it/s] 29%|██▊       | 32/112 [00:12<00:31,  2.57it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.56it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.57it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.57it/s] 30%|███       | 34/112 [00:13<00:30,  2.56it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.58it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.55it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.59it/s] 32%|███▏      | 36/112 [00:14<00:30,  2.53it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.60it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.54it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.61it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.55it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.61it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.55it/s]100%|██████████| 112/112 [00:43<00:00,  2.61it/s]100%|██████████| 112/112 [00:43<00:00,  2.56it/s]
 36%|███▌      | 40/112 [00:16<00:28,  2.56it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.56it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.56it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.56it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.56it/s] 40%|████      | 45/112 [00:18<00:26,  2.56it/s] 41%|████      | 46/112 [00:18<00:25,  2.56it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.56it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.53it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.54it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.54it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.55it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.56it/s] 47%|████▋     | 53/112 [00:21<00:22,  2.57it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.58it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.59it/s] 50%|█████     | 56/112 [00:22<00:21,  2.59it/s] 51%|█████     | 57/112 [00:22<00:21,  2.58it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.57it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.57it/s]W0315 05:53:59.841000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.842000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.842000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.842000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.842000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.842000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.842000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.885000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.886000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.886000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.886000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.886000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.902000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.902000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.902000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.902000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:53:59.902000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.082000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.082000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.083000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.083000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.083000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 54%|█████▎    | 60/112 [00:23<00:20,  2.54it/s]W0315 05:54:00.425000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.425000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.425000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.425000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.425000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.425000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.425000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.458000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.458000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.458000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.458000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.458000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.534000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.534000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.534000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.534000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:00.534000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 54%|█████▍    | 61/112 [00:24<00:19,  2.55it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.56it/s]I0315 05:54:01.371093 344704 finetune.py:68] layer 22_gate @ epoch 3 new loss 0.00013578942161984742 old loss 0.0001363524643238634 BETTER
 56%|█████▋    | 63/112 [00:25<00:19,  2.56it/s]W0315 05:54:01.752000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:01.766000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:01.775000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:01.775000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 57%|█████▋    | 64/112 [00:25<00:18,  2.58it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.58it/s]W0315 05:54:02.245000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.245000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.245000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.245000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.245000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.245000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.245000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.276000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.277000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.277000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.277000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.277000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 59%|█████▉    | 66/112 [00:26<00:17,  2.58it/s]W0315 05:54:02.635000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.635000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.635000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.635000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.635000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.636000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.636000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.636000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
 60%|█████▉    | 67/112 [00:26<00:17,  2.58it/s]W0315 05:54:02.946000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.947000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.947000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.947000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:02.947000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:03.296000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:03.302000 139776240191296 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 61%|██████    | 68/112 [00:27<00:17,  2.58it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.56it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.56it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.54it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.55it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.55it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.56it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.57it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.57it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.57it/s] 71%|███████   | 79/112 [00:31<00:12,  2.57it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.57it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.57it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.57it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.57it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.54it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.54it/s]I0315 05:54:10.118377 345273 finetune.py:68] layer 23_gate @ epoch 2 new loss 0.0001513122406322509 old loss 0.00015187829558271915 BETTER
 77%|███████▋  | 86/112 [00:34<00:10,  2.55it/s]I0315 05:54:10.596401 343568 finetune.py:45] layer 20_down initial loss 0.00018016139802057296
W0315 05:54:10.597048 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 78%|███████▊  | 87/112 [00:34<00:09,  2.56it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.56it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.57it/s] 80%|████████  | 90/112 [00:35<00:08,  2.57it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.57it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.56it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.55it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.55it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.53it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.51it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.53it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.54it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.53it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.54it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.55it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.56it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.57it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.56it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.56it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.55it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.53it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.54it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.55it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.55it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.56it/s]100%|██████████| 112/112 [00:44<00:00,  2.56it/s]100%|██████████| 112/112 [00:44<00:00,  2.53it/s]
W0315 05:54:28.064000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.064000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.064000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.064000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.065000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.065000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.065000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.107000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.107000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.107000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.107000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.107000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.123000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.124000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.124000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.124000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.124000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.302000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.302000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.302000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.302000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.302000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.642000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.643000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.643000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.643000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.643000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.643000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.643000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.679000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.679000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.679000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.679000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.679000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.755000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.756000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.756000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.756000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:28.756000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:29.993000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.006000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.015000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.015000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.488000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.488000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.488000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.488000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.488000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.489000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.489000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.522000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.522000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.522000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.522000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.522000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.886000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.886000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.887000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.887000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.887000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.887000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.887000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:54:30.887000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:31.203000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:54:31.204000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:31.204000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:54:31.204000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:54:31.204000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:54:31.566000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:54:31.572000 140079597524800 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 05:54:34.270117 344704 finetune.py:68] layer 22_gate @ epoch 4 new loss 0.00013530055002775043 old loss 0.00013578942161984742 BETTER
W0315 05:54:35.491943 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 05:54:38.798732 344138 finetune.py:45] layer 21_down initial loss 0.00021119057782925665
W0315 05:54:38.799197 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

22_gate proxy err 0.005600976757705212 tr(WHW.T) 4887.345703125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:38,  1.13it/s]  2%|▏         | 2/112 [00:01<01:05,  1.69it/s]  3%|▎         | 3/112 [00:01<00:54,  2.01it/s]  4%|▎         | 4/112 [00:02<00:49,  2.20it/s]  4%|▍         | 5/112 [00:02<00:45,  2.33it/s]  5%|▌         | 6/112 [00:02<00:43,  2.42it/s]I0315 05:54:42.019354 343568 finetune.py:68] layer 20_down @ epoch 0 new loss 0.0001800981117412448 old loss 0.00018016139802057296 BETTER
  6%|▋         | 7/112 [00:03<00:42,  2.48it/s]I0315 05:54:42.363756 345273 finetune.py:68] layer 23_gate @ epoch 3 new loss 0.00015082163736224174 old loss 0.0001513122406322509 BETTER
  7%|▋         | 8/112 [00:03<00:41,  2.51it/s]  8%|▊         | 9/112 [00:03<00:40,  2.53it/s]  9%|▉         | 10/112 [00:04<00:40,  2.55it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 11%|█         | 12/112 [00:05<00:39,  2.53it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.54it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.55it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.59it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.59it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.59it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.59it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.58it/s] 21%|██        | 23/112 [00:09<00:34,  2.58it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.54it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.55it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.56it/s] 24%|██▍       | 27/112 [00:10<00:33,  2.56it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.57it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.58it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.58it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.59it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s] 30%|███       | 34/112 [00:13<00:30,  2.58it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.58it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.55it/s] 33%|███▎      | 37/112 [00:14<00:29,  2.57it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.58it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.58it/s] 36%|███▌      | 40/112 [00:16<00:27,  2.58it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.59it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.59it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.59it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.59it/s] 40%|████      | 45/112 [00:17<00:25,  2.58it/s] 41%|████      | 46/112 [00:18<00:25,  2.58it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.58it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.56it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.57it/s] 45%|████▍     | 50/112 [00:19<00:24,  2.57it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.58it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.58it/s] 47%|████▋     | 53/112 [00:21<00:22,  2.59it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.59it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.59it/s] 50%|█████     | 56/112 [00:22<00:21,  2.59it/s] 51%|█████     | 57/112 [00:22<00:21,  2.58it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.58it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.58it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.55it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.56it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.57it/s] 56%|█████▋    | 63/112 [00:24<00:19,  2.58it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.58it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.58it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.59it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.59it/s] 61%|██████    | 68/112 [00:26<00:16,  2.60it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.58it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.58it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.58it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.53it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.55it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.56it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.56it/s] 68%|██████▊   | 76/112 [00:29<00:14,  2.57it/s]I0315 05:55:09.224153 344138 finetune.py:68] layer 21_down @ epoch 0 new loss 0.0002111169887939468 old loss 0.00021119057782925665 BETTER
 69%|██████▉   | 77/112 [00:30<00:13,  2.57it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.58it/s] 71%|███████   | 79/112 [00:31<00:12,  2.58it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.59it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.58it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.57it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.55it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.56it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.56it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.57it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.58it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.58it/s] 80%|████████  | 90/112 [00:35<00:08,  2.59it/s]I0315 05:55:14.450038 343568 finetune.py:68] layer 20_down @ epoch 1 new loss 0.0001800732861738652 old loss 0.0001800981117412448 BETTER
I0315 05:55:14.670235 345273 finetune.py:68] layer 23_gate @ epoch 4 new loss 0.0001503595121903345 old loss 0.00015082163736224174 BETTER
 81%|████████▏ | 91/112 [00:35<00:08,  2.59it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.59it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.59it/s]W0315 05:55:15.746133 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 94/112 [00:36<00:06,  2.59it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.58it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.56it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.56it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.57it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.58it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.58it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.59it/s]23_gate proxy err 0.006068551912903786 tr(WHW.T) 4769.7919921875
  0%|          | 0/112 [00:00<?, ?it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.59it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.59it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.59it/s]  2%|▏         | 2/112 [00:01<01:04,  1.71it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.58it/s]  3%|▎         | 3/112 [00:01<00:54,  2.01it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.57it/s]  4%|▎         | 4/112 [00:02<00:48,  2.20it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.58it/s]  4%|▍         | 5/112 [00:02<00:45,  2.33it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.55it/s]  5%|▌         | 6/112 [00:02<00:43,  2.42it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.56it/s]  6%|▋         | 7/112 [00:03<00:42,  2.48it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.57it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.58it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.55it/s]
  9%|▉         | 10/112 [00:04<00:39,  2.57it/s] 10%|▉         | 11/112 [00:04<00:39,  2.57it/s] 11%|█         | 12/112 [00:05<00:38,  2.57it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.57it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.58it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.56it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.58it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.62it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.62it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.63it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.62it/s] 21%|██        | 23/112 [00:09<00:33,  2.62it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.60it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.60it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.60it/s] 24%|██▍       | 27/112 [00:10<00:33,  2.57it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.58it/s]W0315 05:55:30.288000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.289000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.289000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.289000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.289000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.289000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.289000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.334000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.334000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.334000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.334000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.334000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.365000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.365000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.365000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.365000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.365000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 26%|██▌       | 29/112 [00:11<00:32,  2.59it/s]W0315 05:55:30.545000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.545000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.545000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.545000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.545000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 27%|██▋       | 30/112 [00:12<00:31,  2.59it/s]W0315 05:55:30.883000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.883000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.883000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.883000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.884000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.884000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.884000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.916000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.917000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.917000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.917000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.917000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.990000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.990000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.990000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.990000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:30.990000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 31/112 [00:12<00:31,  2.59it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.59it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s]W0315 05:55:32.216000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.230000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.238000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.239000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
 30%|███       | 34/112 [00:13<00:30,  2.59it/s]W0315 05:55:32.705000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.705000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.705000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.705000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.705000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.705000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.705000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.737000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.737000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.737000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.738000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:32.738000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 31%|███▏      | 35/112 [00:13<00:29,  2.59it/s]W0315 05:55:33.098000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.098000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.098000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.098000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.098000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.098000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.099000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.099000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 32%|███▏      | 36/112 [00:14<00:29,  2.59it/s]W0315 05:55:33.406000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.407000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.407000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.407000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.407000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
 33%|███▎      | 37/112 [00:14<00:28,  2.59it/s]W0315 05:55:33.754000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:55:33.760000 140451406747456 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 38/112 [00:15<00:28,  2.58it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.55it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.58it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.58it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.60it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.60it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.60it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 41%|████      | 46/112 [00:18<00:25,  2.60it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.60it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.59it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.58it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.58it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.55it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.56it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.57it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.58it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.58it/s]I0315 05:55:40.712029 344138 finetune.py:68] layer 21_down @ epoch 1 new loss 0.00021110428497195244 old loss 0.0002111169887939468 BETTER
 50%|█████     | 56/112 [00:22<00:21,  2.59it/s]I0315 05:55:41.008141 344704 finetune.py:45] layer 22_down initial loss 0.00022283889120444655
W0315 05:55:41.008536 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 51%|█████     | 57/112 [00:22<00:21,  2.59it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.59it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.59it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.59it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.58it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.58it/s] 56%|█████▋    | 63/112 [00:24<00:19,  2.56it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.56it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.57it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.57it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.58it/s] 61%|██████    | 68/112 [00:26<00:16,  2.59it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.59it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.59it/s]I0315 05:55:46.982784 343568 finetune.py:68] layer 20_down @ epoch 2 new loss 0.00018006860045716166 old loss 0.0001800732861738652 BETTER
 64%|██████▍   | 72/112 [00:28<00:15,  2.59it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.59it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.57it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.55it/s] 68%|██████▊   | 76/112 [00:29<00:14,  2.57it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.58it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.58it/s] 71%|███████   | 79/112 [00:31<00:12,  2.59it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.59it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.60it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.60it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.60it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.60it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.60it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.60it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.57it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.58it/s] 80%|████████  | 90/112 [00:35<00:08,  2.59it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.59it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.59it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.60it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.59it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.59it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.59it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.60it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.59it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.59it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.57it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.58it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.59it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.59it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.61it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.61it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.60it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.59it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.60it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.60it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.59it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.59it/s]100%|██████████| 112/112 [00:43<00:00,  2.56it/s]
W0315 05:56:10.020000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.020000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.021000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.021000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.021000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.021000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.021000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.065000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.065000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.066000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.066000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.066000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.082000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.082000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.082000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.082000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.082000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.259000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.260000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.260000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.260000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.260000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.597000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.598000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.598000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.598000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.598000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.598000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.599000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.631000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.631000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.631000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.631000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.631000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.705000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.705000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.705000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.706000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:10.706000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
I0315 05:56:11.854507 344704 finetune.py:68] layer 22_down @ epoch 0 new loss 0.0002227586810477078 old loss 0.00022283889120444655 BETTER
W0315 05:56:11.941000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:11.947000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:11.954000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:56:11.954000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
I0315 05:56:12.167489 344138 finetune.py:68] layer 21_down @ epoch 2 new loss 0.00021108321379870176 old loss 0.00021110428497195244 BETTER
W0315 05:56:12.430000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.430000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.430000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.430000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.430000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.431000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.431000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.464000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.464000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.464000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.464000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.464000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.825000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.826000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.826000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.826000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.826000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.826000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.826000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:12.826000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:13.135000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:56:13.136000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:56:13.136000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:56:13.136000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:56:13.136000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:56:13.491000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 05:56:13.496000 140141905483584 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 05:56:19.662283 343568 finetune.py:68] layer 20_down @ epoch 3 new loss 0.00018005180754698813 old loss 0.00018006860045716166 BETTER
I0315 05:56:20.529753 345273 finetune.py:45] layer 23_down initial loss 0.000244265072979033
W0315 05:56:20.530210 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:56:43.211341 344704 finetune.py:68] layer 22_down @ epoch 1 new loss 0.00022274671937339008 old loss 0.0002227586810477078 BETTER
I0315 05:56:43.669859 344138 finetune.py:68] layer 21_down @ epoch 3 new loss 0.00021108187502250075 old loss 0.00021108321379870176 BETTER
I0315 05:56:50.543900 345273 finetune.py:68] layer 23_down @ epoch 0 new loss 0.0002441722899675369 old loss 0.000244265072979033 BETTER
I0315 05:56:52.369686 343568 finetune.py:68] layer 20_down @ epoch 4 new loss 0.00018004360026679933 old loss 0.00018005180754698813 BETTER
W0315 05:56:53.147566 343568 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

20_down proxy err 0.012810325250029564 tr(WHW.T) 24.00002670288086
I0315 05:57:15.185029 344704 finetune.py:68] layer 22_down @ epoch 2 new loss 0.00022273340437095612 old loss 0.00022274671937339008 BETTER
I0315 05:57:15.824501 344138 finetune.py:68] layer 21_down @ epoch 4 new loss 0.00021106073108967394 old loss 0.00021108187502250075 BETTER
W0315 05:57:16.606614 344138 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

21_down proxy err 0.012089759111404419 tr(WHW.T) 29.211200714111328
I0315 05:57:21.832384 345273 finetune.py:68] layer 23_down @ epoch 1 new loss 0.0002441570977680385 old loss 0.0002441722899675369 BETTER
I0315 05:57:47.031208 344704 finetune.py:68] layer 22_down @ epoch 3 new loss 0.0002227298973593861 old loss 0.00022273340437095612 BETTER
I0315 05:57:53.069024 345273 finetune.py:68] layer 23_down @ epoch 2 new loss 0.0002441419055685401 old loss 0.0002441570977680385 BETTER
I0315 05:58:18.687319 344704 finetune.py:68] layer 22_down @ epoch 4 new loss 0.0002227099030278623 old loss 0.0002227298973593861 BETTER
W0315 05:58:19.433888 344704 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

22_down proxy err 0.01218498032540083 tr(WHW.T) 30.848859786987305
I0315 05:58:24.124827 345273 finetune.py:68] layer 23_down @ epoch 3 new loss 0.0002441349788568914 old loss 0.0002441419055685401 BETTER
I0315 05:58:26.960736 273992 quantize_finetune_llama.py:186] computed original embedding for layer 24 in 65.93416833877563s
I0315 05:58:27.345057 273992 quantize_finetune_llama.py:159] layer 25 gpu 1
I0315 05:58:29.372296 369033 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 05:58:29.372407 369033 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 05:58:29.372463 369033 utils.py:162] NumExpr defaulting to 16 threads.
I0315 05:58:29.574022 369033 config.py:58] PyTorch version 2.4.0 available.
I0315 05:58:31.789639 369033 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 05:58:32.206315 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:43,  1.40s/it]  6%|▋         | 2/32 [00:01<00:23,  1.30it/s]  9%|▉         | 3/32 [00:02<00:16,  1.75it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.32it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.49it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:05<00:06,  2.88it/s] 41%|████      | 13/32 [00:05<00:06,  2.89it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:06<00:05,  3.00it/s] 50%|█████     | 16/32 [00:06<00:05,  2.98it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.98it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.98it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.97it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.96it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.94it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.91it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.92it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.89it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.91it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.92it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.92it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
W0315 05:58:48.209000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.209000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.209000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.209000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.209000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.209000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.209000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.251000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.251000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.251000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.251000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.251000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.268000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.268000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.268000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.269000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.269000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.602000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.602000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.602000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.602000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:58:48.602000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.522000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.522000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.522000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.522000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.522000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.523000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.523000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.541000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.541000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.541000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.541000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.541000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.790000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.790000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.790000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.790000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:58:49.790000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.017000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.017000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.017000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.017000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.017000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.017000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.017000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.040000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.040000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.040000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.040000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.040000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.992000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.992000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.992000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.992000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:58:51.993000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 05:58:55.384914 345273 finetune.py:68] layer 23_down @ epoch 4 new loss 0.00024411588674411178 old loss 0.0002441349788568914 BETTER
W0315 05:58:56.202907 345273 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

23_down proxy err 0.012277242727577686 tr(WHW.T) 32.827232360839844
I0315 05:58:58.723854 369033 finetune.py:45] layer 24_v initial loss 9.408460755366832e-05
W0315 05:58:58.724354 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 05:59:29.758020 273992 quantize_finetune_llama.py:186] computed original embedding for layer 25 in 61.9925434589386s
I0315 05:59:30.153969 273992 quantize_finetune_llama.py:159] layer 26 gpu 2
I0315 05:59:32.272081 370569 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 05:59:32.272241 370569 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 05:59:32.272309 370569 utils.py:162] NumExpr defaulting to 16 threads.
I0315 05:59:32.499150 370569 config.py:58] PyTorch version 2.4.0 available.
I0315 05:59:34.770789 370569 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 05:59:35.187513 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 05:59:35.190745 369033 finetune.py:68] layer 24_v @ epoch 0 new loss 1.931597216753289e-05 old loss 9.408460755366832e-05 BETTER
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.60s/it]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.91it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.15it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.86it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.88it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.85it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.84it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.84it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.84it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.83it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.83it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
W0315 05:59:51.983000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 05:59:51.983000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:59:51.983000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 05:59:51.983000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:59:51.984000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:59:51.984000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:59:51.984000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.011000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.011000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.011000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.011000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.011000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.028000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.028000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.028000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.028000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.028000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.363000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.363000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.363000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.363000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 05:59:52.363000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.304000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.304000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.304000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.304000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.304000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.305000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.305000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.323000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.323000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.323000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.324000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.324000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.575000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.576000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.576000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.576000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 05:59:53.576000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.775000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.776000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.776000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.776000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.776000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.776000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.776000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.794000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.795000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.795000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.795000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:59:54.795000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 05:59:55.731000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 05:59:55.732000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 05:59:55.732000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 05:59:55.732000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 05:59:55.732000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 06:00:01.850131 370569 finetune.py:45] layer 25_v initial loss 0.00010829701204784214
W0315 06:00:01.850575 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:00:12.675076 369033 finetune.py:68] layer 24_v @ epoch 1 new loss 1.584803976584226e-05 old loss 1.931597216753289e-05 BETTER
I0315 06:00:30.900278 273992 quantize_finetune_llama.py:186] computed original embedding for layer 26 in 60.309240102767944s
I0315 06:00:31.324278 273992 quantize_finetune_llama.py:159] layer 27 gpu 3
I0315 06:00:33.385948 372077 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 06:00:33.386057 372077 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 06:00:33.386119 372077 utils.py:162] NumExpr defaulting to 16 threads.
I0315 06:00:33.594695 372077 config.py:58] PyTorch version 2.4.0 available.
I0315 06:00:35.836432 372077 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0315 06:00:36.077010 370569 finetune.py:68] layer 25_v @ epoch 0 new loss 2.353144736844115e-05 old loss 0.00010829701204784214 BETTER
W0315 06:00:36.208675 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:53,  1.71s/it]  6%|▋         | 2/32 [00:02<00:27,  1.10it/s]  9%|▉         | 3/32 [00:02<00:18,  1.53it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.87it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.14it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.56it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.78it/s] 41%|████      | 13/32 [00:05<00:06,  2.79it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.81it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.82it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.86it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.88it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.85it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.86it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.85it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.86it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0315 06:00:50.309629 369033 finetune.py:68] layer 24_v @ epoch 2 new loss 1.4734839169250336e-05 old loss 1.584803976584226e-05 BETTER
W0315 06:00:52.864000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.864000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.864000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.865000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.865000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.865000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.865000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.892000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.892000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.892000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.892000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.892000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.909000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.909000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.909000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.909000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:00:52.909000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:00:53.236000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:00:53.236000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:00:53.237000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:00:53.237000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:00:53.237000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.156000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.156000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.157000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.157000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.157000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.157000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.157000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.175000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.175000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.175000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.176000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.176000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.423000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.423000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.423000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.423000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:00:54.423000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.626000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.626000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.626000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.627000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.627000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.627000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.627000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.645000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.645000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.645000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.645000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:00:55.645000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:00:56.590000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:00:56.590000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:00:56.590000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:00:56.590000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:00:56.590000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 06:01:02.441872 372077 finetune.py:45] layer 26_v initial loss 0.00011473459016997367
W0315 06:01:02.442105 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:01:11.499861 370569 finetune.py:68] layer 25_v @ epoch 1 new loss 1.9443836208665743e-05 old loss 2.353144736844115e-05 BETTER
I0315 06:01:28.089976 369033 finetune.py:68] layer 24_v @ epoch 3 new loss 1.4200535588315688e-05 old loss 1.4734839169250336e-05 BETTER
I0315 06:01:31.766586 273992 quantize_finetune_llama.py:186] computed original embedding for layer 27 in 59.98015999794006s
I0315 06:01:32.197722 273992 quantize_finetune_llama.py:159] layer 28 gpu 0
I0315 06:01:34.294644 373601 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 06:01:34.294767 373601 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 06:01:34.294852 373601 utils.py:162] NumExpr defaulting to 16 threads.
I0315 06:01:34.511940 373601 config.py:58] PyTorch version 2.4.0 available.
I0315 06:01:36.761542 373601 data_utils.py:336] using 256 training seqs, 128 validation seqs
I0315 06:01:36.985264 372077 finetune.py:68] layer 26_v @ epoch 0 new loss 3.39909311151132e-05 old loss 0.00011473459016997367 BETTER
W0315 06:01:37.220861 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:49,  1.61s/it]  6%|▋         | 2/32 [00:01<00:26,  1.15it/s]  9%|▉         | 3/32 [00:02<00:18,  1.57it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.91it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.16it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.76it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.81it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.79it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.79it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.79it/s]I0315 06:01:47.316788 370569 finetune.py:68] layer 25_v @ epoch 2 new loss 1.8160879335482605e-05 old loss 1.9443836208665743e-05 BETTER
 69%|██████▉   | 22/32 [00:09<00:03,  2.78it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.80it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.80it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.79it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.81it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.80it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.81it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.81it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0315 06:01:54.137000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.137000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.137000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.137000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.137000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.137000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.137000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.164000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.165000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.165000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.165000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.165000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.182000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.182000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.182000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.182000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.182000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.517000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.517000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.518000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.518000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:01:54.518000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.446000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.446000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.446000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.446000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.446000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.446000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.447000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.465000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.465000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.465000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.465000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.465000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.715000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.715000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.715000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.715000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:01:55.716000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.931000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.931000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.931000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.931000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.931000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.931000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.932000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.950000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.950000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.950000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.950000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:01:56.950000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:01:57.901000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:01:57.901000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:01:57.902000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:01:57.902000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:01:57.902000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 06:02:05.334659 373601 finetune.py:45] layer 27_v initial loss 0.00012612974387593567
W0315 06:02:05.334902 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:02:06.624264 369033 finetune.py:68] layer 24_v @ epoch 4 new loss 1.3854973076377064e-05 old loss 1.4200535588315688e-05 BETTER
W0315 06:02:08.662434 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_v proxy err 0.011920065619051456 tr(WHW.T) 136.39785766601562
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.05s/it]  6%|▋         | 2/32 [00:01<00:19,  1.52it/s]  9%|▉         | 3/32 [00:01<00:15,  1.89it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.14it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s]I0315 06:02:12.753610 372077 finetune.py:68] layer 26_v @ epoch 1 new loss 2.966772262880113e-05 old loss 3.39909311151132e-05 BETTER
 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.65it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.69it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.69it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.69it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
I0315 06:02:23.360855 370569 finetune.py:68] layer 25_v @ epoch 3 new loss 1.7563081200933084e-05 old loss 1.8160879335482605e-05 BETTER
W0315 06:02:29.114000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.115000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.115000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.115000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.115000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.115000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.115000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.149000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.149000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.149000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.149000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.149000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.165000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.166000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.166000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.166000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.166000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.332000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.332000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.332000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.332000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.332000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.573000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.573000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.574000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.574000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.574000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.574000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.574000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.597000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.597000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.597000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.597000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.597000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.666000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.666000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.666000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.666000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:02:29.666000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.581000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.900000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.900000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.900000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.900000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.900000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.900000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.901000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.923000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.923000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.924000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.924000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:02:30.924000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:02:31.191000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:02:31.191000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:02:31.191000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:02:31.191000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:02:31.192000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:02:31.464000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 06:02:38.900569 369033 finetune.py:45] layer 24_q initial loss 2.0068007870577276e-05
W0315 06:02:38.900872 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:02:39.532471 373601 finetune.py:68] layer 27_v @ epoch 0 new loss 3.4316646633669734e-05 old loss 0.00012612974387593567 BETTER
I0315 06:02:48.577458 372077 finetune.py:68] layer 26_v @ epoch 2 new loss 2.8111364372307435e-05 old loss 2.966772262880113e-05 BETTER
I0315 06:02:59.805029 370569 finetune.py:68] layer 25_v @ epoch 4 new loss 1.7140675481641665e-05 old loss 1.7563081200933084e-05 BETTER
W0315 06:03:01.788658 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

25_v proxy err 0.01044139452278614 tr(WHW.T) 164.04367065429688
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.00it/s]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:15,  1.90it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.10it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.25it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.42it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.47it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.50it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.54it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.57it/s] 50%|█████     | 16/32 [00:06<00:06,  2.55it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.55it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.55it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.56it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.57it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.57it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.57it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.56it/s]I0315 06:03:14.412846 373601 finetune.py:68] layer 27_v @ epoch 1 new loss 3.123734495602548e-05 old loss 3.4316646633669734e-05 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.55it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.55it/s]I0315 06:03:15.398338 369033 finetune.py:68] layer 24_q @ epoch 0 new loss 1.9373668692423962e-05 old loss 2.0068007870577276e-05 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:13<00:00,  2.57it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
W0315 06:03:22.545000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.545000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.545000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.545000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.546000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.546000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.546000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.576000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.576000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.576000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.577000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.577000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.593000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.593000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.593000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.593000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.593000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.754000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.754000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.754000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.754000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.754000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.989000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.989000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.989000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.989000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.989000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.989000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:03:22.990000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.010000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.010000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.010000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.010000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.010000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.076000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.077000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.077000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.077000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.077000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:03:23.969000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.296000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.296000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.296000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.296000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.297000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.297000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.297000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.320000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.320000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.320000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.320000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.320000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.586000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.586000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.586000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.586000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:03:24.586000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
I0315 06:03:24.761041 372077 finetune.py:68] layer 26_v @ epoch 3 new loss 2.72597317234613e-05 old loss 2.8111364372307435e-05 BETTER
W0315 06:03:24.857000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 06:03:33.081549 370569 finetune.py:45] layer 25_q initial loss 2.9282906325533986e-05
W0315 06:03:33.081972 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:03:49.517428 373601 finetune.py:68] layer 27_v @ epoch 2 new loss 3.0866471206536517e-05 old loss 3.123734495602548e-05 BETTER
I0315 06:03:53.162736 369033 finetune.py:68] layer 24_q @ epoch 1 new loss 1.9018312741536647e-05 old loss 1.9373668692423962e-05 BETTER
I0315 06:04:00.873494 372077 finetune.py:68] layer 26_v @ epoch 4 new loss 2.67179621005198e-05 old loss 2.72597317234613e-05 BETTER
W0315 06:04:02.483739 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_v proxy err 0.013865821994841099 tr(WHW.T) 123.81900024414062
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:14,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s]I0315 06:04:07.972259 370569 finetune.py:68] layer 25_q @ epoch 0 new loss 2.774168024188839e-05 old loss 2.9282906325533986e-05 BETTER
 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.60it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.62it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.61it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
W0315 06:04:22.608000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.608000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.608000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.609000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.609000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.609000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.609000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.639000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.639000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.639000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.639000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.639000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.657000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.657000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.657000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.657000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.657000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.820000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.820000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.820000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.820000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:04:22.820000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.057000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.058000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.058000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.058000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.058000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.058000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.058000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.080000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.080000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.080000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.081000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.081000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.147000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.147000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.148000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.148000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:04:23.148000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.030000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.346000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.346000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.346000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.346000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.346000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.346000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.347000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.369000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.369000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.369000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.370000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.370000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.632000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.632000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.632000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.632000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:04:24.632000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
I0315 06:04:24.887290 373601 finetune.py:68] layer 27_v @ epoch 3 new loss 3.038658178411424e-05 old loss 3.0866471206536517e-05 BETTER
W0315 06:04:24.900000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 06:04:30.961639 369033 finetune.py:68] layer 24_q @ epoch 2 new loss 1.8745738998404704e-05 old loss 1.9018312741536647e-05 BETTER
I0315 06:04:31.735608 372077 finetune.py:45] layer 26_q initial loss 3.5991924960399047e-05
W0315 06:04:31.735944 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:04:43.814563 370569 finetune.py:68] layer 25_q @ epoch 1 new loss 2.6942860131384805e-05 old loss 2.774168024188839e-05 BETTER
I0315 06:05:00.155899 373601 finetune.py:68] layer 27_v @ epoch 4 new loss 2.900392973970156e-05 old loss 3.038658178411424e-05 BETTER
W0315 06:05:02.016745 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

27_v proxy err 0.011005000211298466 tr(WHW.T) 203.18115234375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.07s/it]  6%|▋         | 2/32 [00:01<00:20,  1.49it/s]  9%|▉         | 3/32 [00:01<00:15,  1.84it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.22it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.32it/s]I0315 06:05:06.474616 372077 finetune.py:68] layer 26_q @ epoch 0 new loss 3.4726366720860824e-05 old loss 3.5991924960399047e-05 BETTER
 22%|██▏       | 7/32 [00:03<00:10,  2.39it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.44it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.47it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.49it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.49it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.51it/s]I0315 06:05:08.829925 369033 finetune.py:68] layer 24_q @ epoch 3 new loss 1.854482434282545e-05 old loss 1.8745738998404704e-05 BETTER
 41%|████      | 13/32 [00:05<00:07,  2.52it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.51it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.51it/s] 50%|█████     | 16/32 [00:06<00:06,  2.51it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.53it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.54it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.54it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.56it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.56it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:10<00:03,  2.55it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.52it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.53it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.53it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.55it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:13<00:00,  2.55it/s]100%|██████████| 32/32 [00:13<00:00,  2.41it/s]
I0315 06:05:20.066240 370569 finetune.py:68] layer 25_q @ epoch 2 new loss 2.6382547730463557e-05 old loss 2.6942860131384805e-05 BETTER
W0315 06:05:23.225000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.225000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.226000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.226000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.226000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.226000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.226000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.258000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.258000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.258000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.258000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.259000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.279000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.280000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.280000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.280000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.280000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.445000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.445000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.445000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.445000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.445000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.683000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.684000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.684000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.684000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.684000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.684000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.685000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.706000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.706000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.706000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.707000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.707000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.776000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.776000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.776000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.776000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:05:23.776000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:05:24.699000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.025000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.025000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.025000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.025000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.026000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.026000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.026000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.048000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.048000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.049000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.049000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.049000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.322000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.322000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.322000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.322000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.322000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:05:25.595000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 06:05:32.189125 373601 finetune.py:45] layer 27_q initial loss 4.31571934313979e-05
W0315 06:05:32.189433 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:05:42.770886 372077 finetune.py:68] layer 26_q @ epoch 1 new loss 3.406601899769157e-05 old loss 3.4726366720860824e-05 BETTER
I0315 06:05:47.098366 369033 finetune.py:68] layer 24_q @ epoch 4 new loss 1.838794196373783e-05 old loss 1.854482434282545e-05 BETTER
W0315 06:05:48.786010 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_q proxy err 0.0019374703988432884 tr(WHW.T) 6552.87890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.06it/s]  6%|▋         | 2/32 [00:01<00:18,  1.65it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.59it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.64it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.66it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.69it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.69it/s] 41%|████      | 13/32 [00:05<00:07,  2.70it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.70it/s]I0315 06:05:55.944576 370569 finetune.py:68] layer 25_q @ epoch 3 new loss 2.592489545349963e-05 old loss 2.6382547730463557e-05 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.71it/s] 50%|█████     | 16/32 [00:06<00:05,  2.73it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.73it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.74it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.74it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.74it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.74it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.75it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
I0315 06:06:06.134034 373601 finetune.py:68] layer 27_q @ epoch 0 new loss 4.134008486289531e-05 old loss 4.31571934313979e-05 BETTER
I0315 06:06:09.235842 369033 finetune.py:45] layer 24_k initial loss 2.456248512316961e-05
W0315 06:06:09.236176 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:06:18.934785 372077 finetune.py:68] layer 26_q @ epoch 2 new loss 3.353761348989792e-05 old loss 3.406601899769157e-05 BETTER
I0315 06:06:32.263356 370569 finetune.py:68] layer 25_q @ epoch 4 new loss 2.5549537895130925e-05 old loss 2.592489545349963e-05 BETTER
W0315 06:06:34.018016 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

25_q proxy err 0.001661907765083015 tr(WHW.T) 7685.8828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.03it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:14,  1.96it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.19it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.56it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.58it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.60it/s]I0315 06:06:41.065135 373601 finetune.py:68] layer 27_q @ epoch 1 new loss 4.055833778693341e-05 old loss 4.134008486289531e-05 BETTER
 47%|████▋     | 15/32 [00:06<00:06,  2.62it/s] 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.62it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.63it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.62it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.62it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.62it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.61it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.60it/s]I0315 06:06:45.954592 369033 finetune.py:68] layer 24_k @ epoch 0 new loss 2.394354305579327e-05 old loss 2.456248512316961e-05 BETTER
 84%|████████▍ | 27/32 [00:10<00:01,  2.60it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.61it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:12<00:00,  2.50it/s]
I0315 06:06:55.026188 370569 finetune.py:45] layer 25_k initial loss 3.946163269574754e-05
W0315 06:06:55.026451 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:06:55.040805 372077 finetune.py:68] layer 26_q @ epoch 3 new loss 3.3140247978735715e-05 old loss 3.353761348989792e-05 BETTER
I0315 06:07:16.052855 373601 finetune.py:68] layer 27_q @ epoch 2 new loss 4.013366196886636e-05 old loss 4.055833778693341e-05 BETTER
I0315 06:07:23.926937 369033 finetune.py:68] layer 24_k @ epoch 1 new loss 2.379184115852695e-05 old loss 2.394354305579327e-05 BETTER
I0315 06:07:30.185029 370569 finetune.py:68] layer 25_k @ epoch 0 new loss 3.638248745119199e-05 old loss 3.946163269574754e-05 BETTER
I0315 06:07:31.697649 372077 finetune.py:68] layer 26_q @ epoch 4 new loss 3.279990778537467e-05 old loss 3.3140247978735715e-05 BETTER
W0315 06:07:33.364163 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_q proxy err 0.0019354079850018024 tr(WHW.T) 6101.3564453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:18,  1.67it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.52it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.66it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.66it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.65it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.63it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.64it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.55it/s]
I0315 06:07:51.338515 373601 finetune.py:68] layer 27_q @ epoch 3 new loss 3.970192119595595e-05 old loss 4.013366196886636e-05 BETTER
I0315 06:07:54.521845 372077 finetune.py:45] layer 26_k initial loss 4.132353569730185e-05
W0315 06:07:54.522139 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:08:02.041555 369033 finetune.py:68] layer 24_k @ epoch 2 new loss 2.3672491806792095e-05 old loss 2.379184115852695e-05 BETTER
I0315 06:08:06.099775 370569 finetune.py:68] layer 25_k @ epoch 1 new loss 3.600904528866522e-05 old loss 3.638248745119199e-05 BETTER
I0315 06:08:26.519456 373601 finetune.py:68] layer 27_q @ epoch 4 new loss 3.949020538129844e-05 old loss 3.970192119595595e-05 BETTER
W0315 06:08:28.558042 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 06:08:29.550865 372077 finetune.py:68] layer 26_k @ epoch 0 new loss 4.006185554317199e-05 old loss 4.132353569730185e-05 BETTER
27_q proxy err 0.0020098600070923567 tr(WHW.T) 6390.794921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:29,  1.04it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:14,  1.94it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.16it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.37it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.48it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.52it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.52it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.52it/s] 50%|█████     | 16/32 [00:06<00:06,  2.53it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.54it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.54it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.55it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.55it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.55it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.55it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.55it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.54it/s]I0315 06:08:40.554134 369033 finetune.py:68] layer 24_k @ epoch 3 new loss 2.3586293536936864e-05 old loss 2.3672491806792095e-05 BETTER
 81%|████████▏ | 26/32 [00:10<00:02,  2.54it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.53it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.53it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.54it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.55it/s]I0315 06:08:42.557305 370569 finetune.py:68] layer 25_k @ epoch 2 new loss 3.573856520233676e-05 old loss 3.600904528866522e-05 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.55it/s]100%|██████████| 32/32 [00:13<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
I0315 06:08:50.339837 373601 finetune.py:45] layer 27_k initial loss 5.404912735684775e-05
W0315 06:08:50.340099 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:09:06.053620 372077 finetune.py:68] layer 26_k @ epoch 1 new loss 3.971512705902569e-05 old loss 4.006185554317199e-05 BETTER
I0315 06:09:18.644660 369033 finetune.py:68] layer 24_k @ epoch 4 new loss 2.3515738575952128e-05 old loss 2.3586293536936864e-05 BETTER
I0315 06:09:18.839847 370569 finetune.py:68] layer 25_k @ epoch 3 new loss 3.54968142346479e-05 old loss 3.573856520233676e-05 BETTER
W0315 06:09:20.480465 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_k proxy err 0.0016591622261330485 tr(WHW.T) 4134.79052734375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.23it/s]  6%|▋         | 2/32 [00:01<00:16,  1.81it/s]  9%|▉         | 3/32 [00:01<00:13,  2.13it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.32it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s]I0315 06:09:24.394834 373601 finetune.py:68] layer 27_k @ epoch 0 new loss 5.221536412136629e-05 old loss 5.404912735684775e-05 BETTER
 19%|█▉        | 6/32 [00:02<00:10,  2.52it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.62it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.64it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.67it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.67it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.69it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.70it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.68it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.67it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.67it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.69it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.72it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.71it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]
I0315 06:09:41.886960 369033 finetune.py:45] layer 24_o initial loss 4.299276042729616e-05
W0315 06:09:41.887399 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:09:42.588565 372077 finetune.py:68] layer 26_k @ epoch 2 new loss 3.944402851630002e-05 old loss 3.971512705902569e-05 BETTER
I0315 06:09:55.690180 370569 finetune.py:68] layer 25_k @ epoch 4 new loss 3.525455394992605e-05 old loss 3.54968142346479e-05 BETTER
W0315 06:09:57.503513 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

25_k proxy err 0.001660989597439766 tr(WHW.T) 4235.2685546875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s]I0315 06:09:59.894107 373601 finetune.py:68] layer 27_k @ epoch 1 new loss 5.17180233146064e-05 old loss 5.221536412136629e-05 BETTER
  6%|▋         | 2/32 [00:01<00:17,  1.75it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.35it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.47it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.58it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.56it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.57it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.58it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.58it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.56it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]100%|██████████| 32/32 [00:12<00:00,  2.49it/s]
I0315 06:10:19.063630 369033 finetune.py:68] layer 24_o @ epoch 0 new loss 4.097781493328512e-05 old loss 4.299276042729616e-05 BETTER
I0315 06:10:19.262037 372077 finetune.py:68] layer 26_k @ epoch 3 new loss 3.9206141082104295e-05 old loss 3.944402851630002e-05 BETTER
I0315 06:10:19.327067 370569 finetune.py:45] layer 25_o initial loss 5.717379099223763e-05
W0315 06:10:19.327311 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:10:35.440513 373601 finetune.py:68] layer 27_k @ epoch 2 new loss 5.156632687430829e-05 old loss 5.17180233146064e-05 BETTER
I0315 06:10:54.453133 370569 finetune.py:68] layer 25_o @ epoch 0 new loss 5.4569831263506785e-05 old loss 5.717379099223763e-05 BETTER
I0315 06:10:56.239883 372077 finetune.py:68] layer 26_k @ epoch 4 new loss 3.906829806510359e-05 old loss 3.9206141082104295e-05 BETTER
I0315 06:10:57.008863 369033 finetune.py:68] layer 24_o @ epoch 1 new loss 4.0666574932402e-05 old loss 4.097781493328512e-05 BETTER
W0315 06:10:58.132097 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_k proxy err 0.0015630533453077078 tr(WHW.T) 4388.74609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:16,  1.77it/s]  9%|▉         | 3/32 [00:01<00:14,  2.07it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.44it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.53it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.54it/s] 41%|████      | 13/32 [00:05<00:07,  2.55it/s] 44%|████▍     | 14/32 [00:05<00:07,  2.56it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.57it/s] 50%|█████     | 16/32 [00:06<00:06,  2.57it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.57it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.57it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.56it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.54it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.54it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.55it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.56it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.56it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.56it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.57it/s]I0315 06:11:10.657665 373601 finetune.py:68] layer 27_k @ epoch 3 new loss 5.150017386768013e-05 old loss 5.156632687430829e-05 BETTER
 88%|████████▊ | 28/32 [00:11<00:01,  2.58it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.58it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]100%|██████████| 32/32 [00:12<00:00,  2.48it/s]
I0315 06:11:20.215542 372077 finetune.py:45] layer 26_o initial loss 6.972519622649997e-05
W0315 06:11:20.216023 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:11:30.798984 370569 finetune.py:68] layer 25_o @ epoch 1 new loss 5.40956825716421e-05 old loss 5.4569831263506785e-05 BETTER
I0315 06:11:34.970137 369033 finetune.py:68] layer 24_o @ epoch 2 new loss 4.046812318847515e-05 old loss 4.0666574932402e-05 BETTER
I0315 06:11:45.988435 373601 finetune.py:68] layer 27_k @ epoch 4 new loss 5.138236883794889e-05 old loss 5.150017386768013e-05 BETTER
W0315 06:11:47.960290 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

27_k proxy err 0.001798227895051241 tr(WHW.T) 4195.9892578125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.24it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.25it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.36it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.48it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.52it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.58it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.59it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s]I0315 06:11:55.455983 372077 finetune.py:68] layer 26_o @ epoch 0 new loss 6.661309453193098e-05 old loss 6.972519622649997e-05 BETTER
 50%|█████     | 16/32 [00:06<00:06,  2.62it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.62it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.58it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.59it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.60it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.61it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.60it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0315 06:12:06.911201 370569 finetune.py:68] layer 25_o @ epoch 2 new loss 5.3754931286675856e-05 old loss 5.40956825716421e-05 BETTER
I0315 06:12:09.313184 373601 finetune.py:45] layer 27_o initial loss 8.679968595970422e-05
W0315 06:12:09.313597 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:12:12.673181 369033 finetune.py:68] layer 24_o @ epoch 3 new loss 4.0312850615009665e-05 old loss 4.046812318847515e-05 BETTER
I0315 06:12:31.553016 372077 finetune.py:68] layer 26_o @ epoch 1 new loss 6.598923937417567e-05 old loss 6.661309453193098e-05 BETTER
I0315 06:12:43.133547 370569 finetune.py:68] layer 25_o @ epoch 3 new loss 5.349644197849557e-05 old loss 5.3754931286675856e-05 BETTER
I0315 06:12:43.461829 373601 finetune.py:68] layer 27_o @ epoch 0 new loss 8.326076931552961e-05 old loss 8.679968595970422e-05 BETTER
I0315 06:12:50.417804 369033 finetune.py:68] layer 24_o @ epoch 4 new loss 4.0201510273618624e-05 old loss 4.0312850615009665e-05 BETTER
W0315 06:12:52.046900 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_o proxy err 0.010688616894185543 tr(WHW.T) 6.543091297149658
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:07,  2.18s/it]  6%|▋         | 2/32 [00:03<00:53,  1.77s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.53s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:11<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.49s/it]I0315 06:13:07.380575 372077 finetune.py:68] layer 26_o @ epoch 2 new loss 6.557183951372281e-05 old loss 6.598923937417567e-05 BETTER
 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 50%|█████     | 16/32 [00:24<00:23,  1.49s/it]I0315 06:13:18.511649 373601 finetune.py:68] layer 27_o @ epoch 1 new loss 8.246559445979074e-05 old loss 8.326076931552961e-05 BETTER
I0315 06:13:19.056906 370569 finetune.py:68] layer 25_o @ epoch 4 new loss 5.328957922756672e-05 old loss 5.349644197849557e-05 BETTER
 53%|█████▎    | 17/32 [00:25<00:22,  1.49s/it]W0315 06:13:20.680007 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it]25_o proxy err 0.00863554421812296 tr(WHW.T) 8.829554557800293
  0%|          | 0/32 [00:00<?, ?it/s] 59%|█████▉    | 19/32 [00:28<00:19,  1.49s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.49s/it]  3%|▎         | 1/32 [00:02<01:03,  2.05s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it]  9%|▉         | 3/32 [00:05<00:47,  1.65s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.61s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.47s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.47s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.55s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.48s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it]100%|██████████| 32/32 [00:48<00:00,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]
 41%|████      | 13/32 [00:20<00:29,  1.56s/it]I0315 06:13:43.321150 372077 finetune.py:68] layer 26_o @ epoch 3 new loss 6.521061732200906e-05 old loss 6.557183951372281e-05 BETTER
 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it]I0315 06:13:50.334288 369033 finetune.py:45] layer 24_up initial loss 0.00011928586900467053
W0315 06:13:50.334789 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.56s/it]I0315 06:13:53.807447 373601 finetune.py:68] layer 27_o @ epoch 2 new loss 8.193124085664749e-05 old loss 8.246559445979074e-05 BETTER
 66%|██████▌   | 21/32 [00:33<00:17,  1.56s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.56s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.56s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.56s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.55s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.56s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.56s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.56s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.56s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]100%|██████████| 32/32 [00:50<00:00,  1.57s/it]
I0315 06:14:19.319023 372077 finetune.py:68] layer 26_o @ epoch 4 new loss 6.495924753835425e-05 old loss 6.521061732200906e-05 BETTER
I0315 06:14:20.767359 370569 finetune.py:45] layer 25_up initial loss 0.00014342099893838167
W0315 06:14:20.767704 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0315 06:14:21.165753 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

26_o proxy err 0.00654632830992341 tr(WHW.T) 16.23250961303711
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:04,  2.08s/it]I0315 06:14:26.122297 369033 finetune.py:68] layer 24_up @ epoch 0 new loss 0.00011771262506954372 old loss 0.00011928586900467053 BETTER
  6%|▋         | 2/32 [00:03<00:52,  1.76s/it]  9%|▉         | 3/32 [00:05<00:47,  1.65s/it]I0315 06:14:29.249891 373601 finetune.py:68] layer 27_o @ epoch 3 new loss 8.15621460787952e-05 old loss 8.193124085664749e-05 BETTER
 12%|█▎        | 4/32 [00:06<00:44,  1.60s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.53s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.53s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it]I0315 06:14:54.421150 370569 finetune.py:68] layer 25_up @ epoch 0 new loss 0.00014156334509607404 old loss 0.00014342099893838167 BETTER
 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.54s/it]I0315 06:15:02.531564 369033 finetune.py:68] layer 24_up @ epoch 1 new loss 0.00011661838652798906 old loss 0.00011771262506954372 BETTER
 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it]I0315 06:15:04.560768 373601 finetune.py:68] layer 27_o @ epoch 4 new loss 8.132778748404235e-05 old loss 8.15621460787952e-05 BETTER
 84%|████████▍ | 27/32 [00:41<00:07,  1.54s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it]W0315 06:15:06.266249 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it]27_o proxy err 0.007754983846098185 tr(WHW.T) 16.188697814941406
  0%|          | 0/32 [00:00<?, ?it/s] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it]  3%|▎         | 1/32 [00:02<01:04,  2.10s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.53s/it]  6%|▋         | 2/32 [00:03<00:53,  1.77s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
  9%|▉         | 3/32 [00:05<00:48,  1.67s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.58s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it]I0315 06:15:20.439510 372077 finetune.py:45] layer 26_up initial loss 0.00017203474999405444
W0315 06:15:20.439919 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 25%|██▌       | 8/32 [00:12<00:37,  1.58s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.56s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.55s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 38%|███▊      | 12/32 [00:19<00:30,  1.55s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it]I0315 06:15:28.780083 370569 finetune.py:68] layer 25_up @ epoch 1 new loss 0.00014039597590453923 old loss 0.00014156334509607404 BETTER
 44%|████▍     | 14/32 [00:22<00:27,  1.54s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it]I0315 06:15:38.539046 369033 finetune.py:68] layer 24_up @ epoch 2 new loss 0.0001157525839516893 old loss 0.00011661838652798906 BETTER
 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.54s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.55s/it] 78%|███████▊  | 25/32 [00:39<00:10,  1.54s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.55s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.55s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it]I0315 06:15:54.306581 372077 finetune.py:68] layer 26_up @ epoch 0 new loss 0.000169743609149009 old loss 0.00017203474999405444 BETTER
 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.54s/it]100%|██████████| 32/32 [00:50<00:00,  1.56s/it]
I0315 06:16:03.197790 370569 finetune.py:68] layer 25_up @ epoch 2 new loss 0.00013942315126769245 old loss 0.00014039597590453923 BETTER
I0315 06:16:06.157612 373601 finetune.py:45] layer 27_up initial loss 0.0002093627699650824
W0315 06:16:06.158033 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:16:14.900345 369033 finetune.py:68] layer 24_up @ epoch 3 new loss 0.00011500237451400608 old loss 0.0001157525839516893 BETTER
I0315 06:16:28.966812 372077 finetune.py:68] layer 26_up @ epoch 1 new loss 0.00016831979155540466 old loss 0.000169743609149009 BETTER
I0315 06:16:38.509334 370569 finetune.py:68] layer 25_up @ epoch 3 new loss 0.00013856988516636193 old loss 0.00013942315126769245 BETTER
I0315 06:16:39.716351 373601 finetune.py:68] layer 27_up @ epoch 0 new loss 0.0002065302396658808 old loss 0.0002093627699650824 BETTER
I0315 06:16:51.274405 369033 finetune.py:68] layer 24_up @ epoch 4 new loss 0.0001143462213804014 old loss 0.00011500237451400608 BETTER
W0315 06:16:52.694677 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_up proxy err 0.012600578367710114 tr(WHW.T) 1343.2432861328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:58,  1.90s/it]  6%|▋         | 2/32 [00:03<00:49,  1.65s/it]  9%|▉         | 3/32 [00:04<00:45,  1.58s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.54s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it]I0315 06:17:03.476574 372077 finetune.py:68] layer 26_up @ epoch 2 new loss 0.00016713025979697704 old loss 0.00016831979155540466 BETTER
 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.50s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.50s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.50s/it]I0315 06:17:12.937787 370569 finetune.py:68] layer 25_up @ epoch 4 new loss 0.00013782810128759593 old loss 0.00013856988516636193 BETTER
I0315 06:17:13.474720 373601 finetune.py:68] layer 27_up @ epoch 1 new loss 0.00020469068840611726 old loss 0.0002065302396658808 BETTER
 41%|████      | 13/32 [00:19<00:28,  1.50s/it]W0315 06:17:14.719358 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it]25_up proxy err 0.0126616470515728 tr(WHW.T) 1429.2860107421875
  0%|          | 0/32 [00:00<?, ?it/s] 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it] 50%|█████     | 16/32 [00:24<00:23,  1.50s/it]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.50s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.50s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.57s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.50s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.56s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.49s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.49s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.53s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.48s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it]I0315 06:17:37.746414 372077 finetune.py:68] layer 26_up @ epoch 3 new loss 0.0001661025598878041 old loss 0.00016713025979697704 BETTER
 44%|████▍     | 14/32 [00:21<00:27,  1.54s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.48s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.47s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it]100%|██████████| 32/32 [00:48<00:00,  1.47s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]
 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it]I0315 06:17:47.190097 373601 finetune.py:68] layer 27_up @ epoch 2 new loss 0.00020323782518971711 old loss 0.00020469068840611726 BETTER
 62%|██████▎   | 20/32 [00:31<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it]I0315 06:17:50.459058 369033 finetune.py:45] layer 24_gate initial loss 0.00017479131929576397
W0315 06:17:50.459459 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.53s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
I0315 06:18:12.160390 372077 finetune.py:68] layer 26_up @ epoch 4 new loss 0.0001652462233323604 old loss 0.0001661025598878041 BETTER
W0315 06:18:13.673584 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 06:18:14.582975 370569 finetune.py:45] layer 25_gate initial loss 0.00020880759984720498
W0315 06:18:14.583361 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

26_up proxy err 0.01243174634873867 tr(WHW.T) 1584.43603515625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:03<00:51,  1.72s/it]  9%|▉         | 3/32 [00:05<00:47,  1.64s/it]I0315 06:18:20.994673 373601 finetune.py:68] layer 27_up @ epoch 3 new loss 0.000201985050807707 old loss 0.00020323782518971711 BETTER
 12%|█▎        | 4/32 [00:06<00:45,  1.61s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.59s/it]I0315 06:18:24.472112 369033 finetune.py:68] layer 24_gate @ epoch 0 new loss 0.00017355597810819745 old loss 0.00017479131929576397 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.56s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.53s/it] 41%|████      | 13/32 [00:20<00:29,  1.53s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.54s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:25<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.52s/it]I0315 06:18:46.809737 370569 finetune.py:68] layer 25_gate @ epoch 0 new loss 0.00020750929252244532 old loss 0.00020880759984720498 BETTER
 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.51s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.51s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it]I0315 06:18:54.319096 373601 finetune.py:68] layer 27_up @ epoch 4 new loss 0.00020093232160434127 old loss 0.000201985050807707 BETTER
 81%|████████▏ | 26/32 [00:40<00:09,  1.51s/it]W0315 06:18:55.745059 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it]27_up proxy err 0.011583986692130566 tr(WHW.T) 1873.71728515625
  0%|          | 0/32 [00:00<?, ?it/s] 88%|████████▊ | 28/32 [00:43<00:06,  1.50s/it]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]I0315 06:18:59.314872 369033 finetune.py:68] layer 24_gate @ epoch 1 new loss 0.00017287298396695405 old loss 0.00017355597810819745 BETTER
 91%|█████████ | 29/32 [00:44<00:04,  1.50s/it]  6%|▋         | 2/32 [00:03<00:51,  1.71s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.50s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.50s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it]100%|██████████| 32/32 [00:49<00:00,  1.50s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
 16%|█▌        | 5/32 [00:08<00:42,  1.58s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.56s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.55s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.55s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it]I0315 06:19:12.143139 372077 finetune.py:45] layer 26_gate initial loss 0.0002509042387828231
W0315 06:19:12.143606 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.54s/it]I0315 06:19:19.874659 370569 finetune.py:68] layer 25_gate @ epoch 1 new loss 0.00020672169921454042 old loss 0.00020750929252244532 BETTER
 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.54s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.54s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.53s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it]I0315 06:19:34.403925 369033 finetune.py:68] layer 24_gate @ epoch 2 new loss 0.00017233201651833951 old loss 0.00017287298396695405 BETTER
 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.53s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.53s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it]I0315 06:19:44.745859 372077 finetune.py:68] layer 26_gate @ epoch 0 new loss 0.0002492392959538847 old loss 0.0002509042387828231 BETTER
 97%|█████████▋| 31/32 [00:48<00:01,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]100%|██████████| 32/32 [00:49<00:00,  1.55s/it]
I0315 06:19:53.325926 370569 finetune.py:68] layer 25_gate @ epoch 2 new loss 0.0002060988190351054 old loss 0.00020672169921454042 BETTER
I0315 06:19:54.773864 373601 finetune.py:45] layer 27_gate initial loss 0.00030844606226310134
W0315 06:19:54.774351 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:20:09.308219 369033 finetune.py:68] layer 24_gate @ epoch 3 new loss 0.0001718396379146725 old loss 0.00017233201651833951 BETTER
I0315 06:20:17.915295 372077 finetune.py:68] layer 26_gate @ epoch 1 new loss 0.00024831766495481133 old loss 0.0002492392959538847 BETTER
I0315 06:20:26.418431 373601 finetune.py:68] layer 27_gate @ epoch 0 new loss 0.00030623067868873477 old loss 0.00030844606226310134 BETTER
I0315 06:20:26.436287 370569 finetune.py:68] layer 25_gate @ epoch 3 new loss 0.00020556403615046293 old loss 0.0002060988190351054 BETTER
I0315 06:20:44.357123 369033 finetune.py:68] layer 24_gate @ epoch 4 new loss 0.0001714173995424062 old loss 0.0001718396379146725 BETTER
W0315 06:20:45.594031 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

24_gate proxy err 0.006512215826660395 tr(WHW.T) 4747.1298828125
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:35,  1.17it/s]  2%|▏         | 2/112 [00:01<01:02,  1.75it/s]  3%|▎         | 3/112 [00:01<00:52,  2.08it/s]I0315 06:20:51.126829 372077 finetune.py:68] layer 26_gate @ epoch 2 new loss 0.0002476086956448853 old loss 0.00024831766495481133 BETTER
  4%|▎         | 4/112 [00:01<00:47,  2.28it/s]  4%|▍         | 5/112 [00:02<00:44,  2.40it/s]  5%|▌         | 6/112 [00:02<00:42,  2.48it/s]  6%|▋         | 7/112 [00:03<00:41,  2.54it/s]  7%|▋         | 8/112 [00:03<00:40,  2.58it/s]  8%|▊         | 9/112 [00:03<00:39,  2.60it/s]  9%|▉         | 10/112 [00:04<00:39,  2.62it/s] 10%|▉         | 11/112 [00:04<00:38,  2.63it/s] 11%|█         | 12/112 [00:04<00:38,  2.63it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.65it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.65it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.66it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.67it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.67it/s] 17%|█▋        | 19/112 [00:07<00:34,  2.67it/s] 18%|█▊        | 20/112 [00:07<00:34,  2.67it/s] 19%|█▉        | 21/112 [00:08<00:33,  2.68it/s] 20%|█▉        | 22/112 [00:08<00:33,  2.67it/s] 21%|██        | 23/112 [00:09<00:33,  2.68it/s]I0315 06:20:58.879463 373601 finetune.py:68] layer 27_gate @ epoch 1 new loss 0.00030502426670864224 old loss 0.00030623067868873477 BETTER
 21%|██▏       | 24/112 [00:09<00:32,  2.68it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.68it/s]I0315 06:20:59.474832 370569 finetune.py:68] layer 25_gate @ epoch 4 new loss 0.00020508139277808368 old loss 0.00020556403615046293 BETTER
 23%|██▎       | 26/112 [00:10<00:32,  2.65it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.66it/s] 25%|██▌       | 28/112 [00:10<00:31,  2.66it/s]W0315 06:21:00.680500 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 26%|██▌       | 29/112 [00:11<00:31,  2.66it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.67it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.68it/s] 29%|██▊       | 32/112 [00:12<00:29,  2.69it/s] 29%|██▉       | 33/112 [00:12<00:29,  2.69it/s] 30%|███       | 34/112 [00:13<00:29,  2.69it/s] 31%|███▏      | 35/112 [00:13<00:28,  2.68it/s] 32%|███▏      | 36/112 [00:13<00:28,  2.68it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.68it/s] 34%|███▍      | 38/112 [00:14<00:27,  2.68it/s]25_gate proxy err 0.0064783538691699505 tr(WHW.T) 5088.21484375
  0%|          | 0/112 [00:00<?, ?it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.68it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s]  1%|          | 1/112 [00:00<01:39,  1.12it/s] 37%|███▋      | 41/112 [00:15<00:26,  2.66it/s]  2%|▏         | 2/112 [00:01<01:06,  1.66it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.66it/s]  3%|▎         | 3/112 [00:01<00:55,  1.98it/s] 38%|███▊      | 43/112 [00:16<00:25,  2.67it/s]  4%|▎         | 4/112 [00:02<00:49,  2.18it/s] 39%|███▉      | 44/112 [00:16<00:25,  2.67it/s]  4%|▍         | 5/112 [00:02<00:46,  2.31it/s] 40%|████      | 45/112 [00:17<00:25,  2.68it/s]  5%|▌         | 6/112 [00:02<00:43,  2.41it/s] 41%|████      | 46/112 [00:17<00:24,  2.69it/s]  6%|▋         | 7/112 [00:03<00:42,  2.47it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.69it/s]  7%|▋         | 8/112 [00:03<00:41,  2.51it/s] 43%|████▎     | 48/112 [00:18<00:23,  2.69it/s]  8%|▊         | 9/112 [00:03<00:40,  2.54it/s] 44%|████▍     | 49/112 [00:18<00:23,  2.69it/s]  9%|▉         | 10/112 [00:04<00:39,  2.56it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.68it/s] 10%|▉         | 11/112 [00:04<00:39,  2.56it/s] 46%|████▌     | 51/112 [00:19<00:22,  2.67it/s] 11%|█         | 12/112 [00:05<00:38,  2.58it/s] 46%|████▋     | 52/112 [00:19<00:22,  2.67it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.57it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.63it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.58it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.65it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.59it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.66it/s] 50%|█████     | 56/112 [00:21<00:20,  2.67it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.56it/s] 51%|█████     | 57/112 [00:21<00:20,  2.68it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.58it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.68it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.58it/s] 53%|█████▎    | 59/112 [00:22<00:19,  2.69it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.60it/s] 54%|█████▎    | 60/112 [00:22<00:19,  2.69it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.60it/s] 54%|█████▍    | 61/112 [00:23<00:18,  2.69it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.60it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.69it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.60it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.68it/s] 21%|██        | 23/112 [00:09<00:34,  2.60it/s] 57%|█████▋    | 64/112 [00:24<00:17,  2.69it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.61it/s] 58%|█████▊    | 65/112 [00:24<00:17,  2.69it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.61it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.70it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.61it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.67it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.61it/s] 61%|██████    | 68/112 [00:25<00:16,  2.67it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.57it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.68it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.59it/s] 62%|██████▎   | 70/112 [00:26<00:15,  2.69it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.59it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.70it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.60it/s] 64%|██████▍   | 72/112 [00:27<00:14,  2.70it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.61it/s] 65%|██████▌   | 73/112 [00:27<00:14,  2.71it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.61it/s] 66%|██████▌   | 74/112 [00:28<00:13,  2.71it/s] 30%|███       | 34/112 [00:13<00:29,  2.61it/s] 67%|██████▋   | 75/112 [00:28<00:13,  2.71it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.61it/s] 68%|██████▊   | 76/112 [00:28<00:13,  2.70it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.61it/s] 69%|██████▉   | 77/112 [00:29<00:12,  2.70it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 70%|██████▉   | 78/112 [00:29<00:12,  2.70it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.62it/s] 71%|███████   | 79/112 [00:29<00:12,  2.70it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.62it/s] 71%|███████▏  | 80/112 [00:30<00:11,  2.68it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.62it/s] 72%|███████▏  | 81/112 [00:30<00:11,  2.69it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.62it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.70it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.62it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.71it/s] 75%|███████▌  | 84/112 [00:31<00:10,  2.72it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.59it/s] 76%|███████▌  | 85/112 [00:32<00:09,  2.74it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.61it/s] 77%|███████▋  | 86/112 [00:32<00:09,  2.75it/s] 40%|████      | 45/112 [00:17<00:25,  2.63it/s] 78%|███████▊  | 87/112 [00:32<00:09,  2.75it/s] 41%|████      | 46/112 [00:18<00:24,  2.64it/s] 79%|███████▊  | 88/112 [00:33<00:08,  2.76it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.65it/s] 79%|███████▉  | 89/112 [00:33<00:08,  2.75it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.65it/s] 80%|████████  | 90/112 [00:34<00:08,  2.73it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.64it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.72it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.63it/s] 82%|████████▏ | 92/112 [00:34<00:07,  2.72it/s]I0315 06:21:24.197122 372077 finetune.py:68] layer 26_gate @ epoch 3 new loss 0.0002469663158990443 old loss 0.0002476086956448853 BETTER
 46%|████▌     | 51/112 [00:20<00:23,  2.64it/s] 83%|████████▎ | 93/112 [00:35<00:06,  2.72it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.65it/s] 84%|████████▍ | 94/112 [00:35<00:06,  2.68it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.62it/s] 85%|████████▍ | 95/112 [00:35<00:06,  2.69it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.61it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.70it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.61it/s] 87%|████████▋ | 97/112 [00:36<00:05,  2.70it/s] 50%|█████     | 56/112 [00:22<00:21,  2.60it/s] 88%|████████▊ | 98/112 [00:36<00:05,  2.70it/s] 51%|█████     | 57/112 [00:22<00:21,  2.57it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.72it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.57it/s] 89%|████████▉ | 100/112 [00:37<00:04,  2.71it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.58it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.71it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.58it/s] 91%|█████████ | 102/112 [00:38<00:03,  2.71it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.59it/s] 92%|█████████▏| 103/112 [00:38<00:03,  2.71it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.60it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.72it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.60it/s] 94%|█████████▍| 105/112 [00:39<00:02,  2.72it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.60it/s] 95%|█████████▍| 106/112 [00:39<00:02,  2.70it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.59it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.66it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.59it/s] 96%|█████████▋| 108/112 [00:40<00:01,  2.67it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.59it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.68it/s] 61%|██████    | 68/112 [00:26<00:16,  2.59it/s] 98%|█████████▊| 110/112 [00:41<00:00,  2.69it/s] 99%|█████████▉| 111/112 [00:41<00:00,  2.69it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.59it/s]I0315 06:21:31.284599 373601 finetune.py:68] layer 27_gate @ epoch 2 new loss 0.0003040521696675569 old loss 0.00030502426670864224 BETTER
100%|██████████| 112/112 [00:42<00:00,  2.70it/s]100%|██████████| 112/112 [00:42<00:00,  2.66it/s]
 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s] 63%|██████▎   | 71/112 [00:27<00:16,  2.55it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.56it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.57it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.58it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.58it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.58it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.58it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.58it/s] 71%|███████   | 79/112 [00:30<00:12,  2.59it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.58it/s] 72%|███████▏  | 81/112 [00:31<00:12,  2.57it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.57it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.58it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.54it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.55it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.57it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.58it/s]W0315 06:21:38.929000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.929000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.929000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.929000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.929000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.929000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.930000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.975000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.975000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.975000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.975000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.975000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 79%|███████▉  | 89/112 [00:34<00:08,  2.59it/s]W0315 06:21:38.991000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.991000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.992000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.992000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:38.992000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.165000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.165000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.165000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.166000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.166000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 80%|████████  | 90/112 [00:35<00:08,  2.59it/s]W0315 06:21:39.499000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.499000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.499000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.500000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.500000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.500000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.500000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.534000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.534000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.534000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.534000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.534000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.610000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.610000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.610000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.611000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:39.611000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
 81%|████████▏ | 91/112 [00:35<00:08,  2.59it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.60it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.60it/s]W0315 06:21:40.853000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:40.860000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:40.866000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:40.867000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 84%|████████▍ | 94/112 [00:36<00:06,  2.60it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.60it/s]W0315 06:21:41.335000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.336000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.336000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.336000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.336000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.336000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.336000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.368000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.368000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.368000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.368000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.368000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 86%|████████▌ | 96/112 [00:37<00:06,  2.59it/s]W0315 06:21:41.726000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.727000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.727000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.727000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.727000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.727000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.727000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:41.727000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:42.036000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:42.036000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:42.036000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:42.036000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:42.036000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
 87%|████████▋ | 97/112 [00:37<00:05,  2.59it/s]W0315 06:21:42.382000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:42.387000 140380832307008 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 88%|████████▊ | 98/112 [00:38<00:05,  2.59it/s] 88%|████████▊ | 99/112 [00:38<00:05,  2.54it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.56it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.57it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.57it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.57it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.57it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.57it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.57it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.57it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.58it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.59it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.58it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.58it/s]100%|██████████| 112/112 [00:43<00:00,  2.55it/s]100%|██████████| 112/112 [00:43<00:00,  2.56it/s]
I0315 06:21:49.682333 369033 finetune.py:45] layer 24_down initial loss 0.000271458673523739
W0315 06:21:49.682714 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0315 06:21:55.717000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.717000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.717000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.718000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.718000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.718000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.718000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.762000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.762000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.762000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.762000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.763000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.778000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.778000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.779000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.779000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.779000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.951000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.951000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.952000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.952000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:55.952000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.281000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.281000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.281000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.282000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.282000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.282000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.282000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.314000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.314000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.315000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.315000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.315000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.388000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.388000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.388000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.388000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:56.388000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
I0315 06:21:57.354331 372077 finetune.py:68] layer 26_gate @ epoch 4 new loss 0.00024641878553666174 old loss 0.0002469663158990443 BETTER
W0315 06:21:57.609000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:57.622000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:57.630000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:57.630000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.091000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.091000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.091000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.091000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.091000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.091000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.091000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.127000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.127000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.127000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.127000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.127000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.499000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.499000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.499000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.499000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.499000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.499000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.499000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.500000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.526637 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

W0315 06:21:58.804000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.804000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.804000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.804000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:58.804000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:21:59.146000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:21:59.152000 140140975748928 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
26_gate proxy err 0.005975694395601749 tr(WHW.T) 5986.31982421875
  0%|          | 0/112 [00:00<?, ?it/s]  1%|          | 1/112 [00:00<01:41,  1.09it/s]  2%|▏         | 2/112 [00:01<01:06,  1.65it/s]I0315 06:22:03.706014 373601 finetune.py:68] layer 27_gate @ epoch 3 new loss 0.0003032558597624302 old loss 0.0003040521696675569 BETTER
  3%|▎         | 3/112 [00:01<00:54,  1.98it/s]  4%|▎         | 4/112 [00:02<00:49,  2.19it/s]  4%|▍         | 5/112 [00:02<00:46,  2.33it/s]  5%|▌         | 6/112 [00:02<00:43,  2.42it/s]  6%|▋         | 7/112 [00:03<00:42,  2.48it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s]I0315 06:22:06.389890 370569 finetune.py:45] layer 25_down initial loss 0.0003148369141854346
W0315 06:22:06.390507 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  9%|▉         | 10/112 [00:04<00:39,  2.57it/s] 10%|▉         | 11/112 [00:04<00:39,  2.58it/s] 11%|█         | 12/112 [00:05<00:38,  2.59it/s] 12%|█▏        | 13/112 [00:05<00:38,  2.60it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.57it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.58it/s] 14%|█▍        | 16/112 [00:06<00:37,  2.59it/s] 15%|█▌        | 17/112 [00:07<00:36,  2.59it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.60it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.61it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.62it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.62it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.62it/s] 21%|██        | 23/112 [00:09<00:33,  2.62it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.62it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.61it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.61it/s] 24%|██▍       | 27/112 [00:10<00:33,  2.57it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.58it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.59it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.60it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.62it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.62it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.62it/s] 30%|███       | 34/112 [00:13<00:29,  2.62it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.63it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.62it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.61it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.61it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.60it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.56it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.57it/s] 38%|███▊      | 43/112 [00:17<00:26,  2.58it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.58it/s] 40%|████      | 45/112 [00:17<00:25,  2.59it/s] 41%|████      | 46/112 [00:18<00:25,  2.60it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.61it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.61it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.62it/s]I0315 06:22:21.905886 369033 finetune.py:68] layer 24_down @ epoch 0 new loss 0.00027137165307067335 old loss 0.000271458673523739 BETTER
 45%|████▍     | 50/112 [00:19<00:23,  2.62it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.63it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.62it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.61it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.61it/s] 49%|████▉     | 55/112 [00:21<00:22,  2.57it/s] 50%|█████     | 56/112 [00:22<00:21,  2.59it/s] 51%|█████     | 57/112 [00:22<00:21,  2.60it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.61it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.61it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.62it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.62it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.62it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.62it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.62it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.62it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.62it/s] 61%|██████    | 68/112 [00:26<00:16,  2.61it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.57it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.59it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.60it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.60it/s] 65%|██████▌   | 73/112 [00:28<00:15,  2.60it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.61it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.61it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.62it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.61it/s] 70%|██████▉   | 78/112 [00:30<00:13,  2.61it/s] 71%|███████   | 79/112 [00:30<00:12,  2.61it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.60it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.60it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.56it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.57it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.59it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.59it/s]I0315 06:22:36.041385 373601 finetune.py:68] layer 27_gate @ epoch 4 new loss 0.00030255710589699447 old loss 0.0003032558597624302 BETTER
 78%|███████▊  | 87/112 [00:33<00:09,  2.60it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.60it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.60it/s]I0315 06:22:37.181535 370569 finetune.py:68] layer 25_down @ epoch 0 new loss 0.0003147254465147853 old loss 0.0003148369141854346 BETTER
W0315 06:22:37.237198 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 80%|████████  | 90/112 [00:35<00:08,  2.61it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.61it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.62it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.62it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.62it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.62it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.58it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.59it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.60it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.61it/s]27_gate proxy err 0.0053921593353152275 tr(WHW.T) 7245.16015625
  0%|          | 0/112 [00:00<?, ?it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.62it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.63it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.63it/s]  2%|▏         | 2/112 [00:01<01:04,  1.69it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.63it/s]  3%|▎         | 3/112 [00:01<00:54,  2.00it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.64it/s]  4%|▎         | 4/112 [00:02<00:49,  2.18it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.62it/s]  4%|▍         | 5/112 [00:02<00:46,  2.30it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.62it/s]  5%|▌         | 6/112 [00:02<00:44,  2.38it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.61it/s]  6%|▋         | 7/112 [00:03<00:43,  2.43it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.61it/s]  7%|▋         | 8/112 [00:03<00:42,  2.46it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.57it/s]  8%|▊         | 9/112 [00:04<00:41,  2.49it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.59it/s]  9%|▉         | 10/112 [00:04<00:40,  2.51it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.60it/s] 10%|▉         | 11/112 [00:04<00:40,  2.52it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]100%|██████████| 112/112 [00:43<00:00,  2.57it/s]
 11%|█         | 12/112 [00:05<00:39,  2.53it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.53it/s] 12%|█▎        | 14/112 [00:05<00:38,  2.52it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.53it/s] 14%|█▍        | 16/112 [00:06<00:38,  2.51it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.52it/s] 16%|█▌        | 18/112 [00:07<00:37,  2.53it/s] 17%|█▋        | 19/112 [00:07<00:36,  2.54it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.55it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.56it/s] 20%|█▉        | 22/112 [00:09<00:35,  2.56it/s] 21%|██        | 23/112 [00:09<00:34,  2.56it/s] 21%|██▏       | 24/112 [00:09<00:34,  2.56it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.56it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.56it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.55it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.55it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.54it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.51it/s]W0315 06:22:53.486000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.486000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.487000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.487000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.487000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.487000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.487000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.534000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.535000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.535000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.535000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.535000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.551000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.551000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.551000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.552000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.552000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
 28%|██▊       | 31/112 [00:12<00:32,  2.52it/s]W0315 06:22:53.725000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.725000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.725000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.725000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:22:53.726000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.057000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.057000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.057000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.058000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.058000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.058000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.058000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.093000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.093000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.093000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.093000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.093000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 29%|██▊       | 32/112 [00:13<00:31,  2.53it/s]W0315 06:22:54.166000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.167000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.167000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.167000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:54.167000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
 29%|██▉       | 33/112 [00:13<00:31,  2.53it/s] 30%|███       | 34/112 [00:13<00:30,  2.54it/s]I0315 06:22:54.980150 369033 finetune.py:68] layer 24_down @ epoch 1 new loss 0.00027135832351632416 old loss 0.00027137165307067335 BETTER
 31%|███▏      | 35/112 [00:14<00:30,  2.55it/s]W0315 06:22:55.383000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.395000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.403000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.403000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 32%|███▏      | 36/112 [00:14<00:29,  2.55it/s]W0315 06:22:55.867000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.867000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.867000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.867000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.867000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.867000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.867000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.902000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.902000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.902000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.903000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:22:55.903000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 33%|███▎      | 37/112 [00:15<00:29,  2.56it/s]W0315 06:22:56.253000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.253000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.254000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.254000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.254000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.254000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.254000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.254000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 34%|███▍      | 38/112 [00:15<00:28,  2.57it/s]W0315 06:22:56.559000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.559000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.559000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.559000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.559000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 35%|███▍      | 39/112 [00:15<00:28,  2.56it/s]W0315 06:22:56.902000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:22:56.907000 140086853703488 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 36%|███▌      | 40/112 [00:16<00:28,  2.55it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.55it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.53it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.49it/s] 39%|███▉      | 44/112 [00:17<00:27,  2.50it/s] 40%|████      | 45/112 [00:18<00:26,  2.52it/s] 41%|████      | 46/112 [00:18<00:26,  2.53it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.54it/s] 43%|████▎     | 48/112 [00:19<00:25,  2.55it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.55it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.55it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.55it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.55it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.55it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.55it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.55it/s] 50%|█████     | 56/112 [00:22<00:21,  2.55it/s] 51%|█████     | 57/112 [00:22<00:22,  2.50it/s]I0315 06:23:04.130711 372077 finetune.py:45] layer 26_down initial loss 0.0003735555219464004
W0315 06:23:04.131221 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 52%|█████▏    | 58/112 [00:23<00:21,  2.51it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.53it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.54it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.53it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.55it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.56it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.57it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.58it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.59it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.59it/s] 61%|██████    | 68/112 [00:27<00:17,  2.59it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.59it/s]I0315 06:23:08.716229 370569 finetune.py:68] layer 25_down @ epoch 1 new loss 0.0003147200040984899 old loss 0.0003147254465147853 BETTER
 62%|██████▎   | 70/112 [00:27<00:16,  2.60it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.59it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.54it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.56it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.56it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.55it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.56it/s] 69%|██████▉   | 77/112 [00:30<00:13,  2.57it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.57it/s] 71%|███████   | 79/112 [00:31<00:12,  2.58it/s] 71%|███████▏  | 80/112 [00:31<00:12,  2.58it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.58it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.58it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.58it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.57it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.53it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.54it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.56it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.56it/s] 80%|████████  | 90/112 [00:35<00:08,  2.56it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.57it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.58it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.58it/s] 84%|████████▍ | 94/112 [00:37<00:06,  2.58it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.58it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.57it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.57it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.56it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.53it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.54it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.55it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.55it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.56it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.56it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.57it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.56it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.57it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.57it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.56it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.57it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.56it/s]100%|██████████| 112/112 [00:44<00:00,  2.52it/s]100%|██████████| 112/112 [00:44<00:00,  2.52it/s]
I0315 06:23:28.018188 369033 finetune.py:68] layer 24_down @ epoch 2 new loss 0.0002713449648581445 old loss 0.00027135832351632416 BETTER
W0315 06:23:33.211000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.212000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.212000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.212000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.212000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.212000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.212000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.257000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.257000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.257000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.257000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.257000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.273000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.273000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.273000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.274000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.274000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.449000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.449000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.449000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.449000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.449000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.782000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.782000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.783000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.783000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.783000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.783000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.783000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.820000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.820000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.820000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.820000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.820000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.893000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.893000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.893000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.893000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:33.893000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.140000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.154000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.163000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.163000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
I0315 06:23:35.306682 372077 finetune.py:68] layer 26_down @ epoch 0 new loss 0.000373433023924008 old loss 0.0003735555219464004 BETTER
W0315 06:23:35.620000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.620000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.621000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.621000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.621000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.621000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.621000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.651000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.651000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.652000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.652000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:35.652000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.009000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.009000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.009000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.009000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.010000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.010000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.010000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.010000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.321000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.321000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.321000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.321000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.321000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.697000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:23:36.703000 140392729077568 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 06:23:41.036654 370569 finetune.py:68] layer 25_down @ epoch 2 new loss 0.0003146852250210941 old loss 0.0003147200040984899 BETTER
I0315 06:23:44.211240 373601 finetune.py:45] layer 27_down initial loss 0.00045290004345588386
W0315 06:23:44.211867 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:24:01.114729 369033 finetune.py:68] layer 24_down @ epoch 3 new loss 0.00027132523246109486 old loss 0.0002713449648581445 BETTER
I0315 06:24:06.767913 372077 finetune.py:68] layer 26_down @ epoch 1 new loss 0.0003734093625098467 old loss 0.000373433023924008 BETTER
I0315 06:24:13.244340 370569 finetune.py:68] layer 25_down @ epoch 3 new loss 0.0003146615927107632 old loss 0.0003146852250210941 BETTER
I0315 06:24:14.150586 373601 finetune.py:68] layer 27_down @ epoch 0 new loss 0.0004527287383098155 old loss 0.00045290004345588386 BETTER
I0315 06:24:35.385149 369033 finetune.py:68] layer 24_down @ epoch 4 new loss 0.0002713229914661497 old loss 0.00027132523246109486 BETTER
W0315 06:24:36.814859 369033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

24_down proxy err 0.012304197065532207 tr(WHW.T) 35.02656936645508
I0315 06:24:39.442239 372077 finetune.py:68] layer 26_down @ epoch 2 new loss 0.0003733934718184173 old loss 0.0003734093625098467 BETTER
I0315 06:24:45.922540 370569 finetune.py:68] layer 25_down @ epoch 4 new loss 0.000314647622872144 old loss 0.0003146615927107632 BETTER
I0315 06:24:46.746214 373601 finetune.py:68] layer 27_down @ epoch 1 new loss 0.0004526861594058573 old loss 0.0004527287383098155 BETTER
W0315 06:24:46.767085 370569 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

25_down proxy err 0.012395898811519146 tr(WHW.T) 38.038169860839844
I0315 06:25:11.664090 372077 finetune.py:68] layer 26_down @ epoch 3 new loss 0.0003733730409294367 old loss 0.0003733934718184173 BETTER
I0315 06:25:17.760162 373601 finetune.py:76] layer 27_down @ epoch 2 new loss 0.00045268714893609285 old loss 0.0004526861594058573 WORSE
I0315 06:25:43.385319 372077 finetune.py:68] layer 26_down @ epoch 4 new loss 0.00037336075911298394 old loss 0.0003733730409294367 BETTER
W0315 06:25:44.335726 372077 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

26_down proxy err 0.012554970569908619 tr(WHW.T) 43.394657135009766
I0315 06:25:47.895217 373601 finetune.py:68] layer 27_down @ epoch 3 new loss 0.0004526523989625275 old loss 0.0004526861594058573 BETTER
I0315 06:25:58.493126 273992 quantize_finetune_llama.py:186] computed original embedding for layer 28 in 67.28239369392395s
I0315 06:25:59.053511 273992 quantize_finetune_llama.py:159] layer 29 gpu 1
I0315 06:26:01.232865 398602 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 06:26:01.232985 398602 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 06:26:01.233046 398602 utils.py:162] NumExpr defaulting to 16 threads.
I0315 06:26:01.450528 398602 config.py:58] PyTorch version 2.4.0 available.
I0315 06:26:03.778589 398602 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 06:26:04.153479 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:44,  1.45s/it]  6%|▋         | 2/32 [00:01<00:24,  1.21it/s]  9%|▉         | 3/32 [00:02<00:17,  1.65it/s] 12%|█▎        | 4/32 [00:02<00:14,  1.99it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.24it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.43it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.56it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.65it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.77it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.82it/s] 47%|████▋     | 15/32 [00:06<00:05,  2.86it/s] 50%|█████     | 16/32 [00:06<00:05,  2.84it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.84it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.84it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.83it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.82it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.80it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.82it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.77it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.76it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.87it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.88it/s]100%|██████████| 32/32 [00:12<00:00,  2.87it/s]100%|██████████| 32/32 [00:12<00:00,  2.58it/s]
I0315 06:26:18.995024 373601 finetune.py:68] layer 27_down @ epoch 4 new loss 0.00045262492494657636 old loss 0.0004526523989625275 BETTER
W0315 06:26:20.029300 373601 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

27_down proxy err 0.010525134392082691 tr(WHW.T) 61.504066467285156
W0315 06:26:21.061000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.062000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.062000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.062000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.062000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.062000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.062000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.089000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.089000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.089000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.089000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.089000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.106000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.106000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.106000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.106000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.106000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.440000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.440000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.441000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.441000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:26:21.441000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.414000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.414000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.415000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.415000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.415000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.415000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.415000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.433000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.433000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.433000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.434000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.434000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.686000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.687000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.687000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.687000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:26:22.687000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.908000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.908000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.908000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.908000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.909000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.909000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.909000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.927000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.927000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.927000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.927000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:26:23.927000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:26:24.932000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:26:24.932000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:26:24.932000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:26:24.933000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:26:24.933000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 06:26:31.968609 398602 finetune.py:45] layer 28_v initial loss 0.00018481169536244124
W0315 06:26:31.969192 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:27:01.947593 273992 quantize_finetune_llama.py:186] computed original embedding for layer 29 in 62.453025579452515s
I0315 06:27:02.511708 273992 quantize_finetune_llama.py:159] layer 30 gpu 2
I0315 06:27:04.753579 400113 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 06:27:04.753819 400113 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 06:27:04.753934 400113 utils.py:162] NumExpr defaulting to 16 threads.
I0315 06:27:04.993586 400113 config.py:58] PyTorch version 2.4.0 available.
I0315 06:27:07.482600 400113 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 06:27:08.661608 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 06:27:08.979472 398602 finetune.py:68] layer 28_v @ epoch 0 new loss 5.2442963351495564e-05 old loss 0.00018481169536244124 BETTER
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:52,  1.70s/it]  6%|▋         | 2/32 [00:02<00:27,  1.10it/s]  9%|▉         | 3/32 [00:02<00:18,  1.53it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.87it/s] 16%|█▌        | 5/32 [00:03<00:13,  2.07it/s] 19%|█▉        | 6/32 [00:03<00:12,  2.17it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.35it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.50it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.59it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:05<00:07,  2.71it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.72it/s] 41%|████      | 13/32 [00:06<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 50%|█████     | 16/32 [00:07<00:05,  2.73it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.76it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.78it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.78it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.71it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.74it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.78it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.78it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.79it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.78it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.78it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.81it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.78it/s]100%|██████████| 32/32 [00:12<00:00,  2.47it/s]
W0315 06:27:25.875000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.876000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.876000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.876000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.876000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.876000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.876000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.903000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.903000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.903000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.904000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.904000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.920000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.921000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.921000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.921000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:27:25.921000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:27:26.280000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:27:26.281000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:27:26.281000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:27:26.281000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:27:26.281000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.241000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.242000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.242000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.242000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.242000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.242000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.242000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.260000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.260000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.261000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.261000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.261000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.516000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.516000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.516000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.516000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:27:27.516000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.748000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.748000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.749000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.749000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.749000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.749000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.749000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.767000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.767000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.767000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.768000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:27:28.768000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:27:29.754000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:27:29.755000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:27:29.755000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:27:29.755000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:27:29.755000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 06:27:36.420449 400113 finetune.py:45] layer 29_v initial loss 0.0002472637570463121
W0315 06:27:36.420977 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:27:46.566850 398602 finetune.py:68] layer 28_v @ epoch 1 new loss 4.8271507694153115e-05 old loss 5.2442963351495564e-05 BETTER
I0315 06:28:05.828000 273992 quantize_finetune_llama.py:186] computed original embedding for layer 30 in 62.85745859146118s
I0315 06:28:06.388311 273992 quantize_finetune_llama.py:159] layer 31 gpu 3
I0315 06:28:08.598453 401669 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 06:28:08.598644 401669 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 06:28:08.598752 401669 utils.py:162] NumExpr defaulting to 16 threads.
I0315 06:28:08.792338 401669 config.py:58] PyTorch version 2.4.0 available.
I0315 06:28:11.140209 401669 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 06:28:11.517597 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 06:28:12.235212 400113 finetune.py:68] layer 29_v @ epoch 0 new loss 6.499724258901551e-05 old loss 0.0002472637570463121 BETTER
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:50,  1.63s/it]  6%|▋         | 2/32 [00:01<00:26,  1.13it/s]  9%|▉         | 3/32 [00:02<00:18,  1.55it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.84it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.10it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.29it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.55it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.63it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.70it/s] 34%|███▍      | 11/32 [00:05<00:09,  2.23it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.37it/s] 41%|████      | 13/32 [00:06<00:07,  2.50it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:07<00:05,  2.71it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.79it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.74it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:09<00:03,  2.79it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.81it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.83it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.72it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.74it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.76it/s]I0315 06:28:24.806092 398602 finetune.py:68] layer 28_v @ epoch 2 new loss 4.658356192521751e-05 old loss 4.8271507694153115e-05 BETTER
 94%|█████████▍| 30/32 [00:12<00:00,  2.76it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.79it/s]100%|██████████| 32/32 [00:13<00:00,  2.80it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
W0315 06:28:28.897000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.898000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.898000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.898000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.898000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.898000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.899000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.925000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.926000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.926000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.926000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.926000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.943000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.943000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.943000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.943000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:28:28.943000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:28:29.281000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:28:29.282000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:28:29.282000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:28:29.282000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:28:29.282000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.203000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.203000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.203000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.203000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.204000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.204000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.204000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.222000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.222000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.222000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.222000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.222000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.471000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.472000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.472000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.472000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:28:30.472000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.756000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.757000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.757000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.757000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.757000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.757000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.757000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.775000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.775000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.775000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.775000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:28:31.776000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:28:32.714000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:28:32.715000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:28:32.715000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:28:32.715000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:28:32.715000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
I0315 06:28:40.865551 401669 finetune.py:45] layer 30_v initial loss 0.0003374465159140527
W0315 06:28:40.866325 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:28:49.285921 400113 finetune.py:68] layer 29_v @ epoch 1 new loss 6.267966091400012e-05 old loss 6.499724258901551e-05 BETTER
I0315 06:29:03.514917 398602 finetune.py:68] layer 28_v @ epoch 3 new loss 4.5868800953030586e-05 old loss 4.658356192521751e-05 BETTER
I0315 06:29:14.189371 273992 quantize_finetune_llama.py:186] computed original embedding for layer 31 in 67.3132152557373s
I0315 06:29:17.437827 403307 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 06:29:17.438091 403307 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 06:29:17.438212 403307 utils.py:162] NumExpr defaulting to 16 threads.
I0315 06:29:17.667632 403307 config.py:58] PyTorch version 2.4.0 available.
I0315 06:29:17.794152 401669 finetune.py:68] layer 30_v @ epoch 0 new loss 0.00014498454402200878 old loss 0.0003374465159140527 BETTER
I0315 06:29:20.399969 403307 data_utils.py:336] using 256 training seqs, 128 validation seqs
W0315 06:29:20.936101 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:53,  1.72s/it]  6%|▋         | 2/32 [00:02<00:27,  1.08it/s]  9%|▉         | 3/32 [00:02<00:19,  1.50it/s] 12%|█▎        | 4/32 [00:02<00:15,  1.83it/s] 16%|█▌        | 5/32 [00:03<00:12,  2.08it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.26it/s]I0315 06:29:26.158795 400113 finetune.py:68] layer 29_v @ epoch 2 new loss 6.227724952623248e-05 old loss 6.267966091400012e-05 BETTER
 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:04<00:09,  2.51it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.57it/s] 31%|███▏      | 10/32 [00:05<00:08,  2.55it/s] 34%|███▍      | 11/32 [00:05<00:08,  2.55it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:06<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.62it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:07<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.71it/s] 56%|█████▋    | 18/32 [00:08<00:05,  2.73it/s] 59%|█████▉    | 19/32 [00:08<00:04,  2.74it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:09<00:04,  2.75it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.75it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.76it/s] 75%|███████▌  | 24/32 [00:10<00:02,  2.77it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.70it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:12<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:13<00:00,  2.57it/s]100%|██████████| 32/32 [00:13<00:00,  2.41it/s]
W0315 06:29:39.793000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.793000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.794000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.794000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.794000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.794000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.794000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.822000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.822000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.822000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.822000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.822000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.840000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.840000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.840000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.840000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:29:39.840000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:29:40.195000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:29:40.196000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:29:40.196000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:29:40.196000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:29:40.196000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.192000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.193000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.193000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.193000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.193000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.193000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.193000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.212000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.212000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.212000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.212000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.212000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.464000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.464000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.465000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.465000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:29:41.465000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] z1 is not in var_ranges, defaulting to unknown range.
I0315 06:29:42.437311 398602 finetune.py:68] layer 28_v @ epoch 4 new loss 4.539423025562428e-05 old loss 4.5868800953030586e-05 BETTER
W0315 06:29:42.693000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.693000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.694000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.694000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.694000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.694000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.694000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.712000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.712000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.712000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.713000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:29:42.713000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:29:43.659000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:29:43.659000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:29:43.660000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:29:43.660000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:29:43.660000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/0] x1 is not in var_ranges, defaulting to unknown range.
The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.
W0315 06:29:44.369056 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_v proxy err 0.013195355422794819 tr(WHW.T) 175.40756225585938
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:34,  1.10s/it]  6%|▋         | 2/32 [00:01<00:20,  1.47it/s]  9%|▉         | 3/32 [00:01<00:15,  1.84it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.35it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.43it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.49it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.54it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.56it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.59it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.59it/s] 41%|████      | 13/32 [00:05<00:07,  2.61it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s]I0315 06:29:52.180902 403307 finetune.py:45] layer 31_v initial loss 0.00040808465564623475
W0315 06:29:52.181347 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 50%|█████     | 16/32 [00:06<00:06,  2.67it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.66it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.64it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.57it/s]I0315 06:29:54.797541 401669 finetune.py:68] layer 30_v @ epoch 1 new loss 0.00013788427168037742 old loss 0.00014498454402200878 BETTER
 69%|██████▉   | 22/32 [00:09<00:03,  2.57it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.56it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.50it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.53it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.53it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.57it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.58it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.60it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.62it/s]100%|██████████| 32/32 [00:13<00:00,  2.63it/s]100%|██████████| 32/32 [00:13<00:00,  2.46it/s]
I0315 06:30:03.730628 400113 finetune.py:68] layer 29_v @ epoch 3 new loss 6.020083310431801e-05 old loss 6.227724952623248e-05 BETTER
W0315 06:30:05.606000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.607000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.607000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.607000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.607000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.608000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.608000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.639000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.639000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.639000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.639000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.639000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.655000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.655000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.655000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.655000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.656000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.819000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.819000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.820000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.820000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:30:05.820000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.054000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.054000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.054000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.054000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.054000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.054000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.054000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.074000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.074000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.075000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.075000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.075000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.141000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.141000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.141000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.141000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:30:06.142000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.076000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.409000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.409000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.409000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.409000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.409000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.409000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.410000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.431000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.431000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.431000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.431000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.431000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.703000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.704000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.704000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.704000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.704000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:30:07.979000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 06:30:16.111915 398602 finetune.py:45] layer 28_q initial loss 6.308253068709746e-05
W0315 06:30:16.112279 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:30:26.155412 403307 finetune.py:68] layer 31_v @ epoch 0 new loss 0.00022137464839033782 old loss 0.00040808465564623475 BETTER
I0315 06:30:31.282381 401669 finetune.py:76] layer 30_v @ epoch 2 new loss 0.00013803165347781032 old loss 0.00013788427168037742 WORSE
I0315 06:30:40.692044 400113 finetune.py:68] layer 29_v @ epoch 4 new loss 5.9161811805097386e-05 old loss 6.020083310431801e-05 BETTER
W0315 06:30:42.791417 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

29_v proxy err 0.011003940366208553 tr(WHW.T) 249.68582153320312
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.00it/s]  6%|▋         | 2/32 [00:01<00:19,  1.54it/s]  9%|▉         | 3/32 [00:01<00:15,  1.83it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.21it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.30it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.39it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.51it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.54it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.57it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.60it/s] 41%|████      | 13/32 [00:05<00:07,  2.60it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.60it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.60it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.63it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.63it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.66it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.66it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.62it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s]I0315 06:30:53.965045 398602 finetune.py:68] layer 28_q @ epoch 0 new loss 6.0654037952190265e-05 old loss 6.308253068709746e-05 BETTER
 78%|███████▊  | 25/32 [00:10<00:02,  2.55it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.48it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.52it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.52it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.49it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.54it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.57it/s]100%|██████████| 32/32 [00:13<00:00,  2.58it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
I0315 06:31:01.338201 403307 finetune.py:76] layer 31_v @ epoch 1 new loss 0.0002417756331851706 old loss 0.00022137464839033782 WORSE
W0315 06:31:03.740000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.740000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.740000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.740000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.741000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.741000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.741000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.772000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.772000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.772000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.772000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.772000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.790000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.790000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.790000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.790000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.790000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.961000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.961000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.961000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.961000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:31:03.961000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.211000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.211000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.211000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.212000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.212000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.212000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.212000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.232000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.233000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.233000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.233000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.233000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.302000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.302000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.302000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.302000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:31:04.302000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.239000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.574000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.574000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.574000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.574000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.575000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.575000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.575000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.598000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.598000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.598000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.598000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.599000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.862000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.863000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.863000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.863000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:31:05.863000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:31:06.132000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 06:31:06.844352 401669 finetune.py:68] layer 30_v @ epoch 3 new loss 0.0001372411788906902 old loss 0.00013788427168037742 BETTER
I0315 06:31:14.693000 400113 finetune.py:45] layer 29_q initial loss 0.00010944588575512171
W0315 06:31:14.693517 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:31:32.288357 398602 finetune.py:68] layer 28_q @ epoch 1 new loss 5.9445392253110185e-05 old loss 6.0654037952190265e-05 BETTER
I0315 06:31:36.173084 403307 finetune.py:76] layer 31_v @ epoch 2 new loss 0.0002336026227567345 old loss 0.00022137464839033782 WORSE
I0315 06:31:44.122005 401669 finetune.py:68] layer 30_v @ epoch 4 new loss 0.0001323533506365493 old loss 0.0001372411788906902 BETTER
W0315 06:31:46.574725 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_v proxy err 0.013652844354510307 tr(WHW.T) 252.81201171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.02it/s]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.17it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s]I0315 06:31:50.277622 400113 finetune.py:68] layer 29_q @ epoch 0 new loss 0.00010059165651910007 old loss 0.00010944588575512171 BETTER
 19%|█▉        | 6/32 [00:02<00:10,  2.40it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.41it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.44it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.45it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.46it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.50it/s] 41%|████      | 13/32 [00:05<00:07,  2.54it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.61it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.61it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.62it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.58it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.54it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.54it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.52it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.50it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.52it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.51it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.51it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.53it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.54it/s]100%|██████████| 32/32 [00:13<00:00,  2.56it/s]100%|██████████| 32/32 [00:13<00:00,  2.44it/s]
W0315 06:32:07.580000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.581000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.581000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.581000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.581000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.581000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.582000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.613000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.613000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.613000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.613000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.613000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.629000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.629000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.629000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.629000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.629000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.798000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.798000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.799000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.799000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:32:07.799000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.046000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.046000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.046000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.046000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.046000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.046000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.046000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.071000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.072000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.072000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.072000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.072000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.142000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.142000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.143000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.143000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:32:08.143000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.086000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.450000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.450000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.450000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.450000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.451000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.451000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.451000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.473000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.474000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.474000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.474000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.474000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.745000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.746000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.746000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.746000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:32:09.746000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:32:10.021000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 06:32:11.059735 403307 finetune.py:76] layer 31_v @ epoch 3 new loss 0.0002672674891073257 old loss 0.00022137464839033782 WORSE
I0315 06:32:11.180063 398602 finetune.py:68] layer 28_q @ epoch 2 new loss 5.866037463420071e-05 old loss 5.9445392253110185e-05 BETTER
I0315 06:32:16.969517 401669 finetune.py:45] layer 30_q initial loss 0.0001592931803315878
W0315 06:32:16.970578 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:32:27.068648 400113 finetune.py:68] layer 29_q @ epoch 1 new loss 9.718910587253049e-05 old loss 0.00010059165651910007 BETTER
I0315 06:32:45.883302 403307 finetune.py:76] layer 31_v @ epoch 4 new loss 0.00025556888431310654 old loss 0.00022137464839033782 WORSE
W0315 06:32:47.132874 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

31_v proxy err 0.005586537998169661 tr(WHW.T) 365.68487548828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.09s/it]I0315 06:32:49.503580 398602 finetune.py:68] layer 28_q @ epoch 3 new loss 5.7991594076156616e-05 old loss 5.866037463420071e-05 BETTER
  6%|▋         | 2/32 [00:01<00:20,  1.48it/s]  9%|▉         | 3/32 [00:01<00:15,  1.84it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:12,  2.23it/s] 19%|█▉        | 6/32 [00:03<00:11,  2.34it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.41it/s]I0315 06:32:51.892356 401669 finetune.py:68] layer 30_q @ epoch 0 new loss 0.0001533691247459501 old loss 0.0001592931803315878 BETTER
 25%|██▌       | 8/32 [00:03<00:09,  2.46it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.49it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.51it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.53it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:06<00:06,  2.58it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.59it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.59it/s] 59%|█████▉    | 19/32 [00:08<00:05,  2.59it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.58it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.60it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.59it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.60it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.61it/s] 84%|████████▍ | 27/32 [00:11<00:01,  2.61it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.63it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.62it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.61it/s]100%|██████████| 32/32 [00:13<00:00,  2.60it/s]100%|██████████| 32/32 [00:13<00:00,  2.45it/s]
I0315 06:33:03.943857 400113 finetune.py:68] layer 29_q @ epoch 2 new loss 9.54147326410748e-05 old loss 9.718910587253049e-05 BETTER
W0315 06:33:08.940000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.940000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.940000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.941000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.941000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.941000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.941000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.972000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.972000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.972000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.972000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.972000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.988000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.988000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.988000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.988000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:33:08.989000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.155000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.156000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.156000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.156000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.156000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.398000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.398000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.398000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.398000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.398000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.398000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.398000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.421000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.421000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.421000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.421000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.421000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.490000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.490000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.490000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.490000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:33:09.490000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.416000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.742000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.743000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.743000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.743000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.743000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.743000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.743000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.765000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.765000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.765000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.765000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:33:10.765000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:33:11.035000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:33:11.035000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:33:11.035000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:33:11.035000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:33:11.035000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:33:11.308000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/1] ps0 is not in var_ranges, defaulting to unknown range.
I0315 06:33:18.309295 403307 finetune.py:45] layer 31_q initial loss 0.00031766141182743013
W0315 06:33:18.309739 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:33:28.330824 401669 finetune.py:68] layer 30_q @ epoch 1 new loss 0.00015115301357582211 old loss 0.0001533691247459501 BETTER
I0315 06:33:28.877475 398602 finetune.py:68] layer 28_q @ epoch 4 new loss 5.764117668149993e-05 old loss 5.7991594076156616e-05 BETTER
W0315 06:33:31.106642 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_q proxy err 0.001827123574912548 tr(WHW.T) 6758.5986328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:30,  1.00it/s]  6%|▋         | 2/32 [00:01<00:18,  1.58it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.18it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.34it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.44it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.43it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.48it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.50it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.50it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.55it/s] 41%|████      | 13/32 [00:05<00:07,  2.59it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.63it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s]I0315 06:33:39.783364 400113 finetune.py:68] layer 29_q @ epoch 3 new loss 9.428397606825456e-05 old loss 9.54147326410748e-05 BETTER
 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.67it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.64it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.65it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.66it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.67it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.67it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.62it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.62it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.51it/s]
I0315 06:33:53.975124 403307 finetune.py:68] layer 31_q @ epoch 0 new loss 0.00028828525682911277 old loss 0.00031766141182743013 BETTER
I0315 06:33:58.652606 398602 finetune.py:45] layer 28_k initial loss 7.278262637555599e-05
W0315 06:33:58.653697 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:34:07.078262 401669 finetune.py:68] layer 30_q @ epoch 2 new loss 0.00014859250222798437 old loss 0.00015115301357582211 BETTER
I0315 06:34:18.572869 400113 finetune.py:68] layer 29_q @ epoch 4 new loss 9.212363511323929e-05 old loss 9.428397606825456e-05 BETTER
W0315 06:34:20.814063 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

29_q proxy err 0.0021981443278491497 tr(WHW.T) 6064.724609375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.31it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.63it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.68it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.71it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.72it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.74it/s] 41%|████      | 13/32 [00:05<00:06,  2.73it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.74it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.77it/s] 50%|█████     | 16/32 [00:06<00:05,  2.76it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.75it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.74it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.75it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.74it/s]I0315 06:34:31.118663 403307 finetune.py:76] layer 31_q @ epoch 1 new loss 0.00029657562845386565 old loss 0.00028828525682911277 WORSE
 75%|███████▌  | 24/32 [00:09<00:02,  2.74it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.73it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.73it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.74it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
I0315 06:34:35.472790 398602 finetune.py:68] layer 28_k @ epoch 0 new loss 7.055552123347297e-05 old loss 7.278262637555599e-05 BETTER
I0315 06:34:41.092616 400113 finetune.py:45] layer 29_k initial loss 0.00011976160749327391
W0315 06:34:41.092856 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:34:43.429087 401669 finetune.py:76] layer 30_q @ epoch 3 new loss 0.0001507955603301525 old loss 0.00014859250222798437 WORSE
I0315 06:35:05.404597 403307 finetune.py:76] layer 31_q @ epoch 2 new loss 0.0003146248636767268 old loss 0.00028828525682911277 WORSE
I0315 06:35:13.505341 398602 finetune.py:68] layer 28_k @ epoch 1 new loss 7.009648106759414e-05 old loss 7.055552123347297e-05 BETTER
I0315 06:35:15.982655 400113 finetune.py:68] layer 29_k @ epoch 0 new loss 0.00011295374861219898 old loss 0.00011976160749327391 BETTER
I0315 06:35:18.967687 401669 finetune.py:76] layer 30_q @ epoch 4 new loss 0.00015137607988435775 old loss 0.00014859250222798437 WORSE
W0315 06:35:20.352916 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_q proxy err 0.0015027745394036174 tr(WHW.T) 7049.4287109375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.10it/s]  6%|▋         | 2/32 [00:01<00:17,  1.71it/s]  9%|▉         | 3/32 [00:01<00:14,  2.05it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.29it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.57it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.62it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.65it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.64it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.67it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.68it/s] 50%|█████     | 16/32 [00:06<00:05,  2.68it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.68it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.69it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.70it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.71it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.59it/s]
I0315 06:35:39.677221 403307 finetune.py:76] layer 31_q @ epoch 3 new loss 0.00029569267644546926 old loss 0.00028828525682911277 WORSE
I0315 06:35:40.772350 401669 finetune.py:45] layer 30_k initial loss 0.00017781282076612115
W0315 06:35:40.772660 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:35:51.723189 398602 finetune.py:68] layer 28_k @ epoch 2 new loss 6.961751932976767e-05 old loss 7.009648106759414e-05 BETTER
I0315 06:35:52.207371 400113 finetune.py:68] layer 29_k @ epoch 1 new loss 0.00011243638437008485 old loss 0.00011295374861219898 BETTER
I0315 06:36:13.968223 403307 finetune.py:68] layer 31_q @ epoch 4 new loss 0.0002648090885486454 old loss 0.00028828525682911277 BETTER
I0315 06:36:15.483954 401669 finetune.py:68] layer 30_k @ epoch 0 new loss 0.00017383800877723843 old loss 0.00017781282076612115 BETTER
W0315 06:36:15.489113 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

31_q proxy err 0.0010200463002547622 tr(WHW.T) 9336.15625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:28,  1.08it/s]  6%|▋         | 2/32 [00:01<00:18,  1.66it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.38it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.47it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.54it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.59it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.62it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.63it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.65it/s] 41%|████      | 13/32 [00:05<00:07,  2.66it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.65it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.66it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.66it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.68it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.67it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.68it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.68it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.66it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.65it/s]I0315 06:36:28.402123 400113 finetune.py:68] layer 29_k @ epoch 2 new loss 0.00011115212691947818 old loss 0.00011243638437008485 BETTER
 97%|█████████▋| 31/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.56it/s]
I0315 06:36:30.063047 398602 finetune.py:76] layer 28_k @ epoch 3 new loss 6.971758557483554e-05 old loss 6.961751932976767e-05 WORSE
I0315 06:36:35.454040 403307 finetune.py:45] layer 31_k initial loss 0.0003609058621805161
W0315 06:36:35.454401 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:36:51.307915 401669 finetune.py:68] layer 30_k @ epoch 1 new loss 0.00017360461060889065 old loss 0.00017383800877723843 BETTER
I0315 06:37:04.509230 400113 finetune.py:68] layer 29_k @ epoch 3 new loss 0.00011009299487341195 old loss 0.00011115212691947818 BETTER
I0315 06:37:07.282273 398602 finetune.py:76] layer 28_k @ epoch 4 new loss 6.962931365706027e-05 old loss 6.961751932976767e-05 WORSE
W0315 06:37:08.635622 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 06:37:09.246189 403307 finetune.py:68] layer 31_k @ epoch 0 new loss 0.0003350860206410289 old loss 0.0003609058621805161 BETTER
28_k proxy err 0.001509511494077742 tr(WHW.T) 4364.328125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:24,  1.25it/s]  6%|▋         | 2/32 [00:01<00:16,  1.84it/s]  9%|▉         | 3/32 [00:01<00:13,  2.15it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.34it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.58it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.60it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.64it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.65it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.68it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.68it/s] 41%|████      | 13/32 [00:05<00:07,  2.69it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.71it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.71it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.69it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.68it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.67it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.67it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.70it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.72it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.72it/s]100%|██████████| 32/32 [00:12<00:00,  2.61it/s]
I0315 06:37:27.538673 401669 finetune.py:76] layer 30_k @ epoch 2 new loss 0.0001738383580232039 old loss 0.00017360461060889065 WORSE
I0315 06:37:29.647304 398602 finetune.py:45] layer 28_o initial loss 0.0001249970227945596
W0315 06:37:29.647531 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:37:40.688659 400113 finetune.py:68] layer 29_k @ epoch 4 new loss 0.00010868166282307357 old loss 0.00011009299487341195 BETTER
W0315 06:37:42.473317 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

29_k proxy err 0.0016384931514039636 tr(WHW.T) 4796.0595703125
  0%|          | 0/32 [00:00<?, ?it/s]I0315 06:37:44.079414 403307 finetune.py:76] layer 31_k @ epoch 1 new loss 0.00033903762232512236 old loss 0.0003350860206410289 WORSE
  3%|▎         | 1/32 [00:00<00:25,  1.22it/s]  6%|▋         | 2/32 [00:01<00:16,  1.78it/s]  9%|▉         | 3/32 [00:01<00:13,  2.08it/s] 12%|█▎        | 4/32 [00:01<00:12,  2.26it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.37it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.45it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.50it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.54it/s] 28%|██▊       | 9/32 [00:03<00:09,  2.55it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.57it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.56it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.57it/s] 41%|████      | 13/32 [00:05<00:07,  2.57it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.57it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.58it/s] 50%|█████     | 16/32 [00:06<00:06,  2.59it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.60it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.60it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.61it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.61it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.61it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.60it/s] 75%|███████▌  | 24/32 [00:09<00:03,  2.59it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.59it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.59it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.60it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.60it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.61it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.63it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
I0315 06:38:03.469298 401669 finetune.py:68] layer 30_k @ epoch 3 new loss 0.00017096623196266592 old loss 0.00017360461060889065 BETTER
I0315 06:38:03.849878 400113 finetune.py:45] layer 29_o initial loss 0.00016125923139043152
W0315 06:38:03.850135 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:38:07.145133 398602 finetune.py:68] layer 28_o @ epoch 0 new loss 0.00011859140795422718 old loss 0.0001249970227945596 BETTER
I0315 06:38:18.392020 403307 finetune.py:76] layer 31_k @ epoch 2 new loss 0.0003465620684437454 old loss 0.0003350860206410289 WORSE
I0315 06:38:38.748444 400113 finetune.py:68] layer 29_o @ epoch 0 new loss 0.0001534152979729697 old loss 0.00016125923139043152 BETTER
I0315 06:38:39.862918 401669 finetune.py:68] layer 30_k @ epoch 4 new loss 0.00017090447363443673 old loss 0.00017096623196266592 BETTER
W0315 06:38:41.611592 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_k proxy err 0.0012926608324050903 tr(WHW.T) 4103.45703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:25,  1.20it/s]  6%|▋         | 2/32 [00:01<00:17,  1.73it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s]I0315 06:38:45.703069 398602 finetune.py:68] layer 28_o @ epoch 1 new loss 0.00011730362166417763 old loss 0.00011859140795422718 BETTER
 19%|█▉        | 6/32 [00:02<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.40it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.45it/s] 28%|██▊       | 9/32 [00:04<00:09,  2.45it/s] 31%|███▏      | 10/32 [00:04<00:09,  2.42it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.41it/s] 38%|███▊      | 12/32 [00:05<00:08,  2.46it/s] 41%|████      | 13/32 [00:05<00:07,  2.50it/s] 44%|████▍     | 14/32 [00:06<00:07,  2.54it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.56it/s] 50%|█████     | 16/32 [00:06<00:06,  2.58it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.56it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.58it/s] 59%|█████▉    | 19/32 [00:07<00:05,  2.60it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.59it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.60it/s] 69%|██████▉   | 22/32 [00:09<00:03,  2.59it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.58it/s]I0315 06:38:52.812126 403307 finetune.py:68] layer 31_k @ epoch 3 new loss 0.00032032441231422126 old loss 0.0003350860206410289 BETTER
 75%|███████▌  | 24/32 [00:09<00:03,  2.58it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.58it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.55it/s] 84%|████████▍ | 27/32 [00:11<00:02,  2.49it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.51it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.47it/s] 94%|█████████▍| 30/32 [00:12<00:00,  2.45it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.43it/s]100%|██████████| 32/32 [00:13<00:00,  2.32it/s]100%|██████████| 32/32 [00:13<00:00,  2.42it/s]
I0315 06:39:03.799475 401669 finetune.py:45] layer 30_o initial loss 0.00027913571102544665
W0315 06:39:03.799755 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:39:14.804009 400113 finetune.py:68] layer 29_o @ epoch 1 new loss 0.0001518265635240823 old loss 0.0001534152979729697 BETTER
I0315 06:39:23.315083 398602 finetune.py:68] layer 28_o @ epoch 2 new loss 0.00011620554141700268 old loss 0.00011730362166417763 BETTER
I0315 06:39:27.910472 403307 finetune.py:68] layer 31_k @ epoch 4 new loss 0.0003139394102618098 old loss 0.00032032441231422126 BETTER
W0315 06:39:29.387569 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

31_k proxy err 0.0011296611046418548 tr(WHW.T) 4133.4580078125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:00<00:23,  1.34it/s]  6%|▋         | 2/32 [00:01<00:15,  1.93it/s]  9%|▉         | 3/32 [00:01<00:12,  2.26it/s] 12%|█▎        | 4/32 [00:01<00:11,  2.44it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.56it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.64it/s] 22%|██▏       | 7/32 [00:02<00:09,  2.69it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.72it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:03<00:07,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.79it/s] 50%|█████     | 16/32 [00:06<00:05,  2.79it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:06<00:05,  2.78it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.74it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.75it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.77it/s]I0315 06:39:38.761595 401669 finetune.py:68] layer 30_o @ epoch 0 new loss 0.00026714240084402263 old loss 0.00027913571102544665 BETTER
 72%|███████▏  | 23/32 [00:08<00:03,  2.77it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.78it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.77it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.75it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.75it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.73it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.72it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.73it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.73it/s]100%|██████████| 32/32 [00:11<00:00,  2.72it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
I0315 06:39:49.042915 403307 finetune.py:45] layer 31_o initial loss 0.0004814741259906441
W0315 06:39:49.043234 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:39:50.496397 400113 finetune.py:68] layer 29_o @ epoch 2 new loss 0.00015100641758181155 old loss 0.0001518265635240823 BETTER
I0315 06:40:00.801462 398602 finetune.py:68] layer 28_o @ epoch 3 new loss 0.00011585809988901019 old loss 0.00011620554141700268 BETTER
I0315 06:40:14.362127 401669 finetune.py:68] layer 30_o @ epoch 1 new loss 0.00026478618383407593 old loss 0.00026714240084402263 BETTER
I0315 06:40:22.902413 403307 finetune.py:68] layer 31_o @ epoch 0 new loss 0.00043258818914182484 old loss 0.0004814741259906441 BETTER
I0315 06:40:26.030624 400113 finetune.py:68] layer 29_o @ epoch 3 new loss 0.00015034292300697416 old loss 0.00015100641758181155 BETTER
I0315 06:40:38.188398 398602 finetune.py:68] layer 28_o @ epoch 4 new loss 0.0001153079210780561 old loss 0.00011585809988901019 BETTER
W0315 06:40:39.705531 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_o proxy err 0.007490972522646189 tr(WHW.T) 24.578819274902344
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]  6%|▋         | 2/32 [00:03<00:50,  1.68s/it]  9%|▉         | 3/32 [00:04<00:46,  1.59s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.56s/it] 16%|█▌        | 5/32 [00:07<00:41,  1.53s/it]I0315 06:40:50.076810 401669 finetune.py:68] layer 30_o @ epoch 2 new loss 0.0002640257589519024 old loss 0.00026478618383407593 BETTER
 19%|█▉        | 6/32 [00:09<00:39,  1.52s/it] 22%|██▏       | 7/32 [00:10<00:37,  1.51s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.49s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it]I0315 06:40:58.049364 403307 finetune.py:68] layer 31_o @ epoch 1 new loss 0.0004142295219935477 old loss 0.00043258818914182484 BETTER
 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it]I0315 06:41:01.522439 400113 finetune.py:68] layer 29_o @ epoch 4 new loss 0.00014945822476875037 old loss 0.00015034292300697416 BETTER
 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it]W0315 06:41:03.089429 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 47%|████▋     | 15/32 [00:22<00:25,  1.49s/it]29_o proxy err 0.004668422043323517 tr(WHW.T) 36.676414489746094
  0%|          | 0/32 [00:00<?, ?it/s] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it]  3%|▎         | 1/32 [00:01<01:01,  2.00s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it]  9%|▉         | 3/32 [00:05<00:46,  1.61s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.57s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:39,  1.53s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.48s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.53s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it] 28%|██▊       | 9/32 [00:14<00:34,  1.52s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.49s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.49s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.49s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.49s/it] 41%|████      | 13/32 [00:20<00:28,  1.51s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it]I0315 06:41:25.848782 401669 finetune.py:76] layer 30_o @ epoch 3 new loss 0.00026423591771163046 old loss 0.0002640257589519024 WORSE
 44%|████▍     | 14/32 [00:21<00:27,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 47%|████▋     | 15/32 [00:23<00:25,  1.51s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.48s/it] 50%|█████     | 16/32 [00:24<00:24,  1.51s/it]100%|██████████| 32/32 [00:47<00:00,  1.48s/it]100%|██████████| 32/32 [00:47<00:00,  1.50s/it]
 53%|█████▎    | 17/32 [00:26<00:22,  1.51s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it]I0315 06:41:33.065179 403307 finetune.py:68] layer 31_o @ epoch 2 new loss 0.00041348583181388676 old loss 0.0004142295219935477 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.52s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.52s/it]I0315 06:41:37.377836 398602 finetune.py:45] layer 28_up initial loss 0.00028139082132838666
W0315 06:41:37.378244 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 69%|██████▉   | 22/32 [00:33<00:15,  1.52s/it] 72%|███████▏  | 23/32 [00:35<00:13,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.52s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.52s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.51s/it] 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.51s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
I0315 06:42:00.794853 401669 finetune.py:68] layer 30_o @ epoch 4 new loss 0.00026386624085716903 old loss 0.0002640257589519024 BETTER
I0315 06:42:01.114928 400113 finetune.py:45] layer 29_up initial loss 0.00038661560392938554
W0315 06:42:01.115354 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

W0315 06:42:02.375031 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_o proxy err 0.004198345821350813 tr(WHW.T) 84.22958374023438
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:02<01:03,  2.06s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it]I0315 06:42:07.929084 403307 finetune.py:76] layer 31_o @ epoch 3 new loss 0.0004162663535680622 old loss 0.00041348583181388676 WORSE
  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.59s/it]I0315 06:42:13.040575 398602 finetune.py:68] layer 28_up @ epoch 0 new loss 0.00027668356779031456 old loss 0.00028139082132838666 BETTER
 19%|█▉        | 6/32 [00:09<00:40,  1.57s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.57s/it] 25%|██▌       | 8/32 [00:12<00:37,  1.56s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.55s/it] 31%|███▏      | 10/32 [00:15<00:34,  1.56s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.55s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.55s/it] 41%|████      | 13/32 [00:20<00:29,  1.55s/it] 44%|████▍     | 14/32 [00:22<00:27,  1.55s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.55s/it] 50%|█████     | 16/32 [00:25<00:24,  1.55s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.55s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.55s/it] 59%|█████▉    | 19/32 [00:29<00:20,  1.55s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.55s/it]I0315 06:42:35.393047 400113 finetune.py:68] layer 29_up @ epoch 0 new loss 0.00037926502409391105 old loss 0.00038661560392938554 BETTER
 66%|██████▌   | 21/32 [00:33<00:17,  1.55s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.54s/it] 72%|███████▏  | 23/32 [00:36<00:13,  1.55s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.54s/it]I0315 06:42:42.237331 403307 finetune.py:76] layer 31_o @ epoch 4 new loss 0.00042538961861282587 old loss 0.00041348583181388676 WORSE
 78%|███████▊  | 25/32 [00:39<00:10,  1.54s/it]W0315 06:42:43.592946 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 81%|████████▏ | 26/32 [00:40<00:09,  1.54s/it]31_o proxy err 0.0027742607053369284 tr(WHW.T) 182.30975341796875
  0%|          | 0/32 [00:00<?, ?it/s] 84%|████████▍ | 27/32 [00:42<00:07,  1.54s/it]  3%|▎         | 1/32 [00:02<01:05,  2.10s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.54s/it]  6%|▋         | 2/32 [00:03<00:54,  1.81s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.54s/it]I0315 06:42:49.765231 398602 finetune.py:68] layer 28_up @ epoch 1 new loss 0.00027402042178437114 old loss 0.00027668356779031456 BETTER
  9%|▉         | 3/32 [00:05<00:49,  1.71s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.54s/it] 12%|█▎        | 4/32 [00:06<00:46,  1.66s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.53s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.62s/it]100%|██████████| 32/32 [00:49<00:00,  1.53s/it]100%|██████████| 32/32 [00:49<00:00,  1.56s/it]
 19%|█▉        | 6/32 [00:10<00:42,  1.62s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.60s/it] 25%|██▌       | 8/32 [00:13<00:38,  1.59s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it] 31%|███▏      | 10/32 [00:16<00:34,  1.59s/it]I0315 06:43:01.672844 401669 finetune.py:45] layer 30_up initial loss 0.0007504797540605068
W0315 06:43:01.673231 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 34%|███▍      | 11/32 [00:17<00:33,  1.58s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.58s/it] 41%|████      | 13/32 [00:21<00:29,  1.58s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.58s/it] 47%|████▋     | 15/32 [00:24<00:26,  1.58s/it]I0315 06:43:10.152439 400113 finetune.py:68] layer 29_up @ epoch 1 new loss 0.0003750249743461609 old loss 0.00037926502409391105 BETTER
 50%|█████     | 16/32 [00:25<00:25,  1.58s/it] 53%|█████▎    | 17/32 [00:27<00:23,  1.58s/it] 56%|█████▋    | 18/32 [00:28<00:22,  1.58s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.58s/it] 62%|██████▎   | 20/32 [00:32<00:19,  1.58s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.58s/it] 69%|██████▉   | 22/32 [00:35<00:15,  1.58s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.58s/it] 75%|███████▌  | 24/32 [00:38<00:12,  1.58s/it] 78%|███████▊  | 25/32 [00:40<00:11,  1.58s/it]I0315 06:43:26.135654 398602 finetune.py:68] layer 28_up @ epoch 2 new loss 0.0002719006151892245 old loss 0.00027402042178437114 BETTER
 81%|████████▏ | 26/32 [00:41<00:09,  1.58s/it] 84%|████████▍ | 27/32 [00:43<00:07,  1.58s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.58s/it] 91%|█████████ | 29/32 [00:46<00:04,  1.57s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.57s/it] 97%|█████████▋| 31/32 [00:49<00:01,  1.57s/it]I0315 06:43:35.509760 401669 finetune.py:68] layer 30_up @ epoch 0 new loss 0.0007301237201318145 old loss 0.0007504797540605068 BETTER
100%|██████████| 32/32 [00:51<00:00,  1.57s/it]100%|██████████| 32/32 [00:51<00:00,  1.59s/it]
I0315 06:43:44.403392 403307 finetune.py:45] layer 31_up initial loss 0.002523640636354685
W0315 06:43:44.403988 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:43:44.451257 400113 finetune.py:68] layer 29_up @ epoch 2 new loss 0.00037165716639719903 old loss 0.0003750249743461609 BETTER
I0315 06:44:02.425361 398602 finetune.py:68] layer 28_up @ epoch 3 new loss 0.00027014166698791087 old loss 0.0002719006151892245 BETTER
I0315 06:44:09.788630 401669 finetune.py:68] layer 30_up @ epoch 1 new loss 0.000717934628482908 old loss 0.0007301237201318145 BETTER
I0315 06:44:16.782532 403307 finetune.py:68] layer 31_up @ epoch 0 new loss 0.0023814812302589417 old loss 0.002523640636354685 BETTER
I0315 06:44:18.711603 400113 finetune.py:68] layer 29_up @ epoch 3 new loss 0.0003688708820845932 old loss 0.00037165716639719903 BETTER
I0315 06:44:38.840406 398602 finetune.py:68] layer 28_up @ epoch 4 new loss 0.00026857067132368684 old loss 0.00027014166698791087 BETTER
W0315 06:44:40.391880 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

28_up proxy err 0.009736454114317894 tr(WHW.T) 2477.90673828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<01:01,  1.98s/it]I0315 06:44:43.981382 401669 finetune.py:68] layer 30_up @ epoch 2 new loss 0.0007085062679834664 old loss 0.000717934628482908 BETTER
  6%|▋         | 2/32 [00:03<00:51,  1.71s/it]  9%|▉         | 3/32 [00:05<00:46,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:43,  1.57s/it]I0315 06:44:49.740110 403307 finetune.py:68] layer 31_up @ epoch 1 new loss 0.002299143699929118 old loss 0.0023814812302589417 BETTER
 16%|█▌        | 5/32 [00:08<00:41,  1.55s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.54s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.52s/it]I0315 06:44:52.918857 400113 finetune.py:68] layer 29_up @ epoch 4 new loss 0.0003663422539830208 old loss 0.0003688708820845932 BETTER
W0315 06:44:54.262309 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 25%|██▌       | 8/32 [00:12<00:36,  1.51s/it]29_up proxy err 0.007947765290737152 tr(WHW.T) 3252.737548828125
  0%|          | 0/32 [00:00<?, ?it/s] 28%|██▊       | 9/32 [00:13<00:34,  1.50s/it] 31%|███▏      | 10/32 [00:15<00:32,  1.50s/it]  3%|▎         | 1/32 [00:01<01:00,  1.94s/it] 34%|███▍      | 11/32 [00:16<00:31,  1.49s/it]  6%|▋         | 2/32 [00:03<00:51,  1.70s/it] 38%|███▊      | 12/32 [00:18<00:29,  1.49s/it]  9%|▉         | 3/32 [00:05<00:47,  1.63s/it] 41%|████      | 13/32 [00:19<00:28,  1.49s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.59s/it] 44%|████▍     | 14/32 [00:21<00:26,  1.49s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 47%|████▋     | 15/32 [00:22<00:25,  1.48s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 50%|█████     | 16/32 [00:24<00:23,  1.48s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 53%|█████▎    | 17/32 [00:25<00:22,  1.48s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.53s/it] 56%|█████▋    | 18/32 [00:27<00:20,  1.48s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.53s/it] 59%|█████▉    | 19/32 [00:28<00:19,  1.48s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.52s/it] 62%|██████▎   | 20/32 [00:30<00:17,  1.48s/it] 34%|███▍      | 11/32 [00:17<00:31,  1.52s/it] 66%|██████▌   | 21/32 [00:31<00:16,  1.48s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.52s/it] 69%|██████▉   | 22/32 [00:33<00:14,  1.47s/it] 41%|████      | 13/32 [00:20<00:28,  1.52s/it] 72%|███████▏  | 23/32 [00:34<00:13,  1.48s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.52s/it] 75%|███████▌  | 24/32 [00:36<00:11,  1.48s/it]I0315 06:45:18.164254 401669 finetune.py:68] layer 30_up @ epoch 3 new loss 0.0007003062637522817 old loss 0.0007085062679834664 BETTER
 47%|████▋     | 15/32 [00:23<00:25,  1.52s/it] 78%|███████▊  | 25/32 [00:37<00:10,  1.47s/it] 50%|█████     | 16/32 [00:24<00:24,  1.52s/it] 81%|████████▏ | 26/32 [00:39<00:08,  1.47s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.52s/it] 84%|████████▍ | 27/32 [00:40<00:07,  1.48s/it]I0315 06:45:23.159822 403307 finetune.py:68] layer 31_up @ epoch 2 new loss 0.002230800921097398 old loss 0.002299143699929118 BETTER
 56%|█████▋    | 18/32 [00:27<00:21,  1.52s/it] 88%|████████▊ | 28/32 [00:42<00:05,  1.48s/it] 59%|█████▉    | 19/32 [00:29<00:19,  1.51s/it] 91%|█████████ | 29/32 [00:43<00:04,  1.49s/it] 62%|██████▎   | 20/32 [00:30<00:18,  1.51s/it] 94%|█████████▍| 30/32 [00:45<00:02,  1.49s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.50s/it] 97%|█████████▋| 31/32 [00:46<00:01,  1.49s/it] 69%|██████▉   | 22/32 [00:33<00:15,  1.50s/it]100%|██████████| 32/32 [00:48<00:00,  1.48s/it]100%|██████████| 32/32 [00:48<00:00,  1.50s/it]
 72%|███████▏  | 23/32 [00:35<00:13,  1.50s/it] 75%|███████▌  | 24/32 [00:36<00:12,  1.51s/it] 78%|███████▊  | 25/32 [00:38<00:10,  1.51s/it] 81%|████████▏ | 26/32 [00:39<00:09,  1.51s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.52s/it]I0315 06:45:38.237648 398602 finetune.py:45] layer 28_gate initial loss 0.0004073570598848164
W0315 06:45:38.238013 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 88%|████████▊ | 28/32 [00:42<00:06,  1.52s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.52s/it] 94%|█████████▍| 30/32 [00:45<00:03,  1.51s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.52s/it]100%|██████████| 32/32 [00:48<00:00,  1.53s/it]
I0315 06:45:52.314925 400113 finetune.py:45] layer 29_gate initial loss 0.0005477670929394662
W0315 06:45:52.315248 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:45:52.646968 401669 finetune.py:68] layer 30_up @ epoch 4 new loss 0.0006933974800631404 old loss 0.0007003062637522817 BETTER
W0315 06:45:54.182085 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

30_up proxy err 0.004897840321063995 tr(WHW.T) 5511.30078125
  0%|          | 0/32 [00:00<?, ?it/s]I0315 06:45:56.786016 403307 finetune.py:68] layer 31_up @ epoch 3 new loss 0.0021730319131165743 old loss 0.002230800921097398 BETTER
  3%|▎         | 1/32 [00:01<01:00,  1.94s/it]  6%|▋         | 2/32 [00:03<00:50,  1.70s/it]  9%|▉         | 3/32 [00:05<00:47,  1.62s/it] 12%|█▎        | 4/32 [00:06<00:44,  1.58s/it] 16%|█▌        | 5/32 [00:08<00:42,  1.56s/it] 19%|█▉        | 6/32 [00:09<00:40,  1.55s/it] 22%|██▏       | 7/32 [00:11<00:38,  1.54s/it] 25%|██▌       | 8/32 [00:12<00:36,  1.54s/it] 28%|██▊       | 9/32 [00:14<00:35,  1.54s/it] 31%|███▏      | 10/32 [00:15<00:33,  1.54s/it]I0315 06:46:12.614760 398602 finetune.py:68] layer 28_gate @ epoch 0 new loss 0.0004039662017021328 old loss 0.0004073570598848164 BETTER
 34%|███▍      | 11/32 [00:17<00:32,  1.54s/it] 38%|███▊      | 12/32 [00:18<00:30,  1.54s/it] 41%|████      | 13/32 [00:20<00:29,  1.54s/it] 44%|████▍     | 14/32 [00:21<00:27,  1.54s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.53s/it] 50%|█████     | 16/32 [00:24<00:24,  1.53s/it] 53%|█████▎    | 17/32 [00:26<00:22,  1.53s/it] 56%|█████▋    | 18/32 [00:27<00:21,  1.53s/it]I0315 06:46:24.704007 400113 finetune.py:68] layer 29_gate @ epoch 0 new loss 0.0005435995990410447 old loss 0.0005477670929394662 BETTER
 59%|█████▉    | 19/32 [00:29<00:19,  1.53s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.53s/it] 66%|██████▌   | 21/32 [00:32<00:16,  1.53s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.53s/it]I0315 06:46:30.068534 403307 finetune.py:68] layer 31_up @ epoch 4 new loss 0.0021192766726017 old loss 0.0021730319131165743 BETTER
 72%|███████▏  | 23/32 [00:35<00:13,  1.53s/it]W0315 06:46:31.560391 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 75%|███████▌  | 24/32 [00:37<00:12,  1.53s/it]31_up proxy err 0.0019337112316861749 tr(WHW.T) 12288.3876953125
  0%|          | 0/32 [00:00<?, ?it/s] 78%|███████▊  | 25/32 [00:38<00:10,  1.53s/it]  3%|▎         | 1/32 [00:02<01:02,  2.00s/it] 81%|████████▏ | 26/32 [00:40<00:09,  1.53s/it]  6%|▋         | 2/32 [00:03<00:52,  1.75s/it] 84%|████████▍ | 27/32 [00:41<00:07,  1.53s/it]  9%|▉         | 3/32 [00:05<00:48,  1.66s/it] 88%|████████▊ | 28/32 [00:43<00:06,  1.52s/it] 12%|█▎        | 4/32 [00:06<00:45,  1.62s/it] 91%|█████████ | 29/32 [00:44<00:04,  1.51s/it] 16%|█▌        | 5/32 [00:08<00:43,  1.60s/it] 94%|█████████▍| 30/32 [00:46<00:03,  1.52s/it] 19%|█▉        | 6/32 [00:09<00:41,  1.59s/it] 97%|█████████▋| 31/32 [00:47<00:01,  1.51s/it] 22%|██▏       | 7/32 [00:11<00:39,  1.58s/it]100%|██████████| 32/32 [00:49<00:00,  1.51s/it]100%|██████████| 32/32 [00:49<00:00,  1.54s/it]
 25%|██▌       | 8/32 [00:12<00:37,  1.57s/it] 28%|██▊       | 9/32 [00:14<00:36,  1.58s/it]I0315 06:46:47.824906 398602 finetune.py:68] layer 28_gate @ epoch 1 new loss 0.00040224113035947084 old loss 0.0004039662017021328 BETTER
 31%|███▏      | 10/32 [00:16<00:34,  1.57s/it] 34%|███▍      | 11/32 [00:17<00:32,  1.56s/it] 38%|███▊      | 12/32 [00:19<00:31,  1.56s/it]I0315 06:46:53.075574 401669 finetune.py:45] layer 30_gate initial loss 0.0009703557589091361
W0315 06:46:53.075981 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 41%|████      | 13/32 [00:20<00:29,  1.56s/it] 44%|████▍     | 14/32 [00:22<00:28,  1.56s/it] 47%|████▋     | 15/32 [00:23<00:26,  1.56s/it]I0315 06:46:57.764366 400113 finetune.py:68] layer 29_gate @ epoch 1 new loss 0.0005411923048086464 old loss 0.0005435995990410447 BETTER
 50%|█████     | 16/32 [00:25<00:24,  1.56s/it] 53%|█████▎    | 17/32 [00:26<00:23,  1.56s/it] 56%|█████▋    | 18/32 [00:28<00:21,  1.56s/it] 59%|█████▉    | 19/32 [00:30<00:20,  1.56s/it] 62%|██████▎   | 20/32 [00:31<00:18,  1.57s/it] 66%|██████▌   | 21/32 [00:33<00:17,  1.57s/it] 69%|██████▉   | 22/32 [00:34<00:15,  1.57s/it] 72%|███████▏  | 23/32 [00:36<00:14,  1.57s/it] 75%|███████▌  | 24/32 [00:37<00:12,  1.57s/it] 78%|███████▊  | 25/32 [00:39<00:11,  1.57s/it] 81%|████████▏ | 26/32 [00:41<00:09,  1.57s/it] 84%|████████▍ | 27/32 [00:42<00:07,  1.57s/it] 88%|████████▊ | 28/32 [00:44<00:06,  1.57s/it] 91%|█████████ | 29/32 [00:45<00:04,  1.56s/it] 94%|█████████▍| 30/32 [00:47<00:03,  1.57s/it] 97%|█████████▋| 31/32 [00:48<00:01,  1.56s/it]I0315 06:47:22.762447 398602 finetune.py:68] layer 28_gate @ epoch 2 new loss 0.0004008532559964806 old loss 0.00040224113035947084 BETTER
100%|██████████| 32/32 [00:50<00:00,  1.55s/it]100%|██████████| 32/32 [00:50<00:00,  1.58s/it]
I0315 06:47:25.290503 401669 finetune.py:68] layer 30_gate @ epoch 0 new loss 0.0009601252386346459 old loss 0.0009703557589091361 BETTER
I0315 06:47:30.505504 400113 finetune.py:68] layer 29_gate @ epoch 2 new loss 0.0005392585881054401 old loss 0.0005411923048086464 BETTER
I0315 06:47:31.674287 403307 finetune.py:45] layer 31_gate initial loss 0.0027596866711974144
W0315 06:47:31.674668 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:47:57.540489 398602 finetune.py:68] layer 28_gate @ epoch 3 new loss 0.00039969495264813304 old loss 0.0004008532559964806 BETTER
I0315 06:47:57.971777 401669 finetune.py:68] layer 30_gate @ epoch 1 new loss 0.0009537737350910902 old loss 0.0009601252386346459 BETTER
I0315 06:48:03.056542 403307 finetune.py:68] layer 31_gate @ epoch 0 new loss 0.0027022885624319315 old loss 0.0027596866711974144 BETTER
I0315 06:48:03.393013 400113 finetune.py:68] layer 29_gate @ epoch 3 new loss 0.0005376145127229393 old loss 0.0005392585881054401 BETTER
I0315 06:48:30.726706 401669 finetune.py:68] layer 30_gate @ epoch 2 new loss 0.0009486862691119313 old loss 0.0009537737350910902 BETTER
I0315 06:48:32.189437 398602 finetune.py:68] layer 28_gate @ epoch 4 new loss 0.00039869974716566503 old loss 0.00039969495264813304 BETTER
W0315 06:48:33.537009 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 06:48:35.139846 403307 finetune.py:68] layer 31_gate @ epoch 1 new loss 0.0026657739654183388 old loss 0.0027022885624319315 BETTER
I0315 06:48:36.160432 400113 finetune.py:68] layer 29_gate @ epoch 4 new loss 0.0005361339426599443 old loss 0.0005376145127229393 BETTER
28_gate proxy err 0.004740102216601372 tr(WHW.T) 8693.26171875
  0%|          | 0/112 [00:00<?, ?it/s]W0315 06:48:37.253254 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  1%|          | 1/112 [00:00<01:39,  1.12it/s]  2%|▏         | 2/112 [00:01<01:05,  1.68it/s]  3%|▎         | 3/112 [00:01<00:54,  2.00it/s]  4%|▎         | 4/112 [00:02<00:48,  2.22it/s]  4%|▍         | 5/112 [00:02<00:46,  2.31it/s]  5%|▌         | 6/112 [00:02<00:43,  2.42it/s]29_gate proxy err 0.004286368377506733 tr(WHW.T) 9691.478515625
  0%|          | 0/112 [00:00<?, ?it/s]  6%|▋         | 7/112 [00:03<00:42,  2.48it/s]  7%|▋         | 8/112 [00:03<00:41,  2.52it/s]  1%|          | 1/112 [00:00<01:36,  1.15it/s]  8%|▊         | 9/112 [00:03<00:40,  2.55it/s]  2%|▏         | 2/112 [00:01<01:03,  1.73it/s]  9%|▉         | 10/112 [00:04<00:39,  2.59it/s]  3%|▎         | 3/112 [00:01<00:53,  2.04it/s] 10%|▉         | 11/112 [00:04<00:38,  2.61it/s]  4%|▎         | 4/112 [00:02<00:48,  2.24it/s] 11%|█         | 12/112 [00:05<00:38,  2.62it/s]  4%|▍         | 5/112 [00:02<00:45,  2.37it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.64it/s]  5%|▌         | 6/112 [00:02<00:43,  2.44it/s] 13%|█▎        | 15/112 [00:06<00:36,  2.65it/s]  6%|▋         | 7/112 [00:03<00:41,  2.51it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.66it/s]  7%|▋         | 8/112 [00:03<00:40,  2.54it/s] 15%|█▌        | 17/112 [00:06<00:35,  2.65it/s]  8%|▊         | 9/112 [00:03<00:40,  2.56it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.65it/s]  9%|▉         | 10/112 [00:04<00:39,  2.58it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.65it/s] 10%|▉         | 11/112 [00:04<00:38,  2.60it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.62it/s] 11%|█         | 12/112 [00:05<00:38,  2.61it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.64it/s] 12%|█▏        | 13/112 [00:05<00:37,  2.62it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.64it/s] 12%|█▎        | 14/112 [00:05<00:37,  2.62it/s] 21%|██        | 23/112 [00:09<00:33,  2.64it/s] 13%|█▎        | 15/112 [00:06<00:37,  2.61it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.65it/s] 14%|█▍        | 16/112 [00:06<00:36,  2.62it/s] 22%|██▏       | 25/112 [00:09<00:32,  2.65it/s] 15%|█▌        | 17/112 [00:06<00:36,  2.62it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.66it/s] 16%|█▌        | 18/112 [00:07<00:35,  2.61it/s] 24%|██▍       | 27/112 [00:10<00:31,  2.66it/s] 17%|█▋        | 19/112 [00:07<00:35,  2.59it/s] 25%|██▌       | 28/112 [00:11<00:31,  2.66it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.60it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.66it/s] 19%|█▉        | 21/112 [00:08<00:34,  2.60it/s] 27%|██▋       | 30/112 [00:11<00:30,  2.65it/s] 20%|█▉        | 22/112 [00:08<00:34,  2.60it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.65it/s] 21%|██        | 23/112 [00:09<00:34,  2.61it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.64it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.62it/s] 29%|██▉       | 33/112 [00:13<00:29,  2.64it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.62it/s] 30%|███       | 34/112 [00:13<00:29,  2.61it/s] 23%|██▎       | 26/112 [00:10<00:32,  2.62it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.62it/s] 24%|██▍       | 27/112 [00:10<00:32,  2.62it/s] 32%|███▏      | 36/112 [00:14<00:28,  2.63it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.62it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.64it/s] 26%|██▌       | 29/112 [00:11<00:31,  2.62it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.64it/s] 27%|██▋       | 30/112 [00:11<00:31,  2.62it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.65it/s] 28%|██▊       | 31/112 [00:12<00:30,  2.62it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.64it/s] 29%|██▊       | 32/112 [00:12<00:30,  2.62it/s] 37%|███▋      | 41/112 [00:16<00:26,  2.65it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.65it/s] 30%|███       | 34/112 [00:13<00:29,  2.60it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.65it/s] 31%|███▏      | 35/112 [00:13<00:29,  2.61it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.65it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.61it/s] 40%|████      | 45/112 [00:17<00:25,  2.64it/s] 33%|███▎      | 37/112 [00:14<00:28,  2.62it/s] 41%|████      | 46/112 [00:17<00:24,  2.64it/s] 34%|███▍      | 38/112 [00:14<00:28,  2.63it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.62it/s] 35%|███▍      | 39/112 [00:15<00:27,  2.63it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.63it/s] 36%|███▌      | 40/112 [00:15<00:27,  2.63it/s] 44%|████▍     | 49/112 [00:19<00:23,  2.64it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.63it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.64it/s] 38%|███▊      | 42/112 [00:16<00:26,  2.62it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.65it/s] 38%|███▊      | 43/112 [00:16<00:26,  2.63it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.66it/s] 39%|███▉      | 44/112 [00:17<00:25,  2.62it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.66it/s] 40%|████      | 45/112 [00:17<00:25,  2.62it/s] 48%|████▊     | 54/112 [00:20<00:21,  2.66it/s] 41%|████      | 46/112 [00:18<00:25,  2.59it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.66it/s] 42%|████▏     | 47/112 [00:18<00:24,  2.61it/s] 50%|█████     | 56/112 [00:21<00:21,  2.66it/s] 43%|████▎     | 48/112 [00:18<00:24,  2.62it/s] 51%|█████     | 57/112 [00:22<00:20,  2.65it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.62it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.65it/s] 45%|████▍     | 50/112 [00:19<00:23,  2.62it/s] 53%|█████▎    | 59/112 [00:22<00:20,  2.65it/s] 46%|████▌     | 51/112 [00:19<00:23,  2.63it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.61it/s] 46%|████▋     | 52/112 [00:20<00:22,  2.62it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.62it/s] 47%|████▋     | 53/112 [00:20<00:22,  2.63it/s] 55%|█████▌    | 62/112 [00:23<00:18,  2.63it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.63it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.64it/s] 49%|████▉     | 55/112 [00:21<00:21,  2.63it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.65it/s] 50%|█████     | 56/112 [00:21<00:21,  2.63it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.65it/s] 51%|█████     | 57/112 [00:22<00:20,  2.62it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.65it/s] 52%|█████▏    | 58/112 [00:22<00:20,  2.63it/s] 60%|█████▉    | 67/112 [00:25<00:16,  2.65it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.60it/s] 61%|██████    | 68/112 [00:26<00:16,  2.65it/s] 54%|█████▎    | 60/112 [00:23<00:19,  2.62it/s]I0315 06:49:03.519548 401669 finetune.py:68] layer 30_gate @ epoch 3 new loss 0.0009441269212402403 old loss 0.0009486862691119313 BETTER
 62%|██████▏   | 69/112 [00:26<00:16,  2.65it/s] 54%|█████▍    | 61/112 [00:23<00:19,  2.62it/s] 62%|██████▎   | 70/112 [00:27<00:15,  2.65it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.62it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.64it/s] 56%|█████▋    | 63/112 [00:24<00:18,  2.62it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.65it/s] 57%|█████▋    | 64/112 [00:24<00:18,  2.63it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.63it/s] 58%|█████▊    | 65/112 [00:25<00:17,  2.63it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.60it/s] 59%|█████▉    | 66/112 [00:25<00:17,  2.63it/s] 67%|██████▋   | 75/112 [00:28<00:14,  2.62it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.63it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.62it/s] 61%|██████    | 68/112 [00:26<00:16,  2.63it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.63it/s] 62%|██████▏   | 69/112 [00:26<00:16,  2.63it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.64it/s]I0315 06:49:07.155333 403307 finetune.py:68] layer 31_gate @ epoch 2 new loss 0.0026345686055719852 old loss 0.0026657739654183388 BETTER
 62%|██████▎   | 70/112 [00:27<00:15,  2.63it/s] 71%|███████   | 79/112 [00:30<00:12,  2.65it/s] 63%|██████▎   | 71/112 [00:27<00:15,  2.63it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.66it/s] 64%|██████▍   | 72/112 [00:27<00:15,  2.63it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.65it/s] 65%|██████▌   | 73/112 [00:28<00:14,  2.61it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.65it/s] 66%|██████▌   | 74/112 [00:28<00:14,  2.62it/s] 74%|███████▍  | 83/112 [00:31<00:10,  2.65it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.62it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.65it/s] 68%|██████▊   | 76/112 [00:29<00:13,  2.63it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.64it/s] 69%|██████▉   | 77/112 [00:29<00:13,  2.63it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.64it/s] 70%|██████▉   | 78/112 [00:30<00:12,  2.64it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.62it/s] 71%|███████   | 79/112 [00:30<00:12,  2.64it/s] 79%|███████▊  | 88/112 [00:33<00:09,  2.64it/s] 71%|███████▏  | 80/112 [00:30<00:12,  2.64it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.65it/s] 72%|███████▏  | 81/112 [00:31<00:11,  2.65it/s] 80%|████████  | 90/112 [00:34<00:08,  2.65it/s] 73%|███████▎  | 82/112 [00:31<00:11,  2.65it/s] 81%|████████▏ | 91/112 [00:34<00:07,  2.66it/s] 74%|███████▍  | 83/112 [00:32<00:10,  2.65it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.66it/s] 75%|███████▌  | 84/112 [00:32<00:10,  2.64it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.66it/s] 76%|███████▌  | 85/112 [00:32<00:10,  2.65it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.67it/s] 77%|███████▋  | 86/112 [00:33<00:09,  2.62it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.67it/s] 78%|███████▊  | 87/112 [00:33<00:09,  2.62it/s] 86%|████████▌ | 96/112 [00:36<00:05,  2.68it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.63it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.68it/s] 79%|███████▉  | 89/112 [00:34<00:08,  2.64it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.67it/s] 80%|████████  | 90/112 [00:34<00:08,  2.64it/s] 88%|████████▊ | 99/112 [00:37<00:04,  2.66it/s] 81%|████████▏ | 91/112 [00:35<00:07,  2.65it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.64it/s] 82%|████████▏ | 92/112 [00:35<00:07,  2.65it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.65it/s] 83%|████████▎ | 93/112 [00:35<00:07,  2.65it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.66it/s] 84%|████████▍ | 94/112 [00:36<00:06,  2.65it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.67it/s] 85%|████████▍ | 95/112 [00:36<00:06,  2.64it/s] 93%|█████████▎| 104/112 [00:39<00:02,  2.67it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.63it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.68it/s] 87%|████████▋ | 97/112 [00:37<00:05,  2.63it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.67it/s] 88%|████████▊ | 98/112 [00:37<00:05,  2.63it/s] 96%|█████████▌| 107/112 [00:40<00:01,  2.67it/s] 88%|████████▊ | 99/112 [00:38<00:04,  2.60it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.66it/s] 89%|████████▉ | 100/112 [00:38<00:04,  2.61it/s] 97%|█████████▋| 109/112 [00:41<00:01,  2.66it/s] 90%|█████████ | 101/112 [00:38<00:04,  2.62it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.67it/s] 91%|█████████ | 102/112 [00:39<00:03,  2.64it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.68it/s] 92%|█████████▏| 103/112 [00:39<00:03,  2.64it/s]100%|██████████| 112/112 [00:42<00:00,  2.68it/s]100%|██████████| 112/112 [00:42<00:00,  2.62it/s]
 93%|█████████▎| 104/112 [00:40<00:03,  2.64it/s] 94%|█████████▍| 105/112 [00:40<00:02,  2.64it/s] 95%|█████████▍| 106/112 [00:40<00:02,  2.64it/s] 96%|█████████▌| 107/112 [00:41<00:01,  2.64it/s] 96%|█████████▋| 108/112 [00:41<00:01,  2.62it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.62it/s] 98%|█████████▊| 110/112 [00:42<00:00,  2.62it/s] 99%|█████████▉| 111/112 [00:42<00:00,  2.62it/s]100%|██████████| 112/112 [00:43<00:00,  2.63it/s]100%|██████████| 112/112 [00:43<00:00,  2.60it/s]
W0315 06:49:27.243000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.243000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.243000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.243000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.243000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.243000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.244000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.286000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.286000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.286000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.286000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.286000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.302000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.302000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.303000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.303000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.303000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.477000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.477000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.477000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.477000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.478000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.812000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.812000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.812000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.812000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.812000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.812000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.813000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.845000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.845000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.845000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.845000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.845000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.919000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.919000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.919000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.919000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:27.919000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.130000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.136000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.143000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.143000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.610000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.610000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.610000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.611000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.611000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.611000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.611000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.643000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.643000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.643000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.643000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.643000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.996000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.996000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.996000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.997000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.997000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.997000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.997000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:29.997000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.301000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.301000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.301000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.301000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.301000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.488000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.489000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.489000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.489000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.489000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.489000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.489000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.532000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.532000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.532000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.533000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.533000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.549000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.549000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.549000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.549000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.549000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.644000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.649000 140565396682560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.724000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.724000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.724000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.724000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:30.724000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.058000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.058000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.058000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.058000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.059000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.059000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.059000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.090000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.091000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.091000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.091000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.091000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.164000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.164000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.164000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.164000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:31.164000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.384000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.396000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.404000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.405000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.862000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.862000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.863000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.863000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.863000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.863000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.863000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.894000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.894000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.894000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.894000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:32.894000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.254000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.254000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.255000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.255000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.255000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.255000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.255000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.255000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.562000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.562000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.562000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.562000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.563000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.913000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:49:33.918000 139945014863680 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 06:49:36.198358 401669 finetune.py:68] layer 30_gate @ epoch 4 new loss 0.0009398147813044488 old loss 0.0009441269212402403 BETTER
W0315 06:49:37.562608 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

I0315 06:49:37.838691 398602 finetune.py:45] layer 28_down initial loss 0.000592113530728966
W0315 06:49:37.839072 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:49:39.231005 403307 finetune.py:68] layer 31_gate @ epoch 3 new loss 0.0026045474223792553 old loss 0.0026345686055719852 BETTER
30_gate proxy err 0.003354730550199747 tr(WHW.T) 13188.3212890625
  0%|          | 0/112 [00:00<?, ?it/s]I0315 06:49:41.098256 400113 finetune.py:45] layer 29_down initial loss 0.0007959938375279307
W0315 06:49:41.098640 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

  1%|          | 1/112 [00:00<01:43,  1.07it/s]  2%|▏         | 2/112 [00:01<01:07,  1.63it/s]  3%|▎         | 3/112 [00:01<00:55,  1.96it/s]  4%|▎         | 4/112 [00:02<00:49,  2.17it/s]  4%|▍         | 5/112 [00:02<00:46,  2.31it/s]  5%|▌         | 6/112 [00:02<00:44,  2.40it/s]  6%|▋         | 7/112 [00:03<00:42,  2.46it/s]  7%|▋         | 8/112 [00:03<00:41,  2.50it/s]  8%|▊         | 9/112 [00:04<00:40,  2.53it/s]  9%|▉         | 10/112 [00:04<00:39,  2.55it/s] 10%|▉         | 11/112 [00:04<00:44,  2.27it/s] 11%|█         | 12/112 [00:05<00:42,  2.37it/s] 12%|█▏        | 13/112 [00:05<00:40,  2.43it/s] 12%|█▎        | 14/112 [00:06<00:40,  2.44it/s] 13%|█▎        | 15/112 [00:06<00:39,  2.48it/s] 14%|█▍        | 16/112 [00:06<00:38,  2.52it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.54it/s] 16%|█▌        | 18/112 [00:07<00:36,  2.55it/s] 17%|█▋        | 19/112 [00:08<00:36,  2.56it/s] 18%|█▊        | 20/112 [00:08<00:35,  2.58it/s] 19%|█▉        | 21/112 [00:08<00:35,  2.58it/s] 20%|█▉        | 22/112 [00:09<00:34,  2.59it/s] 21%|██        | 23/112 [00:09<00:34,  2.60it/s] 21%|██▏       | 24/112 [00:09<00:33,  2.59it/s] 22%|██▏       | 25/112 [00:10<00:33,  2.59it/s] 23%|██▎       | 26/112 [00:10<00:33,  2.59it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.54it/s] 25%|██▌       | 28/112 [00:11<00:32,  2.55it/s] 26%|██▌       | 29/112 [00:11<00:32,  2.56it/s] 27%|██▋       | 30/112 [00:12<00:31,  2.57it/s] 28%|██▊       | 31/112 [00:12<00:31,  2.58it/s] 29%|██▊       | 32/112 [00:13<00:30,  2.59it/s] 29%|██▉       | 33/112 [00:13<00:30,  2.59it/s] 30%|███       | 34/112 [00:13<00:30,  2.60it/s] 31%|███▏      | 35/112 [00:14<00:29,  2.60it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.60it/s] 33%|███▎      | 37/112 [00:15<00:28,  2.60it/s] 34%|███▍      | 38/112 [00:15<00:28,  2.60it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.59it/s] 36%|███▌      | 40/112 [00:16<00:27,  2.58it/s] 37%|███▋      | 41/112 [00:16<00:27,  2.58it/s] 38%|███▊      | 42/112 [00:16<00:27,  2.54it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.55it/s] 39%|███▉      | 44/112 [00:17<00:26,  2.56it/s] 40%|████      | 45/112 [00:18<00:26,  2.57it/s] 41%|████      | 46/112 [00:18<00:25,  2.58it/s] 42%|████▏     | 47/112 [00:18<00:25,  2.58it/s] 43%|████▎     | 48/112 [00:19<00:24,  2.58it/s] 44%|████▍     | 49/112 [00:19<00:24,  2.59it/s] 45%|████▍     | 50/112 [00:20<00:23,  2.59it/s] 46%|████▌     | 51/112 [00:20<00:23,  2.59it/s] 46%|████▋     | 52/112 [00:20<00:23,  2.59it/s] 47%|████▋     | 53/112 [00:21<00:22,  2.59it/s] 48%|████▊     | 54/112 [00:21<00:22,  2.55it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.56it/s] 50%|█████     | 56/112 [00:22<00:21,  2.57it/s] 51%|█████     | 57/112 [00:22<00:21,  2.58it/s] 52%|█████▏    | 58/112 [00:23<00:20,  2.58it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.58it/s] 54%|█████▎    | 60/112 [00:23<00:20,  2.58it/s] 54%|█████▍    | 61/112 [00:24<00:19,  2.59it/s] 55%|█████▌    | 62/112 [00:24<00:19,  2.59it/s] 56%|█████▋    | 63/112 [00:25<00:18,  2.59it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.59it/s] 58%|█████▊    | 65/112 [00:25<00:18,  2.59it/s] 59%|█████▉    | 66/112 [00:26<00:17,  2.59it/s] 60%|█████▉    | 67/112 [00:26<00:17,  2.59it/s] 61%|██████    | 68/112 [00:27<00:17,  2.58it/s] 62%|██████▏   | 69/112 [00:27<00:16,  2.56it/s] 62%|██████▎   | 70/112 [00:27<00:16,  2.57it/s] 63%|██████▎   | 71/112 [00:28<00:15,  2.57it/s] 64%|██████▍   | 72/112 [00:28<00:15,  2.57it/s]I0315 06:50:09.906706 398602 finetune.py:68] layer 28_down @ epoch 0 new loss 0.0005918595707044005 old loss 0.000592113530728966 BETTER
 65%|██████▌   | 73/112 [00:28<00:15,  2.58it/s] 66%|██████▌   | 74/112 [00:29<00:14,  2.59it/s] 67%|██████▋   | 75/112 [00:29<00:14,  2.60it/s] 68%|██████▊   | 76/112 [00:30<00:13,  2.60it/s]I0315 06:50:11.350582 403307 finetune.py:68] layer 31_gate @ epoch 4 new loss 0.002577953739091754 old loss 0.0026045474223792553 BETTER
 69%|██████▉   | 77/112 [00:30<00:13,  2.60it/s]I0315 06:50:11.838474 400113 finetune.py:68] layer 29_down @ epoch 0 new loss 0.0007955662440508604 old loss 0.0007959938375279307 BETTER
 70%|██████▉   | 78/112 [00:30<00:13,  2.60it/s] 71%|███████   | 79/112 [00:31<00:12,  2.60it/s]W0315 06:50:12.595663 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:120: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

 71%|███████▏  | 80/112 [00:31<00:12,  2.59it/s] 72%|███████▏  | 81/112 [00:32<00:11,  2.59it/s] 73%|███████▎  | 82/112 [00:32<00:11,  2.60it/s] 74%|███████▍  | 83/112 [00:32<00:11,  2.58it/s] 75%|███████▌  | 84/112 [00:33<00:10,  2.58it/s] 76%|███████▌  | 85/112 [00:33<00:10,  2.59it/s] 77%|███████▋  | 86/112 [00:33<00:10,  2.59it/s] 78%|███████▊  | 87/112 [00:34<00:09,  2.59it/s] 79%|███████▊  | 88/112 [00:34<00:09,  2.59it/s]31_gate proxy err 0.0014670067466795444 tr(WHW.T) 25609.76171875
  0%|          | 0/112 [00:00<?, ?it/s] 79%|███████▉  | 89/112 [00:35<00:08,  2.59it/s] 80%|████████  | 90/112 [00:35<00:08,  2.59it/s] 81%|████████▏ | 91/112 [00:35<00:08,  2.59it/s]  1%|          | 1/112 [00:00<01:38,  1.13it/s] 82%|████████▏ | 92/112 [00:36<00:07,  2.59it/s]  2%|▏         | 2/112 [00:01<01:05,  1.67it/s] 83%|████████▎ | 93/112 [00:36<00:07,  2.59it/s]  3%|▎         | 3/112 [00:01<00:55,  1.98it/s] 84%|████████▍ | 94/112 [00:37<00:06,  2.58it/s]  4%|▎         | 4/112 [00:02<00:50,  2.16it/s] 85%|████████▍ | 95/112 [00:37<00:06,  2.58it/s]  4%|▍         | 5/112 [00:02<00:47,  2.28it/s] 86%|████████▌ | 96/112 [00:37<00:06,  2.56it/s]  5%|▌         | 6/112 [00:02<00:44,  2.36it/s] 87%|████████▋ | 97/112 [00:38<00:05,  2.57it/s]  6%|▋         | 7/112 [00:03<00:43,  2.41it/s] 88%|████████▊ | 98/112 [00:38<00:05,  2.58it/s]  7%|▋         | 8/112 [00:03<00:42,  2.44it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.58it/s]  8%|▊         | 9/112 [00:04<00:42,  2.45it/s] 89%|████████▉ | 100/112 [00:39<00:04,  2.59it/s]  9%|▉         | 10/112 [00:04<00:41,  2.47it/s] 90%|█████████ | 101/112 [00:39<00:04,  2.60it/s] 10%|▉         | 11/112 [00:04<00:40,  2.49it/s] 91%|█████████ | 102/112 [00:40<00:03,  2.62it/s] 11%|█         | 12/112 [00:05<00:39,  2.50it/s] 92%|█████████▏| 103/112 [00:40<00:03,  2.62it/s] 12%|█▏        | 13/112 [00:05<00:39,  2.51it/s] 93%|█████████▎| 104/112 [00:40<00:03,  2.63it/s] 12%|█▎        | 14/112 [00:06<00:39,  2.50it/s] 94%|█████████▍| 105/112 [00:41<00:02,  2.61it/s] 13%|█▎        | 15/112 [00:06<00:38,  2.49it/s] 95%|█████████▍| 106/112 [00:41<00:02,  2.61it/s] 14%|█▍        | 16/112 [00:06<00:38,  2.50it/s] 96%|█████████▌| 107/112 [00:42<00:01,  2.60it/s] 15%|█▌        | 17/112 [00:07<00:37,  2.51it/s] 96%|█████████▋| 108/112 [00:42<00:01,  2.60it/s] 16%|█▌        | 18/112 [00:07<00:37,  2.51it/s] 97%|█████████▋| 109/112 [00:42<00:01,  2.60it/s] 17%|█▋        | 19/112 [00:08<00:36,  2.52it/s] 98%|█████████▊| 110/112 [00:43<00:00,  2.56it/s] 18%|█▊        | 20/112 [00:08<00:36,  2.52it/s] 99%|█████████▉| 111/112 [00:43<00:00,  2.59it/s] 19%|█▉        | 21/112 [00:08<00:36,  2.52it/s]100%|██████████| 112/112 [00:44<00:00,  2.59it/s]100%|██████████| 112/112 [00:44<00:00,  2.54it/s]
 20%|█▉        | 22/112 [00:09<00:35,  2.52it/s] 21%|██        | 23/112 [00:09<00:35,  2.52it/s] 21%|██▏       | 24/112 [00:10<00:34,  2.52it/s] 22%|██▏       | 25/112 [00:10<00:34,  2.51it/s] 23%|██▎       | 26/112 [00:10<00:34,  2.51it/s] 24%|██▍       | 27/112 [00:11<00:33,  2.51it/s] 25%|██▌       | 28/112 [00:11<00:33,  2.48it/s] 26%|██▌       | 29/112 [00:12<00:33,  2.50it/s] 27%|██▋       | 30/112 [00:12<00:32,  2.51it/s] 28%|██▊       | 31/112 [00:12<00:32,  2.51it/s] 29%|██▊       | 32/112 [00:13<00:31,  2.52it/s] 29%|██▉       | 33/112 [00:13<00:31,  2.53it/s] 30%|███       | 34/112 [00:14<00:30,  2.54it/s] 31%|███▏      | 35/112 [00:14<00:30,  2.53it/s] 32%|███▏      | 36/112 [00:14<00:29,  2.54it/s] 33%|███▎      | 37/112 [00:15<00:29,  2.53it/s] 34%|███▍      | 38/112 [00:15<00:29,  2.52it/s] 35%|███▍      | 39/112 [00:15<00:28,  2.52it/s]W0315 06:50:32.231000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.231000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.231000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.231000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.232000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.232000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.232000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.275000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.275000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.275000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.275000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.275000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.291000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.291000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.291000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.291000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.291000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
 36%|███▌      | 40/112 [00:16<00:28,  2.51it/s]W0315 06:50:32.464000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.464000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.464000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.464000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.464000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.796000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.796000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.796000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.796000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.796000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.796000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.797000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.832000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.832000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.832000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.832000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.832000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 37%|███▋      | 41/112 [00:16<00:28,  2.48it/s]W0315 06:50:32.908000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.908000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.908000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.908000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:32.908000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
 38%|███▊      | 42/112 [00:17<00:28,  2.49it/s] 38%|███▊      | 43/112 [00:17<00:27,  2.50it/s] 39%|███▉      | 44/112 [00:17<00:27,  2.50it/s]W0315 06:50:34.131000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.138000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.144000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.144000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
 40%|████      | 45/112 [00:18<00:26,  2.51it/s]W0315 06:50:34.601000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.601000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.601000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.601000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.602000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.602000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.602000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.647000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.647000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.647000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.647000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:34.647000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 41%|████      | 46/112 [00:18<00:26,  2.51it/s]W0315 06:50:35.002000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.002000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.002000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.002000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.002000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.002000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.002000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.003000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 42%|████▏     | 47/112 [00:19<00:25,  2.51it/s]W0315 06:50:35.318000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.318000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.318000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.318000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.318000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
 43%|████▎     | 48/112 [00:19<00:25,  2.52it/s]W0315 06:50:35.667000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:50:35.673000 140502570706752 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
 44%|████▍     | 49/112 [00:19<00:25,  2.52it/s] 45%|████▍     | 50/112 [00:20<00:24,  2.52it/s] 46%|████▌     | 51/112 [00:20<00:24,  2.51it/s] 46%|████▋     | 52/112 [00:21<00:23,  2.51it/s] 47%|████▋     | 53/112 [00:21<00:23,  2.52it/s] 48%|████▊     | 54/112 [00:21<00:23,  2.49it/s] 49%|████▉     | 55/112 [00:22<00:22,  2.50it/s] 50%|█████     | 56/112 [00:22<00:22,  2.51it/s] 51%|█████     | 57/112 [00:23<00:21,  2.52it/s] 52%|█████▏    | 58/112 [00:23<00:21,  2.53it/s] 53%|█████▎    | 59/112 [00:23<00:20,  2.53it/s] 54%|█████▎    | 60/112 [00:24<00:20,  2.54it/s] 54%|█████▍    | 61/112 [00:24<00:20,  2.54it/s] 55%|█████▌    | 62/112 [00:25<00:19,  2.54it/s] 56%|█████▋    | 63/112 [00:25<00:19,  2.54it/s] 57%|█████▋    | 64/112 [00:25<00:18,  2.54it/s] 58%|█████▊    | 65/112 [00:26<00:18,  2.54it/s]I0315 06:50:42.753410 401669 finetune.py:45] layer 30_down initial loss 0.001326761324889958
W0315 06:50:42.753792 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

 59%|█████▉    | 66/112 [00:26<00:18,  2.53it/s]I0315 06:50:42.873218 398602 finetune.py:68] layer 28_down @ epoch 1 new loss 0.0005918144597671926 old loss 0.0005918595707044005 BETTER
 60%|█████▉    | 67/112 [00:27<00:17,  2.52it/s]I0315 06:50:43.487580 400113 finetune.py:68] layer 29_down @ epoch 1 new loss 0.0007954631000757217 old loss 0.0007955662440508604 BETTER
 61%|██████    | 68/112 [00:27<00:17,  2.49it/s] 62%|██████▏   | 69/112 [00:27<00:17,  2.50it/s] 62%|██████▎   | 70/112 [00:28<00:16,  2.51it/s] 63%|██████▎   | 71/112 [00:28<00:16,  2.51it/s] 64%|██████▍   | 72/112 [00:29<00:15,  2.51it/s] 65%|██████▌   | 73/112 [00:29<00:15,  2.51it/s] 66%|██████▌   | 74/112 [00:29<00:15,  2.52it/s] 67%|██████▋   | 75/112 [00:30<00:14,  2.52it/s] 68%|██████▊   | 76/112 [00:30<00:14,  2.52it/s] 69%|██████▉   | 77/112 [00:31<00:13,  2.52it/s] 70%|██████▉   | 78/112 [00:31<00:13,  2.52it/s] 71%|███████   | 79/112 [00:31<00:13,  2.53it/s] 71%|███████▏  | 80/112 [00:32<00:12,  2.52it/s] 72%|███████▏  | 81/112 [00:32<00:12,  2.52it/s] 73%|███████▎  | 82/112 [00:33<00:12,  2.49it/s] 74%|███████▍  | 83/112 [00:33<00:11,  2.50it/s] 75%|███████▌  | 84/112 [00:33<00:11,  2.51it/s] 76%|███████▌  | 85/112 [00:34<00:10,  2.51it/s] 77%|███████▋  | 86/112 [00:34<00:10,  2.52it/s] 78%|███████▊  | 87/112 [00:35<00:09,  2.52it/s] 79%|███████▊  | 88/112 [00:35<00:09,  2.53it/s] 79%|███████▉  | 89/112 [00:35<00:09,  2.53it/s] 80%|████████  | 90/112 [00:36<00:08,  2.53it/s] 81%|████████▏ | 91/112 [00:36<00:08,  2.52it/s] 82%|████████▏ | 92/112 [00:37<00:07,  2.52it/s] 83%|████████▎ | 93/112 [00:37<00:07,  2.52it/s] 84%|████████▍ | 94/112 [00:37<00:07,  2.50it/s] 85%|████████▍ | 95/112 [00:38<00:06,  2.51it/s] 86%|████████▌ | 96/112 [00:38<00:06,  2.52it/s] 87%|████████▋ | 97/112 [00:39<00:05,  2.52it/s] 88%|████████▊ | 98/112 [00:39<00:05,  2.52it/s] 88%|████████▊ | 99/112 [00:39<00:05,  2.53it/s] 89%|████████▉ | 100/112 [00:40<00:04,  2.53it/s] 90%|█████████ | 101/112 [00:40<00:04,  2.53it/s] 91%|█████████ | 102/112 [00:41<00:03,  2.53it/s] 92%|█████████▏| 103/112 [00:41<00:03,  2.53it/s] 93%|█████████▎| 104/112 [00:41<00:03,  2.53it/s] 94%|█████████▍| 105/112 [00:42<00:02,  2.52it/s] 95%|█████████▍| 106/112 [00:42<00:02,  2.52it/s] 96%|█████████▌| 107/112 [00:43<00:02,  2.49it/s] 96%|█████████▋| 108/112 [00:43<00:01,  2.51it/s] 97%|█████████▋| 109/112 [00:43<00:01,  2.51it/s] 98%|█████████▊| 110/112 [00:44<00:00,  2.52it/s] 99%|█████████▉| 111/112 [00:44<00:00,  2.51it/s]100%|██████████| 112/112 [00:44<00:00,  2.52it/s]100%|██████████| 112/112 [00:44<00:00,  2.49it/s]
W0315 06:51:08.459000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.459000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.460000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.460000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.460000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.460000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.460000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.505000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.506000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.506000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.506000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.506000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.522000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.522000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.522000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.522000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.522000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.701000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.701000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.701000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.701000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:08.702000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.040000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.041000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.041000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.041000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.041000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.041000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.041000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.080000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.080000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.080000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.080000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.080000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.154000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.155000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.155000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.155000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:09.155000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.400000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.406000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.430000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.431000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.899000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.899000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.899000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.899000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.899000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.900000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.900000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.931000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.931000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.931000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.931000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:51:10.931000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.291000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.291000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.291000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.291000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.291000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.292000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.292000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.292000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.602000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.603000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.603000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.603000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.603000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.954000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:51:11.960000 140433629386560 torch/fx/experimental/symbolic_shapes.py:4449] [1/2] ps5 is not in var_ranges, defaulting to unknown range.
I0315 06:51:13.255175 401669 finetune.py:68] layer 30_down @ epoch 0 new loss 0.0013259686529636383 old loss 0.001326761324889958 BETTER
I0315 06:51:14.758742 400113 finetune.py:68] layer 29_down @ epoch 2 new loss 0.0007953785825520754 old loss 0.0007954631000757217 BETTER
I0315 06:51:15.740427 398602 finetune.py:68] layer 28_down @ epoch 2 new loss 0.0005917848320677876 old loss 0.0005918144597671926 BETTER
I0315 06:51:18.965397 403307 finetune.py:45] layer 31_down initial loss 0.003377515822649002
W0315 06:51:18.965806 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:46: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler(enabled=(orig_dtype==torch.float16))

I0315 06:51:44.430228 401669 finetune.py:68] layer 30_down @ epoch 1 new loss 0.0013258142862468958 old loss 0.0013259686529636383 BETTER
I0315 06:51:46.066666 400113 finetune.py:68] layer 29_down @ epoch 3 new loss 0.0007953228778205812 old loss 0.0007953785825520754 BETTER
I0315 06:51:48.531563 398602 finetune.py:68] layer 28_down @ epoch 3 new loss 0.0005917555536143482 old loss 0.0005917848320677876 BETTER
I0315 06:51:48.798007 403307 finetune.py:68] layer 31_down @ epoch 0 new loss 0.003374353051185608 old loss 0.003377515822649002 BETTER
I0315 06:52:15.662363 401669 finetune.py:68] layer 30_down @ epoch 2 new loss 0.0013256388483569026 old loss 0.0013258142862468958 BETTER
I0315 06:52:17.374558 400113 finetune.py:68] layer 29_down @ epoch 4 new loss 0.000795231550000608 old loss 0.0007953228778205812 BETTER
W0315 06:52:18.249383 400113 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

29_down proxy err 0.008472810499370098 tr(WHW.T) 132.77667236328125
I0315 06:52:19.219354 403307 finetune.py:68] layer 31_down @ epoch 1 new loss 0.003373548621311784 old loss 0.003374353051185608 BETTER
I0315 06:52:21.694945 398602 finetune.py:68] layer 28_down @ epoch 4 new loss 0.00059171998873353 old loss 0.0005917555536143482 BETTER
W0315 06:52:22.423046 398602 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

28_down proxy err 0.01010475680232048 tr(WHW.T) 82.41047668457031
I0315 06:52:46.872602 401669 finetune.py:68] layer 30_down @ epoch 3 new loss 0.0013255920493975282 old loss 0.0013256388483569026 BETTER
I0315 06:52:49.815857 403307 finetune.py:68] layer 31_down @ epoch 2 new loss 0.0033731921575963497 old loss 0.003373548621311784 BETTER
I0315 06:53:18.090794 401669 finetune.py:68] layer 30_down @ epoch 4 new loss 0.0013254676014184952 old loss 0.0013255920493975282 BETTER
W0315 06:53:18.825085 401669 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

30_down proxy err 0.004578725900501013 tr(WHW.T) 369.75
I0315 06:53:20.617924 403307 finetune.py:68] layer 31_down @ epoch 3 new loss 0.0033724247477948666 old loss 0.0033731921575963497 BETTER
I0315 06:53:51.510559 403307 finetune.py:68] layer 31_down @ epoch 4 new loss 0.003371883649379015 old loss 0.0033724247477948666 BETTER
W0315 06:53:52.261869 403307 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune.py:323: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

31_down proxy err 0.0014107292518019676 tr(WHW.T) 2772.77685546875
I0315 06:54:23.055906 428838 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0315 06:54:23.056043 428838 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0315 06:54:23.056087 428838 utils.py:162] NumExpr defaulting to 16 threads.
I0315 06:54:23.238744 428838 config.py:58] PyTorch version 2.4.0 available.
W0315 06:54:25.048798 428838 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0315 06:54:25.049422 428838 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 3,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 128256
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  7.29it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  7.86it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.11it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.21it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.35it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.39it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.56it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.31it/s]
Some weights of the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/meta-llama--Meta-Llama-3-8B and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:00,  8.17it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:00,  8.07it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:00<00:00,  8.09it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:00<00:00,  8.19it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:00<00:00,  8.22it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:00<00:00,  8.26it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.39it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:00<00:00,  8.27it/s]
W0315 06:54:30.447021 428838 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0315 06:54:30.448361 428838 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0315 06:54:30.463028 428838 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0315 06:54:30.475255 428838 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0315 06:54:30.487842 428838 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0315 06:54:30.500142 428838 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0315 06:54:30.519427 428838 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0315 06:54:30.538353 428838 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0315 06:54:30.565612 428838 hfize_llama.py:113] loaded layer 0
I0315 06:54:30.672832 428838 hfize_llama.py:113] loaded layer 1
I0315 06:54:30.799921 428838 hfize_llama.py:113] loaded layer 2
I0315 06:54:30.914286 428838 hfize_llama.py:113] loaded layer 3
I0315 06:54:31.040044 428838 hfize_llama.py:113] loaded layer 4
I0315 06:54:31.149653 428838 hfize_llama.py:113] loaded layer 5
I0315 06:54:31.265196 428838 hfize_llama.py:113] loaded layer 6
I0315 06:54:31.391931 428838 hfize_llama.py:113] loaded layer 7
I0315 06:54:31.477214 428838 hfize_llama.py:113] loaded layer 8
I0315 06:54:31.603926 428838 hfize_llama.py:113] loaded layer 9
I0315 06:54:31.725571 428838 hfize_llama.py:113] loaded layer 10
I0315 06:54:31.853522 428838 hfize_llama.py:113] loaded layer 11
I0315 06:54:31.957640 428838 hfize_llama.py:113] loaded layer 12
I0315 06:54:32.087910 428838 hfize_llama.py:113] loaded layer 13
I0315 06:54:32.218006 428838 hfize_llama.py:113] loaded layer 14
I0315 06:54:32.326232 428838 hfize_llama.py:113] loaded layer 15
I0315 06:54:32.443269 428838 hfize_llama.py:113] loaded layer 16
I0315 06:54:32.579023 428838 hfize_llama.py:113] loaded layer 17
I0315 06:54:32.693296 428838 hfize_llama.py:113] loaded layer 18
I0315 06:54:32.823953 428838 hfize_llama.py:113] loaded layer 19
I0315 06:54:32.944686 428838 hfize_llama.py:113] loaded layer 20
I0315 06:54:33.058611 428838 hfize_llama.py:113] loaded layer 21
I0315 06:54:33.161706 428838 hfize_llama.py:113] loaded layer 22
I0315 06:54:33.285442 428838 hfize_llama.py:113] loaded layer 23
I0315 06:54:33.428046 428838 hfize_llama.py:113] loaded layer 24
I0315 06:54:33.564411 428838 hfize_llama.py:113] loaded layer 25
I0315 06:54:33.698337 428838 hfize_llama.py:113] loaded layer 26
I0315 06:54:33.827926 428838 hfize_llama.py:113] loaded layer 27
I0315 06:54:33.970843 428838 hfize_llama.py:113] loaded layer 28
I0315 06:54:34.067854 428838 hfize_llama.py:113] loaded layer 29
I0315 06:54:34.187988 428838 hfize_llama.py:113] loaded layer 30
I0315 06:54:34.319939 428838 hfize_llama.py:113] loaded layer 31
I0315 06:54:34.320038 428838 hfize_llama.py:115] saving model...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.31it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.00it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.66it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.73s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.04s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]
I0315 06:54:52.287098 428838 hfize_llama.py:122] successfully loaded hfized model
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  4.24it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.90it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.57it/s]
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.81s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]
  0%|          | 0/141 [00:00<?, ?it/s]W0315 06:55:35.478000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.479000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.479000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.479000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.479000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.479000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.479000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.505000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.505000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.505000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.505000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.505000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.521000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.521000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.522000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.522000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.522000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.831000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.831000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.831000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.831000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:35.831000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.692000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.692000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.692000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.692000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.693000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.693000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.693000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.710000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.711000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.711000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.711000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.711000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.956000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.956000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.957000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.957000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:36.957000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.514000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.514000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.514000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.514000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.514000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.514000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.515000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.533000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.533000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.533000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.533000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:38.533000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:39.417000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:39.417000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:39.417000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:39.417000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:39.417000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.136000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.136000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.137000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.137000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.137000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.137000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.137000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.164000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.164000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.164000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.164000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.164000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.179000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.179000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.179000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.179000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.179000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.328000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.328000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.328000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.328000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.328000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.542000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.542000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.542000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.542000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.542000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.543000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.543000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.562000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.562000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.562000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.562000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.562000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.624000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.624000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.624000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.624000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:45.625000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.442000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.735000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.735000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.735000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.735000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.735000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.735000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.736000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.755000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.755000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.755000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.755000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.755000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.999000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.999000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.999000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.999000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:46.999000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:47.251000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.075000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.075000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.075000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.075000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.075000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.076000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.076000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q3 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.114000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.115000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.115000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.115000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.115000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.130000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.130000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.130000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.130000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.130000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.292000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.292000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.292000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.292000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.292000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.607000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.607000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.607000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.607000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.608000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.608000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.608000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z3 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.710000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.710000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.710000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.710000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:54.710000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:55.850000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:55.856000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:55.861000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:55.862000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.286000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.286000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.286000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.287000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.287000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.287000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.287000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.313000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.313000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.313000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.313000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.314000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.641000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.925000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.926000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.926000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.926000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0315 06:55:56.926000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0315 06:55:57.240000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0315 06:55:57.245000 140423738083136 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps5 is not in var_ranges, defaulting to unknown range.
W0315 06:56:10.021491 429932 logging.py:328] Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)
avg_loss = 1.6642042398452759:   0%|          | 0/141 [00:36<?, ?it/s]avg_loss = 1.6642042398452759:   1%|          | 1/141 [00:36<1:26:04, 36.89s/it]avg_loss = 1.9537139534950256:   1%|          | 1/141 [00:37<1:26:04, 36.89s/it]avg_loss = 1.9537139534950256:   1%|▏         | 2/141 [00:37<36:05, 15.58s/it]  avg_loss = 2.0851603746414185:   1%|▏         | 2/141 [00:38<36:05, 15.58s/it]avg_loss = 2.0851603746414185:   2%|▏         | 3/141 [00:38<20:11,  8.78s/it]avg_loss = 2.034169226884842:   2%|▏         | 3/141 [00:38<20:11,  8.78s/it] avg_loss = 2.034169226884842:   3%|▎         | 4/141 [00:38<12:44,  5.58s/it]avg_loss = 1.9823277235031127:   3%|▎         | 4/141 [00:39<12:44,  5.58s/it]avg_loss = 1.9823277235031127:   4%|▎         | 5/141 [00:39<08:39,  3.82s/it]avg_loss = 1.8858495155970256:   4%|▎         | 5/141 [00:40<08:39,  3.82s/it]avg_loss = 1.8858495155970256:   4%|▍         | 6/141 [00:40<06:11,  2.75s/it]avg_loss = 1.8240670476640974:   4%|▍         | 6/141 [00:40<06:11,  2.75s/it]avg_loss = 1.8240670476640974:   5%|▍         | 7/141 [00:40<04:38,  2.08s/it]avg_loss = 1.8180546313524246:   5%|▍         | 7/141 [00:41<04:38,  2.08s/it]avg_loss = 1.8180546313524246:   6%|▌         | 8/141 [00:41<03:37,  1.64s/it]avg_loss = 1.8494906293021307:   6%|▌         | 8/141 [00:42<03:37,  1.64s/it]avg_loss = 1.8494906293021307:   6%|▋         | 9/141 [00:42<02:56,  1.34s/it]avg_loss = 1.850603199005127:   6%|▋         | 9/141 [00:43<02:56,  1.34s/it] avg_loss = 1.850603199005127:   7%|▋         | 10/141 [00:43<02:29,  1.14s/it]avg_loss = 1.8463055654005571:   7%|▋         | 10/141 [00:43<02:29,  1.14s/it]avg_loss = 1.8463055654005571:   8%|▊         | 11/141 [00:43<02:10,  1.00s/it]avg_loss = 1.8673532009124756:   8%|▊         | 11/141 [00:44<02:10,  1.00s/it]avg_loss = 1.8673532009124756:   9%|▊         | 12/141 [00:44<01:57,  1.10it/s]avg_loss = 1.8791354436140795:   9%|▊         | 12/141 [00:45<01:57,  1.10it/s]avg_loss = 1.8791354436140795:   9%|▉         | 13/141 [00:45<01:47,  1.19it/s]avg_loss = 1.895330207688468:   9%|▉         | 13/141 [00:45<01:47,  1.19it/s] avg_loss = 1.895330207688468:  10%|▉         | 14/141 [00:45<01:41,  1.25it/s]avg_loss = 1.9041757265726724:  10%|▉         | 14/141 [00:46<01:41,  1.25it/s]avg_loss = 1.9041757265726724:  11%|█         | 15/141 [00:46<01:36,  1.31it/s]avg_loss = 1.927129864692688:  11%|█         | 15/141 [00:47<01:36,  1.31it/s] avg_loss = 1.927129864692688:  11%|█▏        | 16/141 [00:47<01:32,  1.35it/s]avg_loss = 1.9302443336038029:  11%|█▏        | 16/141 [00:47<01:32,  1.35it/s]avg_loss = 1.9302443336038029:  12%|█▏        | 17/141 [00:47<01:30,  1.37it/s]avg_loss = 1.9323137005170186:  12%|█▏        | 17/141 [00:48<01:30,  1.37it/s]avg_loss = 1.9323137005170186:  13%|█▎        | 18/141 [00:48<01:28,  1.39it/s]avg_loss = 1.919153138210899:  13%|█▎        | 18/141 [00:49<01:28,  1.39it/s] avg_loss = 1.919153138210899:  13%|█▎        | 19/141 [00:49<01:27,  1.40it/s]avg_loss = 1.918418288230896:  13%|█▎        | 19/141 [00:50<01:27,  1.40it/s]avg_loss = 1.918418288230896:  14%|█▍        | 20/141 [00:50<01:26,  1.41it/s]avg_loss = 1.9229744161878313:  14%|█▍        | 20/141 [00:50<01:26,  1.41it/s]avg_loss = 1.9229744161878313:  15%|█▍        | 21/141 [00:50<01:25,  1.41it/s]avg_loss = 1.9254057082262905:  15%|█▍        | 21/141 [00:51<01:25,  1.41it/s]avg_loss = 1.9254057082262905:  16%|█▌        | 22/141 [00:51<01:24,  1.41it/s]avg_loss = 1.9270318228265513:  16%|█▌        | 22/141 [00:52<01:24,  1.41it/s]avg_loss = 1.9270318228265513:  16%|█▋        | 23/141 [00:52<01:23,  1.42it/s]avg_loss = 1.9312302023172379:  16%|█▋        | 23/141 [00:52<01:23,  1.42it/s]avg_loss = 1.9312302023172379:  17%|█▋        | 24/141 [00:52<01:22,  1.42it/s]avg_loss = 1.937165207862854:  17%|█▋        | 24/141 [00:53<01:22,  1.42it/s] avg_loss = 1.937165207862854:  18%|█▊        | 25/141 [00:53<01:21,  1.42it/s]avg_loss = 1.9490309632741487:  18%|█▊        | 25/141 [00:54<01:21,  1.42it/s]avg_loss = 1.9490309632741487:  18%|█▊        | 26/141 [00:54<01:21,  1.42it/s]avg_loss = 1.9612192180421617:  18%|█▊        | 26/141 [00:54<01:21,  1.42it/s]avg_loss = 1.9612192180421617:  19%|█▉        | 27/141 [00:54<01:20,  1.42it/s]avg_loss = 1.967007692371096:  19%|█▉        | 27/141 [00:55<01:20,  1.42it/s] avg_loss = 1.967007692371096:  20%|█▉        | 28/141 [00:55<01:19,  1.41it/s]avg_loss = 1.9634089880976184:  20%|█▉        | 28/141 [00:56<01:19,  1.41it/s]avg_loss = 1.9634089880976184:  21%|██        | 29/141 [00:56<01:19,  1.42it/s]avg_loss = 1.9534653504689534:  21%|██        | 29/141 [00:57<01:19,  1.42it/s]avg_loss = 1.9534653504689534:  21%|██▏       | 30/141 [00:57<01:18,  1.42it/s]avg_loss = 1.9400150122181061:  21%|██▏       | 30/141 [00:57<01:18,  1.42it/s]avg_loss = 1.9400150122181061:  22%|██▏       | 31/141 [00:57<01:17,  1.42it/s]avg_loss = 1.9289407655596733:  22%|██▏       | 31/141 [00:58<01:17,  1.42it/s]avg_loss = 1.9289407655596733:  23%|██▎       | 32/141 [00:58<01:16,  1.42it/s]avg_loss = 1.9278453371741555:  23%|██▎       | 32/141 [00:59<01:16,  1.42it/s]avg_loss = 1.9278453371741555:  23%|██▎       | 33/141 [00:59<01:16,  1.42it/s]avg_loss = 1.926248799352085:  23%|██▎       | 33/141 [00:59<01:16,  1.42it/s] avg_loss = 1.926248799352085:  24%|██▍       | 34/141 [00:59<01:15,  1.42it/s]avg_loss = 1.9285311051777432:  24%|██▍       | 34/141 [01:00<01:15,  1.42it/s]avg_loss = 1.9285311051777432:  25%|██▍       | 35/141 [01:00<01:14,  1.42it/s]avg_loss = 1.9117269582218595:  25%|██▍       | 35/141 [01:01<01:14,  1.42it/s]avg_loss = 1.9117269582218595:  26%|██▌       | 36/141 [01:01<01:14,  1.41it/s]avg_loss = 1.896014793499096:  26%|██▌       | 36/141 [01:02<01:14,  1.41it/s] avg_loss = 1.896014793499096:  26%|██▌       | 37/141 [01:02<01:13,  1.41it/s]avg_loss = 1.8809732292827808:  26%|██▌       | 37/141 [01:02<01:13,  1.41it/s]avg_loss = 1.8809732292827808:  27%|██▋       | 38/141 [01:02<01:13,  1.40it/s]avg_loss = 1.8668232819972894:  27%|██▋       | 38/141 [01:03<01:13,  1.40it/s]avg_loss = 1.8668232819972894:  28%|██▊       | 39/141 [01:03<01:12,  1.41it/s]avg_loss = 1.8582959711551665:  28%|██▊       | 39/141 [01:04<01:12,  1.41it/s]avg_loss = 1.8582959711551665:  28%|██▊       | 40/141 [01:04<01:12,  1.40it/s]avg_loss = 1.8634777766902273:  28%|██▊       | 40/141 [01:04<01:12,  1.40it/s]avg_loss = 1.8634777766902273:  29%|██▉       | 41/141 [01:04<01:11,  1.40it/s]avg_loss = 1.8800961063021706:  29%|██▉       | 41/141 [01:05<01:11,  1.40it/s]avg_loss = 1.8800961063021706:  30%|██▉       | 42/141 [01:05<01:10,  1.40it/s]avg_loss = 1.8960083695345147:  30%|██▉       | 42/141 [01:06<01:10,  1.40it/s]avg_loss = 1.8960083695345147:  30%|███       | 43/141 [01:06<01:09,  1.40it/s]avg_loss = 1.8995509906248613:  30%|███       | 43/141 [01:07<01:09,  1.40it/s]avg_loss = 1.8995509906248613:  31%|███       | 44/141 [01:07<01:09,  1.40it/s]avg_loss = 1.9039044857025147:  31%|███       | 44/141 [01:07<01:09,  1.40it/s]avg_loss = 1.9039044857025147:  32%|███▏      | 45/141 [01:07<01:08,  1.40it/s]avg_loss = 1.9089916270712148:  32%|███▏      | 45/141 [01:08<01:08,  1.40it/s]avg_loss = 1.9089916270712148:  33%|███▎      | 46/141 [01:08<01:07,  1.40it/s]avg_loss = 1.9150525204678799:  33%|███▎      | 46/141 [01:09<01:07,  1.40it/s]avg_loss = 1.9150525204678799:  33%|███▎      | 47/141 [01:09<01:07,  1.40it/s]avg_loss = 1.917980710665385:  33%|███▎      | 47/141 [01:09<01:07,  1.40it/s] avg_loss = 1.917980710665385:  34%|███▍      | 48/141 [01:09<01:06,  1.40it/s]avg_loss = 1.9170175620487757:  34%|███▍      | 48/141 [01:10<01:06,  1.40it/s]avg_loss = 1.9170175620487757:  35%|███▍      | 49/141 [01:10<01:05,  1.39it/s]avg_loss = 1.9167870378494263:  35%|███▍      | 49/141 [01:11<01:05,  1.39it/s]avg_loss = 1.9167870378494263:  35%|███▌      | 50/141 [01:11<01:05,  1.40it/s]avg_loss = 1.9108855023103601:  35%|███▌      | 50/141 [01:12<01:05,  1.40it/s]avg_loss = 1.9108855023103601:  36%|███▌      | 51/141 [01:12<01:04,  1.39it/s]avg_loss = 1.9067261127325206:  36%|███▌      | 51/141 [01:12<01:04,  1.39it/s]avg_loss = 1.9067261127325206:  37%|███▋      | 52/141 [01:12<01:03,  1.39it/s]avg_loss = 1.900025003361252:  37%|███▋      | 52/141 [01:13<01:03,  1.39it/s] avg_loss = 1.900025003361252:  38%|███▊      | 53/141 [01:13<01:03,  1.39it/s]avg_loss = 1.8969337785685505:  38%|███▊      | 53/141 [01:14<01:03,  1.39it/s]avg_loss = 1.8969337785685505:  38%|███▊      | 54/141 [01:14<01:02,  1.39it/s]avg_loss = 1.8891447739167646:  38%|███▊      | 54/141 [01:14<01:02,  1.39it/s]avg_loss = 1.8891447739167646:  39%|███▉      | 55/141 [01:14<01:01,  1.39it/s]avg_loss = 1.8812005817890167:  39%|███▉      | 55/141 [01:15<01:01,  1.39it/s]avg_loss = 1.8812005817890167:  40%|███▉      | 56/141 [01:15<01:01,  1.39it/s]avg_loss = 1.8769784266488594:  40%|███▉      | 56/141 [01:16<01:01,  1.39it/s]avg_loss = 1.8769784266488594:  40%|████      | 57/141 [01:16<01:00,  1.38it/s]avg_loss = 1.8741123470766792:  40%|████      | 57/141 [01:17<01:00,  1.38it/s]avg_loss = 1.8741123470766792:  41%|████      | 58/141 [01:17<00:59,  1.38it/s]avg_loss = 1.8763537447331315:  41%|████      | 58/141 [01:17<00:59,  1.38it/s]avg_loss = 1.8763537447331315:  42%|████▏     | 59/141 [01:17<00:59,  1.38it/s]avg_loss = 1.8819389661153159:  42%|████▏     | 59/141 [01:18<00:59,  1.38it/s]avg_loss = 1.8819389661153159:  43%|████▎     | 60/141 [01:18<00:58,  1.38it/s]avg_loss = 1.8874135760010267:  43%|████▎     | 60/141 [01:19<00:58,  1.38it/s]avg_loss = 1.8874135760010267:  43%|████▎     | 61/141 [01:19<00:57,  1.38it/s]avg_loss = 1.8945163026932748:  43%|████▎     | 61/141 [01:20<00:57,  1.38it/s]avg_loss = 1.8945163026932748:  44%|████▍     | 62/141 [01:20<00:57,  1.38it/s]avg_loss = 1.8857911522426303:  44%|████▍     | 62/141 [01:20<00:57,  1.38it/s]avg_loss = 1.8857911522426303:  45%|████▍     | 63/141 [01:20<00:56,  1.38it/s]avg_loss = 1.8835857324302197:  45%|████▍     | 63/141 [01:21<00:56,  1.38it/s]avg_loss = 1.8835857324302197:  45%|████▌     | 64/141 [01:21<00:55,  1.38it/s]avg_loss = 1.8808899622697097:  45%|████▌     | 64/141 [01:22<00:55,  1.38it/s]avg_loss = 1.8808899622697097:  46%|████▌     | 65/141 [01:22<00:55,  1.38it/s]avg_loss = 1.8748980724450313:  46%|████▌     | 65/141 [01:22<00:55,  1.38it/s]avg_loss = 1.8748980724450313:  47%|████▋     | 66/141 [01:22<00:54,  1.38it/s]avg_loss = 1.8721294492038327:  47%|████▋     | 66/141 [01:23<00:54,  1.38it/s]avg_loss = 1.8721294492038327:  48%|████▊     | 67/141 [01:23<00:53,  1.38it/s]avg_loss = 1.869355303399703:  48%|████▊     | 67/141 [01:24<00:53,  1.38it/s] avg_loss = 1.869355303399703:  48%|████▊     | 68/141 [01:24<00:53,  1.38it/s]avg_loss = 1.8664285791093025:  48%|████▊     | 68/141 [01:25<00:53,  1.38it/s]avg_loss = 1.8664285791093025:  49%|████▉     | 69/141 [01:25<00:52,  1.37it/s]avg_loss = 1.8672742911747524:  49%|████▉     | 69/141 [01:25<00:52,  1.37it/s]avg_loss = 1.8672742911747524:  50%|████▉     | 70/141 [01:25<00:51,  1.37it/s]avg_loss = 1.870777321533418:  50%|████▉     | 70/141 [01:26<00:51,  1.37it/s] avg_loss = 1.870777321533418:  50%|█████     | 71/141 [01:26<00:50,  1.38it/s]avg_loss = 1.8730757037798564:  50%|█████     | 71/141 [01:27<00:50,  1.38it/s]avg_loss = 1.8730757037798564:  51%|█████     | 72/141 [01:27<00:50,  1.37it/s]avg_loss = 1.8714322439611775:  51%|█████     | 72/141 [01:28<00:50,  1.37it/s]avg_loss = 1.8714322439611775:  52%|█████▏    | 73/141 [01:28<00:49,  1.37it/s]avg_loss = 1.8729604176572852:  52%|█████▏    | 73/141 [01:28<00:49,  1.37it/s]avg_loss = 1.8729604176572852:  52%|█████▏    | 74/141 [01:28<00:48,  1.37it/s]avg_loss = 1.872925664583842:  52%|█████▏    | 74/141 [01:29<00:48,  1.37it/s] avg_loss = 1.872925664583842:  53%|█████▎    | 75/141 [01:29<00:48,  1.37it/s]avg_loss = 1.8716739588662197:  53%|█████▎    | 75/141 [01:30<00:48,  1.37it/s]avg_loss = 1.8716739588662197:  54%|█████▍    | 76/141 [01:30<00:47,  1.37it/s]avg_loss = 1.8728842116021491:  54%|█████▍    | 76/141 [01:30<00:47,  1.37it/s]avg_loss = 1.8728842116021491:  55%|█████▍    | 77/141 [01:30<00:46,  1.37it/s]avg_loss = 1.8753339571830554:  55%|█████▍    | 77/141 [01:31<00:46,  1.37it/s]avg_loss = 1.8753339571830554:  55%|█████▌    | 78/141 [01:31<00:45,  1.37it/s]avg_loss = 1.8792483444455303:  55%|█████▌    | 78/141 [01:32<00:45,  1.37it/s]avg_loss = 1.8792483444455303:  56%|█████▌    | 79/141 [01:32<00:45,  1.37it/s]avg_loss = 1.8760829016566276:  56%|█████▌    | 79/141 [01:33<00:45,  1.37it/s]avg_loss = 1.8760829016566276:  57%|█████▋    | 80/141 [01:33<00:44,  1.37it/s]avg_loss = 1.8747750182210663:  57%|█████▋    | 80/141 [01:33<00:44,  1.37it/s]avg_loss = 1.8747750182210663:  57%|█████▋    | 81/141 [01:33<00:43,  1.37it/s]avg_loss = 1.8739909602374565:  57%|█████▋    | 81/141 [01:34<00:43,  1.37it/s]avg_loss = 1.8739909602374565:  58%|█████▊    | 82/141 [01:34<00:43,  1.36it/s]avg_loss = 1.8721935835229344:  58%|█████▊    | 82/141 [01:35<00:43,  1.36it/s]avg_loss = 1.8721935835229344:  59%|█████▉    | 83/141 [01:35<00:42,  1.36it/s]avg_loss = 1.8700493588334037:  59%|█████▉    | 83/141 [01:36<00:42,  1.36it/s]avg_loss = 1.8700493588334037:  60%|█████▉    | 84/141 [01:36<00:41,  1.36it/s]avg_loss = 1.8675063876544729:  60%|█████▉    | 84/141 [01:36<00:41,  1.36it/s]avg_loss = 1.8675063876544729:  60%|██████    | 85/141 [01:36<00:41,  1.36it/s]avg_loss = 1.8691350318664728:  60%|██████    | 85/141 [01:37<00:41,  1.36it/s]avg_loss = 1.8691350318664728:  61%|██████    | 86/141 [01:37<00:40,  1.36it/s]avg_loss = 1.871013001463879:  61%|██████    | 86/141 [01:38<00:40,  1.36it/s] avg_loss = 1.871013001463879:  62%|██████▏   | 87/141 [01:38<00:39,  1.36it/s]avg_loss = 1.8716793669895693:  62%|██████▏   | 87/141 [01:39<00:39,  1.36it/s]avg_loss = 1.8716793669895693:  62%|██████▏   | 88/141 [01:39<00:38,  1.36it/s]avg_loss = 1.8804595832074626:  62%|██████▏   | 88/141 [01:39<00:38,  1.36it/s]avg_loss = 1.8804595832074626:  63%|██████▎   | 89/141 [01:39<00:38,  1.36it/s]avg_loss = 1.8878007054328918:  63%|██████▎   | 89/141 [01:40<00:38,  1.36it/s]avg_loss = 1.8878007054328918:  64%|██████▍   | 90/141 [01:40<00:37,  1.36it/s]avg_loss = 1.8909836981322738:  64%|██████▍   | 90/141 [01:41<00:37,  1.36it/s]avg_loss = 1.8909836981322738:  65%|██████▍   | 91/141 [01:41<00:36,  1.36it/s]avg_loss = 1.8958370957685553:  65%|██████▍   | 91/141 [01:41<00:36,  1.36it/s]avg_loss = 1.8958370957685553:  65%|██████▌   | 92/141 [01:41<00:36,  1.36it/s]avg_loss = 1.9009106633483723:  65%|██████▌   | 92/141 [01:42<00:36,  1.36it/s]avg_loss = 1.9009106633483723:  66%|██████▌   | 93/141 [01:42<00:35,  1.36it/s]avg_loss = 1.9017120168564168:  66%|██████▌   | 93/141 [01:43<00:35,  1.36it/s]avg_loss = 1.9017120168564168:  67%|██████▋   | 94/141 [01:43<00:34,  1.36it/s]avg_loss = 1.9054825255745336:  67%|██████▋   | 94/141 [01:44<00:34,  1.36it/s]avg_loss = 1.9054825255745336:  67%|██████▋   | 95/141 [01:44<00:33,  1.36it/s]avg_loss = 1.9060581376155217:  67%|██████▋   | 95/141 [01:44<00:33,  1.36it/s]avg_loss = 1.9060581376155217:  68%|██████▊   | 96/141 [01:44<00:33,  1.36it/s]avg_loss = 1.907844914603479:  68%|██████▊   | 96/141 [01:45<00:33,  1.36it/s] avg_loss = 1.907844914603479:  69%|██████▉   | 97/141 [01:45<00:32,  1.36it/s]avg_loss = 1.9045196576994292:  69%|██████▉   | 97/141 [01:46<00:32,  1.36it/s]avg_loss = 1.9045196576994292:  70%|██████▉   | 98/141 [01:46<00:31,  1.35it/s]avg_loss = 1.9054928813317809:  70%|██████▉   | 98/141 [01:47<00:31,  1.35it/s]avg_loss = 1.9054928813317809:  70%|███████   | 99/141 [01:47<00:31,  1.35it/s]avg_loss = 1.9078465008735657:  70%|███████   | 99/141 [01:47<00:31,  1.35it/s]avg_loss = 1.9078465008735657:  71%|███████   | 100/141 [01:47<00:30,  1.35it/s]avg_loss = 1.906743356496981:  71%|███████   | 100/141 [01:48<00:30,  1.35it/s] avg_loss = 1.906743356496981:  72%|███████▏  | 101/141 [01:48<00:29,  1.35it/s]avg_loss = 1.9071331772149778:  72%|███████▏  | 101/141 [01:49<00:29,  1.35it/s]avg_loss = 1.9071331772149778:  72%|███████▏  | 102/141 [01:49<00:28,  1.35it/s]avg_loss = 1.9058720382671912:  72%|███████▏  | 102/141 [01:50<00:28,  1.35it/s]avg_loss = 1.9058720382671912:  73%|███████▎  | 103/141 [01:50<00:28,  1.35it/s]avg_loss = 1.9086729139089584:  73%|███████▎  | 103/141 [01:50<00:28,  1.35it/s]avg_loss = 1.9086729139089584:  74%|███████▍  | 104/141 [01:50<00:27,  1.35it/s]avg_loss = 1.9075086866106306:  74%|███████▍  | 104/141 [01:51<00:27,  1.35it/s]avg_loss = 1.9075086866106306:  74%|███████▍  | 105/141 [01:51<00:26,  1.35it/s]avg_loss = 1.9067919175579864:  74%|███████▍  | 105/141 [01:52<00:26,  1.35it/s]avg_loss = 1.9067919175579864:  75%|███████▌  | 106/141 [01:52<00:25,  1.35it/s]avg_loss = 1.9046241334665601:  75%|███████▌  | 106/141 [01:53<00:25,  1.35it/s]avg_loss = 1.9046241334665601:  76%|███████▌  | 107/141 [01:53<00:25,  1.35it/s]avg_loss = 1.9022853286178023:  76%|███████▌  | 107/141 [01:53<00:25,  1.35it/s]avg_loss = 1.9022853286178023:  77%|███████▋  | 108/141 [01:53<00:24,  1.35it/s]avg_loss = 1.900053605027155:  77%|███████▋  | 108/141 [01:54<00:24,  1.35it/s] avg_loss = 1.900053605027155:  77%|███████▋  | 109/141 [01:54<00:23,  1.35it/s]avg_loss = 1.8975875095887618:  77%|███████▋  | 109/141 [01:55<00:23,  1.35it/s]avg_loss = 1.8975875095887618:  78%|███████▊  | 110/141 [01:55<00:23,  1.35it/s]avg_loss = 1.8999495699598983:  78%|███████▊  | 110/141 [01:56<00:23,  1.35it/s]avg_loss = 1.8999495699598983:  79%|███████▊  | 111/141 [01:56<00:22,  1.35it/s]avg_loss = 1.8996243413005556:  79%|███████▊  | 111/141 [01:56<00:22,  1.35it/s]avg_loss = 1.8996243413005556:  79%|███████▉  | 112/141 [01:56<00:21,  1.35it/s]avg_loss = 1.9006864665883831:  79%|███████▉  | 112/141 [01:57<00:21,  1.35it/s]avg_loss = 1.9006864665883831:  80%|████████  | 113/141 [01:57<00:20,  1.35it/s]avg_loss = 1.9016172342133104:  80%|████████  | 113/141 [01:58<00:20,  1.35it/s]avg_loss = 1.9016172342133104:  81%|████████  | 114/141 [01:58<00:20,  1.35it/s]avg_loss = 1.9009245986523835:  81%|████████  | 114/141 [01:58<00:20,  1.35it/s]avg_loss = 1.9009245986523835:  82%|████████▏ | 115/141 [01:58<00:19,  1.34it/s]avg_loss = 1.899784337857674:  82%|████████▏ | 115/141 [01:59<00:19,  1.34it/s] avg_loss = 1.899784337857674:  82%|████████▏ | 116/141 [01:59<00:18,  1.34it/s]avg_loss = 1.901901525309962:  82%|████████▏ | 116/141 [02:00<00:18,  1.34it/s]avg_loss = 1.901901525309962:  83%|████████▎ | 117/141 [02:00<00:17,  1.34it/s]avg_loss = 1.9015297172433239:  83%|████████▎ | 117/141 [02:01<00:17,  1.34it/s]avg_loss = 1.9015297172433239:  84%|████████▎ | 118/141 [02:01<00:17,  1.34it/s]avg_loss = 1.9001279738770813:  84%|████████▎ | 118/141 [02:01<00:17,  1.34it/s]avg_loss = 1.9001279738770813:  84%|████████▍ | 119/141 [02:01<00:16,  1.34it/s]avg_loss = 1.8983159869909287:  84%|████████▍ | 119/141 [02:02<00:16,  1.34it/s]avg_loss = 1.8983159869909287:  85%|████████▌ | 120/141 [02:02<00:15,  1.34it/s]avg_loss = 1.8982636810334261:  85%|████████▌ | 120/141 [02:03<00:15,  1.34it/s]avg_loss = 1.8982636810334261:  86%|████████▌ | 121/141 [02:03<00:14,  1.34it/s]avg_loss = 1.8986788470236982:  86%|████████▌ | 121/141 [02:04<00:14,  1.34it/s]avg_loss = 1.8986788470236982:  87%|████████▋ | 122/141 [02:04<00:14,  1.34it/s]avg_loss = 1.8984466422864092:  87%|████████▋ | 122/141 [02:04<00:14,  1.34it/s]avg_loss = 1.8984466422864092:  87%|████████▋ | 123/141 [02:04<00:13,  1.34it/s]avg_loss = 1.8985887900475533:  87%|████████▋ | 123/141 [02:05<00:13,  1.34it/s]avg_loss = 1.8985887900475533:  88%|████████▊ | 124/141 [02:05<00:12,  1.34it/s]avg_loss = 1.8973109130859376:  88%|████████▊ | 124/141 [02:06<00:12,  1.34it/s]avg_loss = 1.8973109130859376:  89%|████████▊ | 125/141 [02:06<00:11,  1.34it/s]avg_loss = 1.8976975490176489:  89%|████████▊ | 125/141 [02:07<00:11,  1.34it/s]avg_loss = 1.8976975490176489:  89%|████████▉ | 126/141 [02:07<00:11,  1.34it/s]avg_loss = 1.8974588602546631:  89%|████████▉ | 126/141 [02:07<00:11,  1.34it/s]avg_loss = 1.8974588602546631:  90%|█████████ | 127/141 [02:07<00:10,  1.34it/s]avg_loss = 1.8962392685934901:  90%|█████████ | 127/141 [02:08<00:10,  1.34it/s]avg_loss = 1.8962392685934901:  91%|█████████ | 128/141 [02:08<00:09,  1.34it/s]avg_loss = 1.8963492766831271:  91%|█████████ | 128/141 [02:09<00:09,  1.34it/s]avg_loss = 1.8963492766831271:  91%|█████████▏| 129/141 [02:09<00:08,  1.34it/s]avg_loss = 1.8972446533349845:  91%|█████████▏| 129/141 [02:10<00:08,  1.34it/s]avg_loss = 1.8972446533349845:  92%|█████████▏| 130/141 [02:10<00:08,  1.34it/s]avg_loss = 1.8981423650989095:  92%|█████████▏| 130/141 [02:10<00:08,  1.34it/s]avg_loss = 1.8981423650989095:  93%|█████████▎| 131/141 [02:10<00:07,  1.34it/s]avg_loss = 1.898549249677947:  93%|█████████▎| 131/141 [02:11<00:07,  1.34it/s] avg_loss = 1.898549249677947:  94%|█████████▎| 132/141 [02:11<00:06,  1.34it/s]avg_loss = 1.8955098856660657:  94%|█████████▎| 132/141 [02:12<00:06,  1.34it/s]avg_loss = 1.8955098856660657:  94%|█████████▍| 133/141 [02:12<00:05,  1.34it/s]avg_loss = 1.8909647802808391:  94%|█████████▍| 133/141 [02:13<00:05,  1.34it/s]avg_loss = 1.8909647802808391:  95%|█████████▌| 134/141 [02:13<00:05,  1.34it/s]avg_loss = 1.8934033623448125:  95%|█████████▌| 134/141 [02:13<00:05,  1.34it/s]avg_loss = 1.8934033623448125:  96%|█████████▌| 135/141 [02:13<00:04,  1.34it/s]avg_loss = 1.8968256641836727:  96%|█████████▌| 135/141 [02:14<00:04,  1.34it/s]avg_loss = 1.8968256641836727:  96%|█████████▋| 136/141 [02:14<00:03,  1.34it/s]avg_loss = 1.8980640199062597:  96%|█████████▋| 136/141 [02:15<00:03,  1.34it/s]avg_loss = 1.8980640199062597:  97%|█████████▋| 137/141 [02:15<00:02,  1.33it/s]avg_loss = 1.896844165912573:  97%|█████████▋| 137/141 [02:16<00:02,  1.33it/s] avg_loss = 1.896844165912573:  98%|█████████▊| 138/141 [02:16<00:02,  1.33it/s]avg_loss = 1.8971867964422102:  98%|█████████▊| 138/141 [02:16<00:02,  1.33it/s]avg_loss = 1.8971867964422102:  99%|█████████▊| 139/141 [02:16<00:01,  1.33it/s]avg_loss = 1.8978539160319736:  99%|█████████▊| 139/141 [02:17<00:01,  1.33it/s]avg_loss = 1.8978539160319736:  99%|█████████▉| 140/141 [02:17<00:00,  1.34it/s]avg_loss = 1.8990504268213366:  99%|█████████▉| 140/141 [02:18<00:00,  1.34it/s]avg_loss = 1.8990504268213366: 100%|██████████| 141/141 [02:18<00:00,  1.34it/s]avg_loss = 1.8990504268213366: 100%|██████████| 141/141 [02:18<00:00,  1.02it/s]
I0315 06:57:51.614747 429932 eval_ppl.py:66] wikitext2 perplexity: 6.679549217224121
