I0416 08:14:26.264882 3179536 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:26.264986 3179536 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:26.265033 3179536 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:26.381046 3179536 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:28.615646 3179536 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

I0416 08:14:29.990040 3179536 quantize_finetune_clip.py:141] loaded model
I0416 08:14:29.990173 3179536 quantize_finetune_clip.py:143] loaded dataset and devset
I0416 08:14:30.061785 3179536 quantize_finetune_clip.py:151] vision layer 0 gpu 0
I0416 08:14:31.171227 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 0 in 0.93s
I0416 08:14:31.961624 3179536 quantize_finetune_clip.py:151] vision layer 1 gpu 0
I0416 08:14:34.245661 3182628 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:34.245785 3182628 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:34.245844 3182628 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:34.360695 3182628 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:37.111035 3182628 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:03<00:25,  3.70s/it] 25%|██▌       | 2/8 [00:04<00:10,  1.71s/it] 38%|███▊      | 3/8 [00:04<00:05,  1.08s/it] 50%|█████     | 4/8 [00:04<00:03,  1.28it/s] 62%|██████▎   | 5/8 [00:04<00:01,  1.63it/s] 75%|███████▌  | 6/8 [00:05<00:01,  1.96it/s] 88%|████████▊ | 7/8 [00:05<00:00,  2.24it/s]100%|██████████| 8/8 [00:05<00:00,  2.47it/s]100%|██████████| 8/8 [00:05<00:00,  1.35it/s]
Process Process-1:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_0_v proxy err 6.202562508406118e-05 tr(WHW.T) 28.12407684326172
I0416 08:14:46.439701 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 1 in 0.20s
I0416 08:14:46.564707 3179536 quantize_finetune_clip.py:151] vision layer 2 gpu 0
I0416 08:14:49.019840 3186257 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:49.019966 3186257 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:49.020026 3186257 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:49.136681 3186257 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:51.430951 3186257 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.27s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.44it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.95it/s] 50%|█████     | 4/8 [00:02<00:01,  2.34it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.63it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.86it/s]100%|██████████| 8/8 [00:03<00:00,  3.00it/s]100%|██████████| 8/8 [00:03<00:00,  2.36it/s]
Process Process-2:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_1_v proxy err 9.862549632089213e-05 tr(WHW.T) 68.71031951904297
I0416 08:14:57.185876 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 2 in 0.19s
I0416 08:14:57.307855 3179536 quantize_finetune_clip.py:151] vision layer 3 gpu 0
I0416 08:14:59.779035 3188199 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:59.779184 3188199 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:59.779244 3188199 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:59.900862 3188199 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:02.740374 3188199 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.28s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.33it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.80it/s] 50%|█████     | 4/8 [00:02<00:01,  2.19it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.53it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.77it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.94it/s]100%|██████████| 8/8 [00:03<00:00,  3.04it/s]100%|██████████| 8/8 [00:03<00:00,  2.30it/s]
Process Process-3:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_2_v proxy err 0.00016279215924441814 tr(WHW.T) 61.11924743652344
I0416 08:15:09.498181 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 3 in 0.18s
I0416 08:15:09.625091 3179536 quantize_finetune_clip.py:151] vision layer 4 gpu 0
I0416 08:15:11.932437 3190948 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:11.932565 3190948 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:11.932627 3190948 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:12.052782 3190948 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:14.485972 3190948 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.35s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.37it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.90it/s] 50%|█████     | 4/8 [00:02<00:01,  2.29it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.56it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.81it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.95it/s]100%|██████████| 8/8 [00:03<00:00,  3.06it/s]100%|██████████| 8/8 [00:03<00:00,  2.33it/s]
Process Process-4:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_3_v proxy err 0.0002340648352401331 tr(WHW.T) 78.36602783203125
I0416 08:15:21.257665 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 4 in 0.14s
I0416 08:15:21.377072 3179536 quantize_finetune_clip.py:151] vision layer 5 gpu 0
I0416 08:15:23.741856 3193408 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:23.741999 3193408 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:23.742094 3193408 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:23.873148 3193408 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:26.202572 3193408 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.28s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.43it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.92it/s] 50%|█████     | 4/8 [00:02<00:01,  2.31it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.60it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.82it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.91it/s]100%|██████████| 8/8 [00:03<00:00,  3.04it/s]100%|██████████| 8/8 [00:03<00:00,  2.36it/s]
Process Process-5:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_4_v proxy err 0.00021585813374258578 tr(WHW.T) 95.10258483886719
I0416 08:15:32.429804 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 5 in 0.14s
I0416 08:15:32.556988 3179536 quantize_finetune_clip.py:151] vision layer 6 gpu 0
I0416 08:15:34.855641 3196247 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:34.855766 3196247 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:34.855942 3196247 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:34.979148 3196247 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:37.358058 3196247 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.27s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.43it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.93it/s] 50%|█████     | 4/8 [00:02<00:01,  2.32it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.54it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.74it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.87it/s]100%|██████████| 8/8 [00:03<00:00,  3.00it/s]100%|██████████| 8/8 [00:03<00:00,  2.34it/s]
Process Process-6:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_5_v proxy err 0.00026181049179285765 tr(WHW.T) 151.44627380371094
I0416 08:15:44.082719 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 6 in 0.40s
I0416 08:15:44.204854 3179536 quantize_finetune_clip.py:151] vision layer 7 gpu 0
I0416 08:15:46.464532 3199172 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:46.464662 3199172 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:46.464725 3199172 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:46.583615 3199172 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:48.855360 3199172 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.35s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.36it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.87it/s] 50%|█████     | 4/8 [00:02<00:01,  2.26it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.55it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.88it/s]100%|██████████| 8/8 [00:03<00:00,  3.00it/s]100%|██████████| 8/8 [00:03<00:00,  2.30it/s]
Process Process-7:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_6_v proxy err 0.0004219932307023555 tr(WHW.T) 170.1556854248047
I0416 08:15:54.822945 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 7 in 0.18s
I0416 08:15:54.953020 3179536 quantize_finetune_clip.py:151] vision layer 8 gpu 0
I0416 08:15:57.265384 3201752 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:57.265582 3201752 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:57.265695 3201752 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:57.391521 3201752 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:59.807743 3201752 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.30s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.40it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.89it/s] 50%|█████     | 4/8 [00:02<00:01,  2.27it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.55it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.83it/s]100%|██████████| 8/8 [00:03<00:00,  2.94it/s]100%|██████████| 8/8 [00:03<00:00,  2.30it/s]
Process Process-8:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_7_v proxy err 0.0004516042536124587 tr(WHW.T) 231.85989379882812
I0416 08:16:05.876096 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 8 in 0.14s
I0416 08:16:05.996515 3179536 quantize_finetune_clip.py:151] vision layer 9 gpu 0
I0416 08:16:08.538981 3204487 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:08.539127 3204487 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:08.539196 3204487 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:08.669887 3204487 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:11.362383 3204487 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.29s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.41it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.90it/s] 50%|█████     | 4/8 [00:02<00:01,  2.27it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.53it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.75it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.92it/s]100%|██████████| 8/8 [00:03<00:00,  3.03it/s]100%|██████████| 8/8 [00:03<00:00,  2.33it/s]
Process Process-9:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_8_v proxy err 0.00042256698361597955 tr(WHW.T) 293.3328857421875
I0416 08:16:17.937342 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 9 in 0.14s
I0416 08:16:18.067945 3179536 quantize_finetune_clip.py:151] vision layer 10 gpu 0
I0416 08:16:20.580854 3208099 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:20.580997 3208099 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:20.581057 3208099 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:20.700132 3208099 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:22.955658 3208099 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.33s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.38it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.87it/s] 50%|█████     | 4/8 [00:02<00:01,  2.22it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.50it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.65it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.85it/s]100%|██████████| 8/8 [00:03<00:00,  3.00it/s]100%|██████████| 8/8 [00:03<00:00,  2.29it/s]
Process Process-10:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_9_v proxy err 0.00044706225162371993 tr(WHW.T) 314.0332336425781
I0416 08:16:29.800987 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 10 in 0.14s
I0416 08:16:29.933794 3179536 quantize_finetune_clip.py:151] vision layer 11 gpu 0
I0416 08:16:32.241630 3210705 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:32.241763 3210705 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:32.241826 3210705 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:32.364130 3210705 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:34.866700 3210705 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.36s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.32it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.81it/s] 50%|█████     | 4/8 [00:02<00:01,  2.19it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.49it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.72it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.88it/s]100%|██████████| 8/8 [00:03<00:00,  3.00it/s]100%|██████████| 8/8 [00:03<00:00,  2.27it/s]
Process Process-11:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_10_v proxy err 0.0005400637164711952 tr(WHW.T) 327.5375671386719
I0416 08:16:40.758922 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 11 in 0.15s
I0416 08:16:40.888437 3179536 quantize_finetune_clip.py:151] vision layer 12 gpu 0
I0416 08:16:43.192578 3213059 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:43.192704 3213059 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:43.192768 3213059 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:43.311424 3213059 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:45.622636 3213059 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.29s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.41it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.92it/s] 50%|█████     | 4/8 [00:02<00:01,  2.25it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.56it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.78it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.95it/s]100%|██████████| 8/8 [00:03<00:00,  3.06it/s]100%|██████████| 8/8 [00:03<00:00,  2.35it/s]
Process Process-12:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_11_v proxy err 0.0006227814592421055 tr(WHW.T) 309.68670654296875
I0416 08:16:51.491428 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 12 in 0.19s
I0416 08:16:51.614206 3179536 quantize_finetune_clip.py:151] vision layer 13 gpu 0
I0416 08:16:53.918963 3215914 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:53.919091 3215914 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:53.919172 3215914 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:54.039059 3215914 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:56.486982 3215914 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.31s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.35it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.85it/s] 50%|█████     | 4/8 [00:02<00:01,  2.26it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.52it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.74it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.87it/s]100%|██████████| 8/8 [00:03<00:00,  2.99it/s]100%|██████████| 8/8 [00:03<00:00,  2.30it/s]
Process Process-13:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_12_v proxy err 0.0007133936160244048 tr(WHW.T) 274.07757568359375
I0416 08:17:03.019578 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 13 in 0.15s
I0416 08:17:03.172768 3179536 quantize_finetune_clip.py:151] vision layer 14 gpu 0
I0416 08:17:05.623584 3219210 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:05.623722 3219210 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:05.623793 3219210 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:05.744276 3219210 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:08.313410 3219210 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.30s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.38it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.86it/s] 50%|█████     | 4/8 [00:02<00:01,  2.18it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.47it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.68it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.83it/s]100%|██████████| 8/8 [00:03<00:00,  2.95it/s]100%|██████████| 8/8 [00:03<00:00,  2.27it/s]
Process Process-14:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_13_v proxy err 0.000811902282293886 tr(WHW.T) 298.0824890136719
I0416 08:17:14.426314 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 14 in 0.16s
I0416 08:17:14.544093 3179536 quantize_finetune_clip.py:151] vision layer 15 gpu 0
I0416 08:17:16.819053 3221779 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:16.819234 3221779 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:16.819318 3221779 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:16.939794 3221779 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:19.574933 3221779 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.39s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.33it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.83it/s] 50%|█████     | 4/8 [00:02<00:01,  2.16it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.48it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.72it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.91it/s]100%|██████████| 8/8 [00:03<00:00,  3.04it/s]100%|██████████| 8/8 [00:03<00:00,  2.27it/s]
Process Process-15:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_14_v proxy err 0.0008706608787178993 tr(WHW.T) 284.17218017578125
I0416 08:17:25.929681 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 15 in 0.16s
I0416 08:17:26.068140 3179536 quantize_finetune_clip.py:151] vision layer 16 gpu 0
I0416 08:17:28.599580 3224802 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:28.599720 3224802 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:28.599778 3224802 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:28.724391 3224802 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:31.179128 3224802 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.29s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.40it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.89it/s] 50%|█████     | 4/8 [00:02<00:01,  2.17it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.48it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.70it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.86it/s]100%|██████████| 8/8 [00:03<00:00,  2.98it/s]100%|██████████| 8/8 [00:03<00:00,  2.29it/s]
Process Process-16:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_15_v proxy err 0.0008728501852601767 tr(WHW.T) 285.29705810546875
I0416 08:17:37.178087 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 16 in 0.14s
I0416 08:17:37.300066 3179536 quantize_finetune_clip.py:151] vision layer 17 gpu 0
I0416 08:17:39.723621 3227292 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:39.723780 3227292 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:39.723902 3227292 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:39.844721 3227292 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:42.359868 3227292 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.34s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.36it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.87it/s] 50%|█████     | 4/8 [00:02<00:01,  2.22it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.55it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.73it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.88it/s]100%|██████████| 8/8 [00:03<00:00,  2.98it/s]100%|██████████| 8/8 [00:03<00:00,  2.29it/s]
Process Process-17:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_16_v proxy err 0.0009762296685948968 tr(WHW.T) 323.1610412597656
I0416 08:17:48.526319 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 17 in 0.13s
I0416 08:17:48.646422 3179536 quantize_finetune_clip.py:151] vision layer 18 gpu 0
I0416 08:17:51.017606 3230717 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:51.017735 3230717 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:51.017796 3230717 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:51.141491 3230717 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:53.458944 3230717 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.30s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.42it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.94it/s] 50%|█████     | 4/8 [00:02<00:01,  2.35it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.66it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s] 88%|████████▊ | 7/8 [00:03<00:00,  3.06it/s]100%|██████████| 8/8 [00:03<00:00,  3.18it/s]100%|██████████| 8/8 [00:03<00:00,  2.41it/s]
Process Process-18:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_17_v proxy err 0.0010884987423196435 tr(WHW.T) 311.25750732421875
I0416 08:17:59.303199 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 18 in 0.14s
I0416 08:17:59.469391 3179536 quantize_finetune_clip.py:151] vision layer 19 gpu 0
I0416 08:18:01.813001 3233327 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:01.813159 3233327 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:01.813226 3233327 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:01.940215 3233327 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:04.436763 3233327 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.30s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.42it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.92it/s] 50%|█████     | 4/8 [00:02<00:01,  2.35it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.67it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s] 88%|████████▊ | 7/8 [00:03<00:00,  3.10it/s]100%|██████████| 8/8 [00:03<00:00,  3.23it/s]100%|██████████| 8/8 [00:03<00:00,  2.43it/s]
Process Process-19:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_18_v proxy err 0.0011849036673083901 tr(WHW.T) 355.8935546875
I0416 08:18:10.078234 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 19 in 0.14s
I0416 08:18:10.204223 3179536 quantize_finetune_clip.py:151] vision layer 20 gpu 0
I0416 08:18:12.750165 3235600 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:12.750322 3235600 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:12.750390 3235600 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:12.876901 3235600 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:15.300909 3235600 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:10,  1.47s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.28it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.78it/s] 50%|█████     | 4/8 [00:02<00:01,  2.19it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.49it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.75it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.94it/s]100%|██████████| 8/8 [00:03<00:00,  3.09it/s]100%|██████████| 8/8 [00:03<00:00,  2.26it/s]
Process Process-20:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_19_v proxy err 0.0012176979798823595 tr(WHW.T) 348.79376220703125
I0416 08:18:21.494764 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 20 in 0.31s
I0416 08:18:21.610832 3179536 quantize_finetune_clip.py:151] vision layer 21 gpu 0
I0416 08:18:24.093779 3238450 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:24.093950 3238450 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:24.094033 3238450 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:24.216955 3238450 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:26.633704 3238450 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.35s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.36it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.88it/s] 50%|█████     | 4/8 [00:02<00:01,  2.27it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.57it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.91it/s]100%|██████████| 8/8 [00:03<00:00,  2.88it/s]100%|██████████| 8/8 [00:03<00:00,  2.28it/s]
Process Process-21:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_20_v proxy err 0.0012333031045272946 tr(WHW.T) 375.24658203125
I0416 08:18:33.312472 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 21 in 0.16s
I0416 08:18:33.439851 3179536 quantize_finetune_clip.py:151] vision layer 22 gpu 0
I0416 08:18:35.839711 3241800 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:35.839867 3241800 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:35.839932 3241800 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:35.965151 3241800 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:38.405876 3241800 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.29s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.42it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.93it/s] 50%|█████     | 4/8 [00:02<00:01,  2.32it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.62it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.95it/s]100%|██████████| 8/8 [00:03<00:00,  3.07it/s]100%|██████████| 8/8 [00:03<00:00,  2.37it/s]
Process Process-22:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_21_v proxy err 0.0011944198049604893 tr(WHW.T) 334.0597229003906
I0416 08:18:44.177219 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 22 in 0.16s
I0416 08:18:44.309182 3179536 quantize_finetune_clip.py:151] vision layer 23 gpu 0
I0416 08:18:46.749406 3244561 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:46.749525 3244561 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:46.749588 3244561 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:46.876923 3244561 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:49.291269 3244561 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.37s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.34it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.86it/s] 50%|█████     | 4/8 [00:02<00:01,  2.18it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.50it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.73it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.90it/s]100%|██████████| 8/8 [00:03<00:00,  3.03it/s]100%|██████████| 8/8 [00:03<00:00,  2.28it/s]
Process Process-23:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_22_v proxy err 0.0014823435340076685 tr(WHW.T) 295.9122619628906
I0416 08:18:55.265972 3179536 quantize_finetune_clip.py:168] computed original embedding for vision layer 23 in 0.16s
I0416 08:18:57.845046 3247172 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:57.845198 3247172 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:57.845260 3247172 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:57.964271 3247172 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:00.279732 3247172 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.31s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.41it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.94it/s] 50%|█████     | 4/8 [00:02<00:01,  2.35it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.67it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.90it/s] 88%|████████▊ | 7/8 [00:03<00:00,  3.05it/s]100%|██████████| 8/8 [00:03<00:00,  3.16it/s]100%|██████████| 8/8 [00:03<00:00,  2.41it/s]
Process Process-24:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (4096) must match the size of tensor b (0) at non-singleton dimension 0
vision_23_v proxy err 0.0014649603981524706 tr(WHW.T) 353.4617919921875
I0416 08:19:05.544157 3179536 quantize_finetune_clip.py:151] text layer 0 gpu 0
I0416 08:19:06.084689 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 0 in 0.39s
I0416 08:19:06.277554 3179536 quantize_finetune_clip.py:151] text layer 1 gpu 0
I0416 08:19:08.715910 3249840 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:08.716029 3249840 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:08.716099 3249840 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:08.837940 3249840 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:11.325106 3249840 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:03<00:17,  3.48s/it] 33%|███▎      | 2/6 [00:03<00:06,  1.66s/it] 50%|█████     | 3/6 [00:04<00:03,  1.06s/it] 67%|██████▋   | 4/6 [00:04<00:01,  1.28it/s] 83%|████████▎ | 5/6 [00:04<00:00,  1.58it/s]100%|██████████| 6/6 [00:05<00:00,  1.92it/s]100%|██████████| 6/6 [00:05<00:00,  1.14it/s]
Process Process-25:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_0_v proxy err 0.0011656313436105847 tr(WHW.T) 75.04249572753906
I0416 08:19:19.132884 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 1 in 0.25s
I0416 08:19:19.252252 3179536 quantize_finetune_clip.py:151] text layer 2 gpu 0
I0416 08:19:21.562711 3253033 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:21.562824 3253033 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:21.562882 3253033 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:21.679726 3253033 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:23.942394 3253033 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.26s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.43it/s] 50%|█████     | 3/6 [00:01<00:01,  1.93it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.31it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.60it/s]100%|██████████| 6/6 [00:02<00:00,  2.81it/s]100%|██████████| 6/6 [00:02<00:00,  2.16it/s]
Process Process-26:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_1_v proxy err 0.00023130075715016574 tr(WHW.T) 73.62937927246094
I0416 08:19:29.239298 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 2 in 0.22s
I0416 08:19:29.353043 3179536 quantize_finetune_clip.py:151] text layer 3 gpu 0
I0416 08:19:31.741901 3254466 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:31.742070 3254466 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:31.742133 3254466 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:31.864088 3254466 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:34.120607 3254466 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:07,  1.41s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.33it/s] 50%|█████     | 3/6 [00:01<00:01,  1.84it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.24it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.56it/s]100%|██████████| 6/6 [00:02<00:00,  2.79it/s]100%|██████████| 6/6 [00:02<00:00,  2.08it/s]
Process Process-27:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_2_v proxy err 0.00023645340115763247 tr(WHW.T) 129.4635009765625
I0416 08:19:39.324799 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 3 in 0.14s
I0416 08:19:39.439646 3179536 quantize_finetune_clip.py:151] text layer 4 gpu 0
I0416 08:19:41.790375 3255546 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:41.790562 3255546 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:41.790625 3255546 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:41.909922 3255546 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:44.244209 3255546 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.28s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.44it/s] 50%|█████     | 3/6 [00:01<00:01,  1.97it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.38it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.70it/s]100%|██████████| 6/6 [00:02<00:00,  2.91it/s]100%|██████████| 6/6 [00:02<00:00,  2.21it/s]
Process Process-28:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_3_v proxy err 0.0002574238751549274 tr(WHW.T) 130.8394775390625
I0416 08:19:49.289924 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 4 in 0.14s
I0416 08:19:49.402174 3179536 quantize_finetune_clip.py:151] text layer 5 gpu 0
I0416 08:19:51.785571 3256626 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:51.785746 3256626 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:51.785813 3256626 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:51.907124 3256626 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:54.295560 3256626 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:07,  1.41s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.33it/s] 50%|█████     | 3/6 [00:01<00:01,  1.85it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.25it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.57it/s]100%|██████████| 6/6 [00:02<00:00,  2.81it/s]100%|██████████| 6/6 [00:02<00:00,  2.09it/s]
Process Process-29:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_4_v proxy err 0.0003010413493029773 tr(WHW.T) 180.7900848388672
I0416 08:19:59.531561 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 5 in 0.17s
I0416 08:19:59.641668 3179536 quantize_finetune_clip.py:151] text layer 6 gpu 0
I0416 08:20:01.891466 3258239 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:20:01.891585 3258239 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:20:01.891646 3258239 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:20:02.007637 3258239 config.py:58] PyTorch version 2.4.0 available.
W0416 08:20:04.685333 3258239 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.29s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.41it/s] 50%|█████     | 3/6 [00:01<00:01,  1.92it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.31it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.60it/s]100%|██████████| 6/6 [00:02<00:00,  2.81it/s]100%|██████████| 6/6 [00:02<00:00,  2.15it/s]
Process Process-30:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_5_v proxy err 0.00019998745119664818 tr(WHW.T) 249.72238159179688
I0416 08:20:09.747065 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 6 in 0.13s
I0416 08:20:09.857272 3179536 quantize_finetune_clip.py:151] text layer 7 gpu 0
I0416 08:20:12.197802 3259814 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:20:12.197969 3259814 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:20:12.198031 3259814 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:20:12.318304 3259814 config.py:58] PyTorch version 2.4.0 available.
W0416 08:20:14.685789 3259814 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.34s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.38it/s] 50%|█████     | 3/6 [00:01<00:01,  1.89it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.30it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.60it/s]100%|██████████| 6/6 [00:02<00:00,  2.82it/s]100%|██████████| 6/6 [00:02<00:00,  2.13it/s]
Process Process-31:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_6_v proxy err 0.00027832385967485607 tr(WHW.T) 246.55203247070312
I0416 08:20:19.823537 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 7 in 0.14s
I0416 08:20:19.940792 3179536 quantize_finetune_clip.py:151] text layer 8 gpu 0
I0416 08:20:22.407704 3260926 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:20:22.407879 3260926 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:20:22.407942 3260926 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:20:22.528447 3260926 config.py:58] PyTorch version 2.4.0 available.
W0416 08:20:24.754430 3260926 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:07,  1.44s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.31it/s] 50%|█████     | 3/6 [00:02<00:01,  1.83it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.25it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.59it/s]100%|██████████| 6/6 [00:02<00:00,  2.84it/s]100%|██████████| 6/6 [00:02<00:00,  2.09it/s]
Process Process-32:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_7_v proxy err 0.00025446555810049176 tr(WHW.T) 263.7965087890625
I0416 08:20:30.060246 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 8 in 0.15s
I0416 08:20:30.173069 3179536 quantize_finetune_clip.py:151] text layer 9 gpu 0
I0416 08:20:32.510125 3262004 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:20:32.510258 3262004 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:20:32.510318 3262004 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:20:32.630966 3262004 config.py:58] PyTorch version 2.4.0 available.
W0416 08:20:34.890117 3262004 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.33s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.39it/s] 50%|█████     | 3/6 [00:01<00:01,  1.90it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.31it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.61it/s]100%|██████████| 6/6 [00:02<00:00,  2.84it/s]100%|██████████| 6/6 [00:02<00:00,  2.14it/s]
Process Process-33:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_8_v proxy err 0.0002149815991288051 tr(WHW.T) 368.84979248046875
I0416 08:20:39.963662 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 9 in 0.14s
I0416 08:20:40.077148 3179536 quantize_finetune_clip.py:151] text layer 10 gpu 0
I0416 08:20:42.485171 3263196 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:20:42.485318 3263196 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:20:42.485383 3263196 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:20:42.614260 3263196 config.py:58] PyTorch version 2.4.0 available.
W0416 08:20:45.053215 3263196 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.35s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.38it/s] 50%|█████     | 3/6 [00:01<00:01,  1.90it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.27it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.59it/s]100%|██████████| 6/6 [00:02<00:00,  2.83it/s]100%|██████████| 6/6 [00:02<00:00,  2.13it/s]
Process Process-34:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_9_v proxy err 0.0002157378039555624 tr(WHW.T) 361.0838623046875
I0416 08:20:50.197977 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 10 in 0.15s
I0416 08:20:50.307431 3179536 quantize_finetune_clip.py:151] text layer 11 gpu 0
I0416 08:20:52.576030 3265127 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:20:52.576187 3265127 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:20:52.576250 3265127 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:20:52.692257 3265127 config.py:58] PyTorch version 2.4.0 available.
W0416 08:20:55.052974 3265127 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.30s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.41it/s] 50%|█████     | 3/6 [00:01<00:01,  1.91it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.28it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.57it/s]100%|██████████| 6/6 [00:02<00:00,  2.78it/s]100%|██████████| 6/6 [00:02<00:00,  2.14it/s]
Process Process-35:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_10_v proxy err 0.00027011852944269776 tr(WHW.T) 381.42498779296875
I0416 08:21:00.152783 3179536 quantize_finetune_clip.py:168] computed original embedding for text layer 11 in 0.13s
I0416 08:21:02.659647 3266211 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:21:02.659810 3266211 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:21:02.659911 3266211 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:21:02.779505 3266211 config.py:58] PyTorch version 2.4.0 available.
W0416 08:21:05.150738 3266211 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:07,  1.41s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.33it/s] 50%|█████     | 3/6 [00:01<00:01,  1.84it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.25it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.54it/s]100%|██████████| 6/6 [00:02<00:00,  2.78it/s]100%|██████████| 6/6 [00:02<00:00,  2.08it/s]
Process Process-36:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/opt/conda/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/workspace/Weight_compression/qtip/quantize_llama/quantize_finetune_clip.py", line 92, in quantize_clip_decoder
    finetune_clip.quantize_finetune_decoder_layer_clip(layer, quant_order, idx, cb, args,
  File "/workspace/Weight_compression/qtip/lib/algo/finetune_clip.py", line 216, in quantize_finetune_decoder_layer_clip
    q_linear.trellis.copy_(packed)
RuntimeError: The size of tensor a (2304) must match the size of tensor b (0) at non-singleton dimension 0
text_11_v proxy err 0.0003213656600564718 tr(WHW.T) 514.353271484375
I0416 08:21:15.338541 3267474 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:21:15.338645 3267474 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:21:15.338688 3267474 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:21:15.453891 3267474 config.py:58] PyTorch version 2.4.0 available.
W0416 08:21:17.369616 3267474 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0416 08:21:17.370280 3267474 hfize_clip.py:43] CLIPConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/openai--clip-vit-large-patch14",
  "architectures": [
    "CLIPModel"
  ],
  "initializer_factor": 1.0,
  "logit_scale_init_value": 2.6592,
  "model_type": "clip",
  "projection_dim": 768,
  "quip_params": {
    "K": 8,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "text_config": {
    "dropout": 0.0,
    "hidden_size": 768,
    "intermediate_size": 3072,
    "model_type": "clip_text_model",
    "num_attention_heads": 12,
    "projection_dim": 768,
    "torch_dtype": "float32"
  },
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "vision_config": {
    "dropout": 0.0,
    "hidden_size": 1024,
    "intermediate_size": 4096,
    "model_type": "clip_vision_model",
    "num_attention_heads": 16,
    "num_hidden_layers": 24,
    "patch_size": 14,
    "projection_dim": 768,
    "torch_dtype": "float32"
  }
}

Some weights of the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 were not used when initializing CLIPModel: ['text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing CLIPModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CLIPModel were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 and are newly initialized: ['text_model.encoder.layers.0.mlp.fc1.SU', 'text_model.encoder.layers.0.mlp.fc1.SV', 'text_model.encoder.layers.0.mlp.fc1.rcp', 'text_model.encoder.layers.0.mlp.fc1.tlut', 'text_model.encoder.layers.0.mlp.fc1.tp_rank', 'text_model.encoder.layers.0.mlp.fc1.trellis', 'text_model.encoder.layers.0.mlp.fc2.SU', 'text_model.encoder.layers.0.mlp.fc2.SV', 'text_model.encoder.layers.0.mlp.fc2.rcp', 'text_model.encoder.layers.0.mlp.fc2.tlut', 'text_model.encoder.layers.0.mlp.fc2.tp_rank', 'text_model.encoder.layers.0.mlp.fc2.trellis', 'text_model.encoder.layers.0.self_attn.k_proj.SU', 'text_model.encoder.layers.0.self_attn.k_proj.SV', 'text_model.encoder.layers.0.self_attn.k_proj.rcp', 'text_model.encoder.layers.0.self_attn.k_proj.tlut', 'text_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.k_proj.trellis', 'text_model.encoder.layers.0.self_attn.out_proj.SU', 'text_model.encoder.layers.0.self_attn.out_proj.SV', 'text_model.encoder.layers.0.self_attn.out_proj.rcp', 'text_model.encoder.layers.0.self_attn.out_proj.tlut', 'text_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.out_proj.trellis', 'text_model.encoder.layers.0.self_attn.q_proj.SU', 'text_model.encoder.layers.0.self_attn.q_proj.SV', 'text_model.encoder.layers.0.self_attn.q_proj.rcp', 'text_model.encoder.layers.0.self_attn.q_proj.tlut', 'text_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.q_proj.trellis', 'text_model.encoder.layers.0.self_attn.v_proj.SU', 'text_model.encoder.layers.0.self_attn.v_proj.SV', 'text_model.encoder.layers.0.self_attn.v_proj.rcp', 'text_model.encoder.layers.0.self_attn.v_proj.tlut', 'text_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.v_proj.trellis', 'text_model.encoder.layers.1.mlp.fc1.SU', 'text_model.encoder.layers.1.mlp.fc1.SV', 'text_model.encoder.layers.1.mlp.fc1.rcp', 'text_model.encoder.layers.1.mlp.fc1.tlut', 'text_model.encoder.layers.1.mlp.fc1.tp_rank', 'text_model.encoder.layers.1.mlp.fc1.trellis', 'text_model.encoder.layers.1.mlp.fc2.SU', 'text_model.encoder.layers.1.mlp.fc2.SV', 'text_model.encoder.layers.1.mlp.fc2.rcp', 'text_model.encoder.layers.1.mlp.fc2.tlut', 'text_model.encoder.layers.1.mlp.fc2.tp_rank', 'text_model.encoder.layers.1.mlp.fc2.trellis', 'text_model.encoder.layers.1.self_attn.k_proj.SU', 'text_model.encoder.layers.1.self_attn.k_proj.SV', 'text_model.encoder.layers.1.self_attn.k_proj.rcp', 'text_model.encoder.layers.1.self_attn.k_proj.tlut', 'text_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.k_proj.trellis', 'text_model.encoder.layers.1.self_attn.out_proj.SU', 'text_model.encoder.layers.1.self_attn.out_proj.SV', 'text_model.encoder.layers.1.self_attn.out_proj.rcp', 'text_model.encoder.layers.1.self_attn.out_proj.tlut', 'text_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.out_proj.trellis', 'text_model.encoder.layers.1.self_attn.q_proj.SU', 'text_model.encoder.layers.1.self_attn.q_proj.SV', 'text_model.encoder.layers.1.self_attn.q_proj.rcp', 'text_model.encoder.layers.1.self_attn.q_proj.tlut', 'text_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.q_proj.trellis', 'text_model.encoder.layers.1.self_attn.v_proj.SU', 'text_model.encoder.layers.1.self_attn.v_proj.SV', 'text_model.encoder.layers.1.self_attn.v_proj.rcp', 'text_model.encoder.layers.1.self_attn.v_proj.tlut', 'text_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.v_proj.trellis', 'text_model.encoder.layers.10.mlp.fc1.SU', 'text_model.encoder.layers.10.mlp.fc1.SV', 'text_model.encoder.layers.10.mlp.fc1.rcp', 'text_model.encoder.layers.10.mlp.fc1.tlut', 'text_model.encoder.layers.10.mlp.fc1.tp_rank', 'text_model.encoder.layers.10.mlp.fc1.trellis', 'text_model.encoder.layers.10.mlp.fc2.SU', 'text_model.encoder.layers.10.mlp.fc2.SV', 'text_model.encoder.layers.10.mlp.fc2.rcp', 'text_model.encoder.layers.10.mlp.fc2.tlut', 'text_model.encoder.layers.10.mlp.fc2.tp_rank', 'text_model.encoder.layers.10.mlp.fc2.trellis', 'text_model.encoder.layers.10.self_attn.k_proj.SU', 'text_model.encoder.layers.10.self_attn.k_proj.SV', 'text_model.encoder.layers.10.self_attn.k_proj.rcp', 'text_model.encoder.layers.10.self_attn.k_proj.tlut', 'text_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.k_proj.trellis', 'text_model.encoder.layers.10.self_attn.out_proj.SU', 'text_model.encoder.layers.10.self_attn.out_proj.SV', 'text_model.encoder.layers.10.self_attn.out_proj.rcp', 'text_model.encoder.layers.10.self_attn.out_proj.tlut', 'text_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.out_proj.trellis', 'text_model.encoder.layers.10.self_attn.q_proj.SU', 'text_model.encoder.layers.10.self_attn.q_proj.SV', 'text_model.encoder.layers.10.self_attn.q_proj.rcp', 'text_model.encoder.layers.10.self_attn.q_proj.tlut', 'text_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.q_proj.trellis', 'text_model.encoder.layers.10.self_attn.v_proj.SU', 'text_model.encoder.layers.10.self_attn.v_proj.SV', 'text_model.encoder.layers.10.self_attn.v_proj.rcp', 'text_model.encoder.layers.10.self_attn.v_proj.tlut', 'text_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.v_proj.trellis', 'text_model.encoder.layers.11.mlp.fc1.SU', 'text_model.encoder.layers.11.mlp.fc1.SV', 'text_model.encoder.layers.11.mlp.fc1.rcp', 'text_model.encoder.layers.11.mlp.fc1.tlut', 'text_model.encoder.layers.11.mlp.fc1.tp_rank', 'text_model.encoder.layers.11.mlp.fc1.trellis', 'text_model.encoder.layers.11.mlp.fc2.SU', 'text_model.encoder.layers.11.mlp.fc2.SV', 'text_model.encoder.layers.11.mlp.fc2.rcp', 'text_model.encoder.layers.11.mlp.fc2.tlut', 'text_model.encoder.layers.11.mlp.fc2.tp_rank', 'text_model.encoder.layers.11.mlp.fc2.trellis', 'text_model.encoder.layers.11.self_attn.k_proj.SU', 'text_model.encoder.layers.11.self_attn.k_proj.SV', 'text_model.encoder.layers.11.self_attn.k_proj.rcp', 'text_model.encoder.layers.11.self_attn.k_proj.tlut', 'text_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.k_proj.trellis', 'text_model.encoder.layers.11.self_attn.out_proj.SU', 'text_model.encoder.layers.11.self_attn.out_proj.SV', 'text_model.encoder.layers.11.self_attn.out_proj.rcp', 'text_model.encoder.layers.11.self_attn.out_proj.tlut', 'text_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.out_proj.trellis', 'text_model.encoder.layers.11.self_attn.q_proj.SU', 'text_model.encoder.layers.11.self_attn.q_proj.SV', 'text_model.encoder.layers.11.self_attn.q_proj.rcp', 'text_model.encoder.layers.11.self_attn.q_proj.tlut', 'text_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.q_proj.trellis', 'text_model.encoder.layers.11.self_attn.v_proj.SU', 'text_model.encoder.layers.11.self_attn.v_proj.SV', 'text_model.encoder.layers.11.self_attn.v_proj.rcp', 'text_model.encoder.layers.11.self_attn.v_proj.tlut', 'text_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.v_proj.trellis', 'text_model.encoder.layers.2.mlp.fc1.SU', 'text_model.encoder.layers.2.mlp.fc1.SV', 'text_model.encoder.layers.2.mlp.fc1.rcp', 'text_model.encoder.layers.2.mlp.fc1.tlut', 'text_model.encoder.layers.2.mlp.fc1.tp_rank', 'text_model.encoder.layers.2.mlp.fc1.trellis', 'text_model.encoder.layers.2.mlp.fc2.SU', 'text_model.encoder.layers.2.mlp.fc2.SV', 'text_model.encoder.layers.2.mlp.fc2.rcp', 'text_model.encoder.layers.2.mlp.fc2.tlut', 'text_model.encoder.layers.2.mlp.fc2.tp_rank', 'text_model.encoder.layers.2.mlp.fc2.trellis', 'text_model.encoder.layers.2.self_attn.k_proj.SU', 'text_model.encoder.layers.2.self_attn.k_proj.SV', 'text_model.encoder.layers.2.self_attn.k_proj.rcp', 'text_model.encoder.layers.2.self_attn.k_proj.tlut', 'text_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.k_proj.trellis', 'text_model.encoder.layers.2.self_attn.out_proj.SU', 'text_model.encoder.layers.2.self_attn.out_proj.SV', 'text_model.encoder.layers.2.self_attn.out_proj.rcp', 'text_model.encoder.layers.2.self_attn.out_proj.tlut', 'text_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.out_proj.trellis', 'text_model.encoder.layers.2.self_attn.q_proj.SU', 'text_model.encoder.layers.2.self_attn.q_proj.SV', 'text_model.encoder.layers.2.self_attn.q_proj.rcp', 'text_model.encoder.layers.2.self_attn.q_proj.tlut', 'text_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.q_proj.trellis', 'text_model.encoder.layers.2.self_attn.v_proj.SU', 'text_model.encoder.layers.2.self_attn.v_proj.SV', 'text_model.encoder.layers.2.self_attn.v_proj.rcp', 'text_model.encoder.layers.2.self_attn.v_proj.tlut', 'text_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.v_proj.trellis', 'text_model.encoder.layers.3.mlp.fc1.SU', 'text_model.encoder.layers.3.mlp.fc1.SV', 'text_model.encoder.layers.3.mlp.fc1.rcp', 'text_model.encoder.layers.3.mlp.fc1.tlut', 'text_model.encoder.layers.3.mlp.fc1.tp_rank', 'text_model.encoder.layers.3.mlp.fc1.trellis', 'text_model.encoder.layers.3.mlp.fc2.SU', 'text_model.encoder.layers.3.mlp.fc2.SV', 'text_model.encoder.layers.3.mlp.fc2.rcp', 'text_model.encoder.layers.3.mlp.fc2.tlut', 'text_model.encoder.layers.3.mlp.fc2.tp_rank', 'text_model.encoder.layers.3.mlp.fc2.trellis', 'text_model.encoder.layers.3.self_attn.k_proj.SU', 'text_model.encoder.layers.3.self_attn.k_proj.SV', 'text_model.encoder.layers.3.self_attn.k_proj.rcp', 'text_model.encoder.layers.3.self_attn.k_proj.tlut', 'text_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.k_proj.trellis', 'text_model.encoder.layers.3.self_attn.out_proj.SU', 'text_model.encoder.layers.3.self_attn.out_proj.SV', 'text_model.encoder.layers.3.self_attn.out_proj.rcp', 'text_model.encoder.layers.3.self_attn.out_proj.tlut', 'text_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.out_proj.trellis', 'text_model.encoder.layers.3.self_attn.q_proj.SU', 'text_model.encoder.layers.3.self_attn.q_proj.SV', 'text_model.encoder.layers.3.self_attn.q_proj.rcp', 'text_model.encoder.layers.3.self_attn.q_proj.tlut', 'text_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.q_proj.trellis', 'text_model.encoder.layers.3.self_attn.v_proj.SU', 'text_model.encoder.layers.3.self_attn.v_proj.SV', 'text_model.encoder.layers.3.self_attn.v_proj.rcp', 'text_model.encoder.layers.3.self_attn.v_proj.tlut', 'text_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.v_proj.trellis', 'text_model.encoder.layers.4.mlp.fc1.SU', 'text_model.encoder.layers.4.mlp.fc1.SV', 'text_model.encoder.layers.4.mlp.fc1.rcp', 'text_model.encoder.layers.4.mlp.fc1.tlut', 'text_model.encoder.layers.4.mlp.fc1.tp_rank', 'text_model.encoder.layers.4.mlp.fc1.trellis', 'text_model.encoder.layers.4.mlp.fc2.SU', 'text_model.encoder.layers.4.mlp.fc2.SV', 'text_model.encoder.layers.4.mlp.fc2.rcp', 'text_model.encoder.layers.4.mlp.fc2.tlut', 'text_model.encoder.layers.4.mlp.fc2.tp_rank', 'text_model.encoder.layers.4.mlp.fc2.trellis', 'text_model.encoder.layers.4.self_attn.k_proj.SU', 'text_model.encoder.layers.4.self_attn.k_proj.SV', 'text_model.encoder.layers.4.self_attn.k_proj.rcp', 'text_model.encoder.layers.4.self_attn.k_proj.tlut', 'text_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.k_proj.trellis', 'text_model.encoder.layers.4.self_attn.out_proj.SU', 'text_model.encoder.layers.4.self_attn.out_proj.SV', 'text_model.encoder.layers.4.self_attn.out_proj.rcp', 'text_model.encoder.layers.4.self_attn.out_proj.tlut', 'text_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.out_proj.trellis', 'text_model.encoder.layers.4.self_attn.q_proj.SU', 'text_model.encoder.layers.4.self_attn.q_proj.SV', 'text_model.encoder.layers.4.self_attn.q_proj.rcp', 'text_model.encoder.layers.4.self_attn.q_proj.tlut', 'text_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.q_proj.trellis', 'text_model.encoder.layers.4.self_attn.v_proj.SU', 'text_model.encoder.layers.4.self_attn.v_proj.SV', 'text_model.encoder.layers.4.self_attn.v_proj.rcp', 'text_model.encoder.layers.4.self_attn.v_proj.tlut', 'text_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.v_proj.trellis', 'text_model.encoder.layers.5.mlp.fc1.SU', 'text_model.encoder.layers.5.mlp.fc1.SV', 'text_model.encoder.layers.5.mlp.fc1.rcp', 'text_model.encoder.layers.5.mlp.fc1.tlut', 'text_model.encoder.layers.5.mlp.fc1.tp_rank', 'text_model.encoder.layers.5.mlp.fc1.trellis', 'text_model.encoder.layers.5.mlp.fc2.SU', 'text_model.encoder.layers.5.mlp.fc2.SV', 'text_model.encoder.layers.5.mlp.fc2.rcp', 'text_model.encoder.layers.5.mlp.fc2.tlut', 'text_model.encoder.layers.5.mlp.fc2.tp_rank', 'text_model.encoder.layers.5.mlp.fc2.trellis', 'text_model.encoder.layers.5.self_attn.k_proj.SU', 'text_model.encoder.layers.5.self_attn.k_proj.SV', 'text_model.encoder.layers.5.self_attn.k_proj.rcp', 'text_model.encoder.layers.5.self_attn.k_proj.tlut', 'text_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.k_proj.trellis', 'text_model.encoder.layers.5.self_attn.out_proj.SU', 'text_model.encoder.layers.5.self_attn.out_proj.SV', 'text_model.encoder.layers.5.self_attn.out_proj.rcp', 'text_model.encoder.layers.5.self_attn.out_proj.tlut', 'text_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.out_proj.trellis', 'text_model.encoder.layers.5.self_attn.q_proj.SU', 'text_model.encoder.layers.5.self_attn.q_proj.SV', 'text_model.encoder.layers.5.self_attn.q_proj.rcp', 'text_model.encoder.layers.5.self_attn.q_proj.tlut', 'text_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.q_proj.trellis', 'text_model.encoder.layers.5.self_attn.v_proj.SU', 'text_model.encoder.layers.5.self_attn.v_proj.SV', 'text_model.encoder.layers.5.self_attn.v_proj.rcp', 'text_model.encoder.layers.5.self_attn.v_proj.tlut', 'text_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.v_proj.trellis', 'text_model.encoder.layers.6.mlp.fc1.SU', 'text_model.encoder.layers.6.mlp.fc1.SV', 'text_model.encoder.layers.6.mlp.fc1.rcp', 'text_model.encoder.layers.6.mlp.fc1.tlut', 'text_model.encoder.layers.6.mlp.fc1.tp_rank', 'text_model.encoder.layers.6.mlp.fc1.trellis', 'text_model.encoder.layers.6.mlp.fc2.SU', 'text_model.encoder.layers.6.mlp.fc2.SV', 'text_model.encoder.layers.6.mlp.fc2.rcp', 'text_model.encoder.layers.6.mlp.fc2.tlut', 'text_model.encoder.layers.6.mlp.fc2.tp_rank', 'text_model.encoder.layers.6.mlp.fc2.trellis', 'text_model.encoder.layers.6.self_attn.k_proj.SU', 'text_model.encoder.layers.6.self_attn.k_proj.SV', 'text_model.encoder.layers.6.self_attn.k_proj.rcp', 'text_model.encoder.layers.6.self_attn.k_proj.tlut', 'text_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.k_proj.trellis', 'text_model.encoder.layers.6.self_attn.out_proj.SU', 'text_model.encoder.layers.6.self_attn.out_proj.SV', 'text_model.encoder.layers.6.self_attn.out_proj.rcp', 'text_model.encoder.layers.6.self_attn.out_proj.tlut', 'text_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.out_proj.trellis', 'text_model.encoder.layers.6.self_attn.q_proj.SU', 'text_model.encoder.layers.6.self_attn.q_proj.SV', 'text_model.encoder.layers.6.self_attn.q_proj.rcp', 'text_model.encoder.layers.6.self_attn.q_proj.tlut', 'text_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.q_proj.trellis', 'text_model.encoder.layers.6.self_attn.v_proj.SU', 'text_model.encoder.layers.6.self_attn.v_proj.SV', 'text_model.encoder.layers.6.self_attn.v_proj.rcp', 'text_model.encoder.layers.6.self_attn.v_proj.tlut', 'text_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.v_proj.trellis', 'text_model.encoder.layers.7.mlp.fc1.SU', 'text_model.encoder.layers.7.mlp.fc1.SV', 'text_model.encoder.layers.7.mlp.fc1.rcp', 'text_model.encoder.layers.7.mlp.fc1.tlut', 'text_model.encoder.layers.7.mlp.fc1.tp_rank', 'text_model.encoder.layers.7.mlp.fc1.trellis', 'text_model.encoder.layers.7.mlp.fc2.SU', 'text_model.encoder.layers.7.mlp.fc2.SV', 'text_model.encoder.layers.7.mlp.fc2.rcp', 'text_model.encoder.layers.7.mlp.fc2.tlut', 'text_model.encoder.layers.7.mlp.fc2.tp_rank', 'text_model.encoder.layers.7.mlp.fc2.trellis', 'text_model.encoder.layers.7.self_attn.k_proj.SU', 'text_model.encoder.layers.7.self_attn.k_proj.SV', 'text_model.encoder.layers.7.self_attn.k_proj.rcp', 'text_model.encoder.layers.7.self_attn.k_proj.tlut', 'text_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.k_proj.trellis', 'text_model.encoder.layers.7.self_attn.out_proj.SU', 'text_model.encoder.layers.7.self_attn.out_proj.SV', 'text_model.encoder.layers.7.self_attn.out_proj.rcp', 'text_model.encoder.layers.7.self_attn.out_proj.tlut', 'text_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.out_proj.trellis', 'text_model.encoder.layers.7.self_attn.q_proj.SU', 'text_model.encoder.layers.7.self_attn.q_proj.SV', 'text_model.encoder.layers.7.self_attn.q_proj.rcp', 'text_model.encoder.layers.7.self_attn.q_proj.tlut', 'text_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.q_proj.trellis', 'text_model.encoder.layers.7.self_attn.v_proj.SU', 'text_model.encoder.layers.7.self_attn.v_proj.SV', 'text_model.encoder.layers.7.self_attn.v_proj.rcp', 'text_model.encoder.layers.7.self_attn.v_proj.tlut', 'text_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.v_proj.trellis', 'text_model.encoder.layers.8.mlp.fc1.SU', 'text_model.encoder.layers.8.mlp.fc1.SV', 'text_model.encoder.layers.8.mlp.fc1.rcp', 'text_model.encoder.layers.8.mlp.fc1.tlut', 'text_model.encoder.layers.8.mlp.fc1.tp_rank', 'text_model.encoder.layers.8.mlp.fc1.trellis', 'text_model.encoder.layers.8.mlp.fc2.SU', 'text_model.encoder.layers.8.mlp.fc2.SV', 'text_model.encoder.layers.8.mlp.fc2.rcp', 'text_model.encoder.layers.8.mlp.fc2.tlut', 'text_model.encoder.layers.8.mlp.fc2.tp_rank', 'text_model.encoder.layers.8.mlp.fc2.trellis', 'text_model.encoder.layers.8.self_attn.k_proj.SU', 'text_model.encoder.layers.8.self_attn.k_proj.SV', 'text_model.encoder.layers.8.self_attn.k_proj.rcp', 'text_model.encoder.layers.8.self_attn.k_proj.tlut', 'text_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.k_proj.trellis', 'text_model.encoder.layers.8.self_attn.out_proj.SU', 'text_model.encoder.layers.8.self_attn.out_proj.SV', 'text_model.encoder.layers.8.self_attn.out_proj.rcp', 'text_model.encoder.layers.8.self_attn.out_proj.tlut', 'text_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.out_proj.trellis', 'text_model.encoder.layers.8.self_attn.q_proj.SU', 'text_model.encoder.layers.8.self_attn.q_proj.SV', 'text_model.encoder.layers.8.self_attn.q_proj.rcp', 'text_model.encoder.layers.8.self_attn.q_proj.tlut', 'text_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.q_proj.trellis', 'text_model.encoder.layers.8.self_attn.v_proj.SU', 'text_model.encoder.layers.8.self_attn.v_proj.SV', 'text_model.encoder.layers.8.self_attn.v_proj.rcp', 'text_model.encoder.layers.8.self_attn.v_proj.tlut', 'text_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.v_proj.trellis', 'text_model.encoder.layers.9.mlp.fc1.SU', 'text_model.encoder.layers.9.mlp.fc1.SV', 'text_model.encoder.layers.9.mlp.fc1.rcp', 'text_model.encoder.layers.9.mlp.fc1.tlut', 'text_model.encoder.layers.9.mlp.fc1.tp_rank', 'text_model.encoder.layers.9.mlp.fc1.trellis', 'text_model.encoder.layers.9.mlp.fc2.SU', 'text_model.encoder.layers.9.mlp.fc2.SV', 'text_model.encoder.layers.9.mlp.fc2.rcp', 'text_model.encoder.layers.9.mlp.fc2.tlut', 'text_model.encoder.layers.9.mlp.fc2.tp_rank', 'text_model.encoder.layers.9.mlp.fc2.trellis', 'text_model.encoder.layers.9.self_attn.k_proj.SU', 'text_model.encoder.layers.9.self_attn.k_proj.SV', 'text_model.encoder.layers.9.self_attn.k_proj.rcp', 'text_model.encoder.layers.9.self_attn.k_proj.tlut', 'text_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.k_proj.trellis', 'text_model.encoder.layers.9.self_attn.out_proj.SU', 'text_model.encoder.layers.9.self_attn.out_proj.SV', 'text_model.encoder.layers.9.self_attn.out_proj.rcp', 'text_model.encoder.layers.9.self_attn.out_proj.tlut', 'text_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.out_proj.trellis', 'text_model.encoder.layers.9.self_attn.q_proj.SU', 'text_model.encoder.layers.9.self_attn.q_proj.SV', 'text_model.encoder.layers.9.self_attn.q_proj.rcp', 'text_model.encoder.layers.9.self_attn.q_proj.tlut', 'text_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.q_proj.trellis', 'text_model.encoder.layers.9.self_attn.v_proj.SU', 'text_model.encoder.layers.9.self_attn.v_proj.SV', 'text_model.encoder.layers.9.self_attn.v_proj.rcp', 'text_model.encoder.layers.9.self_attn.v_proj.tlut', 'text_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.v_proj.trellis', 'vision_model.encoder.layers.0.mlp.fc1.SU', 'vision_model.encoder.layers.0.mlp.fc1.SV', 'vision_model.encoder.layers.0.mlp.fc1.rcp', 'vision_model.encoder.layers.0.mlp.fc1.tlut', 'vision_model.encoder.layers.0.mlp.fc1.tp_rank', 'vision_model.encoder.layers.0.mlp.fc1.trellis', 'vision_model.encoder.layers.0.mlp.fc2.SU', 'vision_model.encoder.layers.0.mlp.fc2.SV', 'vision_model.encoder.layers.0.mlp.fc2.rcp', 'vision_model.encoder.layers.0.mlp.fc2.tlut', 'vision_model.encoder.layers.0.mlp.fc2.tp_rank', 'vision_model.encoder.layers.0.mlp.fc2.trellis', 'vision_model.encoder.layers.0.self_attn.k_proj.SU', 'vision_model.encoder.layers.0.self_attn.k_proj.SV', 'vision_model.encoder.layers.0.self_attn.k_proj.rcp', 'vision_model.encoder.layers.0.self_attn.k_proj.tlut', 'vision_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.k_proj.trellis', 'vision_model.encoder.layers.0.self_attn.out_proj.SU', 'vision_model.encoder.layers.0.self_attn.out_proj.SV', 'vision_model.encoder.layers.0.self_attn.out_proj.rcp', 'vision_model.encoder.layers.0.self_attn.out_proj.tlut', 'vision_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.out_proj.trellis', 'vision_model.encoder.layers.0.self_attn.q_proj.SU', 'vision_model.encoder.layers.0.self_attn.q_proj.SV', 'vision_model.encoder.layers.0.self_attn.q_proj.rcp', 'vision_model.encoder.layers.0.self_attn.q_proj.tlut', 'vision_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.q_proj.trellis', 'vision_model.encoder.layers.0.self_attn.v_proj.SU', 'vision_model.encoder.layers.0.self_attn.v_proj.SV', 'vision_model.encoder.layers.0.self_attn.v_proj.rcp', 'vision_model.encoder.layers.0.self_attn.v_proj.tlut', 'vision_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.v_proj.trellis', 'vision_model.encoder.layers.1.mlp.fc1.SU', 'vision_model.encoder.layers.1.mlp.fc1.SV', 'vision_model.encoder.layers.1.mlp.fc1.rcp', 'vision_model.encoder.layers.1.mlp.fc1.tlut', 'vision_model.encoder.layers.1.mlp.fc1.tp_rank', 'vision_model.encoder.layers.1.mlp.fc1.trellis', 'vision_model.encoder.layers.1.mlp.fc2.SU', 'vision_model.encoder.layers.1.mlp.fc2.SV', 'vision_model.encoder.layers.1.mlp.fc2.rcp', 'vision_model.encoder.layers.1.mlp.fc2.tlut', 'vision_model.encoder.layers.1.mlp.fc2.tp_rank', 'vision_model.encoder.layers.1.mlp.fc2.trellis', 'vision_model.encoder.layers.1.self_attn.k_proj.SU', 'vision_model.encoder.layers.1.self_attn.k_proj.SV', 'vision_model.encoder.layers.1.self_attn.k_proj.rcp', 'vision_model.encoder.layers.1.self_attn.k_proj.tlut', 'vision_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.k_proj.trellis', 'vision_model.encoder.layers.1.self_attn.out_proj.SU', 'vision_model.encoder.layers.1.self_attn.out_proj.SV', 'vision_model.encoder.layers.1.self_attn.out_proj.rcp', 'vision_model.encoder.layers.1.self_attn.out_proj.tlut', 'vision_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.out_proj.trellis', 'vision_model.encoder.layers.1.self_attn.q_proj.SU', 'vision_model.encoder.layers.1.self_attn.q_proj.SV', 'vision_model.encoder.layers.1.self_attn.q_proj.rcp', 'vision_model.encoder.layers.1.self_attn.q_proj.tlut', 'vision_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.q_proj.trellis', 'vision_model.encoder.layers.1.self_attn.v_proj.SU', 'vision_model.encoder.layers.1.self_attn.v_proj.SV', 'vision_model.encoder.layers.1.self_attn.v_proj.rcp', 'vision_model.encoder.layers.1.self_attn.v_proj.tlut', 'vision_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.v_proj.trellis', 'vision_model.encoder.layers.10.mlp.fc1.SU', 'vision_model.encoder.layers.10.mlp.fc1.SV', 'vision_model.encoder.layers.10.mlp.fc1.rcp', 'vision_model.encoder.layers.10.mlp.fc1.tlut', 'vision_model.encoder.layers.10.mlp.fc1.tp_rank', 'vision_model.encoder.layers.10.mlp.fc1.trellis', 'vision_model.encoder.layers.10.mlp.fc2.SU', 'vision_model.encoder.layers.10.mlp.fc2.SV', 'vision_model.encoder.layers.10.mlp.fc2.rcp', 'vision_model.encoder.layers.10.mlp.fc2.tlut', 'vision_model.encoder.layers.10.mlp.fc2.tp_rank', 'vision_model.encoder.layers.10.mlp.fc2.trellis', 'vision_model.encoder.layers.10.self_attn.k_proj.SU', 'vision_model.encoder.layers.10.self_attn.k_proj.SV', 'vision_model.encoder.layers.10.self_attn.k_proj.rcp', 'vision_model.encoder.layers.10.self_attn.k_proj.tlut', 'vision_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.k_proj.trellis', 'vision_model.encoder.layers.10.self_attn.out_proj.SU', 'vision_model.encoder.layers.10.self_attn.out_proj.SV', 'vision_model.encoder.layers.10.self_attn.out_proj.rcp', 'vision_model.encoder.layers.10.self_attn.out_proj.tlut', 'vision_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.out_proj.trellis', 'vision_model.encoder.layers.10.self_attn.q_proj.SU', 'vision_model.encoder.layers.10.self_attn.q_proj.SV', 'vision_model.encoder.layers.10.self_attn.q_proj.rcp', 'vision_model.encoder.layers.10.self_attn.q_proj.tlut', 'vision_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.q_proj.trellis', 'vision_model.encoder.layers.10.self_attn.v_proj.SU', 'vision_model.encoder.layers.10.self_attn.v_proj.SV', 'vision_model.encoder.layers.10.self_attn.v_proj.rcp', 'vision_model.encoder.layers.10.self_attn.v_proj.tlut', 'vision_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.v_proj.trellis', 'vision_model.encoder.layers.11.mlp.fc1.SU', 'vision_model.encoder.layers.11.mlp.fc1.SV', 'vision_model.encoder.layers.11.mlp.fc1.rcp', 'vision_model.encoder.layers.11.mlp.fc1.tlut', 'vision_model.encoder.layers.11.mlp.fc1.tp_rank', 'vision_model.encoder.layers.11.mlp.fc1.trellis', 'vision_model.encoder.layers.11.mlp.fc2.SU', 'vision_model.encoder.layers.11.mlp.fc2.SV', 'vision_model.encoder.layers.11.mlp.fc2.rcp', 'vision_model.encoder.layers.11.mlp.fc2.tlut', 'vision_model.encoder.layers.11.mlp.fc2.tp_rank', 'vision_model.encoder.layers.11.mlp.fc2.trellis', 'vision_model.encoder.layers.11.self_attn.k_proj.SU', 'vision_model.encoder.layers.11.self_attn.k_proj.SV', 'vision_model.encoder.layers.11.self_attn.k_proj.rcp', 'vision_model.encoder.layers.11.self_attn.k_proj.tlut', 'vision_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.k_proj.trellis', 'vision_model.encoder.layers.11.self_attn.out_proj.SU', 'vision_model.encoder.layers.11.self_attn.out_proj.SV', 'vision_model.encoder.layers.11.self_attn.out_proj.rcp', 'vision_model.encoder.layers.11.self_attn.out_proj.tlut', 'vision_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.out_proj.trellis', 'vision_model.encoder.layers.11.self_attn.q_proj.SU', 'vision_model.encoder.layers.11.self_attn.q_proj.SV', 'vision_model.encoder.layers.11.self_attn.q_proj.rcp', 'vision_model.encoder.layers.11.self_attn.q_proj.tlut', 'vision_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.q_proj.trellis', 'vision_model.encoder.layers.11.self_attn.v_proj.SU', 'vision_model.encoder.layers.11.self_attn.v_proj.SV', 'vision_model.encoder.layers.11.self_attn.v_proj.rcp', 'vision_model.encoder.layers.11.self_attn.v_proj.tlut', 'vision_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.v_proj.trellis', 'vision_model.encoder.layers.12.mlp.fc1.SU', 'vision_model.encoder.layers.12.mlp.fc1.SV', 'vision_model.encoder.layers.12.mlp.fc1.rcp', 'vision_model.encoder.layers.12.mlp.fc1.tlut', 'vision_model.encoder.layers.12.mlp.fc1.tp_rank', 'vision_model.encoder.layers.12.mlp.fc1.trellis', 'vision_model.encoder.layers.12.mlp.fc2.SU', 'vision_model.encoder.layers.12.mlp.fc2.SV', 'vision_model.encoder.layers.12.mlp.fc2.rcp', 'vision_model.encoder.layers.12.mlp.fc2.tlut', 'vision_model.encoder.layers.12.mlp.fc2.tp_rank', 'vision_model.encoder.layers.12.mlp.fc2.trellis', 'vision_model.encoder.layers.12.self_attn.k_proj.SU', 'vision_model.encoder.layers.12.self_attn.k_proj.SV', 'vision_model.encoder.layers.12.self_attn.k_proj.rcp', 'vision_model.encoder.layers.12.self_attn.k_proj.tlut', 'vision_model.encoder.layers.12.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.k_proj.trellis', 'vision_model.encoder.layers.12.self_attn.out_proj.SU', 'vision_model.encoder.layers.12.self_attn.out_proj.SV', 'vision_model.encoder.layers.12.self_attn.out_proj.rcp', 'vision_model.encoder.layers.12.self_attn.out_proj.tlut', 'vision_model.encoder.layers.12.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.out_proj.trellis', 'vision_model.encoder.layers.12.self_attn.q_proj.SU', 'vision_model.encoder.layers.12.self_attn.q_proj.SV', 'vision_model.encoder.layers.12.self_attn.q_proj.rcp', 'vision_model.encoder.layers.12.self_attn.q_proj.tlut', 'vision_model.encoder.layers.12.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.q_proj.trellis', 'vision_model.encoder.layers.12.self_attn.v_proj.SU', 'vision_model.encoder.layers.12.self_attn.v_proj.SV', 'vision_model.encoder.layers.12.self_attn.v_proj.rcp', 'vision_model.encoder.layers.12.self_attn.v_proj.tlut', 'vision_model.encoder.layers.12.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.v_proj.trellis', 'vision_model.encoder.layers.13.mlp.fc1.SU', 'vision_model.encoder.layers.13.mlp.fc1.SV', 'vision_model.encoder.layers.13.mlp.fc1.rcp', 'vision_model.encoder.layers.13.mlp.fc1.tlut', 'vision_model.encoder.layers.13.mlp.fc1.tp_rank', 'vision_model.encoder.layers.13.mlp.fc1.trellis', 'vision_model.encoder.layers.13.mlp.fc2.SU', 'vision_model.encoder.layers.13.mlp.fc2.SV', 'vision_model.encoder.layers.13.mlp.fc2.rcp', 'vision_model.encoder.layers.13.mlp.fc2.tlut', 'vision_model.encoder.layers.13.mlp.fc2.tp_rank', 'vision_model.encoder.layers.13.mlp.fc2.trellis', 'vision_model.encoder.layers.13.self_attn.k_proj.SU', 'vision_model.encoder.layers.13.self_attn.k_proj.SV', 'vision_model.encoder.layers.13.self_attn.k_proj.rcp', 'vision_model.encoder.layers.13.self_attn.k_proj.tlut', 'vision_model.encoder.layers.13.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.k_proj.trellis', 'vision_model.encoder.layers.13.self_attn.out_proj.SU', 'vision_model.encoder.layers.13.self_attn.out_proj.SV', 'vision_model.encoder.layers.13.self_attn.out_proj.rcp', 'vision_model.encoder.layers.13.self_attn.out_proj.tlut', 'vision_model.encoder.layers.13.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.out_proj.trellis', 'vision_model.encoder.layers.13.self_attn.q_proj.SU', 'vision_model.encoder.layers.13.self_attn.q_proj.SV', 'vision_model.encoder.layers.13.self_attn.q_proj.rcp', 'vision_model.encoder.layers.13.self_attn.q_proj.tlut', 'vision_model.encoder.layers.13.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.q_proj.trellis', 'vision_model.encoder.layers.13.self_attn.v_proj.SU', 'vision_model.encoder.layers.13.self_attn.v_proj.SV', 'vision_model.encoder.layers.13.self_attn.v_proj.rcp', 'vision_model.encoder.layers.13.self_attn.v_proj.tlut', 'vision_model.encoder.layers.13.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.v_proj.trellis', 'vision_model.encoder.layers.14.mlp.fc1.SU', 'vision_model.encoder.layers.14.mlp.fc1.SV', 'vision_model.encoder.layers.14.mlp.fc1.rcp', 'vision_model.encoder.layers.14.mlp.fc1.tlut', 'vision_model.encoder.layers.14.mlp.fc1.tp_rank', 'vision_model.encoder.layers.14.mlp.fc1.trellis', 'vision_model.encoder.layers.14.mlp.fc2.SU', 'vision_model.encoder.layers.14.mlp.fc2.SV', 'vision_model.encoder.layers.14.mlp.fc2.rcp', 'vision_model.encoder.layers.14.mlp.fc2.tlut', 'vision_model.encoder.layers.14.mlp.fc2.tp_rank', 'vision_model.encoder.layers.14.mlp.fc2.trellis', 'vision_model.encoder.layers.14.self_attn.k_proj.SU', 'vision_model.encoder.layers.14.self_attn.k_proj.SV', 'vision_model.encoder.layers.14.self_attn.k_proj.rcp', 'vision_model.encoder.layers.14.self_attn.k_proj.tlut', 'vision_model.encoder.layers.14.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.k_proj.trellis', 'vision_model.encoder.layers.14.self_attn.out_proj.SU', 'vision_model.encoder.layers.14.self_attn.out_proj.SV', 'vision_model.encoder.layers.14.self_attn.out_proj.rcp', 'vision_model.encoder.layers.14.self_attn.out_proj.tlut', 'vision_model.encoder.layers.14.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.out_proj.trellis', 'vision_model.encoder.layers.14.self_attn.q_proj.SU', 'vision_model.encoder.layers.14.self_attn.q_proj.SV', 'vision_model.encoder.layers.14.self_attn.q_proj.rcp', 'vision_model.encoder.layers.14.self_attn.q_proj.tlut', 'vision_model.encoder.layers.14.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.q_proj.trellis', 'vision_model.encoder.layers.14.self_attn.v_proj.SU', 'vision_model.encoder.layers.14.self_attn.v_proj.SV', 'vision_model.encoder.layers.14.self_attn.v_proj.rcp', 'vision_model.encoder.layers.14.self_attn.v_proj.tlut', 'vision_model.encoder.layers.14.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.v_proj.trellis', 'vision_model.encoder.layers.15.mlp.fc1.SU', 'vision_model.encoder.layers.15.mlp.fc1.SV', 'vision_model.encoder.layers.15.mlp.fc1.rcp', 'vision_model.encoder.layers.15.mlp.fc1.tlut', 'vision_model.encoder.layers.15.mlp.fc1.tp_rank', 'vision_model.encoder.layers.15.mlp.fc1.trellis', 'vision_model.encoder.layers.15.mlp.fc2.SU', 'vision_model.encoder.layers.15.mlp.fc2.SV', 'vision_model.encoder.layers.15.mlp.fc2.rcp', 'vision_model.encoder.layers.15.mlp.fc2.tlut', 'vision_model.encoder.layers.15.mlp.fc2.tp_rank', 'vision_model.encoder.layers.15.mlp.fc2.trellis', 'vision_model.encoder.layers.15.self_attn.k_proj.SU', 'vision_model.encoder.layers.15.self_attn.k_proj.SV', 'vision_model.encoder.layers.15.self_attn.k_proj.rcp', 'vision_model.encoder.layers.15.self_attn.k_proj.tlut', 'vision_model.encoder.layers.15.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.k_proj.trellis', 'vision_model.encoder.layers.15.self_attn.out_proj.SU', 'vision_model.encoder.layers.15.self_attn.out_proj.SV', 'vision_model.encoder.layers.15.self_attn.out_proj.rcp', 'vision_model.encoder.layers.15.self_attn.out_proj.tlut', 'vision_model.encoder.layers.15.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.out_proj.trellis', 'vision_model.encoder.layers.15.self_attn.q_proj.SU', 'vision_model.encoder.layers.15.self_attn.q_proj.SV', 'vision_model.encoder.layers.15.self_attn.q_proj.rcp', 'vision_model.encoder.layers.15.self_attn.q_proj.tlut', 'vision_model.encoder.layers.15.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.q_proj.trellis', 'vision_model.encoder.layers.15.self_attn.v_proj.SU', 'vision_model.encoder.layers.15.self_attn.v_proj.SV', 'vision_model.encoder.layers.15.self_attn.v_proj.rcp', 'vision_model.encoder.layers.15.self_attn.v_proj.tlut', 'vision_model.encoder.layers.15.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.v_proj.trellis', 'vision_model.encoder.layers.16.mlp.fc1.SU', 'vision_model.encoder.layers.16.mlp.fc1.SV', 'vision_model.encoder.layers.16.mlp.fc1.rcp', 'vision_model.encoder.layers.16.mlp.fc1.tlut', 'vision_model.encoder.layers.16.mlp.fc1.tp_rank', 'vision_model.encoder.layers.16.mlp.fc1.trellis', 'vision_model.encoder.layers.16.mlp.fc2.SU', 'vision_model.encoder.layers.16.mlp.fc2.SV', 'vision_model.encoder.layers.16.mlp.fc2.rcp', 'vision_model.encoder.layers.16.mlp.fc2.tlut', 'vision_model.encoder.layers.16.mlp.fc2.tp_rank', 'vision_model.encoder.layers.16.mlp.fc2.trellis', 'vision_model.encoder.layers.16.self_attn.k_proj.SU', 'vision_model.encoder.layers.16.self_attn.k_proj.SV', 'vision_model.encoder.layers.16.self_attn.k_proj.rcp', 'vision_model.encoder.layers.16.self_attn.k_proj.tlut', 'vision_model.encoder.layers.16.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.k_proj.trellis', 'vision_model.encoder.layers.16.self_attn.out_proj.SU', 'vision_model.encoder.layers.16.self_attn.out_proj.SV', 'vision_model.encoder.layers.16.self_attn.out_proj.rcp', 'vision_model.encoder.layers.16.self_attn.out_proj.tlut', 'vision_model.encoder.layers.16.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.out_proj.trellis', 'vision_model.encoder.layers.16.self_attn.q_proj.SU', 'vision_model.encoder.layers.16.self_attn.q_proj.SV', 'vision_model.encoder.layers.16.self_attn.q_proj.rcp', 'vision_model.encoder.layers.16.self_attn.q_proj.tlut', 'vision_model.encoder.layers.16.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.q_proj.trellis', 'vision_model.encoder.layers.16.self_attn.v_proj.SU', 'vision_model.encoder.layers.16.self_attn.v_proj.SV', 'vision_model.encoder.layers.16.self_attn.v_proj.rcp', 'vision_model.encoder.layers.16.self_attn.v_proj.tlut', 'vision_model.encoder.layers.16.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.v_proj.trellis', 'vision_model.encoder.layers.17.mlp.fc1.SU', 'vision_model.encoder.layers.17.mlp.fc1.SV', 'vision_model.encoder.layers.17.mlp.fc1.rcp', 'vision_model.encoder.layers.17.mlp.fc1.tlut', 'vision_model.encoder.layers.17.mlp.fc1.tp_rank', 'vision_model.encoder.layers.17.mlp.fc1.trellis', 'vision_model.encoder.layers.17.mlp.fc2.SU', 'vision_model.encoder.layers.17.mlp.fc2.SV', 'vision_model.encoder.layers.17.mlp.fc2.rcp', 'vision_model.encoder.layers.17.mlp.fc2.tlut', 'vision_model.encoder.layers.17.mlp.fc2.tp_rank', 'vision_model.encoder.layers.17.mlp.fc2.trellis', 'vision_model.encoder.layers.17.self_attn.k_proj.SU', 'vision_model.encoder.layers.17.self_attn.k_proj.SV', 'vision_model.encoder.layers.17.self_attn.k_proj.rcp', 'vision_model.encoder.layers.17.self_attn.k_proj.tlut', 'vision_model.encoder.layers.17.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.k_proj.trellis', 'vision_model.encoder.layers.17.self_attn.out_proj.SU', 'vision_model.encoder.layers.17.self_attn.out_proj.SV', 'vision_model.encoder.layers.17.self_attn.out_proj.rcp', 'vision_model.encoder.layers.17.self_attn.out_proj.tlut', 'vision_model.encoder.layers.17.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.out_proj.trellis', 'vision_model.encoder.layers.17.self_attn.q_proj.SU', 'vision_model.encoder.layers.17.self_attn.q_proj.SV', 'vision_model.encoder.layers.17.self_attn.q_proj.rcp', 'vision_model.encoder.layers.17.self_attn.q_proj.tlut', 'vision_model.encoder.layers.17.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.q_proj.trellis', 'vision_model.encoder.layers.17.self_attn.v_proj.SU', 'vision_model.encoder.layers.17.self_attn.v_proj.SV', 'vision_model.encoder.layers.17.self_attn.v_proj.rcp', 'vision_model.encoder.layers.17.self_attn.v_proj.tlut', 'vision_model.encoder.layers.17.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.v_proj.trellis', 'vision_model.encoder.layers.18.mlp.fc1.SU', 'vision_model.encoder.layers.18.mlp.fc1.SV', 'vision_model.encoder.layers.18.mlp.fc1.rcp', 'vision_model.encoder.layers.18.mlp.fc1.tlut', 'vision_model.encoder.layers.18.mlp.fc1.tp_rank', 'vision_model.encoder.layers.18.mlp.fc1.trellis', 'vision_model.encoder.layers.18.mlp.fc2.SU', 'vision_model.encoder.layers.18.mlp.fc2.SV', 'vision_model.encoder.layers.18.mlp.fc2.rcp', 'vision_model.encoder.layers.18.mlp.fc2.tlut', 'vision_model.encoder.layers.18.mlp.fc2.tp_rank', 'vision_model.encoder.layers.18.mlp.fc2.trellis', 'vision_model.encoder.layers.18.self_attn.k_proj.SU', 'vision_model.encoder.layers.18.self_attn.k_proj.SV', 'vision_model.encoder.layers.18.self_attn.k_proj.rcp', 'vision_model.encoder.layers.18.self_attn.k_proj.tlut', 'vision_model.encoder.layers.18.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.k_proj.trellis', 'vision_model.encoder.layers.18.self_attn.out_proj.SU', 'vision_model.encoder.layers.18.self_attn.out_proj.SV', 'vision_model.encoder.layers.18.self_attn.out_proj.rcp', 'vision_model.encoder.layers.18.self_attn.out_proj.tlut', 'vision_model.encoder.layers.18.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.out_proj.trellis', 'vision_model.encoder.layers.18.self_attn.q_proj.SU', 'vision_model.encoder.layers.18.self_attn.q_proj.SV', 'vision_model.encoder.layers.18.self_attn.q_proj.rcp', 'vision_model.encoder.layers.18.self_attn.q_proj.tlut', 'vision_model.encoder.layers.18.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.q_proj.trellis', 'vision_model.encoder.layers.18.self_attn.v_proj.SU', 'vision_model.encoder.layers.18.self_attn.v_proj.SV', 'vision_model.encoder.layers.18.self_attn.v_proj.rcp', 'vision_model.encoder.layers.18.self_attn.v_proj.tlut', 'vision_model.encoder.layers.18.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.v_proj.trellis', 'vision_model.encoder.layers.19.mlp.fc1.SU', 'vision_model.encoder.layers.19.mlp.fc1.SV', 'vision_model.encoder.layers.19.mlp.fc1.rcp', 'vision_model.encoder.layers.19.mlp.fc1.tlut', 'vision_model.encoder.layers.19.mlp.fc1.tp_rank', 'vision_model.encoder.layers.19.mlp.fc1.trellis', 'vision_model.encoder.layers.19.mlp.fc2.SU', 'vision_model.encoder.layers.19.mlp.fc2.SV', 'vision_model.encoder.layers.19.mlp.fc2.rcp', 'vision_model.encoder.layers.19.mlp.fc2.tlut', 'vision_model.encoder.layers.19.mlp.fc2.tp_rank', 'vision_model.encoder.layers.19.mlp.fc2.trellis', 'vision_model.encoder.layers.19.self_attn.k_proj.SU', 'vision_model.encoder.layers.19.self_attn.k_proj.SV', 'vision_model.encoder.layers.19.self_attn.k_proj.rcp', 'vision_model.encoder.layers.19.self_attn.k_proj.tlut', 'vision_model.encoder.layers.19.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.k_proj.trellis', 'vision_model.encoder.layers.19.self_attn.out_proj.SU', 'vision_model.encoder.layers.19.self_attn.out_proj.SV', 'vision_model.encoder.layers.19.self_attn.out_proj.rcp', 'vision_model.encoder.layers.19.self_attn.out_proj.tlut', 'vision_model.encoder.layers.19.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.out_proj.trellis', 'vision_model.encoder.layers.19.self_attn.q_proj.SU', 'vision_model.encoder.layers.19.self_attn.q_proj.SV', 'vision_model.encoder.layers.19.self_attn.q_proj.rcp', 'vision_model.encoder.layers.19.self_attn.q_proj.tlut', 'vision_model.encoder.layers.19.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.q_proj.trellis', 'vision_model.encoder.layers.19.self_attn.v_proj.SU', 'vision_model.encoder.layers.19.self_attn.v_proj.SV', 'vision_model.encoder.layers.19.self_attn.v_proj.rcp', 'vision_model.encoder.layers.19.self_attn.v_proj.tlut', 'vision_model.encoder.layers.19.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.v_proj.trellis', 'vision_model.encoder.layers.2.mlp.fc1.SU', 'vision_model.encoder.layers.2.mlp.fc1.SV', 'vision_model.encoder.layers.2.mlp.fc1.rcp', 'vision_model.encoder.layers.2.mlp.fc1.tlut', 'vision_model.encoder.layers.2.mlp.fc1.tp_rank', 'vision_model.encoder.layers.2.mlp.fc1.trellis', 'vision_model.encoder.layers.2.mlp.fc2.SU', 'vision_model.encoder.layers.2.mlp.fc2.SV', 'vision_model.encoder.layers.2.mlp.fc2.rcp', 'vision_model.encoder.layers.2.mlp.fc2.tlut', 'vision_model.encoder.layers.2.mlp.fc2.tp_rank', 'vision_model.encoder.layers.2.mlp.fc2.trellis', 'vision_model.encoder.layers.2.self_attn.k_proj.SU', 'vision_model.encoder.layers.2.self_attn.k_proj.SV', 'vision_model.encoder.layers.2.self_attn.k_proj.rcp', 'vision_model.encoder.layers.2.self_attn.k_proj.tlut', 'vision_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.k_proj.trellis', 'vision_model.encoder.layers.2.self_attn.out_proj.SU', 'vision_model.encoder.layers.2.self_attn.out_proj.SV', 'vision_model.encoder.layers.2.self_attn.out_proj.rcp', 'vision_model.encoder.layers.2.self_attn.out_proj.tlut', 'vision_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.out_proj.trellis', 'vision_model.encoder.layers.2.self_attn.q_proj.SU', 'vision_model.encoder.layers.2.self_attn.q_proj.SV', 'vision_model.encoder.layers.2.self_attn.q_proj.rcp', 'vision_model.encoder.layers.2.self_attn.q_proj.tlut', 'vision_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.q_proj.trellis', 'vision_model.encoder.layers.2.self_attn.v_proj.SU', 'vision_model.encoder.layers.2.self_attn.v_proj.SV', 'vision_model.encoder.layers.2.self_attn.v_proj.rcp', 'vision_model.encoder.layers.2.self_attn.v_proj.tlut', 'vision_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.v_proj.trellis', 'vision_model.encoder.layers.20.mlp.fc1.SU', 'vision_model.encoder.layers.20.mlp.fc1.SV', 'vision_model.encoder.layers.20.mlp.fc1.rcp', 'vision_model.encoder.layers.20.mlp.fc1.tlut', 'vision_model.encoder.layers.20.mlp.fc1.tp_rank', 'vision_model.encoder.layers.20.mlp.fc1.trellis', 'vision_model.encoder.layers.20.mlp.fc2.SU', 'vision_model.encoder.layers.20.mlp.fc2.SV', 'vision_model.encoder.layers.20.mlp.fc2.rcp', 'vision_model.encoder.layers.20.mlp.fc2.tlut', 'vision_model.encoder.layers.20.mlp.fc2.tp_rank', 'vision_model.encoder.layers.20.mlp.fc2.trellis', 'vision_model.encoder.layers.20.self_attn.k_proj.SU', 'vision_model.encoder.layers.20.self_attn.k_proj.SV', 'vision_model.encoder.layers.20.self_attn.k_proj.rcp', 'vision_model.encoder.layers.20.self_attn.k_proj.tlut', 'vision_model.encoder.layers.20.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.k_proj.trellis', 'vision_model.encoder.layers.20.self_attn.out_proj.SU', 'vision_model.encoder.layers.20.self_attn.out_proj.SV', 'vision_model.encoder.layers.20.self_attn.out_proj.rcp', 'vision_model.encoder.layers.20.self_attn.out_proj.tlut', 'vision_model.encoder.layers.20.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.out_proj.trellis', 'vision_model.encoder.layers.20.self_attn.q_proj.SU', 'vision_model.encoder.layers.20.self_attn.q_proj.SV', 'vision_model.encoder.layers.20.self_attn.q_proj.rcp', 'vision_model.encoder.layers.20.self_attn.q_proj.tlut', 'vision_model.encoder.layers.20.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.q_proj.trellis', 'vision_model.encoder.layers.20.self_attn.v_proj.SU', 'vision_model.encoder.layers.20.self_attn.v_proj.SV', 'vision_model.encoder.layers.20.self_attn.v_proj.rcp', 'vision_model.encoder.layers.20.self_attn.v_proj.tlut', 'vision_model.encoder.layers.20.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.v_proj.trellis', 'vision_model.encoder.layers.21.mlp.fc1.SU', 'vision_model.encoder.layers.21.mlp.fc1.SV', 'vision_model.encoder.layers.21.mlp.fc1.rcp', 'vision_model.encoder.layers.21.mlp.fc1.tlut', 'vision_model.encoder.layers.21.mlp.fc1.tp_rank', 'vision_model.encoder.layers.21.mlp.fc1.trellis', 'vision_model.encoder.layers.21.mlp.fc2.SU', 'vision_model.encoder.layers.21.mlp.fc2.SV', 'vision_model.encoder.layers.21.mlp.fc2.rcp', 'vision_model.encoder.layers.21.mlp.fc2.tlut', 'vision_model.encoder.layers.21.mlp.fc2.tp_rank', 'vision_model.encoder.layers.21.mlp.fc2.trellis', 'vision_model.encoder.layers.21.self_attn.k_proj.SU', 'vision_model.encoder.layers.21.self_attn.k_proj.SV', 'vision_model.encoder.layers.21.self_attn.k_proj.rcp', 'vision_model.encoder.layers.21.self_attn.k_proj.tlut', 'vision_model.encoder.layers.21.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.k_proj.trellis', 'vision_model.encoder.layers.21.self_attn.out_proj.SU', 'vision_model.encoder.layers.21.self_attn.out_proj.SV', 'vision_model.encoder.layers.21.self_attn.out_proj.rcp', 'vision_model.encoder.layers.21.self_attn.out_proj.tlut', 'vision_model.encoder.layers.21.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.out_proj.trellis', 'vision_model.encoder.layers.21.self_attn.q_proj.SU', 'vision_model.encoder.layers.21.self_attn.q_proj.SV', 'vision_model.encoder.layers.21.self_attn.q_proj.rcp', 'vision_model.encoder.layers.21.self_attn.q_proj.tlut', 'vision_model.encoder.layers.21.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.q_proj.trellis', 'vision_model.encoder.layers.21.self_attn.v_proj.SU', 'vision_model.encoder.layers.21.self_attn.v_proj.SV', 'vision_model.encoder.layers.21.self_attn.v_proj.rcp', 'vision_model.encoder.layers.21.self_attn.v_proj.tlut', 'vision_model.encoder.layers.21.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.v_proj.trellis', 'vision_model.encoder.layers.22.mlp.fc1.SU', 'vision_model.encoder.layers.22.mlp.fc1.SV', 'vision_model.encoder.layers.22.mlp.fc1.rcp', 'vision_model.encoder.layers.22.mlp.fc1.tlut', 'vision_model.encoder.layers.22.mlp.fc1.tp_rank', 'vision_model.encoder.layers.22.mlp.fc1.trellis', 'vision_model.encoder.layers.22.mlp.fc2.SU', 'vision_model.encoder.layers.22.mlp.fc2.SV', 'vision_model.encoder.layers.22.mlp.fc2.rcp', 'vision_model.encoder.layers.22.mlp.fc2.tlut', 'vision_model.encoder.layers.22.mlp.fc2.tp_rank', 'vision_model.encoder.layers.22.mlp.fc2.trellis', 'vision_model.encoder.layers.22.self_attn.k_proj.SU', 'vision_model.encoder.layers.22.self_attn.k_proj.SV', 'vision_model.encoder.layers.22.self_attn.k_proj.rcp', 'vision_model.encoder.layers.22.self_attn.k_proj.tlut', 'vision_model.encoder.layers.22.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.k_proj.trellis', 'vision_model.encoder.layers.22.self_attn.out_proj.SU', 'vision_model.encoder.layers.22.self_attn.out_proj.SV', 'vision_model.encoder.layers.22.self_attn.out_proj.rcp', 'vision_model.encoder.layers.22.self_attn.out_proj.tlut', 'vision_model.encoder.layers.22.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.out_proj.trellis', 'vision_model.encoder.layers.22.self_attn.q_proj.SU', 'vision_model.encoder.layers.22.self_attn.q_proj.SV', 'vision_model.encoder.layers.22.self_attn.q_proj.rcp', 'vision_model.encoder.layers.22.self_attn.q_proj.tlut', 'vision_model.encoder.layers.22.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.q_proj.trellis', 'vision_model.encoder.layers.22.self_attn.v_proj.SU', 'vision_model.encoder.layers.22.self_attn.v_proj.SV', 'vision_model.encoder.layers.22.self_attn.v_proj.rcp', 'vision_model.encoder.layers.22.self_attn.v_proj.tlut', 'vision_model.encoder.layers.22.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.v_proj.trellis', 'vision_model.encoder.layers.23.mlp.fc1.SU', 'vision_model.encoder.layers.23.mlp.fc1.SV', 'vision_model.encoder.layers.23.mlp.fc1.rcp', 'vision_model.encoder.layers.23.mlp.fc1.tlut', 'vision_model.encoder.layers.23.mlp.fc1.tp_rank', 'vision_model.encoder.layers.23.mlp.fc1.trellis', 'vision_model.encoder.layers.23.mlp.fc2.SU', 'vision_model.encoder.layers.23.mlp.fc2.SV', 'vision_model.encoder.layers.23.mlp.fc2.rcp', 'vision_model.encoder.layers.23.mlp.fc2.tlut', 'vision_model.encoder.layers.23.mlp.fc2.tp_rank', 'vision_model.encoder.layers.23.mlp.fc2.trellis', 'vision_model.encoder.layers.23.self_attn.k_proj.SU', 'vision_model.encoder.layers.23.self_attn.k_proj.SV', 'vision_model.encoder.layers.23.self_attn.k_proj.rcp', 'vision_model.encoder.layers.23.self_attn.k_proj.tlut', 'vision_model.encoder.layers.23.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.k_proj.trellis', 'vision_model.encoder.layers.23.self_attn.out_proj.SU', 'vision_model.encoder.layers.23.self_attn.out_proj.SV', 'vision_model.encoder.layers.23.self_attn.out_proj.rcp', 'vision_model.encoder.layers.23.self_attn.out_proj.tlut', 'vision_model.encoder.layers.23.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.out_proj.trellis', 'vision_model.encoder.layers.23.self_attn.q_proj.SU', 'vision_model.encoder.layers.23.self_attn.q_proj.SV', 'vision_model.encoder.layers.23.self_attn.q_proj.rcp', 'vision_model.encoder.layers.23.self_attn.q_proj.tlut', 'vision_model.encoder.layers.23.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.q_proj.trellis', 'vision_model.encoder.layers.23.self_attn.v_proj.SU', 'vision_model.encoder.layers.23.self_attn.v_proj.SV', 'vision_model.encoder.layers.23.self_attn.v_proj.rcp', 'vision_model.encoder.layers.23.self_attn.v_proj.tlut', 'vision_model.encoder.layers.23.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.v_proj.trellis', 'vision_model.encoder.layers.3.mlp.fc1.SU', 'vision_model.encoder.layers.3.mlp.fc1.SV', 'vision_model.encoder.layers.3.mlp.fc1.rcp', 'vision_model.encoder.layers.3.mlp.fc1.tlut', 'vision_model.encoder.layers.3.mlp.fc1.tp_rank', 'vision_model.encoder.layers.3.mlp.fc1.trellis', 'vision_model.encoder.layers.3.mlp.fc2.SU', 'vision_model.encoder.layers.3.mlp.fc2.SV', 'vision_model.encoder.layers.3.mlp.fc2.rcp', 'vision_model.encoder.layers.3.mlp.fc2.tlut', 'vision_model.encoder.layers.3.mlp.fc2.tp_rank', 'vision_model.encoder.layers.3.mlp.fc2.trellis', 'vision_model.encoder.layers.3.self_attn.k_proj.SU', 'vision_model.encoder.layers.3.self_attn.k_proj.SV', 'vision_model.encoder.layers.3.self_attn.k_proj.rcp', 'vision_model.encoder.layers.3.self_attn.k_proj.tlut', 'vision_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.k_proj.trellis', 'vision_model.encoder.layers.3.self_attn.out_proj.SU', 'vision_model.encoder.layers.3.self_attn.out_proj.SV', 'vision_model.encoder.layers.3.self_attn.out_proj.rcp', 'vision_model.encoder.layers.3.self_attn.out_proj.tlut', 'vision_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.out_proj.trellis', 'vision_model.encoder.layers.3.self_attn.q_proj.SU', 'vision_model.encoder.layers.3.self_attn.q_proj.SV', 'vision_model.encoder.layers.3.self_attn.q_proj.rcp', 'vision_model.encoder.layers.3.self_attn.q_proj.tlut', 'vision_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.q_proj.trellis', 'vision_model.encoder.layers.3.self_attn.v_proj.SU', 'vision_model.encoder.layers.3.self_attn.v_proj.SV', 'vision_model.encoder.layers.3.self_attn.v_proj.rcp', 'vision_model.encoder.layers.3.self_attn.v_proj.tlut', 'vision_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.v_proj.trellis', 'vision_model.encoder.layers.4.mlp.fc1.SU', 'vision_model.encoder.layers.4.mlp.fc1.SV', 'vision_model.encoder.layers.4.mlp.fc1.rcp', 'vision_model.encoder.layers.4.mlp.fc1.tlut', 'vision_model.encoder.layers.4.mlp.fc1.tp_rank', 'vision_model.encoder.layers.4.mlp.fc1.trellis', 'vision_model.encoder.layers.4.mlp.fc2.SU', 'vision_model.encoder.layers.4.mlp.fc2.SV', 'vision_model.encoder.layers.4.mlp.fc2.rcp', 'vision_model.encoder.layers.4.mlp.fc2.tlut', 'vision_model.encoder.layers.4.mlp.fc2.tp_rank', 'vision_model.encoder.layers.4.mlp.fc2.trellis', 'vision_model.encoder.layers.4.self_attn.k_proj.SU', 'vision_model.encoder.layers.4.self_attn.k_proj.SV', 'vision_model.encoder.layers.4.self_attn.k_proj.rcp', 'vision_model.encoder.layers.4.self_attn.k_proj.tlut', 'vision_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.k_proj.trellis', 'vision_model.encoder.layers.4.self_attn.out_proj.SU', 'vision_model.encoder.layers.4.self_attn.out_proj.SV', 'vision_model.encoder.layers.4.self_attn.out_proj.rcp', 'vision_model.encoder.layers.4.self_attn.out_proj.tlut', 'vision_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.out_proj.trellis', 'vision_model.encoder.layers.4.self_attn.q_proj.SU', 'vision_model.encoder.layers.4.self_attn.q_proj.SV', 'vision_model.encoder.layers.4.self_attn.q_proj.rcp', 'vision_model.encoder.layers.4.self_attn.q_proj.tlut', 'vision_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.q_proj.trellis', 'vision_model.encoder.layers.4.self_attn.v_proj.SU', 'vision_model.encoder.layers.4.self_attn.v_proj.SV', 'vision_model.encoder.layers.4.self_attn.v_proj.rcp', 'vision_model.encoder.layers.4.self_attn.v_proj.tlut', 'vision_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.v_proj.trellis', 'vision_model.encoder.layers.5.mlp.fc1.SU', 'vision_model.encoder.layers.5.mlp.fc1.SV', 'vision_model.encoder.layers.5.mlp.fc1.rcp', 'vision_model.encoder.layers.5.mlp.fc1.tlut', 'vision_model.encoder.layers.5.mlp.fc1.tp_rank', 'vision_model.encoder.layers.5.mlp.fc1.trellis', 'vision_model.encoder.layers.5.mlp.fc2.SU', 'vision_model.encoder.layers.5.mlp.fc2.SV', 'vision_model.encoder.layers.5.mlp.fc2.rcp', 'vision_model.encoder.layers.5.mlp.fc2.tlut', 'vision_model.encoder.layers.5.mlp.fc2.tp_rank', 'vision_model.encoder.layers.5.mlp.fc2.trellis', 'vision_model.encoder.layers.5.self_attn.k_proj.SU', 'vision_model.encoder.layers.5.self_attn.k_proj.SV', 'vision_model.encoder.layers.5.self_attn.k_proj.rcp', 'vision_model.encoder.layers.5.self_attn.k_proj.tlut', 'vision_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.k_proj.trellis', 'vision_model.encoder.layers.5.self_attn.out_proj.SU', 'vision_model.encoder.layers.5.self_attn.out_proj.SV', 'vision_model.encoder.layers.5.self_attn.out_proj.rcp', 'vision_model.encoder.layers.5.self_attn.out_proj.tlut', 'vision_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.out_proj.trellis', 'vision_model.encoder.layers.5.self_attn.q_proj.SU', 'vision_model.encoder.layers.5.self_attn.q_proj.SV', 'vision_model.encoder.layers.5.self_attn.q_proj.rcp', 'vision_model.encoder.layers.5.self_attn.q_proj.tlut', 'vision_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.q_proj.trellis', 'vision_model.encoder.layers.5.self_attn.v_proj.SU', 'vision_model.encoder.layers.5.self_attn.v_proj.SV', 'vision_model.encoder.layers.5.self_attn.v_proj.rcp', 'vision_model.encoder.layers.5.self_attn.v_proj.tlut', 'vision_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.v_proj.trellis', 'vision_model.encoder.layers.6.mlp.fc1.SU', 'vision_model.encoder.layers.6.mlp.fc1.SV', 'vision_model.encoder.layers.6.mlp.fc1.rcp', 'vision_model.encoder.layers.6.mlp.fc1.tlut', 'vision_model.encoder.layers.6.mlp.fc1.tp_rank', 'vision_model.encoder.layers.6.mlp.fc1.trellis', 'vision_model.encoder.layers.6.mlp.fc2.SU', 'vision_model.encoder.layers.6.mlp.fc2.SV', 'vision_model.encoder.layers.6.mlp.fc2.rcp', 'vision_model.encoder.layers.6.mlp.fc2.tlut', 'vision_model.encoder.layers.6.mlp.fc2.tp_rank', 'vision_model.encoder.layers.6.mlp.fc2.trellis', 'vision_model.encoder.layers.6.self_attn.k_proj.SU', 'vision_model.encoder.layers.6.self_attn.k_proj.SV', 'vision_model.encoder.layers.6.self_attn.k_proj.rcp', 'vision_model.encoder.layers.6.self_attn.k_proj.tlut', 'vision_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.k_proj.trellis', 'vision_model.encoder.layers.6.self_attn.out_proj.SU', 'vision_model.encoder.layers.6.self_attn.out_proj.SV', 'vision_model.encoder.layers.6.self_attn.out_proj.rcp', 'vision_model.encoder.layers.6.self_attn.out_proj.tlut', 'vision_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.out_proj.trellis', 'vision_model.encoder.layers.6.self_attn.q_proj.SU', 'vision_model.encoder.layers.6.self_attn.q_proj.SV', 'vision_model.encoder.layers.6.self_attn.q_proj.rcp', 'vision_model.encoder.layers.6.self_attn.q_proj.tlut', 'vision_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.q_proj.trellis', 'vision_model.encoder.layers.6.self_attn.v_proj.SU', 'vision_model.encoder.layers.6.self_attn.v_proj.SV', 'vision_model.encoder.layers.6.self_attn.v_proj.rcp', 'vision_model.encoder.layers.6.self_attn.v_proj.tlut', 'vision_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.v_proj.trellis', 'vision_model.encoder.layers.7.mlp.fc1.SU', 'vision_model.encoder.layers.7.mlp.fc1.SV', 'vision_model.encoder.layers.7.mlp.fc1.rcp', 'vision_model.encoder.layers.7.mlp.fc1.tlut', 'vision_model.encoder.layers.7.mlp.fc1.tp_rank', 'vision_model.encoder.layers.7.mlp.fc1.trellis', 'vision_model.encoder.layers.7.mlp.fc2.SU', 'vision_model.encoder.layers.7.mlp.fc2.SV', 'vision_model.encoder.layers.7.mlp.fc2.rcp', 'vision_model.encoder.layers.7.mlp.fc2.tlut', 'vision_model.encoder.layers.7.mlp.fc2.tp_rank', 'vision_model.encoder.layers.7.mlp.fc2.trellis', 'vision_model.encoder.layers.7.self_attn.k_proj.SU', 'vision_model.encoder.layers.7.self_attn.k_proj.SV', 'vision_model.encoder.layers.7.self_attn.k_proj.rcp', 'vision_model.encoder.layers.7.self_attn.k_proj.tlut', 'vision_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.k_proj.trellis', 'vision_model.encoder.layers.7.self_attn.out_proj.SU', 'vision_model.encoder.layers.7.self_attn.out_proj.SV', 'vision_model.encoder.layers.7.self_attn.out_proj.rcp', 'vision_model.encoder.layers.7.self_attn.out_proj.tlut', 'vision_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.out_proj.trellis', 'vision_model.encoder.layers.7.self_attn.q_proj.SU', 'vision_model.encoder.layers.7.self_attn.q_proj.SV', 'vision_model.encoder.layers.7.self_attn.q_proj.rcp', 'vision_model.encoder.layers.7.self_attn.q_proj.tlut', 'vision_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.q_proj.trellis', 'vision_model.encoder.layers.7.self_attn.v_proj.SU', 'vision_model.encoder.layers.7.self_attn.v_proj.SV', 'vision_model.encoder.layers.7.self_attn.v_proj.rcp', 'vision_model.encoder.layers.7.self_attn.v_proj.tlut', 'vision_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.v_proj.trellis', 'vision_model.encoder.layers.8.mlp.fc1.SU', 'vision_model.encoder.layers.8.mlp.fc1.SV', 'vision_model.encoder.layers.8.mlp.fc1.rcp', 'vision_model.encoder.layers.8.mlp.fc1.tlut', 'vision_model.encoder.layers.8.mlp.fc1.tp_rank', 'vision_model.encoder.layers.8.mlp.fc1.trellis', 'vision_model.encoder.layers.8.mlp.fc2.SU', 'vision_model.encoder.layers.8.mlp.fc2.SV', 'vision_model.encoder.layers.8.mlp.fc2.rcp', 'vision_model.encoder.layers.8.mlp.fc2.tlut', 'vision_model.encoder.layers.8.mlp.fc2.tp_rank', 'vision_model.encoder.layers.8.mlp.fc2.trellis', 'vision_model.encoder.layers.8.self_attn.k_proj.SU', 'vision_model.encoder.layers.8.self_attn.k_proj.SV', 'vision_model.encoder.layers.8.self_attn.k_proj.rcp', 'vision_model.encoder.layers.8.self_attn.k_proj.tlut', 'vision_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.k_proj.trellis', 'vision_model.encoder.layers.8.self_attn.out_proj.SU', 'vision_model.encoder.layers.8.self_attn.out_proj.SV', 'vision_model.encoder.layers.8.self_attn.out_proj.rcp', 'vision_model.encoder.layers.8.self_attn.out_proj.tlut', 'vision_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.out_proj.trellis', 'vision_model.encoder.layers.8.self_attn.q_proj.SU', 'vision_model.encoder.layers.8.self_attn.q_proj.SV', 'vision_model.encoder.layers.8.self_attn.q_proj.rcp', 'vision_model.encoder.layers.8.self_attn.q_proj.tlut', 'vision_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.q_proj.trellis', 'vision_model.encoder.layers.8.self_attn.v_proj.SU', 'vision_model.encoder.layers.8.self_attn.v_proj.SV', 'vision_model.encoder.layers.8.self_attn.v_proj.rcp', 'vision_model.encoder.layers.8.self_attn.v_proj.tlut', 'vision_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.v_proj.trellis', 'vision_model.encoder.layers.9.mlp.fc1.SU', 'vision_model.encoder.layers.9.mlp.fc1.SV', 'vision_model.encoder.layers.9.mlp.fc1.rcp', 'vision_model.encoder.layers.9.mlp.fc1.tlut', 'vision_model.encoder.layers.9.mlp.fc1.tp_rank', 'vision_model.encoder.layers.9.mlp.fc1.trellis', 'vision_model.encoder.layers.9.mlp.fc2.SU', 'vision_model.encoder.layers.9.mlp.fc2.SV', 'vision_model.encoder.layers.9.mlp.fc2.rcp', 'vision_model.encoder.layers.9.mlp.fc2.tlut', 'vision_model.encoder.layers.9.mlp.fc2.tp_rank', 'vision_model.encoder.layers.9.mlp.fc2.trellis', 'vision_model.encoder.layers.9.self_attn.k_proj.SU', 'vision_model.encoder.layers.9.self_attn.k_proj.SV', 'vision_model.encoder.layers.9.self_attn.k_proj.rcp', 'vision_model.encoder.layers.9.self_attn.k_proj.tlut', 'vision_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.k_proj.trellis', 'vision_model.encoder.layers.9.self_attn.out_proj.SU', 'vision_model.encoder.layers.9.self_attn.out_proj.SV', 'vision_model.encoder.layers.9.self_attn.out_proj.rcp', 'vision_model.encoder.layers.9.self_attn.out_proj.tlut', 'vision_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.out_proj.trellis', 'vision_model.encoder.layers.9.self_attn.q_proj.SU', 'vision_model.encoder.layers.9.self_attn.q_proj.SV', 'vision_model.encoder.layers.9.self_attn.q_proj.rcp', 'vision_model.encoder.layers.9.self_attn.q_proj.tlut', 'vision_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.q_proj.trellis', 'vision_model.encoder.layers.9.self_attn.v_proj.SU', 'vision_model.encoder.layers.9.self_attn.v_proj.SV', 'vision_model.encoder.layers.9.self_attn.v_proj.rcp', 'vision_model.encoder.layers.9.self_attn.v_proj.tlut', 'vision_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0416 08:21:20.858370 3267474 hfize_clip.py:65] Loading text layer 0
W0416 08:21:20.858752 3267474 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved = torch.load(f'{path_prefix}/{full_key}.pt', map_location='cpu')

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 133, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 80, in main
    load_clip_block('text', model.text_model.encoder.layers, orig_model.text_model.encoder.layers)
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 71, in load_clip_block
    load_proj_or_restore(layer.self_attn, 'q_proj', f'{prefix}_{i}', 'q', orig.self_attn, args.quantized_path, skip_list)
  File "/workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py", line 32, in load_proj_or_restore
    saved = torch.load(f'{path_prefix}/{full_key}.pt', map_location='cpu')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: '../hf_model_comp/qtip/ckpt/clip-vit-large-patch14_8bit/text_0_q.pt'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../hf_model_comp/qtip/hf/clip-vit-large-patch14_8bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/workspace/Weight_compression/qtip/eval/eval_clip_imagenet.py", line 267, in <module>
    main(args)
  File "/workspace/Weight_compression/qtip/eval/eval_clip_imagenet.py", line 176, in main
    model = model_from_hf_path_clip(args.hf_path).to('cuda')
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Weight_compression/qtip/eval/eval_clip_imagenet.py", line 156, in model_from_hf_path_clip
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: '../hf_model_comp/qtip/hf/clip-vit-large-patch14_8bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
