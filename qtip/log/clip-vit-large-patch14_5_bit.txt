I0416 08:14:26.257190 3179539 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:26.257294 3179539 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:26.257336 3179539 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:26.372704 3179539 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:28.429047 3179539 warnings.py:110] /workspace/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

I0416 08:14:29.705912 3179539 quantize_finetune_clip.py:141] loaded model
I0416 08:14:29.706081 3179539 quantize_finetune_clip.py:143] loaded dataset and devset
I0416 08:14:29.762272 3179539 quantize_finetune_clip.py:151] vision layer 0 gpu 0
I0416 08:14:30.573023 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 0 in 0.59s
I0416 08:14:31.243752 3179539 quantize_finetune_clip.py:151] vision layer 1 gpu 0
I0416 08:14:33.633281 3182039 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:14:33.633409 3182039 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:14:33.633470 3182039 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:14:33.751529 3182039 config.py:58] PyTorch version 2.4.0 available.
W0416 08:14:36.519534 3182039 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:03<00:24,  3.51s/it] 25%|██▌       | 2/8 [00:03<00:09,  1.65s/it] 38%|███▊      | 3/8 [00:04<00:05,  1.05s/it] 50%|█████     | 4/8 [00:04<00:03,  1.28it/s] 62%|██████▎   | 5/8 [00:04<00:01,  1.60it/s] 75%|███████▌  | 6/8 [00:05<00:01,  1.87it/s] 88%|████████▊ | 7/8 [00:05<00:00,  2.11it/s]100%|██████████| 8/8 [00:05<00:00,  2.31it/s]100%|██████████| 8/8 [00:05<00:00,  1.34it/s]
W0416 08:14:44.884720 3182039 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_0_v proxy err 6.215793109731749e-05 tr(WHW.T) 28.12407684326172
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.25it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.53it/s] 50%|█████     | 4/8 [00:01<00:01,  2.71it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.72it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.77it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.80it/s]100%|██████████| 8/8 [00:03<00:00,  2.84it/s]100%|██████████| 8/8 [00:03<00:00,  2.66it/s]
W0416 08:14:49.232473 3182039 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_0_q proxy err 1.0201641089224722e-05 tr(WHW.T) 372.64678955078125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.23it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.48it/s] 50%|█████     | 4/8 [00:01<00:01,  2.65it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.72it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.78it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.85it/s]100%|██████████| 8/8 [00:02<00:00,  2.90it/s]100%|██████████| 8/8 [00:02<00:00,  2.68it/s]
W0416 08:14:53.622420 3182039 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_0_k proxy err 1.739160688885022e-05 tr(WHW.T) 187.16102600097656
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.65it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.20it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.46it/s] 50%|█████     | 4/8 [00:01<00:01,  2.58it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.70it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.76it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.83it/s]100%|██████████| 8/8 [00:03<00:00,  2.87it/s]100%|██████████| 8/8 [00:03<00:00,  2.65it/s]
W0416 08:14:58.040930 3182039 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_0_o proxy err 2.8695953005808406e-05 tr(WHW.T) 13.379737854003906
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:03<00:22,  3.17s/it] 25%|██▌       | 2/8 [00:03<00:09,  1.53s/it] 38%|███▊      | 3/8 [00:03<00:05,  1.00s/it] 50%|█████     | 4/8 [00:04<00:03,  1.33it/s] 62%|██████▎   | 5/8 [00:04<00:01,  1.63it/s] 75%|███████▌  | 6/8 [00:05<00:01,  1.89it/s] 88%|████████▊ | 7/8 [00:05<00:00,  2.09it/s]100%|██████████| 8/8 [00:05<00:00,  2.26it/s]100%|██████████| 8/8 [00:05<00:00,  1.39it/s]
W0416 08:15:05.466304 3182039 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_0_fc1 proxy err 0.00010360706801293418 tr(WHW.T) 21288.48046875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:36,  1.17s/it]  6%|▋         | 2/32 [00:01<00:20,  1.43it/s]  9%|▉         | 3/32 [00:01<00:15,  1.84it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.09it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:03<00:10,  2.41it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.58it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.66it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.69it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.70it/s] 41%|████      | 13/32 [00:05<00:07,  2.70it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.69it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.70it/s] 50%|█████     | 16/32 [00:06<00:05,  2.69it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.70it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.72it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.71it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.70it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.71it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.73it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.72it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.69it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.69it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.71it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.70it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.69it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.69it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.54it/s]
W0416 08:15:19.847465 3182039 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_0_fc2 proxy err 5.867808340553893e-06 tr(WHW.T) 112.2016830444336
I0416 08:15:21.560837 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 1 in 0.25s
I0416 08:15:21.682008 3179539 quantize_finetune_clip.py:151] vision layer 2 gpu 0
I0416 08:15:24.173743 3193412 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:15:24.173895 3193412 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:15:24.173963 3193412 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:15:24.296787 3193412 config.py:58] PyTorch version 2.4.0 available.
W0416 08:15:26.666130 3193412 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:12,  1.77s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.08it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.52it/s] 50%|█████     | 4/8 [00:02<00:02,  1.88it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.17it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.39it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.57it/s]100%|██████████| 8/8 [00:04<00:00,  2.69it/s]100%|██████████| 8/8 [00:04<00:00,  1.94it/s]
W0416 08:15:32.278771 3193412 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_1_v proxy err 9.919971489580348e-05 tr(WHW.T) 68.71031951904297
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.75it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.32it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.57it/s] 50%|█████     | 4/8 [00:01<00:01,  2.69it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.78it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.82it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.81it/s]100%|██████████| 8/8 [00:02<00:00,  2.83it/s]100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
W0416 08:15:36.719015 3193412 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_1_q proxy err 1.7566128008184023e-05 tr(WHW.T) 529.9490356445312
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.65it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.23it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.52it/s] 50%|█████     | 4/8 [00:01<00:01,  2.70it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.82it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.91it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]100%|██████████| 8/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
W0416 08:15:41.083897 3193412 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_1_k proxy err 2.9011922379140742e-05 tr(WHW.T) 303.2245788574219
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.71it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.32it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.60it/s] 50%|█████     | 4/8 [00:01<00:01,  2.74it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.91it/s]100%|██████████| 8/8 [00:02<00:00,  2.92it/s]100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
W0416 08:15:45.462329 3193412 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_1_o proxy err 4.2477597162360325e-05 tr(WHW.T) 21.233436584472656
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.22s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.38it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.78it/s] 50%|█████     | 4/8 [00:02<00:01,  2.06it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.25it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.39it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.49it/s]100%|██████████| 8/8 [00:03<00:00,  2.56it/s]100%|██████████| 8/8 [00:03<00:00,  2.10it/s]
W0416 08:15:50.877565 3193412 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_1_fc1 proxy err 1.3251389646029565e-05 tr(WHW.T) 7345.1005859375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.06s/it]  6%|▋         | 2/32 [00:01<00:19,  1.55it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.22it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.55it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.62it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.69it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.73it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.74it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.74it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.79it/s] 41%|████      | 13/32 [00:05<00:06,  2.81it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.79it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.79it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.81it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.82it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.81it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.81it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.82it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.81it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.81it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.80it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.80it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.84it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.80it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
W0416 08:16:04.626965 3193412 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_1_fc2 proxy err 1.5977053408278152e-05 tr(WHW.T) 65.21305847167969
I0416 08:16:06.067323 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 2 in 0.17s
I0416 08:16:06.221462 3179539 quantize_finetune_clip.py:151] vision layer 3 gpu 0
I0416 08:16:08.521680 3204614 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:08.521798 3204614 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:08.521889 3204614 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:08.643892 3204614 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:11.041801 3204614 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.65s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.13it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.58it/s] 50%|█████     | 4/8 [00:02<00:02,  1.94it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.23it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.46it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.62it/s]100%|██████████| 8/8 [00:03<00:00,  2.74it/s]100%|██████████| 8/8 [00:03<00:00,  2.01it/s]
W0416 08:16:16.553864 3204614 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_2_v proxy err 0.0001635363296372816 tr(WHW.T) 61.11924743652344
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.28it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.38it/s] 50%|█████     | 4/8 [00:01<00:01,  2.60it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.75it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.86it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.88it/s]100%|██████████| 8/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
W0416 08:16:21.097890 3204614 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_2_q proxy err 8.364246605196968e-05 tr(WHW.T) 195.00669860839844
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.28it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.57it/s] 50%|█████     | 4/8 [00:01<00:01,  2.73it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.83it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.77it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.86it/s]100%|██████████| 8/8 [00:02<00:00,  2.85it/s]100%|██████████| 8/8 [00:02<00:00,  2.69it/s]
W0416 08:16:25.517164 3204614 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_2_k proxy err 6.954224954824895e-05 tr(WHW.T) 206.7851104736328
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.79it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.39it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.81it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.86it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
W0416 08:16:29.767517 3204614 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_2_o proxy err 4.3820578866871074e-05 tr(WHW.T) 12.587794303894043
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.24s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.39it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.81it/s] 50%|█████     | 4/8 [00:02<00:01,  2.07it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.29it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.43it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.52it/s]100%|██████████| 8/8 [00:03<00:00,  2.59it/s]100%|██████████| 8/8 [00:03<00:00,  2.12it/s]
W0416 08:16:35.193329 3204614 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_2_fc1 proxy err 2.7082254746346734e-05 tr(WHW.T) 11104.5673828125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.08s/it]  6%|▋         | 2/32 [00:01<00:19,  1.52it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.43it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.67it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 41%|████      | 13/32 [00:05<00:06,  2.78it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:05<00:06,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.87it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.84it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.83it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.83it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.84it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.84it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:11<00:00,  2.84it/s]100%|██████████| 32/32 [00:11<00:00,  2.68it/s]
W0416 08:16:48.733702 3204614 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_2_fc2 proxy err 2.1902080334257334e-05 tr(WHW.T) 34.765602111816406
I0416 08:16:50.300525 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 3 in 0.18s
I0416 08:16:50.446098 3179539 quantize_finetune_clip.py:151] vision layer 4 gpu 0
I0416 08:16:52.726066 3215506 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:16:52.726189 3215506 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:16:52.726252 3215506 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:16:52.847629 3215506 config.py:58] PyTorch version 2.4.0 available.
W0416 08:16:55.584629 3215506 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.68s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.12it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.57it/s] 50%|█████     | 4/8 [00:02<00:02,  1.93it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.20it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.41it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.57it/s]100%|██████████| 8/8 [00:04<00:00,  2.69it/s]100%|██████████| 8/8 [00:04<00:00,  1.98it/s]
W0416 08:17:01.467922 3215506 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_3_v proxy err 0.00023483553377445787 tr(WHW.T) 78.36602783203125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.22it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.40it/s] 50%|█████     | 4/8 [00:01<00:01,  2.48it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.62it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.71it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.79it/s]100%|██████████| 8/8 [00:03<00:00,  2.85it/s]100%|██████████| 8/8 [00:03<00:00,  2.61it/s]
W0416 08:17:05.887729 3215506 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_3_q proxy err 8.280044858111069e-05 tr(WHW.T) 300.93536376953125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.60it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.22it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.43it/s] 50%|█████     | 4/8 [00:01<00:01,  2.60it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.75it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.88it/s]100%|██████████| 8/8 [00:02<00:00,  2.90it/s]100%|██████████| 8/8 [00:02<00:00,  2.67it/s]
W0416 08:17:10.285704 3215506 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_3_k proxy err 7.43033378967084e-05 tr(WHW.T) 327.538330078125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.53it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.13it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.44it/s] 50%|█████     | 4/8 [00:01<00:01,  2.63it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.75it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.89it/s]100%|██████████| 8/8 [00:03<00:00,  2.92it/s]100%|██████████| 8/8 [00:03<00:00,  2.67it/s]
W0416 08:17:15.036124 3215506 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_3_o proxy err 5.652748222928494e-05 tr(WHW.T) 11.745527267456055
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.26s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.34it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.75it/s] 50%|█████     | 4/8 [00:02<00:01,  2.03it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.22it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.35it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.43it/s]100%|██████████| 8/8 [00:03<00:00,  2.54it/s]100%|██████████| 8/8 [00:03<00:00,  2.06it/s]
W0416 08:17:21.621244 3215506 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_3_fc1 proxy err 2.4762304747127928e-05 tr(WHW.T) 6100.47265625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:34,  1.11s/it]  6%|▋         | 2/32 [00:01<00:20,  1.46it/s]  9%|▉         | 3/32 [00:01<00:15,  1.83it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.08it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.30it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.49it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.53it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.61it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.61it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.66it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.66it/s] 41%|████      | 13/32 [00:05<00:07,  2.71it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.72it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.73it/s] 50%|█████     | 16/32 [00:06<00:05,  2.70it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.71it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.67it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.68it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.67it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.65it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.65it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.68it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.71it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.70it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.72it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.71it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.70it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.64it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.68it/s]100%|██████████| 32/32 [00:12<00:00,  2.70it/s]100%|██████████| 32/32 [00:12<00:00,  2.53it/s]
W0416 08:17:36.333140 3215506 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_3_fc2 proxy err 3.706500501721166e-05 tr(WHW.T) 40.130794525146484
I0416 08:17:38.732436 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 4 in 0.18s
I0416 08:17:38.858989 3179539 quantize_finetune_clip.py:151] vision layer 5 gpu 0
I0416 08:17:41.147806 3227772 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:17:41.147961 3227772 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:17:41.148026 3227772 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:17:41.273220 3227772 config.py:58] PyTorch version 2.4.0 available.
W0416 08:17:43.715994 3227772 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:12,  1.71s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.11it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.56it/s] 50%|█████     | 4/8 [00:02<00:02,  1.91it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.19it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.43it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.58it/s]100%|██████████| 8/8 [00:04<00:00,  2.69it/s]100%|██████████| 8/8 [00:04<00:00,  1.97it/s]
W0416 08:17:49.336791 3227772 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_4_v proxy err 0.00021519559959415346 tr(WHW.T) 95.10258483886719
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.59it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.23it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.53it/s] 50%|█████     | 4/8 [00:01<00:01,  2.72it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.83it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.92it/s]100%|██████████| 8/8 [00:02<00:00,  2.92it/s]100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
W0416 08:17:53.829391 3227772 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_4_q proxy err 9.761390538187698e-05 tr(WHW.T) 333.9703369140625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.54it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.16it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.49it/s] 50%|█████     | 4/8 [00:01<00:01,  2.67it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.74it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.91it/s]100%|██████████| 8/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  2.70it/s]
W0416 08:17:58.211357 3227772 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_4_k proxy err 8.812831219984218e-05 tr(WHW.T) 323.5253601074219
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.56it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.17it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.49it/s] 50%|█████     | 4/8 [00:01<00:01,  2.68it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.82it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
W0416 08:18:02.567175 3227772 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_4_o proxy err 6.0386897530406713e-05 tr(WHW.T) 18.242263793945312
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.24s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.39it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.82it/s] 50%|█████     | 4/8 [00:02<00:01,  2.14it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.34it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.47it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.57it/s]100%|██████████| 8/8 [00:03<00:00,  2.61it/s]100%|██████████| 8/8 [00:03<00:00,  2.15it/s]
W0416 08:18:07.950515 3227772 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_4_fc1 proxy err 4.352345786173828e-05 tr(WHW.T) 4969.951171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.07s/it]  6%|▋         | 2/32 [00:01<00:19,  1.53it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.28it/s] 19%|█▉        | 6/32 [00:02<00:11,  2.31it/s] 22%|██▏       | 7/32 [00:03<00:10,  2.46it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.67it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.72it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.75it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.76it/s] 41%|████      | 13/32 [00:05<00:07,  2.67it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.73it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.76it/s] 50%|█████     | 16/32 [00:06<00:05,  2.78it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.83it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.87it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.90it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.92it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.91it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.88it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.78it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.77it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.81it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.89it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.90it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:12<00:00,  2.90it/s]100%|██████████| 32/32 [00:12<00:00,  2.64it/s]
W0416 08:18:21.684823 3227772 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_4_fc2 proxy err 8.066343434620649e-05 tr(WHW.T) 29.563274383544922
I0416 08:18:23.739730 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 5 in 0.16s
I0416 08:18:23.868924 3179539 quantize_finetune_clip.py:151] vision layer 6 gpu 0
I0416 08:18:26.200098 3238860 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:18:26.200226 3238860 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:18:26.200320 3238860 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:18:26.329776 3238860 config.py:58] PyTorch version 2.4.0 available.
W0416 08:18:28.700729 3238860 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.70s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.11it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.57it/s] 50%|█████     | 4/8 [00:02<00:02,  1.95it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.22it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.44it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.59it/s]100%|██████████| 8/8 [00:04<00:00,  2.70it/s]100%|██████████| 8/8 [00:04<00:00,  1.98it/s]
W0416 08:18:34.619103 3238860 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_5_v proxy err 0.00026381644420325756 tr(WHW.T) 151.44627380371094
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.30it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.61it/s] 50%|█████     | 4/8 [00:01<00:01,  2.74it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.82it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.87it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.82it/s]100%|██████████| 8/8 [00:02<00:00,  2.88it/s]100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
W0416 08:18:39.127084 3238860 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_5_q proxy err 0.00015653921582270414 tr(WHW.T) 316.5805969238281
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.32it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.58it/s] 50%|█████     | 4/8 [00:01<00:01,  2.74it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.75it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.82it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.88it/s]100%|██████████| 8/8 [00:02<00:00,  2.93it/s]100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
W0416 08:18:43.552980 3238860 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_5_k proxy err 0.00016087038966361433 tr(WHW.T) 303.82757568359375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.69it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.25it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.55it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.90it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
W0416 08:18:47.967525 3238860 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_5_o proxy err 8.173357491614297e-05 tr(WHW.T) 16.882675170898438
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:09,  1.29s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.34it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.77it/s] 50%|█████     | 4/8 [00:02<00:01,  2.07it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.26it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.39it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.52it/s]100%|██████████| 8/8 [00:03<00:00,  2.58it/s]100%|██████████| 8/8 [00:03<00:00,  2.09it/s]
W0416 08:18:53.447094 3238860 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_5_fc1 proxy err 7.804635242791846e-05 tr(WHW.T) 4523.89453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:34,  1.12s/it]  6%|▋         | 2/32 [00:01<00:20,  1.50it/s]  9%|▉         | 3/32 [00:01<00:16,  1.77it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.07it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.48it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.60it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.70it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.75it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.80it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.81it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.86it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.78it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.80it/s] 50%|█████     | 16/32 [00:06<00:05,  2.81it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.82it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.85it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.85it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.90it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.87it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.88it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.82it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.73it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.78it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.80it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.80it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.82it/s]100%|██████████| 32/32 [00:12<00:00,  2.77it/s]100%|██████████| 32/32 [00:12<00:00,  2.63it/s]
W0416 08:19:07.295770 3238860 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_5_fc2 proxy err 0.00013481445785146207 tr(WHW.T) 25.434589385986328
I0416 08:19:08.919398 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 6 in 0.16s
I0416 08:19:09.042182 3179539 quantize_finetune_clip.py:151] vision layer 7 gpu 0
I0416 08:19:11.624937 3250172 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:11.625091 3250172 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:11.625154 3250172 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:11.743672 3250172 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:14.416057 3250172 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:12,  1.74s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.10it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.56it/s] 50%|█████     | 4/8 [00:02<00:02,  1.94it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.24it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.47it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.65it/s]100%|██████████| 8/8 [00:04<00:00,  2.75it/s]100%|██████████| 8/8 [00:04<00:00,  1.99it/s]
W0416 08:19:20.300859 3250172 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_6_v proxy err 0.0004196468216832727 tr(WHW.T) 170.1556854248047
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.74it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.66it/s] 50%|█████     | 4/8 [00:01<00:01,  2.81it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.94it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
W0416 08:19:24.743440 3250172 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_6_q proxy err 0.00018990770331583917 tr(WHW.T) 456.3508605957031
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.59it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.21it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.52it/s] 50%|█████     | 4/8 [00:01<00:01,  2.70it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.81it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  3.07it/s]100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
W0416 08:19:29.084953 3250172 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_6_k proxy err 0.00016150235023815185 tr(WHW.T) 510.3419189453125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.37it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.69it/s] 50%|█████     | 4/8 [00:01<00:01,  2.86it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  3.07it/s]100%|██████████| 8/8 [00:02<00:00,  2.85it/s]
W0416 08:19:33.413758 3250172 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_6_o proxy err 0.0001386382500641048 tr(WHW.T) 14.702461242675781
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.18s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.44it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.86it/s] 50%|█████     | 4/8 [00:02<00:01,  2.18it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.38it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.53it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.64it/s]100%|██████████| 8/8 [00:03<00:00,  2.69it/s]100%|██████████| 8/8 [00:03<00:00,  2.21it/s]
W0416 08:19:38.566142 3250172 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_6_fc1 proxy err 0.00012770123430527747 tr(WHW.T) 3307.18994140625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.08s/it]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:14,  1.99it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.50it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.59it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.70it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.73it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.79it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.86it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.85it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.86it/s] 41%|████      | 13/32 [00:05<00:06,  2.90it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.92it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.93it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.94it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.89it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.86it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.86it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.87it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.88it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.89it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
W0416 08:19:51.849132 3250172 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_6_fc2 proxy err 0.00011490307952044532 tr(WHW.T) 37.4859619140625
I0416 08:19:53.314825 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 7 in 0.17s
I0416 08:19:53.439974 3179539 quantize_finetune_clip.py:151] vision layer 8 gpu 0
I0416 08:19:55.824544 3257116 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:19:55.824673 3257116 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:19:55.824737 3257116 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:19:55.944511 3257116 config.py:58] PyTorch version 2.4.0 available.
W0416 08:19:58.442814 3257116 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:12,  1.83s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.05it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.49it/s] 50%|█████     | 4/8 [00:02<00:02,  1.88it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.16it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.37it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.56it/s]100%|██████████| 8/8 [00:04<00:00,  2.69it/s]100%|██████████| 8/8 [00:04<00:00,  1.92it/s]
W0416 08:20:04.959680 3257116 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_7_v proxy err 0.0004520193615462631 tr(WHW.T) 231.85989379882812
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.71it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.27it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.54it/s] 50%|█████     | 4/8 [00:01<00:01,  2.68it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.78it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.90it/s]100%|██████████| 8/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  2.73it/s]
W0416 08:20:09.526374 3257116 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_7_q proxy err 0.0002652633120305836 tr(WHW.T) 471.35491943359375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.59it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.21it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.56it/s] 50%|█████     | 4/8 [00:01<00:01,  2.76it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
W0416 08:20:13.886727 3257116 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_7_k proxy err 0.00028669138555414975 tr(WHW.T) 440.3499755859375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.75it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.65it/s] 50%|█████     | 4/8 [00:01<00:01,  2.84it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.96it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]100%|██████████| 8/8 [00:02<00:00,  3.13it/s]100%|██████████| 8/8 [00:02<00:00,  2.88it/s]
W0416 08:20:18.097522 3257116 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_7_o proxy err 0.00014505715807899833 tr(WHW.T) 16.805908203125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.19s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.42it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.71it/s] 50%|█████     | 4/8 [00:02<00:01,  2.01it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.22it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.41it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.55it/s]100%|██████████| 8/8 [00:03<00:00,  2.61it/s]100%|██████████| 8/8 [00:03<00:00,  2.11it/s]
W0416 08:20:23.419031 3257116 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_7_fc1 proxy err 0.00014507336891256273 tr(WHW.T) 3763.969970703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.09s/it]  6%|▋         | 2/32 [00:01<00:19,  1.54it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.26it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.64it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.74it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.82it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.84it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.92it/s] 41%|████      | 13/32 [00:05<00:06,  2.94it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.91it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.93it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.95it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.95it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.96it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.96it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.97it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.82it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.86it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.88it/s]100%|██████████| 32/32 [00:11<00:00,  2.90it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
W0416 08:20:36.660972 3257116 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_7_fc2 proxy err 0.00014161771105136722 tr(WHW.T) 36.481971740722656
I0416 08:20:38.139000 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 8 in 0.16s
I0416 08:20:38.267469 3179539 quantize_finetune_clip.py:151] vision layer 9 gpu 0
I0416 08:20:40.584562 3262963 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:20:40.584773 3262963 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:20:40.584886 3262963 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:20:40.707190 3262963 config.py:58] PyTorch version 2.4.0 available.
W0416 08:20:43.007808 3262963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.65s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.16it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.63it/s] 50%|█████     | 4/8 [00:02<00:01,  2.02it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.32it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.54it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.68it/s]100%|██████████| 8/8 [00:03<00:00,  2.81it/s]100%|██████████| 8/8 [00:03<00:00,  2.06it/s]
W0416 08:20:48.428663 3262963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_8_v proxy err 0.0004217723908368498 tr(WHW.T) 293.3328857421875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.82it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.38it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.60it/s] 50%|█████     | 4/8 [00:01<00:01,  2.71it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.76it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.80it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.86it/s]100%|██████████| 8/8 [00:02<00:00,  2.89it/s]100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
W0416 08:20:52.909922 3262963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_8_q proxy err 0.0003289170563220978 tr(WHW.T) 492.7628173828125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.64it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.15it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.46it/s] 50%|█████     | 4/8 [00:01<00:01,  2.65it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.80it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  2.71it/s]
W0416 08:20:57.328917 3262963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_8_k proxy err 0.0003203810192644596 tr(WHW.T) 478.1852722167969
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.71it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.30it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.58it/s] 50%|█████     | 4/8 [00:01<00:01,  2.77it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.93it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
W0416 08:21:01.606199 3262963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_8_o proxy err 0.00016792253882158548 tr(WHW.T) 21.60403823852539
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.24s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.38it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.81it/s] 50%|█████     | 4/8 [00:02<00:01,  2.03it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.24it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.41it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.55it/s]100%|██████████| 8/8 [00:03<00:00,  2.65it/s]100%|██████████| 8/8 [00:03<00:00,  2.13it/s]
W0416 08:21:06.935315 3262963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_8_fc1 proxy err 0.00015371937479358166 tr(WHW.T) 4153.708984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.05s/it]  6%|▋         | 2/32 [00:01<00:18,  1.58it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.44it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.58it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.83it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.90it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.84it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.84it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.89it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.91it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.92it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.93it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.93it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.87it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.84it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.83it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.82it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.81it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
W0416 08:21:20.362691 3262963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_8_fc2 proxy err 0.0001723193418001756 tr(WHW.T) 36.89303207397461
I0416 08:21:21.808153 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 9 in 0.16s
I0416 08:21:21.933020 3179539 quantize_finetune_clip.py:151] vision layer 10 gpu 0
I0416 08:21:24.315516 3268577 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:21:24.315652 3268577 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:21:24.315711 3268577 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:21:24.429573 3268577 config.py:58] PyTorch version 2.4.0 available.
W0416 08:21:26.686317 3268577 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.63s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.16it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.63it/s] 50%|█████     | 4/8 [00:02<00:01,  2.01it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.31it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.56it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.73it/s]100%|██████████| 8/8 [00:03<00:00,  2.87it/s]100%|██████████| 8/8 [00:03<00:00,  2.08it/s]
W0416 08:21:31.998633 3268577 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_9_v proxy err 0.00044677694677375257 tr(WHW.T) 314.0332336425781
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.38it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.68it/s] 50%|█████     | 4/8 [00:01<00:01,  2.85it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.95it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.03it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]100%|██████████| 8/8 [00:02<00:00,  3.13it/s]100%|██████████| 8/8 [00:02<00:00,  2.88it/s]
W0416 08:21:36.094948 3268577 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_9_q proxy err 0.00035220684367232025 tr(WHW.T) 482.75921630859375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.34it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.86it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.96it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]100%|██████████| 8/8 [00:02<00:00,  3.13it/s]100%|██████████| 8/8 [00:02<00:00,  2.88it/s]
W0416 08:21:40.184290 3268577 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_9_k proxy err 0.0003336883964948356 tr(WHW.T) 495.71539306640625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.83it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.47it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.78it/s] 50%|█████     | 4/8 [00:01<00:01,  2.93it/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.02it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.06it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]100%|██████████| 8/8 [00:02<00:00,  3.15it/s]100%|██████████| 8/8 [00:02<00:00,  2.94it/s]
W0416 08:21:44.233387 3268577 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_9_o proxy err 0.00018063189054373652 tr(WHW.T) 20.151992797851562
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.18s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.46it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.90it/s] 50%|█████     | 4/8 [00:02<00:01,  2.21it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.43it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.58it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.70it/s]100%|██████████| 8/8 [00:03<00:00,  2.77it/s]100%|██████████| 8/8 [00:03<00:00,  2.25it/s]
W0416 08:21:49.301487 3268577 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_9_fc1 proxy err 0.00016640551621094346 tr(WHW.T) 4800.1669921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:31,  1.03s/it]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.31it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.50it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.65it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.86it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.87it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.93it/s] 41%|████      | 13/32 [00:05<00:06,  2.96it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.97it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.97it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.96it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.97it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.98it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.98it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.98it/s] 69%|██████▉   | 22/32 [00:08<00:03,  3.00it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.99it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.95it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.90it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.88it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.87it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.92it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.94it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.96it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.97it/s]100%|██████████| 32/32 [00:11<00:00,  2.78it/s]
W0416 08:22:02.397367 3268577 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_9_fc2 proxy err 0.0002072396600851789 tr(WHW.T) 32.10093688964844
I0416 08:22:03.856519 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 10 in 0.16s
I0416 08:22:03.979510 3179539 quantize_finetune_clip.py:151] vision layer 11 gpu 0
I0416 08:22:06.232724 3272229 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:22:06.232858 3272229 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:22:06.232918 3272229 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:22:06.348288 3272229 config.py:58] PyTorch version 2.4.0 available.
W0416 08:22:08.622584 3272229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.59s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.19it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.66it/s] 50%|█████     | 4/8 [00:02<00:01,  2.05it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.34it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.56it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.73it/s]100%|██████████| 8/8 [00:03<00:00,  2.82it/s]100%|██████████| 8/8 [00:03<00:00,  2.09it/s]
W0416 08:22:13.846541 3272229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_10_v proxy err 0.0005389762227423489 tr(WHW.T) 327.5375671386719
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.30it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.57it/s] 50%|█████     | 4/8 [00:01<00:01,  2.67it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.78it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.90it/s]100%|██████████| 8/8 [00:02<00:00,  2.93it/s]100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
W0416 08:22:18.696346 3272229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_10_q proxy err 0.00044037660700269043 tr(WHW.T) 541.7548217773438
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.71it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.37it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.70it/s] 50%|█████     | 4/8 [00:01<00:01,  2.88it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.99it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]100%|██████████| 8/8 [00:02<00:00,  3.08it/s]100%|██████████| 8/8 [00:02<00:00,  2.88it/s]
W0416 08:22:23.008057 3272229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_10_k proxy err 0.00043130319681949914 tr(WHW.T) 543.0531005859375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.80it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.41it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.69it/s] 50%|█████     | 4/8 [00:01<00:01,  2.86it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.96it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]100%|██████████| 8/8 [00:02<00:00,  3.13it/s]100%|██████████| 8/8 [00:02<00:00,  2.90it/s]
W0416 08:22:27.267261 3272229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_10_o proxy err 0.00021734049369115382 tr(WHW.T) 19.04693031311035
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.20s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.43it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.87it/s] 50%|█████     | 4/8 [00:02<00:01,  2.18it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.41it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.58it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.68it/s]100%|██████████| 8/8 [00:03<00:00,  2.75it/s]100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
W0416 08:22:32.460381 3272229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_10_fc1 proxy err 0.00015329256711993366 tr(WHW.T) 6493.8876953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.05s/it]  6%|▋         | 2/32 [00:01<00:18,  1.58it/s]  9%|▉         | 3/32 [00:01<00:14,  2.02it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.33it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.68it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.86it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.88it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.90it/s] 41%|████      | 13/32 [00:05<00:06,  2.92it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.93it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.97it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.99it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.99it/s] 59%|█████▉    | 19/32 [00:07<00:04,  3.01it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.01it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.00it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.99it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.99it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.99it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.97it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.95it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.97it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.99it/s]100%|██████████| 32/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
W0416 08:22:45.514819 3272229 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_10_fc2 proxy err 0.0002226455108029768 tr(WHW.T) 29.747730255126953
I0416 08:22:46.959448 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 11 in 0.17s
I0416 08:22:47.086354 3179539 quantize_finetune_clip.py:151] vision layer 12 gpu 0
I0416 08:22:49.398764 3275780 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:22:49.398885 3275780 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:22:49.398945 3275780 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:22:49.516805 3275780 config.py:58] PyTorch version 2.4.0 available.
W0416 08:22:51.744999 3275780 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.60s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.19it/s] 38%|███▊      | 3/8 [00:02<00:02,  1.68it/s] 50%|█████     | 4/8 [00:02<00:01,  2.08it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.38it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.61it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.80it/s]100%|██████████| 8/8 [00:03<00:00,  2.90it/s]100%|██████████| 8/8 [00:03<00:00,  2.13it/s]
W0416 08:22:56.916765 3275780 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_11_v proxy err 0.0006221868679858744 tr(WHW.T) 309.68670654296875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.66it/s] 50%|█████     | 4/8 [00:01<00:01,  2.79it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.92it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.10it/s]100%|██████████| 8/8 [00:02<00:00,  3.15it/s]100%|██████████| 8/8 [00:02<00:00,  2.87it/s]
W0416 08:23:01.035322 3275780 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_11_q proxy err 0.00041865534149110317 tr(WHW.T) 642.15869140625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.75it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.15it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.48it/s] 50%|█████     | 4/8 [00:01<00:01,  2.68it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.80it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
W0416 08:23:05.380175 3275780 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_11_k proxy err 0.0004129845474381 tr(WHW.T) 620.22705078125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.77it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.42it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.74it/s] 50%|█████     | 4/8 [00:01<00:01,  2.92it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.08it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.13it/s]100%|██████████| 8/8 [00:02<00:00,  3.19it/s]100%|██████████| 8/8 [00:02<00:00,  2.94it/s]
W0416 08:23:09.526906 3275780 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_11_o proxy err 0.00023773293651174754 tr(WHW.T) 19.117759704589844
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.21s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.44it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.88it/s] 50%|█████     | 4/8 [00:02<00:01,  2.18it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.41it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.58it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.67it/s]100%|██████████| 8/8 [00:03<00:00,  2.76it/s]100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
W0416 08:23:14.669121 3275780 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_11_fc1 proxy err 0.00016121365479193628 tr(WHW.T) 7438.43212890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.09s/it]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:14,  1.98it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.50it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.75it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.92it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.97it/s] 41%|████      | 13/32 [00:05<00:06,  2.99it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.98it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.94it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.96it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.96it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.94it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.94it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.95it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.97it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.99it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.99it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.00it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.00it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.00it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  3.03it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
W0416 08:23:27.691452 3275780 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_11_fc2 proxy err 0.0001846294035203755 tr(WHW.T) 33.167724609375
I0416 08:23:29.170104 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 12 in 0.19s
I0416 08:23:29.309220 3179539 quantize_finetune_clip.py:151] vision layer 13 gpu 0
I0416 08:23:31.572969 3279064 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:23:31.573099 3279064 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:23:31.573158 3279064 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:23:31.689690 3279064 config.py:58] PyTorch version 2.4.0 available.
W0416 08:23:33.915381 3279064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.60s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.18it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.65it/s] 50%|█████     | 4/8 [00:02<00:01,  2.03it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.30it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.50it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.64it/s]100%|██████████| 8/8 [00:03<00:00,  2.78it/s]100%|██████████| 8/8 [00:03<00:00,  2.06it/s]
W0416 08:23:39.202308 3279064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_12_v proxy err 0.0007132203318178654 tr(WHW.T) 274.07757568359375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.71it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.66it/s] 50%|█████     | 4/8 [00:01<00:01,  2.81it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.90it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
W0416 08:23:43.425518 3279064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_12_q proxy err 0.0004716258554253727 tr(WHW.T) 615.548828125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.17it/s] 38%|███▊      | 3/8 [00:01<00:02,  2.47it/s] 50%|█████     | 4/8 [00:01<00:01,  2.65it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.74it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.80it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.85it/s]100%|██████████| 8/8 [00:02<00:00,  2.88it/s]100%|██████████| 8/8 [00:02<00:00,  2.67it/s]
W0416 08:23:47.935566 3279064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_12_k proxy err 0.0004760885494761169 tr(WHW.T) 612.162109375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.77it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.38it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.83it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  2.84it/s]
W0416 08:23:52.124786 3279064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_12_o proxy err 0.0002376277989242226 tr(WHW.T) 13.876964569091797
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.21s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.40it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.82it/s] 50%|█████     | 4/8 [00:02<00:01,  2.12it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.33it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.48it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.60it/s]100%|██████████| 8/8 [00:03<00:00,  2.68it/s]100%|██████████| 8/8 [00:03<00:00,  2.17it/s]
W0416 08:23:57.327452 3279064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_12_fc1 proxy err 0.00017907451547216624 tr(WHW.T) 7523.9033203125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.07s/it]  6%|▋         | 2/32 [00:01<00:19,  1.53it/s]  9%|▉         | 3/32 [00:01<00:14,  1.95it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.24it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.42it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.74it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.77it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.79it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.79it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.82it/s] 41%|████      | 13/32 [00:05<00:06,  2.85it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.84it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.85it/s] 50%|█████     | 16/32 [00:06<00:05,  2.86it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.87it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.89it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.88it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.88it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.85it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.85it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.84it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.85it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.85it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.85it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.87it/s]100%|██████████| 32/32 [00:11<00:00,  2.69it/s]
W0416 08:24:10.764891 3279064 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_12_fc2 proxy err 3.8289570511551574e-05 tr(WHW.T) 199.962646484375
I0416 08:24:12.227862 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 13 in 0.16s
I0416 08:24:12.345632 3179539 quantize_finetune_clip.py:151] vision layer 14 gpu 0
I0416 08:24:14.616485 3282621 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:24:14.616623 3282621 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:24:14.616683 3282621 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:24:14.732221 3282621 config.py:58] PyTorch version 2.4.0 available.
W0416 08:24:16.989408 3282621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:12,  1.72s/it] 25%|██▌       | 2/8 [00:02<00:05,  1.12it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.60it/s] 50%|█████     | 4/8 [00:02<00:01,  2.01it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.33it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.58it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.76it/s]100%|██████████| 8/8 [00:03<00:00,  2.90it/s]100%|██████████| 8/8 [00:03<00:00,  2.06it/s]
W0416 08:24:22.264525 3282621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_13_v proxy err 0.0008111951756291091 tr(WHW.T) 298.0824890136719
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.38it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.72it/s] 50%|█████     | 4/8 [00:01<00:01,  2.91it/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.02it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.10it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.16it/s]100%|██████████| 8/8 [00:02<00:00,  3.18it/s]100%|██████████| 8/8 [00:02<00:00,  2.93it/s]
W0416 08:24:26.327409 3282621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_13_q proxy err 0.0004932217998430133 tr(WHW.T) 650.8009033203125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.59it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.19it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.50it/s] 50%|█████     | 4/8 [00:01<00:01,  2.65it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.77it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.87it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.92it/s]100%|██████████| 8/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.70it/s]
W0416 08:24:30.994432 3282621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_13_k proxy err 0.0004815127467736602 tr(WHW.T) 669.0814819335938
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.64it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.26it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.60it/s] 50%|█████     | 4/8 [00:01<00:01,  2.80it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.94it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.08it/s]100%|██████████| 8/8 [00:02<00:00,  3.12it/s]100%|██████████| 8/8 [00:02<00:00,  2.85it/s]
W0416 08:24:35.239100 3282621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_13_o proxy err 0.0003264422994107008 tr(WHW.T) 9.406055450439453
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.18s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.46it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.89it/s] 50%|█████     | 4/8 [00:02<00:01,  2.21it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.41it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.56it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.68it/s]100%|██████████| 8/8 [00:03<00:00,  2.77it/s]100%|██████████| 8/8 [00:03<00:00,  2.24it/s]
W0416 08:24:40.305260 3282621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_13_fc1 proxy err 0.00019973903545178473 tr(WHW.T) 7018.62451171875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.04s/it]  6%|▋         | 2/32 [00:01<00:18,  1.60it/s]  9%|▉         | 3/32 [00:01<00:14,  2.03it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.30it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.45it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.61it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.71it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.91it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.93it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s] 41%|████      | 13/32 [00:05<00:06,  2.97it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.99it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.01it/s] 50%|█████     | 16/32 [00:06<00:05,  3.01it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.02it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.00it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.98it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.99it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.99it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.00it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.00it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.01it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.02it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.01it/s]100%|██████████| 32/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.80it/s]
W0416 08:24:53.307460 3282621 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_13_fc2 proxy err 0.0003011012449860573 tr(WHW.T) 20.70946502685547
I0416 08:24:54.756895 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 14 in 0.16s
I0416 08:24:54.875970 3179539 quantize_finetune_clip.py:151] vision layer 15 gpu 0
I0416 08:24:57.101449 3285986 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:24:57.101575 3285986 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:24:57.101632 3285986 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:24:57.213534 3285986 config.py:58] PyTorch version 2.4.0 available.
W0416 08:24:59.471062 3285986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.60s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.18it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.65it/s] 50%|█████     | 4/8 [00:02<00:01,  2.03it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.33it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.57it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.72it/s]100%|██████████| 8/8 [00:03<00:00,  2.83it/s]100%|██████████| 8/8 [00:03<00:00,  2.09it/s]
W0416 08:25:05.310534 3285986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_14_v proxy err 0.0008734705043025315 tr(WHW.T) 284.17218017578125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.30it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.60it/s] 50%|█████     | 4/8 [00:01<00:01,  2.80it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]100%|██████████| 8/8 [00:02<00:00,  3.09it/s]100%|██████████| 8/8 [00:02<00:00,  2.84it/s]
W0416 08:25:10.084923 3285986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_14_q proxy err 0.0005645639030262828 tr(WHW.T) 660.6932373046875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.64it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.26it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.60it/s] 50%|█████     | 4/8 [00:01<00:01,  2.78it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.89it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
W0416 08:25:14.409768 3285986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_14_k proxy err 0.0005172357195988297 tr(WHW.T) 724.1702880859375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.81it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.40it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.64it/s] 50%|█████     | 4/8 [00:01<00:01,  2.79it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.96it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.05it/s]100%|██████████| 8/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
W0416 08:25:18.642824 3285986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_14_o proxy err 0.00031858496367931366 tr(WHW.T) 8.858718872070312
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.20s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.44it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.87it/s] 50%|█████     | 4/8 [00:02<00:01,  2.19it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.41it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.56it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.67it/s]100%|██████████| 8/8 [00:03<00:00,  2.74it/s]100%|██████████| 8/8 [00:03<00:00,  2.23it/s]
W0416 08:25:23.734664 3285986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_14_fc1 proxy err 0.00021582437329925597 tr(WHW.T) 8141.02294921875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.06s/it]  6%|▋         | 2/32 [00:01<00:19,  1.58it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.32it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.54it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.66it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.75it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.81it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.90it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.94it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.95it/s] 41%|████      | 13/32 [00:05<00:06,  2.96it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.98it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.99it/s] 50%|█████     | 16/32 [00:06<00:05,  2.99it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.00it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.01it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.93it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.91it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.93it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.96it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.97it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.99it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.00it/s] 88%|████████▊ | 28/32 [00:10<00:01,  3.00it/s] 91%|█████████ | 29/32 [00:10<00:01,  3.00it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.00it/s] 97%|█████████▋| 31/32 [00:11<00:00,  3.00it/s]100%|██████████| 32/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
W0416 08:25:36.893653 3285986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_14_fc2 proxy err 0.000284236331935972 tr(WHW.T) 22.776063919067383
I0416 08:25:38.349005 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 15 in 0.16s
I0416 08:25:38.474991 3179539 quantize_finetune_clip.py:151] vision layer 16 gpu 0
I0416 08:25:40.750313 3289471 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:25:40.750442 3289471 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:25:40.750502 3289471 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:25:40.864270 3289471 config.py:58] PyTorch version 2.4.0 available.
W0416 08:25:43.181100 3289471 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.62s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.17it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.66it/s] 50%|█████     | 4/8 [00:02<00:01,  2.04it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.35it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.59it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.75it/s]100%|██████████| 8/8 [00:03<00:00,  2.90it/s]100%|██████████| 8/8 [00:03<00:00,  2.10it/s]
W0416 08:25:48.876379 3289471 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_15_v proxy err 0.0008726983796805143 tr(WHW.T) 285.29705810546875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.68it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.24it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.57it/s] 50%|█████     | 4/8 [00:01<00:01,  2.71it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.81it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.90it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  3.06it/s]100%|██████████| 8/8 [00:02<00:00,  2.78it/s]
W0416 08:25:53.914622 3289471 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_15_q proxy err 0.0005889064632356167 tr(WHW.T) 650.9993286132812
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.63it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.29it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.64it/s] 50%|█████     | 4/8 [00:01<00:01,  2.83it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.94it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  3.06it/s]100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
W0416 08:25:59.038989 3289471 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_15_k proxy err 0.0005109317717142403 tr(WHW.T) 748.6654052734375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.35it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.84it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.93it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]100%|██████████| 8/8 [00:02<00:00,  3.12it/s]100%|██████████| 8/8 [00:02<00:00,  2.87it/s]
W0416 08:26:03.761774 3289471 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_15_o proxy err 0.00027179313474334776 tr(WHW.T) 9.187652587890625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.21s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.43it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.86it/s] 50%|█████     | 4/8 [00:02<00:01,  2.18it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.41it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.57it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.67it/s]100%|██████████| 8/8 [00:03<00:00,  2.73it/s]100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
W0416 08:26:08.887096 3289471 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_15_fc1 proxy err 0.0002347491099499166 tr(WHW.T) 8731.9501953125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.04s/it]  6%|▋         | 2/32 [00:01<00:18,  1.61it/s]  9%|▉         | 3/32 [00:01<00:14,  2.04it/s] 12%|█▎        | 4/32 [00:02<00:11,  2.35it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.57it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.69it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.77it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.83it/s] 28%|██▊       | 9/32 [00:03<00:07,  2.88it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.93it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.96it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.99it/s] 41%|████      | 13/32 [00:05<00:06,  3.01it/s] 44%|████▍     | 14/32 [00:05<00:05,  3.02it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.03it/s] 50%|█████     | 16/32 [00:05<00:05,  3.04it/s] 53%|█████▎    | 17/32 [00:06<00:04,  3.04it/s] 56%|█████▋    | 18/32 [00:06<00:04,  3.04it/s] 59%|█████▉    | 19/32 [00:06<00:04,  3.02it/s] 62%|██████▎   | 20/32 [00:07<00:03,  3.02it/s] 66%|██████▌   | 21/32 [00:07<00:03,  3.02it/s] 69%|██████▉   | 22/32 [00:07<00:03,  3.03it/s] 72%|███████▏  | 23/32 [00:08<00:02,  3.04it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.05it/s] 78%|███████▊  | 25/32 [00:08<00:02,  3.04it/s] 81%|████████▏ | 26/32 [00:09<00:01,  3.04it/s] 84%|████████▍ | 27/32 [00:09<00:01,  3.03it/s] 88%|████████▊ | 28/32 [00:09<00:01,  3.03it/s] 91%|█████████ | 29/32 [00:10<00:00,  3.03it/s] 94%|█████████▍| 30/32 [00:10<00:00,  3.03it/s] 97%|█████████▋| 31/32 [00:10<00:00,  3.04it/s]100%|██████████| 32/32 [00:11<00:00,  2.96it/s]100%|██████████| 32/32 [00:11<00:00,  2.83it/s]
W0416 08:26:21.749122 3289471 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_15_fc2 proxy err 0.0003434770624153316 tr(WHW.T) 21.015804290771484
I0416 08:26:23.167367 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 16 in 0.16s
I0416 08:26:23.289808 3179539 quantize_finetune_clip.py:151] vision layer 17 gpu 0
I0416 08:26:25.541910 3293032 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:26:25.542041 3293032 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:26:25.542102 3293032 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:26:25.658453 3293032 config.py:58] PyTorch version 2.4.0 available.
W0416 08:26:27.982147 3293032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.61s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.18it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.66it/s] 50%|█████     | 4/8 [00:02<00:01,  2.06it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.37it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.60it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.76it/s]100%|██████████| 8/8 [00:03<00:00,  2.88it/s]100%|██████████| 8/8 [00:03<00:00,  2.11it/s]
W0416 08:26:33.215145 3293032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_16_v proxy err 0.0009769174503162503 tr(WHW.T) 323.1610412597656
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.80it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.43it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.73it/s] 50%|█████     | 4/8 [00:01<00:01,  2.88it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.11it/s]100%|██████████| 8/8 [00:02<00:00,  3.14it/s]100%|██████████| 8/8 [00:02<00:00,  2.92it/s]
W0416 08:26:37.292129 3293032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_16_q proxy err 0.0006996437441557646 tr(WHW.T) 631.683349609375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.65it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.28it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.62it/s] 50%|█████     | 4/8 [00:01<00:01,  2.82it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.94it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]100%|██████████| 8/8 [00:02<00:00,  3.09it/s]100%|██████████| 8/8 [00:02<00:00,  2.84it/s]
W0416 08:26:41.490917 3293032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_16_k proxy err 0.0004951588343828917 tr(WHW.T) 873.5093383789062
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.37it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.69it/s] 50%|█████     | 4/8 [00:01<00:01,  2.87it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.99it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.05it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]100%|██████████| 8/8 [00:02<00:00,  3.09it/s]100%|██████████| 8/8 [00:02<00:00,  2.88it/s]
W0416 08:26:45.647252 3293032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_16_o proxy err 0.0003075984714087099 tr(WHW.T) 8.326766967773438
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.21s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.43it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.86it/s] 50%|█████     | 4/8 [00:02<00:01,  2.18it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.38it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.55it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.67it/s]100%|██████████| 8/8 [00:03<00:00,  2.76it/s]100%|██████████| 8/8 [00:03<00:00,  2.22it/s]
W0416 08:26:51.752808 3293032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_16_fc1 proxy err 0.0002352253650315106 tr(WHW.T) 12186.9990234375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.07s/it]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:14,  1.98it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.56it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.76it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.81it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.83it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.86it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.90it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.91it/s] 50%|█████     | 16/32 [00:06<00:05,  2.91it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.90it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.91it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.91it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.92it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.92it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.95it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.97it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.98it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.99it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.98it/s]100%|██████████| 32/32 [00:11<00:00,  2.93it/s]100%|██████████| 32/32 [00:11<00:00,  2.73it/s]
W0416 08:27:05.040167 3293032 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_16_fc2 proxy err 0.00043154595186933875 tr(WHW.T) 18.154970169067383
I0416 08:27:06.501041 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 17 in 0.16s
I0416 08:27:06.616864 3179539 quantize_finetune_clip.py:151] vision layer 18 gpu 0
I0416 08:27:08.898259 3296586 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:27:08.898401 3296586 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:27:08.898468 3296586 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:27:09.021529 3296586 config.py:58] PyTorch version 2.4.0 available.
W0416 08:27:11.322629 3296586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.63s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.14it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.58it/s] 50%|█████     | 4/8 [00:02<00:02,  1.93it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.19it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.40it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.55it/s]100%|██████████| 8/8 [00:04<00:00,  2.63it/s]100%|██████████| 8/8 [00:04<00:00,  1.98it/s]
W0416 08:27:17.623980 3296586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_17_v proxy err 0.0010873235296458006 tr(WHW.T) 311.25750732421875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.34it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.64it/s] 50%|█████     | 4/8 [00:01<00:01,  2.80it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.97it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
W0416 08:27:23.127487 3296586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_17_q proxy err 0.0007523740641772747 tr(WHW.T) 640.507080078125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.67it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.26it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.54it/s] 50%|█████     | 4/8 [00:01<00:01,  2.71it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.80it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.87it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  2.75it/s]
W0416 08:27:27.782717 3296586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_17_k proxy err 0.0005464180721901357 tr(WHW.T) 881.98974609375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.26it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.51it/s] 50%|█████     | 4/8 [00:01<00:01,  2.67it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.80it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
W0416 08:27:33.257513 3296586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_17_o proxy err 0.00031123601365834475 tr(WHW.T) 7.44087028503418
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.22s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.39it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.77it/s] 50%|█████     | 4/8 [00:02<00:01,  2.04it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.24it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.36it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.46it/s]100%|██████████| 8/8 [00:03<00:00,  2.51it/s]100%|██████████| 8/8 [00:03<00:00,  2.08it/s]
W0416 08:27:38.886884 3296586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_17_fc1 proxy err 0.0002625357883516699 tr(WHW.T) 15504.220703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.09s/it]  6%|▋         | 2/32 [00:01<00:20,  1.47it/s]  9%|▉         | 3/32 [00:01<00:15,  1.85it/s] 12%|█▎        | 4/32 [00:02<00:13,  2.11it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.29it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.42it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.51it/s] 25%|██▌       | 8/32 [00:03<00:09,  2.57it/s] 28%|██▊       | 9/32 [00:04<00:08,  2.60it/s] 31%|███▏      | 10/32 [00:04<00:08,  2.59it/s] 34%|███▍      | 11/32 [00:04<00:08,  2.61it/s] 38%|███▊      | 12/32 [00:05<00:07,  2.63it/s] 41%|████      | 13/32 [00:05<00:07,  2.64it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.66it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.65it/s] 50%|█████     | 16/32 [00:06<00:06,  2.66it/s] 53%|█████▎    | 17/32 [00:07<00:05,  2.67it/s] 56%|█████▋    | 18/32 [00:07<00:05,  2.68it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.69it/s] 62%|██████▎   | 20/32 [00:08<00:04,  2.69it/s] 66%|██████▌   | 21/32 [00:08<00:04,  2.70it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.71it/s] 72%|███████▏  | 23/32 [00:09<00:03,  2.71it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.68it/s] 78%|███████▊  | 25/32 [00:10<00:02,  2.68it/s] 81%|████████▏ | 26/32 [00:10<00:02,  2.68it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.69it/s] 88%|████████▊ | 28/32 [00:11<00:01,  2.69it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.67it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.68it/s] 97%|█████████▋| 31/32 [00:12<00:00,  2.67it/s]100%|██████████| 32/32 [00:12<00:00,  2.66it/s]100%|██████████| 32/32 [00:12<00:00,  2.52it/s]
W0416 08:27:53.177463 3296586 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_17_fc2 proxy err 0.0005446278373710811 tr(WHW.T) 18.253433227539062
I0416 08:27:54.784453 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 18 in 0.18s
I0416 08:27:54.906739 3179539 quantize_finetune_clip.py:151] vision layer 19 gpu 0
I0416 08:27:57.341518 3300259 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:27:57.341674 3300259 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:27:57.341731 3300259 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:27:57.461327 3300259 config.py:58] PyTorch version 2.4.0 available.
W0416 08:28:00.755704 3300259 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.66s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.14it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.58it/s] 50%|█████     | 4/8 [00:02<00:02,  1.95it/s] 62%|██████▎   | 5/8 [00:03<00:01,  2.20it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.42it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.59it/s]100%|██████████| 8/8 [00:04<00:00,  2.70it/s]100%|██████████| 8/8 [00:04<00:00,  1.99it/s]
W0416 08:28:06.928260 3300259 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_18_v proxy err 0.0011838484788313508 tr(WHW.T) 355.8935546875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.30it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.55it/s] 50%|█████     | 4/8 [00:01<00:01,  2.68it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.80it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.86it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.91it/s]100%|██████████| 8/8 [00:02<00:00,  2.94it/s]100%|██████████| 8/8 [00:02<00:00,  2.73it/s]
W0416 08:28:11.372225 3300259 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_18_q proxy err 0.0007319278083741665 tr(WHW.T) 765.724365234375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.61it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.21it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.52it/s] 50%|█████     | 4/8 [00:01<00:01,  2.71it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.82it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.89it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.93it/s]100%|██████████| 8/8 [00:02<00:00,  2.95it/s]100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
W0416 08:28:15.694766 3300259 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_18_k proxy err 0.0005675069405697286 tr(WHW.T) 952.23486328125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.28it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.53it/s] 50%|█████     | 4/8 [00:01<00:01,  2.72it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.83it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.96it/s]100%|██████████| 8/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
W0416 08:28:19.976848 3300259 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_18_o proxy err 0.0003202319785486907 tr(WHW.T) 8.747404098510742
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.26s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.36it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.78it/s] 50%|█████     | 4/8 [00:02<00:01,  2.09it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.29it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.43it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.55it/s]100%|██████████| 8/8 [00:03<00:00,  2.63it/s]100%|██████████| 8/8 [00:03<00:00,  2.12it/s]
W0416 08:28:25.282769 3300259 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_18_fc1 proxy err 0.00028291501803323627 tr(WHW.T) 17885.8984375
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.07s/it]  6%|▋         | 2/32 [00:01<00:19,  1.53it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.21it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.53it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.64it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.71it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.74it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.76it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.78it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.78it/s] 41%|████      | 13/32 [00:05<00:06,  2.80it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.80it/s] 47%|████▋     | 15/32 [00:06<00:06,  2.81it/s] 50%|█████     | 16/32 [00:06<00:05,  2.80it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.80it/s] 56%|█████▋    | 18/32 [00:07<00:04,  2.80it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.78it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.76it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.76it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.79it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.79it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.79it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.80it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.78it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.79it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.81it/s] 91%|█████████ | 29/32 [00:11<00:01,  2.82it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.82it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.83it/s]100%|██████████| 32/32 [00:12<00:00,  2.85it/s]100%|██████████| 32/32 [00:12<00:00,  2.65it/s]
W0416 08:28:38.949189 3300259 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_18_fc2 proxy err 0.0005790982395410538 tr(WHW.T) 21.267833709716797
I0416 08:28:40.480469 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 19 in 0.16s
I0416 08:28:40.605060 3179539 quantize_finetune_clip.py:151] vision layer 20 gpu 0
I0416 08:28:42.877956 3303823 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:28:42.878087 3303823 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:28:42.878146 3303823 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:28:42.993868 3303823 config.py:58] PyTorch version 2.4.0 available.
W0416 08:28:45.203194 3303823 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.62s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.18it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.65it/s] 50%|█████     | 4/8 [00:02<00:01,  2.03it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.33it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.52it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.69it/s]100%|██████████| 8/8 [00:03<00:00,  2.83it/s]100%|██████████| 8/8 [00:03<00:00,  2.08it/s]
W0416 08:28:50.472340 3303823 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_19_v proxy err 0.0012207002146169543 tr(WHW.T) 348.79376220703125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.29it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.55it/s] 50%|█████     | 4/8 [00:01<00:01,  2.73it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.85it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.92it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  2.77it/s]
W0416 08:28:55.441505 3303823 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_19_q proxy err 0.0008331911521963775 tr(WHW.T) 673.1248779296875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.75it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.39it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.82it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.95it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.07it/s]100%|██████████| 8/8 [00:02<00:00,  3.10it/s]100%|██████████| 8/8 [00:02<00:00,  2.87it/s]
W0416 08:28:59.651976 3303823 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_19_k proxy err 0.0005477780941873789 tr(WHW.T) 986.3826293945312
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.80it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.43it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.75it/s] 50%|█████     | 4/8 [00:01<00:01,  2.91it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.04it/s]100%|██████████| 8/8 [00:02<00:00,  3.08it/s]100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
W0416 08:29:03.790148 3303823 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_19_o proxy err 0.00032665563048794866 tr(WHW.T) 9.281452178955078
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.19s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.44it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.86it/s] 50%|█████     | 4/8 [00:02<00:01,  2.13it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.34it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.51it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.62it/s]100%|██████████| 8/8 [00:03<00:00,  2.71it/s]100%|██████████| 8/8 [00:03<00:00,  2.20it/s]
W0416 08:29:08.930261 3303823 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_19_fc1 proxy err 0.00027738860808312893 tr(WHW.T) 20043.716796875
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.07s/it]  6%|▋         | 2/32 [00:01<00:19,  1.56it/s]  9%|▉         | 3/32 [00:01<00:14,  1.98it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.28it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.49it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.72it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.89it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.89it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.91it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.90it/s] 50%|█████     | 16/32 [00:06<00:05,  2.91it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.96it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.96it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.96it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.95it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.95it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.89it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.86it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.86it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.74it/s]
W0416 08:29:22.176883 3303823 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_19_fc2 proxy err 0.00042116426629945636 tr(WHW.T) 25.913047790527344
I0416 08:29:23.595788 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 20 in 0.17s
I0416 08:29:23.715353 3179539 quantize_finetune_clip.py:151] vision layer 21 gpu 0
I0416 08:29:25.993310 3307296 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:29:25.993446 3307296 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:29:25.993508 3307296 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:29:26.112112 3307296 config.py:58] PyTorch version 2.4.0 available.
W0416 08:29:28.350572 3307296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.61s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.18it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.66it/s] 50%|█████     | 4/8 [00:02<00:01,  2.06it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.38it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.62it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.80it/s]100%|██████████| 8/8 [00:03<00:00,  2.93it/s]100%|██████████| 8/8 [00:03<00:00,  2.12it/s]
W0416 08:29:33.539333 3307296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_20_v proxy err 0.001231607748195529 tr(WHW.T) 375.24658203125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.78it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.43it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.73it/s] 50%|█████     | 4/8 [00:01<00:01,  2.90it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.97it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.06it/s]100%|██████████| 8/8 [00:02<00:00,  3.08it/s]100%|██████████| 8/8 [00:02<00:00,  2.89it/s]
W0416 08:29:37.678998 3307296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_20_q proxy err 0.0007460833294317126 tr(WHW.T) 722.5367431640625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.30it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.54it/s] 50%|█████     | 4/8 [00:01<00:01,  2.69it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.79it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.84it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.89it/s]100%|██████████| 8/8 [00:02<00:00,  2.91it/s]100%|██████████| 8/8 [00:02<00:00,  2.72it/s]
W0416 08:29:42.675410 3307296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_20_k proxy err 0.0004690776695497334 tr(WHW.T) 1119.14111328125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.76it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.41it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.73it/s] 50%|█████     | 4/8 [00:01<00:01,  2.90it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.98it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.04it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]100%|██████████| 8/8 [00:02<00:00,  3.13it/s]100%|██████████| 8/8 [00:02<00:00,  2.91it/s]
W0416 08:29:47.501550 3307296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_20_o proxy err 0.00028838819707743824 tr(WHW.T) 13.241490364074707
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.18s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.45it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.88it/s] 50%|█████     | 4/8 [00:02<00:01,  2.16it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.37it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.54it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.66it/s]100%|██████████| 8/8 [00:03<00:00,  2.70it/s]100%|██████████| 8/8 [00:03<00:00,  2.21it/s]
W0416 08:29:52.772662 3307296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_20_fc1 proxy err 0.00024953793035820127 tr(WHW.T) 28492.39453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.08s/it]  6%|▋         | 2/32 [00:01<00:19,  1.55it/s]  9%|▉         | 3/32 [00:01<00:14,  1.97it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.23it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.41it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.57it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.68it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.77it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.81it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.86it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.89it/s] 41%|████      | 13/32 [00:05<00:06,  2.88it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.87it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.89it/s] 50%|█████     | 16/32 [00:06<00:05,  2.91it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.93it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.94it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.95it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.97it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.98it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.99it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.98it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.98it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.98it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.95it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.94it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.91it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.75it/s]
W0416 08:30:06.022382 3307296 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_20_fc2 proxy err 0.00013155461056157947 tr(WHW.T) 105.08917999267578
I0416 08:30:07.413155 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 21 in 0.15s
I0416 08:30:07.532721 3179539 quantize_finetune_clip.py:151] vision layer 22 gpu 0
I0416 08:30:09.778453 3310854 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:30:09.778588 3310854 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:30:09.778648 3310854 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:30:09.896627 3310854 config.py:58] PyTorch version 2.4.0 available.
W0416 08:30:12.216038 3310854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.61s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.18it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.65it/s] 50%|█████     | 4/8 [00:02<00:01,  2.04it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.34it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.56it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.72it/s]100%|██████████| 8/8 [00:03<00:00,  2.86it/s]100%|██████████| 8/8 [00:03<00:00,  2.09it/s]
W0416 08:30:17.458323 3310854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_21_v proxy err 0.0011928605381399393 tr(WHW.T) 334.0597229003906
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.66it/s] 50%|█████     | 4/8 [00:01<00:01,  2.80it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.93it/s]100%|██████████| 8/8 [00:02<00:00,  2.79it/s]
W0416 08:30:21.678681 3310854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_21_q proxy err 4.7329940571216866e-05 tr(WHW.T) 10014.0400390625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.70it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.34it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.61it/s] 50%|█████     | 4/8 [00:01<00:01,  2.78it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.91it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.94it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.98it/s]100%|██████████| 8/8 [00:02<00:00,  3.02it/s]100%|██████████| 8/8 [00:02<00:00,  2.80it/s]
W0416 08:30:25.878534 3310854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_21_k proxy err 0.00041172426426783204 tr(WHW.T) 998.8802490234375
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.78it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.40it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.68it/s] 50%|█████     | 4/8 [00:01<00:01,  2.84it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.96it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.01it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.84it/s]
W0416 08:30:30.054180 3310854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_21_o proxy err 0.0003055343695450574 tr(WHW.T) 17.47711181640625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.18s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.45it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.86it/s] 50%|█████     | 4/8 [00:02<00:01,  2.16it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.38it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.53it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.61it/s]100%|██████████| 8/8 [00:03<00:00,  2.68it/s]100%|██████████| 8/8 [00:03<00:00,  2.20it/s]
W0416 08:30:35.166146 3310854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_21_fc1 proxy err 0.00021999265300109982 tr(WHW.T) 26967.453125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.04s/it]  6%|▋         | 2/32 [00:01<00:18,  1.59it/s]  9%|▉         | 3/32 [00:01<00:14,  2.01it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.31it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.48it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.62it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.74it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.82it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.87it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.92it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.95it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.97it/s] 41%|████      | 13/32 [00:05<00:06,  2.97it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.98it/s] 47%|████▋     | 15/32 [00:05<00:05,  3.00it/s] 50%|█████     | 16/32 [00:06<00:05,  3.01it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.97it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.96it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.97it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.97it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.99it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.99it/s] 72%|███████▏  | 23/32 [00:08<00:03,  3.00it/s] 75%|███████▌  | 24/32 [00:08<00:02,  3.00it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.99it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.95it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.94it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.96it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.98it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.99it/s]100%|██████████| 32/32 [00:11<00:00,  2.95it/s]100%|██████████| 32/32 [00:11<00:00,  2.79it/s]
W0416 08:30:48.171389 3310854 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_21_fc2 proxy err 0.00017886614659801126 tr(WHW.T) 76.61061096191406
I0416 08:30:49.587158 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 22 in 0.17s
I0416 08:30:49.703165 3179539 quantize_finetune_clip.py:151] vision layer 23 gpu 0
I0416 08:30:51.983573 3314143 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:30:51.983707 3314143 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:30:51.983772 3314143 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:30:52.107236 3314143 config.py:58] PyTorch version 2.4.0 available.
W0416 08:30:54.326256 3314143 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.57s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.20it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.66it/s] 50%|█████     | 4/8 [00:02<00:01,  2.02it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.29it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.49it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.65it/s]100%|██████████| 8/8 [00:03<00:00,  2.77it/s]100%|██████████| 8/8 [00:03<00:00,  2.07it/s]
W0416 08:31:00.279957 3314143 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_22_v proxy err 0.0014827278209850192 tr(WHW.T) 295.9122619628906
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.72it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.62it/s] 50%|█████     | 4/8 [00:01<00:01,  2.79it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.87it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.99it/s]100%|██████████| 8/8 [00:02<00:00,  3.04it/s]100%|██████████| 8/8 [00:02<00:00,  2.81it/s]
W0416 08:31:05.061897 3314143 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_22_q proxy err 0.00017652845417615026 tr(WHW.T) 127158.65625
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.71it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.33it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.63it/s] 50%|█████     | 4/8 [00:01<00:01,  2.84it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.93it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.98it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.01it/s]100%|██████████| 8/8 [00:02<00:00,  3.06it/s]100%|██████████| 8/8 [00:02<00:00,  2.84it/s]
W0416 08:31:09.996066 3314143 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_22_k proxy err 8.957265526987612e-05 tr(WHW.T) 7097.814453125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.73it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.35it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.65it/s] 50%|█████     | 4/8 [00:01<00:01,  2.82it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.95it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.02it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.09it/s]100%|██████████| 8/8 [00:02<00:00,  3.11it/s]100%|██████████| 8/8 [00:02<00:00,  2.87it/s]
W0416 08:31:15.104393 3314143 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_22_o proxy err 0.0003811255737673491 tr(WHW.T) 19.675331115722656
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.20s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.43it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.86it/s] 50%|█████     | 4/8 [00:02<00:01,  2.17it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.36it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.49it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.58it/s]100%|██████████| 8/8 [00:03<00:00,  2.67it/s]100%|██████████| 8/8 [00:03<00:00,  2.19it/s]
W0416 08:31:20.350803 3314143 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_22_fc1 proxy err 0.0001906292891362682 tr(WHW.T) 20578.845703125
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:32,  1.06s/it]  6%|▋         | 2/32 [00:01<00:19,  1.57it/s]  9%|▉         | 3/32 [00:01<00:14,  2.00it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.27it/s] 16%|█▌        | 5/32 [00:02<00:10,  2.47it/s] 19%|█▉        | 6/32 [00:02<00:09,  2.63it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.73it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.79it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.85it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.89it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.91it/s] 38%|███▊      | 12/32 [00:04<00:06,  2.94it/s] 41%|████      | 13/32 [00:05<00:06,  2.93it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.94it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.95it/s] 50%|█████     | 16/32 [00:06<00:05,  2.94it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.92it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.92it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.94it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.95it/s] 66%|██████▌   | 21/32 [00:07<00:03,  2.95it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.97it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.96it/s] 75%|███████▌  | 24/32 [00:08<00:02,  2.95it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.95it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.94it/s] 84%|████████▍ | 27/32 [00:09<00:01,  2.96it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.95it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.94it/s] 94%|█████████▍| 30/32 [00:10<00:00,  2.96it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.94it/s]100%|██████████| 32/32 [00:11<00:00,  2.91it/s]100%|██████████| 32/32 [00:11<00:00,  2.76it/s]
W0416 08:31:33.568811 3314143 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_22_fc2 proxy err 0.00015672670269850641 tr(WHW.T) 57.731483459472656
I0416 08:31:34.997880 3179539 quantize_finetune_clip.py:168] computed original embedding for vision layer 23 in 0.16s
I0416 08:31:37.417997 3317798 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:31:37.418130 3317798 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:31:37.418190 3317798 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:31:37.535294 3317798 config.py:58] PyTorch version 2.4.0 available.
W0416 08:31:39.790696 3317798 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:11,  1.61s/it] 25%|██▌       | 2/8 [00:01<00:05,  1.18it/s] 38%|███▊      | 3/8 [00:02<00:03,  1.65it/s] 50%|█████     | 4/8 [00:02<00:01,  2.03it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.33it/s] 75%|███████▌  | 6/8 [00:03<00:00,  2.56it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.75it/s]100%|██████████| 8/8 [00:03<00:00,  2.88it/s]100%|██████████| 8/8 [00:03<00:00,  2.10it/s]
W0416 08:31:45.010276 3317798 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_23_v proxy err 0.0014637215062975883 tr(WHW.T) 353.4617919921875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.75it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.36it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.66it/s] 50%|█████     | 4/8 [00:01<00:01,  2.84it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.94it/s] 75%|███████▌  | 6/8 [00:02<00:00,  3.00it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s]100%|██████████| 8/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  2.82it/s]
W0416 08:31:49.836671 3317798 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_23_q proxy err 0.0007834166171960533 tr(WHW.T) 414.4307861328125
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:04,  1.59it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.21it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.53it/s] 50%|█████     | 4/8 [00:01<00:01,  2.69it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.80it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.90it/s] 88%|████████▊ | 7/8 [00:02<00:00,  2.97it/s]100%|██████████| 8/8 [00:02<00:00,  3.00it/s]100%|██████████| 8/8 [00:02<00:00,  2.74it/s]
W0416 08:31:55.407375 3317798 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_23_k proxy err 0.0003564815269783139 tr(WHW.T) 884.6561279296875
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:00<00:03,  1.77it/s] 25%|██▌       | 2/8 [00:00<00:02,  2.39it/s] 38%|███▊      | 3/8 [00:01<00:01,  2.67it/s] 50%|█████     | 4/8 [00:01<00:01,  2.84it/s] 62%|██████▎   | 5/8 [00:01<00:01,  2.92it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.99it/s] 88%|████████▊ | 7/8 [00:02<00:00,  3.04it/s]100%|██████████| 8/8 [00:02<00:00,  3.09it/s]100%|██████████| 8/8 [00:02<00:00,  2.86it/s]
W0416 08:31:59.789082 3317798 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_23_o proxy err 0.00019496695313137025 tr(WHW.T) 40.9009895324707
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:01<00:08,  1.21s/it] 25%|██▌       | 2/8 [00:01<00:04,  1.43it/s] 38%|███▊      | 3/8 [00:01<00:02,  1.84it/s] 50%|█████     | 4/8 [00:02<00:01,  2.13it/s] 62%|██████▎   | 5/8 [00:02<00:01,  2.36it/s] 75%|███████▌  | 6/8 [00:02<00:00,  2.50it/s] 88%|████████▊ | 7/8 [00:03<00:00,  2.61it/s]100%|██████████| 8/8 [00:03<00:00,  2.71it/s]100%|██████████| 8/8 [00:03<00:00,  2.19it/s]
W0416 08:32:04.984416 3317798 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

vision_23_fc1 proxy err 0.0002894300560001284 tr(WHW.T) 5847.96337890625
  0%|          | 0/32 [00:00<?, ?it/s]  3%|▎         | 1/32 [00:01<00:33,  1.07s/it]  6%|▋         | 2/32 [00:01<00:19,  1.55it/s]  9%|▉         | 3/32 [00:01<00:15,  1.93it/s] 12%|█▎        | 4/32 [00:02<00:12,  2.20it/s] 16%|█▌        | 5/32 [00:02<00:11,  2.40it/s] 19%|█▉        | 6/32 [00:02<00:10,  2.54it/s] 22%|██▏       | 7/32 [00:03<00:09,  2.66it/s] 25%|██▌       | 8/32 [00:03<00:08,  2.75it/s] 28%|██▊       | 9/32 [00:03<00:08,  2.80it/s] 31%|███▏      | 10/32 [00:04<00:07,  2.85it/s] 34%|███▍      | 11/32 [00:04<00:07,  2.87it/s] 38%|███▊      | 12/32 [00:04<00:07,  2.85it/s] 41%|████      | 13/32 [00:05<00:06,  2.87it/s] 44%|████▍     | 14/32 [00:05<00:06,  2.89it/s] 47%|████▋     | 15/32 [00:05<00:05,  2.86it/s] 50%|█████     | 16/32 [00:06<00:05,  2.87it/s] 53%|█████▎    | 17/32 [00:06<00:05,  2.86it/s] 56%|█████▋    | 18/32 [00:06<00:04,  2.88it/s] 59%|█████▉    | 19/32 [00:07<00:04,  2.88it/s] 62%|██████▎   | 20/32 [00:07<00:04,  2.89it/s] 66%|██████▌   | 21/32 [00:08<00:03,  2.89it/s] 69%|██████▉   | 22/32 [00:08<00:03,  2.90it/s] 72%|███████▏  | 23/32 [00:08<00:03,  2.91it/s] 75%|███████▌  | 24/32 [00:09<00:02,  2.91it/s] 78%|███████▊  | 25/32 [00:09<00:02,  2.91it/s] 81%|████████▏ | 26/32 [00:09<00:02,  2.91it/s] 84%|████████▍ | 27/32 [00:10<00:01,  2.86it/s] 88%|████████▊ | 28/32 [00:10<00:01,  2.86it/s] 91%|█████████ | 29/32 [00:10<00:01,  2.85it/s] 94%|█████████▍| 30/32 [00:11<00:00,  2.87it/s] 97%|█████████▋| 31/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.89it/s]100%|██████████| 32/32 [00:11<00:00,  2.71it/s]
W0416 08:32:18.454591 3317798 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

vision_23_fc2 proxy err 5.510318442247808e-05 tr(WHW.T) 125.12049102783203
I0416 08:32:19.484585 3179539 quantize_finetune_clip.py:151] text layer 0 gpu 0
I0416 08:32:19.758149 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 0 in 0.15s
I0416 08:32:19.873615 3179539 quantize_finetune_clip.py:151] text layer 1 gpu 0
I0416 08:32:22.153606 3321341 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:32:22.153741 3321341 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:32:22.153803 3321341 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:32:22.270059 3321341 config.py:58] PyTorch version 2.4.0 available.
W0416 08:32:24.515705 3321341 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:03<00:17,  3.60s/it] 33%|███▎      | 2/6 [00:03<00:06,  1.67s/it] 50%|█████     | 3/6 [00:04<00:03,  1.05s/it] 67%|██████▋   | 4/6 [00:04<00:01,  1.30it/s] 83%|████████▎ | 5/6 [00:04<00:00,  1.65it/s]100%|██████████| 6/6 [00:05<00:00,  1.95it/s]100%|██████████| 6/6 [00:05<00:00,  1.15it/s]
W0416 08:32:31.161358 3321341 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_0_v proxy err 0.001166447065770626 tr(WHW.T) 75.04249572753906
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.69it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.31it/s] 50%|█████     | 3/6 [00:01<00:01,  2.64it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.80it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.93it/s]100%|██████████| 6/6 [00:02<00:00,  3.01it/s]100%|██████████| 6/6 [00:02<00:00,  2.75it/s]
W0416 08:32:34.693856 3321341 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_0_q proxy err 0.00043317489326000214 tr(WHW.T) 563.9849853515625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.73it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.30it/s] 50%|█████     | 3/6 [00:01<00:01,  2.62it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.82it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  2.75it/s]
W0416 08:32:38.248358 3321341 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_0_k proxy err 0.0004402141785249114 tr(WHW.T) 544.4554443359375
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.42it/s] 50%|█████     | 3/6 [00:01<00:01,  2.71it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.83it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.97it/s]100%|██████████| 6/6 [00:02<00:00,  3.04it/s]100%|██████████| 6/6 [00:02<00:00,  2.81it/s]
W0416 08:32:41.731426 3321341 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_0_o proxy err 0.00018841844575945288 tr(WHW.T) 5.182252883911133
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.21s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.41it/s] 50%|█████     | 3/6 [00:01<00:01,  1.84it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.14it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.36it/s]100%|██████████| 6/6 [00:02<00:00,  2.51it/s]100%|██████████| 6/6 [00:02<00:00,  2.03it/s]
W0416 08:32:46.151344 3321341 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_0_fc1 proxy err 1.372134738630848e-05 tr(WHW.T) 36859.8359375
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.06s/it]  8%|▊         | 2/24 [00:01<00:13,  1.58it/s] 12%|█▎        | 3/24 [00:01<00:10,  2.00it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.30it/s] 21%|██        | 5/24 [00:02<00:07,  2.49it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.63it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.72it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.79it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.84it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.88it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.90it/s] 50%|█████     | 12/24 [00:04<00:04,  2.91it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.92it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.94it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.95it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.96it/s] 71%|███████   | 17/24 [00:06<00:02,  2.97it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.96it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.96it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.97it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.96it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.93it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.89it/s]100%|██████████| 24/24 [00:08<00:00,  2.90it/s]100%|██████████| 24/24 [00:08<00:00,  2.71it/s]
W0416 08:32:56.492623 3321341 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_0_fc2 proxy err 1.3481125051839626e-06 tr(WHW.T) 2877.713134765625
I0416 08:32:57.912951 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 1 in 0.15s
I0416 08:32:58.050098 3179539 quantize_finetune_clip.py:151] text layer 2 gpu 0
I0416 08:33:00.289762 3324696 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:33:00.289900 3324696 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:33:00.289958 3324696 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:33:00.404889 3324696 config.py:58] PyTorch version 2.4.0 available.
W0416 08:33:03.014824 3324696 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:07,  1.60s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.20it/s] 50%|█████     | 3/6 [00:02<00:01,  1.69it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.07it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.39it/s]100%|██████████| 6/6 [00:03<00:00,  2.63it/s]100%|██████████| 6/6 [00:03<00:00,  1.92it/s]
W0416 08:33:07.542243 3324696 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_1_v proxy err 0.00023072956537362188 tr(WHW.T) 73.62937927246094
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.80it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.42it/s] 50%|█████     | 3/6 [00:01<00:01,  2.72it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.89it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  3.00it/s]100%|██████████| 6/6 [00:02<00:00,  2.80it/s]
W0416 08:33:11.016507 3324696 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_1_q proxy err 0.00010987201676471159 tr(WHW.T) 623.6982421875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.75it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.40it/s] 50%|█████     | 3/6 [00:01<00:01,  2.73it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.91it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.96it/s]100%|██████████| 6/6 [00:02<00:00,  3.03it/s]100%|██████████| 6/6 [00:02<00:00,  2.81it/s]
W0416 08:33:14.606702 3324696 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_1_k proxy err 0.00015001055726315826 tr(WHW.T) 389.26483154296875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.82it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.42it/s] 50%|█████     | 3/6 [00:01<00:01,  2.75it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.93it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.04it/s]100%|██████████| 6/6 [00:02<00:00,  3.09it/s]100%|██████████| 6/6 [00:02<00:00,  2.86it/s]
W0416 08:33:18.258003 3324696 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_1_o proxy err 7.408664532704279e-05 tr(WHW.T) 3.8104982376098633
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:05,  1.19s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.45it/s] 50%|█████     | 3/6 [00:01<00:01,  1.89it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.21it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.44it/s]100%|██████████| 6/6 [00:02<00:00,  2.59it/s]100%|██████████| 6/6 [00:02<00:00,  2.09it/s]
W0416 08:33:22.647044 3324696 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_1_fc1 proxy err 2.357307130296249e-05 tr(WHW.T) 24829.86328125
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:23,  1.03s/it]  8%|▊         | 2/24 [00:01<00:13,  1.59it/s] 12%|█▎        | 3/24 [00:01<00:10,  2.02it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.31it/s] 21%|██        | 5/24 [00:02<00:07,  2.51it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.65it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.75it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.81it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.86it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.90it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.92it/s] 50%|█████     | 12/24 [00:04<00:04,  2.94it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.96it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.96it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.97it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.98it/s] 71%|███████   | 17/24 [00:06<00:02,  2.93it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.94it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.94it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.92it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.91it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.93it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.94it/s]100%|██████████| 24/24 [00:08<00:00,  2.92it/s]100%|██████████| 24/24 [00:08<00:00,  2.72it/s]
W0416 08:33:32.894142 3324696 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_1_fc2 proxy err 6.811328057665378e-05 tr(WHW.T) 2.0527796745300293
I0416 08:33:34.301046 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 2 in 0.14s
I0416 08:33:34.420719 3179539 quantize_finetune_clip.py:151] text layer 3 gpu 0
I0416 08:33:36.714162 3327697 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:33:36.714301 3327697 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:33:36.714360 3327697 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:33:36.829183 3327697 config.py:58] PyTorch version 2.4.0 available.
W0416 08:33:39.122845 3327697 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.65s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.15it/s] 50%|█████     | 3/6 [00:02<00:01,  1.63it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.02it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.34it/s]100%|██████████| 6/6 [00:03<00:00,  2.57it/s]100%|██████████| 6/6 [00:03<00:00,  1.87it/s]
W0416 08:33:43.821169 3327697 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_2_v proxy err 0.00023798839538358152 tr(WHW.T) 129.4635009765625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.79it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.42it/s] 50%|█████     | 3/6 [00:01<00:01,  2.72it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.90it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.00it/s]100%|██████████| 6/6 [00:02<00:00,  2.97it/s]100%|██████████| 6/6 [00:02<00:00,  2.79it/s]
W0416 08:33:47.904617 3327697 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_2_q proxy err 9.85741862677969e-05 tr(WHW.T) 725.2421264648438
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:03,  1.66it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.32it/s] 50%|█████     | 3/6 [00:01<00:01,  2.65it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.77it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.90it/s]100%|██████████| 6/6 [00:02<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  2.74it/s]
W0416 08:33:52.264153 3327697 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_2_k proxy err 0.00014035282947588712 tr(WHW.T) 564.4356079101562
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.41it/s] 50%|█████     | 3/6 [00:01<00:01,  2.72it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.83it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  3.02it/s]100%|██████████| 6/6 [00:02<00:00,  2.80it/s]
W0416 08:33:55.754719 3327697 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_2_o proxy err 0.00012229318963363767 tr(WHW.T) 3.46923828125
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.21s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.42it/s] 50%|█████     | 3/6 [00:01<00:01,  1.85it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.15it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.36it/s]100%|██████████| 6/6 [00:02<00:00,  2.50it/s]100%|██████████| 6/6 [00:02<00:00,  2.03it/s]
W0416 08:34:00.159722 3327697 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_2_fc1 proxy err 2.9773656933684833e-05 tr(WHW.T) 20103.25390625
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.04s/it]  8%|▊         | 2/24 [00:01<00:13,  1.58it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.98it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.27it/s] 21%|██        | 5/24 [00:02<00:07,  2.48it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.61it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.68it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.74it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.79it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.83it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.86it/s] 50%|█████     | 12/24 [00:04<00:04,  2.88it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.91it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.93it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.94it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.95it/s] 71%|███████   | 17/24 [00:06<00:02,  2.96it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.96it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.97it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.97it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.97it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.95it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.95it/s]100%|██████████| 24/24 [00:08<00:00,  2.96it/s]100%|██████████| 24/24 [00:08<00:00,  2.70it/s]
W0416 08:34:10.507739 3327697 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_2_fc2 proxy err 5.495343793882057e-05 tr(WHW.T) 4.276344299316406
I0416 08:34:11.920392 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 3 in 0.13s
I0416 08:34:12.040783 3179539 quantize_finetune_clip.py:151] text layer 4 gpu 0
I0416 08:34:14.377800 3330963 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:34:14.377939 3330963 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:34:14.377999 3330963 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:34:14.492672 3330963 config.py:58] PyTorch version 2.4.0 available.
W0416 08:34:16.767377 3330963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:07,  1.58s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.21it/s] 50%|█████     | 3/6 [00:02<00:01,  1.71it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.11it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.43it/s]100%|██████████| 6/6 [00:03<00:00,  2.62it/s]100%|██████████| 6/6 [00:03<00:00,  1.93it/s]
W0416 08:34:21.260463 3330963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_3_v proxy err 0.00026012418675236404 tr(WHW.T) 130.8394775390625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.79it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.44it/s] 50%|█████     | 3/6 [00:01<00:01,  2.73it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.88it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.82it/s]100%|██████████| 6/6 [00:02<00:00,  2.89it/s]100%|██████████| 6/6 [00:02<00:00,  2.73it/s]
W0416 08:34:24.819933 3330963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_3_q proxy err 0.00013759269495494664 tr(WHW.T) 616.7564697265625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.73it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.39it/s] 50%|█████     | 3/6 [00:01<00:01,  2.66it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.85it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.96it/s]100%|██████████| 6/6 [00:02<00:00,  2.80it/s]100%|██████████| 6/6 [00:02<00:00,  2.69it/s]
W0416 08:34:28.458060 3330963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_3_k proxy err 0.00012375229562167078 tr(WHW.T) 697.400146484375
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.44it/s] 50%|█████     | 3/6 [00:01<00:01,  2.75it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.94it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.06it/s]100%|██████████| 6/6 [00:02<00:00,  3.13it/s]100%|██████████| 6/6 [00:02<00:00,  2.88it/s]
W0416 08:34:31.915336 3330963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_3_o proxy err 8.544837328372523e-05 tr(WHW.T) 5.515538215637207
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.22s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.43it/s] 50%|█████     | 3/6 [00:01<00:01,  1.87it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.19it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.42it/s]100%|██████████| 6/6 [00:02<00:00,  2.58it/s]100%|██████████| 6/6 [00:02<00:00,  2.07it/s]
W0416 08:34:36.242282 3330963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_3_fc1 proxy err 2.942509308923036e-05 tr(WHW.T) 14064.40625
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.06s/it]  8%|▊         | 2/24 [00:01<00:13,  1.58it/s] 12%|█▎        | 3/24 [00:01<00:10,  2.01it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.32it/s] 21%|██        | 5/24 [00:02<00:07,  2.53it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.68it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.78it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.85it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.90it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.93it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.95it/s] 50%|█████     | 12/24 [00:04<00:04,  2.97it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.99it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.99it/s] 62%|██████▎   | 15/24 [00:05<00:02,  3.00it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.99it/s] 71%|███████   | 17/24 [00:06<00:02,  2.99it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.96it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.96it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.95it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.97it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.98it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.99it/s]100%|██████████| 24/24 [00:08<00:00,  3.00it/s]100%|██████████| 24/24 [00:08<00:00,  2.75it/s]
W0416 08:34:46.385731 3330963 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_3_fc2 proxy err 5.5151951528387144e-05 tr(WHW.T) 6.90947151184082
I0416 08:34:47.766764 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 4 in 0.14s
I0416 08:34:47.875444 3179539 quantize_finetune_clip.py:151] text layer 5 gpu 0
I0416 08:34:50.125548 3333961 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:34:50.125681 3333961 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:34:50.125743 3333961 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:34:50.241095 3333961 config.py:58] PyTorch version 2.4.0 available.
W0416 08:34:52.485896 3333961 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.68s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.14it/s] 50%|█████     | 3/6 [00:02<00:01,  1.62it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.01it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.33it/s]100%|██████████| 6/6 [00:03<00:00,  2.57it/s]100%|██████████| 6/6 [00:03<00:00,  1.86it/s]
W0416 08:34:57.116066 3333961 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_4_v proxy err 0.0003050807863473892 tr(WHW.T) 180.7900848388672
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.37it/s] 50%|█████     | 3/6 [00:01<00:01,  2.66it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.84it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.90it/s]100%|██████████| 6/6 [00:02<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  2.77it/s]
W0416 08:35:00.735929 3333961 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_4_q proxy err 0.00011740490299416706 tr(WHW.T) 742.8862915039062
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.70it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.35it/s] 50%|█████     | 3/6 [00:01<00:01,  2.67it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.82it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.91it/s]100%|██████████| 6/6 [00:02<00:00,  2.97it/s]100%|██████████| 6/6 [00:02<00:00,  2.74it/s]
W0416 08:35:04.387588 3333961 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_4_k proxy err 0.0001066289987647906 tr(WHW.T) 890.5114135742188
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.82it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.44it/s] 50%|█████     | 3/6 [00:01<00:01,  2.71it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.89it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.98it/s]100%|██████████| 6/6 [00:02<00:00,  3.06it/s]100%|██████████| 6/6 [00:02<00:00,  2.83it/s]
W0416 08:35:07.831567 3333961 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_4_o proxy err 0.0001307049096794799 tr(WHW.T) 7.300278663635254
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:05,  1.17s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.46it/s] 50%|█████     | 3/6 [00:01<00:01,  1.90it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.18it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.37it/s]100%|██████████| 6/6 [00:02<00:00,  2.53it/s]100%|██████████| 6/6 [00:02<00:00,  2.06it/s]
W0416 08:35:12.171093 3333961 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_4_fc1 proxy err 5.2994535508332774e-05 tr(WHW.T) 9402.642578125
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:23,  1.04s/it]  8%|▊         | 2/24 [00:01<00:13,  1.61it/s] 12%|█▎        | 3/24 [00:01<00:10,  2.05it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.36it/s] 21%|██        | 5/24 [00:02<00:07,  2.56it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.70it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.80it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.86it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.90it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.95it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.97it/s] 50%|█████     | 12/24 [00:04<00:04,  2.99it/s] 54%|█████▍    | 13/24 [00:04<00:03,  3.00it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.97it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.98it/s] 67%|██████▋   | 16/24 [00:06<00:02,  3.01it/s] 71%|███████   | 17/24 [00:06<00:02,  3.00it/s] 75%|███████▌  | 18/24 [00:06<00:01,  3.02it/s] 79%|███████▉  | 19/24 [00:06<00:01,  3.03it/s] 83%|████████▎ | 20/24 [00:07<00:01,  3.03it/s] 88%|████████▊ | 21/24 [00:07<00:00,  3.02it/s] 92%|█████████▏| 22/24 [00:07<00:00,  3.03it/s] 96%|█████████▌| 23/24 [00:08<00:00,  3.04it/s]100%|██████████| 24/24 [00:08<00:00,  3.03it/s]100%|██████████| 24/24 [00:08<00:00,  2.78it/s]
W0416 08:35:22.212089 3333961 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_4_fc2 proxy err 8.005707059055567e-05 tr(WHW.T) 11.091764450073242
I0416 08:35:23.571021 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 5 in 0.14s
I0416 08:35:23.682430 3179539 quantize_finetune_clip.py:151] text layer 6 gpu 0
I0416 08:35:25.938589 3336973 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:35:25.938728 3336973 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:35:25.938791 3336973 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:35:26.056844 3336973 config.py:58] PyTorch version 2.4.0 available.
W0416 08:35:28.310705 3336973 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:07,  1.59s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.20it/s] 50%|█████     | 3/6 [00:02<00:01,  1.68it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.06it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.37it/s]100%|██████████| 6/6 [00:03<00:00,  2.60it/s]100%|██████████| 6/6 [00:03<00:00,  1.91it/s]
W0416 08:35:32.871005 3336973 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_5_v proxy err 0.0001993198093259707 tr(WHW.T) 249.72238159179688
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.76it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.39it/s] 50%|█████     | 3/6 [00:01<00:01,  2.71it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.88it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.98it/s]100%|██████████| 6/6 [00:02<00:00,  3.04it/s]100%|██████████| 6/6 [00:02<00:00,  2.81it/s]
W0416 08:35:36.357032 3336973 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_5_q proxy err 0.00010423183266539127 tr(WHW.T) 855.363525390625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.71it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.35it/s] 50%|█████     | 3/6 [00:01<00:01,  2.63it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.82it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.93it/s]100%|██████████| 6/6 [00:02<00:00,  3.01it/s]100%|██████████| 6/6 [00:02<00:00,  2.76it/s]
W0416 08:35:39.873097 3336973 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_5_k proxy err 0.00011945626465603709 tr(WHW.T) 789.1260986328125
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.43it/s] 50%|█████     | 3/6 [00:01<00:01,  2.73it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.92it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.01it/s]100%|██████████| 6/6 [00:02<00:00,  3.08it/s]100%|██████████| 6/6 [00:02<00:00,  2.84it/s]
W0416 08:35:43.523974 3336973 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_5_o proxy err 0.00013834793935529888 tr(WHW.T) 11.59481430053711
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.22s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.42it/s] 50%|█████     | 3/6 [00:01<00:01,  1.84it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.16it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.39it/s]100%|██████████| 6/6 [00:02<00:00,  2.53it/s]100%|██████████| 6/6 [00:02<00:00,  2.04it/s]
W0416 08:35:47.907319 3336973 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_5_fc1 proxy err 4.7286299377446994e-05 tr(WHW.T) 11533.93359375
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:23,  1.04s/it]  8%|▊         | 2/24 [00:01<00:13,  1.60it/s] 12%|█▎        | 3/24 [00:01<00:10,  2.03it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.33it/s] 21%|██        | 5/24 [00:02<00:07,  2.53it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.67it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.77it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.84it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.85it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.88it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.87it/s] 50%|█████     | 12/24 [00:04<00:04,  2.88it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.92it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.94it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.95it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.96it/s] 71%|███████   | 17/24 [00:06<00:02,  2.96it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.96it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.97it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.96it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.96it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.97it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.98it/s]100%|██████████| 24/24 [00:08<00:00,  2.97it/s]100%|██████████| 24/24 [00:08<00:00,  2.73it/s]
W0416 08:35:58.090358 3336973 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_5_fc2 proxy err 6.0269612731644884e-05 tr(WHW.T) 12.426437377929688
I0416 08:35:59.487725 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 6 in 0.14s
I0416 08:35:59.598549 3179539 quantize_finetune_clip.py:151] text layer 7 gpu 0
I0416 08:36:01.866890 3339986 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:36:01.867027 3339986 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:36:01.867088 3339986 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:36:01.983985 3339986 config.py:58] PyTorch version 2.4.0 available.
W0416 08:36:04.212862 3339986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:07,  1.58s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.20it/s] 50%|█████     | 3/6 [00:02<00:01,  1.68it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.05it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.35it/s]100%|██████████| 6/6 [00:03<00:00,  2.58it/s]100%|██████████| 6/6 [00:03<00:00,  1.90it/s]
W0416 08:36:08.775774 3339986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_6_v proxy err 0.000276754581136629 tr(WHW.T) 246.55203247070312
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.77it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.41it/s] 50%|█████     | 3/6 [00:01<00:01,  2.72it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.86it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.97it/s]100%|██████████| 6/6 [00:02<00:00,  3.03it/s]100%|██████████| 6/6 [00:02<00:00,  2.80it/s]
W0416 08:36:12.302253 3339986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_6_q proxy err 0.00013823306653648615 tr(WHW.T) 831.7105102539062
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:03,  1.64it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.24it/s] 50%|█████     | 3/6 [00:01<00:01,  2.52it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.67it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.73it/s]100%|██████████| 6/6 [00:02<00:00,  2.83it/s]100%|██████████| 6/6 [00:02<00:00,  2.61it/s]
W0416 08:36:17.500133 3339986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_6_k proxy err 0.0001371794060105458 tr(WHW.T) 706.4376831054688
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.79it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.43it/s] 50%|█████     | 3/6 [00:01<00:01,  2.73it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.90it/s] 83%|████████▎ | 5/6 [00:01<00:00,  3.00it/s]100%|██████████| 6/6 [00:02<00:00,  3.07it/s]100%|██████████| 6/6 [00:02<00:00,  2.84it/s]
W0416 08:36:21.360168 3339986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_6_o proxy err 0.0001443226501578465 tr(WHW.T) 7.770773410797119
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.23s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.41it/s] 50%|█████     | 3/6 [00:01<00:01,  1.85it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.15it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.39it/s]100%|██████████| 6/6 [00:02<00:00,  2.52it/s]100%|██████████| 6/6 [00:02<00:00,  2.03it/s]
W0416 08:36:25.795237 3339986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_6_fc1 proxy err 4.94090054417029e-05 tr(WHW.T) 10551.373046875
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.06s/it]  8%|▊         | 2/24 [00:01<00:14,  1.56it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.98it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.27it/s] 21%|██        | 5/24 [00:02<00:07,  2.47it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.61it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.72it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.78it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.82it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.86it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.88it/s] 50%|█████     | 12/24 [00:04<00:04,  2.90it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.91it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.88it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.90it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.89it/s] 71%|███████   | 17/24 [00:06<00:02,  2.91it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.90it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.90it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.89it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.90it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.92it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.91it/s]100%|██████████| 24/24 [00:08<00:00,  2.91it/s]100%|██████████| 24/24 [00:08<00:00,  2.68it/s]
W0416 08:36:36.225023 3339986 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_6_fc2 proxy err 6.406890315702185e-05 tr(WHW.T) 15.198512077331543
I0416 08:36:37.673479 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 7 in 0.14s
I0416 08:36:37.779473 3179539 quantize_finetune_clip.py:151] text layer 8 gpu 0
I0416 08:36:40.057561 3343247 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:36:40.057700 3343247 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:36:40.057759 3343247 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:36:40.171740 3343247 config.py:58] PyTorch version 2.4.0 available.
W0416 08:36:42.423581 3343247 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.71s/it] 33%|███▎      | 2/6 [00:02<00:03,  1.13it/s] 50%|█████     | 3/6 [00:02<00:01,  1.60it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.01it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.32it/s]100%|██████████| 6/6 [00:03<00:00,  2.56it/s]100%|██████████| 6/6 [00:03<00:00,  1.84it/s]
W0416 08:36:47.120280 3343247 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_7_v proxy err 0.00025417140568606555 tr(WHW.T) 263.7965087890625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.80it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.42it/s] 50%|█████     | 3/6 [00:01<00:01,  2.72it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.88it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  3.01it/s]100%|██████████| 6/6 [00:02<00:00,  2.80it/s]
W0416 08:36:50.595981 3343247 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_7_q proxy err 0.00013165274867787957 tr(WHW.T) 725.1131591796875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.72it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.36it/s] 50%|█████     | 3/6 [00:01<00:01,  2.67it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.81it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.93it/s]100%|██████████| 6/6 [00:02<00:00,  3.02it/s]100%|██████████| 6/6 [00:02<00:00,  2.77it/s]
W0416 08:36:54.173505 3343247 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_7_k proxy err 0.00014565823948942125 tr(WHW.T) 651.355712890625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.69it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.33it/s] 50%|█████     | 3/6 [00:01<00:01,  2.64it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.84it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.96it/s]100%|██████████| 6/6 [00:02<00:00,  3.03it/s]100%|██████████| 6/6 [00:02<00:00,  2.77it/s]
W0416 08:36:57.702473 3343247 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_7_o proxy err 0.00012581485498230904 tr(WHW.T) 10.79519271850586
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:05,  1.19s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.45it/s] 50%|█████     | 3/6 [00:01<00:01,  1.88it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.18it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.40it/s]100%|██████████| 6/6 [00:02<00:00,  2.57it/s]100%|██████████| 6/6 [00:02<00:00,  2.07it/s]
W0416 08:37:02.025950 3343247 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_7_fc1 proxy err 4.959154102834873e-05 tr(WHW.T) 12009.625
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:23,  1.04s/it]  8%|▊         | 2/24 [00:01<00:13,  1.59it/s] 12%|█▎        | 3/24 [00:01<00:10,  2.02it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.32it/s] 21%|██        | 5/24 [00:02<00:07,  2.53it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.64it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.74it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.81it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.86it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.90it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.89it/s] 50%|█████     | 12/24 [00:04<00:04,  2.90it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.92it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.95it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.96it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.98it/s] 71%|███████   | 17/24 [00:06<00:02,  2.94it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.94it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.94it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.95it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.96it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.96it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.96it/s]100%|██████████| 24/24 [00:08<00:00,  2.96it/s]100%|██████████| 24/24 [00:08<00:00,  2.72it/s]
W0416 08:37:12.267360 3343247 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_7_fc2 proxy err 6.277403008425608e-05 tr(WHW.T) 11.257575988769531
I0416 08:37:13.673285 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 8 in 0.14s
I0416 08:37:13.785862 3179539 quantize_finetune_clip.py:151] text layer 9 gpu 0
I0416 08:37:16.066327 3346260 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:37:16.066460 3346260 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:37:16.066521 3346260 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:37:16.185324 3346260 config.py:58] PyTorch version 2.4.0 available.
W0416 08:37:18.416543 3346260 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:07,  1.58s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.20it/s] 50%|█████     | 3/6 [00:02<00:01,  1.68it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.07it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.36it/s]100%|██████████| 6/6 [00:03<00:00,  2.58it/s]100%|██████████| 6/6 [00:03<00:00,  1.91it/s]
W0416 08:37:22.976067 3346260 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_8_v proxy err 0.00021528868819586933 tr(WHW.T) 368.84979248046875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.74it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.32it/s] 50%|█████     | 3/6 [00:01<00:01,  2.65it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.83it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.95it/s]100%|██████████| 6/6 [00:02<00:00,  2.98it/s]100%|██████████| 6/6 [00:02<00:00,  2.76it/s]
W0416 08:37:26.483754 3346260 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_8_q proxy err 0.0001412014535162598 tr(WHW.T) 895.6217041015625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.70it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.34it/s] 50%|█████     | 3/6 [00:01<00:01,  2.66it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.83it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  3.02it/s]100%|██████████| 6/6 [00:02<00:00,  2.77it/s]
W0416 08:37:30.018155 3346260 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_8_k proxy err 0.00014553763321600854 tr(WHW.T) 747.58984375
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.68it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.31it/s] 50%|█████     | 3/6 [00:01<00:01,  2.60it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.78it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.87it/s]100%|██████████| 6/6 [00:02<00:00,  2.93it/s]100%|██████████| 6/6 [00:02<00:00,  2.70it/s]
W0416 08:37:33.574726 3346260 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_8_o proxy err 0.00010738228593254462 tr(WHW.T) 10.459186553955078
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:05,  1.19s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.44it/s] 50%|█████     | 3/6 [00:01<00:01,  1.85it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.17it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.37it/s]100%|██████████| 6/6 [00:02<00:00,  2.51it/s]100%|██████████| 6/6 [00:02<00:00,  2.04it/s]
W0416 08:37:37.913305 3346260 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_8_fc1 proxy err 4.948388959746808e-05 tr(WHW.T) 10972.4697265625
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.06s/it]  8%|▊         | 2/24 [00:01<00:14,  1.53it/s] 12%|█▎        | 3/24 [00:01<00:11,  1.90it/s] 17%|█▋        | 4/24 [00:02<00:09,  2.19it/s] 21%|██        | 5/24 [00:02<00:07,  2.40it/s] 25%|██▌       | 6/24 [00:02<00:07,  2.56it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.65it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.74it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.79it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.83it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.87it/s] 50%|█████     | 12/24 [00:04<00:04,  2.90it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.90it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.91it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.92it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.92it/s] 71%|███████   | 17/24 [00:06<00:02,  2.92it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.91it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.90it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.91it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.91it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.90it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.90it/s]100%|██████████| 24/24 [00:09<00:00,  2.89it/s]100%|██████████| 24/24 [00:09<00:00,  2.66it/s]
W0416 08:37:48.552600 3346260 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_8_fc2 proxy err 5.8419853303348646e-05 tr(WHW.T) 20.137924194335938
I0416 08:37:50.133591 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 9 in 0.15s
I0416 08:37:50.245371 3179539 quantize_finetune_clip.py:151] text layer 10 gpu 0
I0416 08:37:52.689300 3349347 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:37:52.689446 3349347 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:37:52.689507 3349347 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:37:52.806637 3349347 config.py:58] PyTorch version 2.4.0 available.
W0416 08:37:55.623282 3349347 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.64s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.15it/s] 50%|█████     | 3/6 [00:02<00:01,  1.62it/s] 67%|██████▋   | 4/6 [00:02<00:01,  1.95it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.21it/s]100%|██████████| 6/6 [00:03<00:00,  2.42it/s]100%|██████████| 6/6 [00:03<00:00,  1.81it/s]
W0416 08:38:01.144185 3349347 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_9_v proxy err 0.0002117473050020635 tr(WHW.T) 361.0838623046875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.73it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.31it/s] 50%|█████     | 3/6 [00:01<00:01,  2.60it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.77it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.88it/s]100%|██████████| 6/6 [00:02<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  2.72it/s]
W0416 08:38:05.078384 3349347 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_9_q proxy err 0.0001316761045018211 tr(WHW.T) 794.7393188476562
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:03,  1.62it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.24it/s] 50%|█████     | 3/6 [00:01<00:01,  2.57it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.73it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.85it/s]100%|██████████| 6/6 [00:02<00:00,  2.92it/s]100%|██████████| 6/6 [00:02<00:00,  2.67it/s]
W0416 08:38:08.860835 3349347 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_9_k proxy err 0.0001407662930432707 tr(WHW.T) 691.6158447265625
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.72it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.31it/s] 50%|█████     | 3/6 [00:01<00:01,  2.60it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.71it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.82it/s]100%|██████████| 6/6 [00:02<00:00,  2.90it/s]100%|██████████| 6/6 [00:02<00:00,  2.68it/s]
W0416 08:38:12.519284 3349347 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_9_o proxy err 0.000128106155898422 tr(WHW.T) 24.63672637939453
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.21s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.41it/s] 50%|█████     | 3/6 [00:01<00:01,  1.82it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.11it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.32it/s]100%|██████████| 6/6 [00:03<00:00,  2.45it/s]100%|██████████| 6/6 [00:03<00:00,  2.00it/s]
W0416 08:38:16.970534 3349347 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_9_fc1 proxy err 6.833625957369804e-05 tr(WHW.T) 10332.9814453125
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.06s/it]  8%|▊         | 2/24 [00:01<00:14,  1.56it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.97it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.24it/s] 21%|██        | 5/24 [00:02<00:07,  2.42it/s] 25%|██▌       | 6/24 [00:02<00:07,  2.55it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.63it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.68it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.72it/s] 42%|████▏     | 10/24 [00:04<00:05,  2.74it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.76it/s] 50%|█████     | 12/24 [00:04<00:04,  2.79it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.80it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.80it/s] 62%|██████▎   | 15/24 [00:06<00:03,  2.81it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.83it/s] 71%|███████   | 17/24 [00:06<00:02,  2.83it/s] 75%|███████▌  | 18/24 [00:07<00:02,  2.82it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.82it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.83it/s] 88%|████████▊ | 21/24 [00:08<00:01,  2.82it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.80it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.81it/s]100%|██████████| 24/24 [00:09<00:00,  2.81it/s]100%|██████████| 24/24 [00:09<00:00,  2.61it/s]
W0416 08:38:27.686310 3349347 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_9_fc2 proxy err 7.341348828049377e-05 tr(WHW.T) 20.32598304748535
I0416 08:38:29.197547 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 10 in 0.16s
I0416 08:38:29.315120 3179539 quantize_finetune_clip.py:151] text layer 11 gpu 0
I0416 08:38:31.602125 3352536 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:38:31.602260 3352536 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:38:31.602319 3352536 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:38:31.717358 3352536 config.py:58] PyTorch version 2.4.0 available.
W0416 08:38:34.048126 3352536 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:07,  1.60s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.18it/s] 50%|█████     | 3/6 [00:02<00:01,  1.65it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.03it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.33it/s]100%|██████████| 6/6 [00:03<00:00,  2.56it/s]100%|██████████| 6/6 [00:03<00:00,  1.88it/s]
W0416 08:38:38.619176 3352536 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_10_v proxy err 0.00026694912230595946 tr(WHW.T) 381.42498779296875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.38it/s] 50%|█████     | 3/6 [00:01<00:01,  2.64it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.82it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  2.75it/s]
W0416 08:38:42.990587 3352536 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_10_q proxy err 0.0001370901009067893 tr(WHW.T) 717.8143310546875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.74it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.34it/s] 50%|█████     | 3/6 [00:01<00:01,  2.65it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.81it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.86it/s]100%|██████████| 6/6 [00:02<00:00,  2.89it/s]100%|██████████| 6/6 [00:02<00:00,  2.71it/s]
W0416 08:38:47.799522 3352536 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_10_k proxy err 0.0001829858374549076 tr(WHW.T) 629.68408203125
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.34it/s] 50%|█████     | 3/6 [00:01<00:01,  2.64it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.82it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.89it/s]100%|██████████| 6/6 [00:02<00:00,  2.98it/s]100%|██████████| 6/6 [00:02<00:00,  2.75it/s]
W0416 08:38:51.607339 3352536 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_10_o proxy err 0.0001367034128634259 tr(WHW.T) 33.12681579589844
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.20s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.42it/s] 50%|█████     | 3/6 [00:01<00:01,  1.84it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.12it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.33it/s]100%|██████████| 6/6 [00:02<00:00,  2.50it/s]100%|██████████| 6/6 [00:02<00:00,  2.02it/s]
W0416 08:38:56.101049 3352536 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_10_fc1 proxy err 5.829226574860513e-05 tr(WHW.T) 9721.9580078125
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.05s/it]  8%|▊         | 2/24 [00:01<00:13,  1.58it/s] 12%|█▎        | 3/24 [00:01<00:10,  2.00it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.28it/s] 21%|██        | 5/24 [00:02<00:07,  2.48it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.62it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.70it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.77it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.81it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.83it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.85it/s] 50%|█████     | 12/24 [00:04<00:04,  2.86it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.88it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.89it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.90it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.90it/s] 71%|███████   | 17/24 [00:06<00:02,  2.89it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.89it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.91it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.92it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.82it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.79it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.77it/s]100%|██████████| 24/24 [00:09<00:00,  2.74it/s]100%|██████████| 24/24 [00:09<00:00,  2.65it/s]
W0416 08:39:06.969621 3352536 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_10_fc2 proxy err 4.748920036945492e-05 tr(WHW.T) 42.26137924194336
I0416 08:39:08.482998 3179539 quantize_finetune_clip.py:168] computed original embedding for text layer 11 in 0.14s
I0416 08:39:10.846022 3355817 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:39:10.846155 3355817 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:39:10.846214 3355817 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:39:10.961889 3355817 config.py:58] PyTorch version 2.4.0 available.
W0416 08:39:13.178092 3355817 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:08,  1.60s/it] 33%|███▎      | 2/6 [00:01<00:03,  1.19it/s] 50%|█████     | 3/6 [00:02<00:01,  1.67it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.04it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.34it/s]100%|██████████| 6/6 [00:03<00:00,  2.57it/s]100%|██████████| 6/6 [00:03<00:00,  1.89it/s]
W0416 08:39:17.728049 3355817 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_11_v proxy err 0.0003238186181988567 tr(WHW.T) 514.353271484375
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.78it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.37it/s] 50%|█████     | 3/6 [00:01<00:01,  2.67it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.85it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.96it/s]100%|██████████| 6/6 [00:02<00:00,  2.98it/s]100%|██████████| 6/6 [00:02<00:00,  2.77it/s]
W0416 08:39:21.408966 3355817 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_11_q proxy err 0.00016161285748239607 tr(WHW.T) 626.7928466796875
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:03,  1.65it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.29it/s] 50%|█████     | 3/6 [00:01<00:01,  2.61it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.80it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.86it/s]100%|██████████| 6/6 [00:02<00:00,  2.94it/s]100%|██████████| 6/6 [00:02<00:00,  2.71it/s]
W0416 08:39:26.454112 3355817 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_11_k proxy err 0.00013678094546776265 tr(WHW.T) 589.9808349609375
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.74it/s] 33%|███▎      | 2/6 [00:00<00:01,  2.37it/s] 50%|█████     | 3/6 [00:01<00:01,  2.67it/s] 67%|██████▋   | 4/6 [00:01<00:00,  2.82it/s] 83%|████████▎ | 5/6 [00:01<00:00,  2.93it/s]100%|██████████| 6/6 [00:02<00:00,  2.99it/s]100%|██████████| 6/6 [00:02<00:00,  2.76it/s]
W0416 08:39:30.547671 3355817 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_11_o proxy err 0.0001153233606601134 tr(WHW.T) 93.02412414550781
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:01<00:06,  1.20s/it] 33%|███▎      | 2/6 [00:01<00:02,  1.42it/s] 50%|█████     | 3/6 [00:01<00:01,  1.83it/s] 67%|██████▋   | 4/6 [00:02<00:00,  2.13it/s] 83%|████████▎ | 5/6 [00:02<00:00,  2.34it/s]100%|██████████| 6/6 [00:02<00:00,  2.49it/s]100%|██████████| 6/6 [00:02<00:00,  2.02it/s]
W0416 08:39:35.125802 3355817 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  H_data = torch.load(in_hess_path, map_location=torch.device('cpu'))

text_11_fc1 proxy err 5.797899211756885e-05 tr(WHW.T) 6430.23974609375
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:01<00:24,  1.07s/it]  8%|▊         | 2/24 [00:01<00:14,  1.56it/s] 12%|█▎        | 3/24 [00:01<00:10,  1.97it/s] 17%|█▋        | 4/24 [00:02<00:08,  2.26it/s] 21%|██        | 5/24 [00:02<00:07,  2.47it/s] 25%|██▌       | 6/24 [00:02<00:06,  2.61it/s] 29%|██▉       | 7/24 [00:03<00:06,  2.70it/s] 33%|███▎      | 8/24 [00:03<00:05,  2.75it/s] 38%|███▊      | 9/24 [00:03<00:05,  2.79it/s] 42%|████▏     | 10/24 [00:04<00:04,  2.81it/s] 46%|████▌     | 11/24 [00:04<00:04,  2.85it/s] 50%|█████     | 12/24 [00:04<00:04,  2.86it/s] 54%|█████▍    | 13/24 [00:05<00:03,  2.87it/s] 58%|█████▊    | 14/24 [00:05<00:03,  2.88it/s] 62%|██████▎   | 15/24 [00:05<00:03,  2.90it/s] 67%|██████▋   | 16/24 [00:06<00:02,  2.92it/s] 71%|███████   | 17/24 [00:06<00:02,  2.94it/s] 75%|███████▌  | 18/24 [00:06<00:02,  2.91it/s] 79%|███████▉  | 19/24 [00:07<00:01,  2.91it/s] 83%|████████▎ | 20/24 [00:07<00:01,  2.90it/s] 88%|████████▊ | 21/24 [00:07<00:01,  2.89it/s] 92%|█████████▏| 22/24 [00:08<00:00,  2.90it/s] 96%|█████████▌| 23/24 [00:08<00:00,  2.90it/s]100%|██████████| 24/24 [00:08<00:00,  2.91it/s]100%|██████████| 24/24 [00:08<00:00,  2.67it/s]
W0416 08:39:45.620290 3355817 warnings.py:110] /workspace/Weight_compression/qtip/lib/algo/finetune_clip.py:264: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  data = torch.load(save_path)

text_11_fc2 proxy err 2.8772907171514817e-05 tr(WHW.T) 66.53541564941406
I0416 08:39:51.985764 3359101 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0416 08:39:51.985876 3359101 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0416 08:39:51.985920 3359101 utils.py:162] NumExpr defaulting to 16 threads.
I0416 08:39:52.102264 3359101 config.py:58] PyTorch version 2.4.0 available.
W0416 08:39:53.524366 3359101 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0416 08:39:53.524965 3359101 hfize_clip.py:43] CLIPConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/openai--clip-vit-large-patch14",
  "architectures": [
    "CLIPModel"
  ],
  "initializer_factor": 1.0,
  "logit_scale_init_value": 2.6592,
  "model_type": "clip",
  "projection_dim": 768,
  "quip_params": {
    "K": 5,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "text_config": {
    "dropout": 0.0,
    "hidden_size": 768,
    "intermediate_size": 3072,
    "model_type": "clip_text_model",
    "num_attention_heads": 12,
    "projection_dim": 768,
    "torch_dtype": "float32"
  },
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "vision_config": {
    "dropout": 0.0,
    "hidden_size": 1024,
    "intermediate_size": 4096,
    "model_type": "clip_vision_model",
    "num_attention_heads": 16,
    "num_hidden_layers": 24,
    "patch_size": 14,
    "projection_dim": 768,
    "torch_dtype": "float32"
  }
}

Some weights of the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 were not used when initializing CLIPModel: ['text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing CLIPModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CLIPModel were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 and are newly initialized: ['text_model.encoder.layers.0.mlp.fc1.SU', 'text_model.encoder.layers.0.mlp.fc1.SV', 'text_model.encoder.layers.0.mlp.fc1.rcp', 'text_model.encoder.layers.0.mlp.fc1.tlut', 'text_model.encoder.layers.0.mlp.fc1.tp_rank', 'text_model.encoder.layers.0.mlp.fc1.trellis', 'text_model.encoder.layers.0.mlp.fc2.SU', 'text_model.encoder.layers.0.mlp.fc2.SV', 'text_model.encoder.layers.0.mlp.fc2.rcp', 'text_model.encoder.layers.0.mlp.fc2.tlut', 'text_model.encoder.layers.0.mlp.fc2.tp_rank', 'text_model.encoder.layers.0.mlp.fc2.trellis', 'text_model.encoder.layers.0.self_attn.k_proj.SU', 'text_model.encoder.layers.0.self_attn.k_proj.SV', 'text_model.encoder.layers.0.self_attn.k_proj.rcp', 'text_model.encoder.layers.0.self_attn.k_proj.tlut', 'text_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.k_proj.trellis', 'text_model.encoder.layers.0.self_attn.out_proj.SU', 'text_model.encoder.layers.0.self_attn.out_proj.SV', 'text_model.encoder.layers.0.self_attn.out_proj.rcp', 'text_model.encoder.layers.0.self_attn.out_proj.tlut', 'text_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.out_proj.trellis', 'text_model.encoder.layers.0.self_attn.q_proj.SU', 'text_model.encoder.layers.0.self_attn.q_proj.SV', 'text_model.encoder.layers.0.self_attn.q_proj.rcp', 'text_model.encoder.layers.0.self_attn.q_proj.tlut', 'text_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.q_proj.trellis', 'text_model.encoder.layers.0.self_attn.v_proj.SU', 'text_model.encoder.layers.0.self_attn.v_proj.SV', 'text_model.encoder.layers.0.self_attn.v_proj.rcp', 'text_model.encoder.layers.0.self_attn.v_proj.tlut', 'text_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.v_proj.trellis', 'text_model.encoder.layers.1.mlp.fc1.SU', 'text_model.encoder.layers.1.mlp.fc1.SV', 'text_model.encoder.layers.1.mlp.fc1.rcp', 'text_model.encoder.layers.1.mlp.fc1.tlut', 'text_model.encoder.layers.1.mlp.fc1.tp_rank', 'text_model.encoder.layers.1.mlp.fc1.trellis', 'text_model.encoder.layers.1.mlp.fc2.SU', 'text_model.encoder.layers.1.mlp.fc2.SV', 'text_model.encoder.layers.1.mlp.fc2.rcp', 'text_model.encoder.layers.1.mlp.fc2.tlut', 'text_model.encoder.layers.1.mlp.fc2.tp_rank', 'text_model.encoder.layers.1.mlp.fc2.trellis', 'text_model.encoder.layers.1.self_attn.k_proj.SU', 'text_model.encoder.layers.1.self_attn.k_proj.SV', 'text_model.encoder.layers.1.self_attn.k_proj.rcp', 'text_model.encoder.layers.1.self_attn.k_proj.tlut', 'text_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.k_proj.trellis', 'text_model.encoder.layers.1.self_attn.out_proj.SU', 'text_model.encoder.layers.1.self_attn.out_proj.SV', 'text_model.encoder.layers.1.self_attn.out_proj.rcp', 'text_model.encoder.layers.1.self_attn.out_proj.tlut', 'text_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.out_proj.trellis', 'text_model.encoder.layers.1.self_attn.q_proj.SU', 'text_model.encoder.layers.1.self_attn.q_proj.SV', 'text_model.encoder.layers.1.self_attn.q_proj.rcp', 'text_model.encoder.layers.1.self_attn.q_proj.tlut', 'text_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.q_proj.trellis', 'text_model.encoder.layers.1.self_attn.v_proj.SU', 'text_model.encoder.layers.1.self_attn.v_proj.SV', 'text_model.encoder.layers.1.self_attn.v_proj.rcp', 'text_model.encoder.layers.1.self_attn.v_proj.tlut', 'text_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.v_proj.trellis', 'text_model.encoder.layers.10.mlp.fc1.SU', 'text_model.encoder.layers.10.mlp.fc1.SV', 'text_model.encoder.layers.10.mlp.fc1.rcp', 'text_model.encoder.layers.10.mlp.fc1.tlut', 'text_model.encoder.layers.10.mlp.fc1.tp_rank', 'text_model.encoder.layers.10.mlp.fc1.trellis', 'text_model.encoder.layers.10.mlp.fc2.SU', 'text_model.encoder.layers.10.mlp.fc2.SV', 'text_model.encoder.layers.10.mlp.fc2.rcp', 'text_model.encoder.layers.10.mlp.fc2.tlut', 'text_model.encoder.layers.10.mlp.fc2.tp_rank', 'text_model.encoder.layers.10.mlp.fc2.trellis', 'text_model.encoder.layers.10.self_attn.k_proj.SU', 'text_model.encoder.layers.10.self_attn.k_proj.SV', 'text_model.encoder.layers.10.self_attn.k_proj.rcp', 'text_model.encoder.layers.10.self_attn.k_proj.tlut', 'text_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.k_proj.trellis', 'text_model.encoder.layers.10.self_attn.out_proj.SU', 'text_model.encoder.layers.10.self_attn.out_proj.SV', 'text_model.encoder.layers.10.self_attn.out_proj.rcp', 'text_model.encoder.layers.10.self_attn.out_proj.tlut', 'text_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.out_proj.trellis', 'text_model.encoder.layers.10.self_attn.q_proj.SU', 'text_model.encoder.layers.10.self_attn.q_proj.SV', 'text_model.encoder.layers.10.self_attn.q_proj.rcp', 'text_model.encoder.layers.10.self_attn.q_proj.tlut', 'text_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.q_proj.trellis', 'text_model.encoder.layers.10.self_attn.v_proj.SU', 'text_model.encoder.layers.10.self_attn.v_proj.SV', 'text_model.encoder.layers.10.self_attn.v_proj.rcp', 'text_model.encoder.layers.10.self_attn.v_proj.tlut', 'text_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.v_proj.trellis', 'text_model.encoder.layers.11.mlp.fc1.SU', 'text_model.encoder.layers.11.mlp.fc1.SV', 'text_model.encoder.layers.11.mlp.fc1.rcp', 'text_model.encoder.layers.11.mlp.fc1.tlut', 'text_model.encoder.layers.11.mlp.fc1.tp_rank', 'text_model.encoder.layers.11.mlp.fc1.trellis', 'text_model.encoder.layers.11.mlp.fc2.SU', 'text_model.encoder.layers.11.mlp.fc2.SV', 'text_model.encoder.layers.11.mlp.fc2.rcp', 'text_model.encoder.layers.11.mlp.fc2.tlut', 'text_model.encoder.layers.11.mlp.fc2.tp_rank', 'text_model.encoder.layers.11.mlp.fc2.trellis', 'text_model.encoder.layers.11.self_attn.k_proj.SU', 'text_model.encoder.layers.11.self_attn.k_proj.SV', 'text_model.encoder.layers.11.self_attn.k_proj.rcp', 'text_model.encoder.layers.11.self_attn.k_proj.tlut', 'text_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.k_proj.trellis', 'text_model.encoder.layers.11.self_attn.out_proj.SU', 'text_model.encoder.layers.11.self_attn.out_proj.SV', 'text_model.encoder.layers.11.self_attn.out_proj.rcp', 'text_model.encoder.layers.11.self_attn.out_proj.tlut', 'text_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.out_proj.trellis', 'text_model.encoder.layers.11.self_attn.q_proj.SU', 'text_model.encoder.layers.11.self_attn.q_proj.SV', 'text_model.encoder.layers.11.self_attn.q_proj.rcp', 'text_model.encoder.layers.11.self_attn.q_proj.tlut', 'text_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.q_proj.trellis', 'text_model.encoder.layers.11.self_attn.v_proj.SU', 'text_model.encoder.layers.11.self_attn.v_proj.SV', 'text_model.encoder.layers.11.self_attn.v_proj.rcp', 'text_model.encoder.layers.11.self_attn.v_proj.tlut', 'text_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.v_proj.trellis', 'text_model.encoder.layers.2.mlp.fc1.SU', 'text_model.encoder.layers.2.mlp.fc1.SV', 'text_model.encoder.layers.2.mlp.fc1.rcp', 'text_model.encoder.layers.2.mlp.fc1.tlut', 'text_model.encoder.layers.2.mlp.fc1.tp_rank', 'text_model.encoder.layers.2.mlp.fc1.trellis', 'text_model.encoder.layers.2.mlp.fc2.SU', 'text_model.encoder.layers.2.mlp.fc2.SV', 'text_model.encoder.layers.2.mlp.fc2.rcp', 'text_model.encoder.layers.2.mlp.fc2.tlut', 'text_model.encoder.layers.2.mlp.fc2.tp_rank', 'text_model.encoder.layers.2.mlp.fc2.trellis', 'text_model.encoder.layers.2.self_attn.k_proj.SU', 'text_model.encoder.layers.2.self_attn.k_proj.SV', 'text_model.encoder.layers.2.self_attn.k_proj.rcp', 'text_model.encoder.layers.2.self_attn.k_proj.tlut', 'text_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.k_proj.trellis', 'text_model.encoder.layers.2.self_attn.out_proj.SU', 'text_model.encoder.layers.2.self_attn.out_proj.SV', 'text_model.encoder.layers.2.self_attn.out_proj.rcp', 'text_model.encoder.layers.2.self_attn.out_proj.tlut', 'text_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.out_proj.trellis', 'text_model.encoder.layers.2.self_attn.q_proj.SU', 'text_model.encoder.layers.2.self_attn.q_proj.SV', 'text_model.encoder.layers.2.self_attn.q_proj.rcp', 'text_model.encoder.layers.2.self_attn.q_proj.tlut', 'text_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.q_proj.trellis', 'text_model.encoder.layers.2.self_attn.v_proj.SU', 'text_model.encoder.layers.2.self_attn.v_proj.SV', 'text_model.encoder.layers.2.self_attn.v_proj.rcp', 'text_model.encoder.layers.2.self_attn.v_proj.tlut', 'text_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.v_proj.trellis', 'text_model.encoder.layers.3.mlp.fc1.SU', 'text_model.encoder.layers.3.mlp.fc1.SV', 'text_model.encoder.layers.3.mlp.fc1.rcp', 'text_model.encoder.layers.3.mlp.fc1.tlut', 'text_model.encoder.layers.3.mlp.fc1.tp_rank', 'text_model.encoder.layers.3.mlp.fc1.trellis', 'text_model.encoder.layers.3.mlp.fc2.SU', 'text_model.encoder.layers.3.mlp.fc2.SV', 'text_model.encoder.layers.3.mlp.fc2.rcp', 'text_model.encoder.layers.3.mlp.fc2.tlut', 'text_model.encoder.layers.3.mlp.fc2.tp_rank', 'text_model.encoder.layers.3.mlp.fc2.trellis', 'text_model.encoder.layers.3.self_attn.k_proj.SU', 'text_model.encoder.layers.3.self_attn.k_proj.SV', 'text_model.encoder.layers.3.self_attn.k_proj.rcp', 'text_model.encoder.layers.3.self_attn.k_proj.tlut', 'text_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.k_proj.trellis', 'text_model.encoder.layers.3.self_attn.out_proj.SU', 'text_model.encoder.layers.3.self_attn.out_proj.SV', 'text_model.encoder.layers.3.self_attn.out_proj.rcp', 'text_model.encoder.layers.3.self_attn.out_proj.tlut', 'text_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.out_proj.trellis', 'text_model.encoder.layers.3.self_attn.q_proj.SU', 'text_model.encoder.layers.3.self_attn.q_proj.SV', 'text_model.encoder.layers.3.self_attn.q_proj.rcp', 'text_model.encoder.layers.3.self_attn.q_proj.tlut', 'text_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.q_proj.trellis', 'text_model.encoder.layers.3.self_attn.v_proj.SU', 'text_model.encoder.layers.3.self_attn.v_proj.SV', 'text_model.encoder.layers.3.self_attn.v_proj.rcp', 'text_model.encoder.layers.3.self_attn.v_proj.tlut', 'text_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.v_proj.trellis', 'text_model.encoder.layers.4.mlp.fc1.SU', 'text_model.encoder.layers.4.mlp.fc1.SV', 'text_model.encoder.layers.4.mlp.fc1.rcp', 'text_model.encoder.layers.4.mlp.fc1.tlut', 'text_model.encoder.layers.4.mlp.fc1.tp_rank', 'text_model.encoder.layers.4.mlp.fc1.trellis', 'text_model.encoder.layers.4.mlp.fc2.SU', 'text_model.encoder.layers.4.mlp.fc2.SV', 'text_model.encoder.layers.4.mlp.fc2.rcp', 'text_model.encoder.layers.4.mlp.fc2.tlut', 'text_model.encoder.layers.4.mlp.fc2.tp_rank', 'text_model.encoder.layers.4.mlp.fc2.trellis', 'text_model.encoder.layers.4.self_attn.k_proj.SU', 'text_model.encoder.layers.4.self_attn.k_proj.SV', 'text_model.encoder.layers.4.self_attn.k_proj.rcp', 'text_model.encoder.layers.4.self_attn.k_proj.tlut', 'text_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.k_proj.trellis', 'text_model.encoder.layers.4.self_attn.out_proj.SU', 'text_model.encoder.layers.4.self_attn.out_proj.SV', 'text_model.encoder.layers.4.self_attn.out_proj.rcp', 'text_model.encoder.layers.4.self_attn.out_proj.tlut', 'text_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.out_proj.trellis', 'text_model.encoder.layers.4.self_attn.q_proj.SU', 'text_model.encoder.layers.4.self_attn.q_proj.SV', 'text_model.encoder.layers.4.self_attn.q_proj.rcp', 'text_model.encoder.layers.4.self_attn.q_proj.tlut', 'text_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.q_proj.trellis', 'text_model.encoder.layers.4.self_attn.v_proj.SU', 'text_model.encoder.layers.4.self_attn.v_proj.SV', 'text_model.encoder.layers.4.self_attn.v_proj.rcp', 'text_model.encoder.layers.4.self_attn.v_proj.tlut', 'text_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.v_proj.trellis', 'text_model.encoder.layers.5.mlp.fc1.SU', 'text_model.encoder.layers.5.mlp.fc1.SV', 'text_model.encoder.layers.5.mlp.fc1.rcp', 'text_model.encoder.layers.5.mlp.fc1.tlut', 'text_model.encoder.layers.5.mlp.fc1.tp_rank', 'text_model.encoder.layers.5.mlp.fc1.trellis', 'text_model.encoder.layers.5.mlp.fc2.SU', 'text_model.encoder.layers.5.mlp.fc2.SV', 'text_model.encoder.layers.5.mlp.fc2.rcp', 'text_model.encoder.layers.5.mlp.fc2.tlut', 'text_model.encoder.layers.5.mlp.fc2.tp_rank', 'text_model.encoder.layers.5.mlp.fc2.trellis', 'text_model.encoder.layers.5.self_attn.k_proj.SU', 'text_model.encoder.layers.5.self_attn.k_proj.SV', 'text_model.encoder.layers.5.self_attn.k_proj.rcp', 'text_model.encoder.layers.5.self_attn.k_proj.tlut', 'text_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.k_proj.trellis', 'text_model.encoder.layers.5.self_attn.out_proj.SU', 'text_model.encoder.layers.5.self_attn.out_proj.SV', 'text_model.encoder.layers.5.self_attn.out_proj.rcp', 'text_model.encoder.layers.5.self_attn.out_proj.tlut', 'text_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.out_proj.trellis', 'text_model.encoder.layers.5.self_attn.q_proj.SU', 'text_model.encoder.layers.5.self_attn.q_proj.SV', 'text_model.encoder.layers.5.self_attn.q_proj.rcp', 'text_model.encoder.layers.5.self_attn.q_proj.tlut', 'text_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.q_proj.trellis', 'text_model.encoder.layers.5.self_attn.v_proj.SU', 'text_model.encoder.layers.5.self_attn.v_proj.SV', 'text_model.encoder.layers.5.self_attn.v_proj.rcp', 'text_model.encoder.layers.5.self_attn.v_proj.tlut', 'text_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.v_proj.trellis', 'text_model.encoder.layers.6.mlp.fc1.SU', 'text_model.encoder.layers.6.mlp.fc1.SV', 'text_model.encoder.layers.6.mlp.fc1.rcp', 'text_model.encoder.layers.6.mlp.fc1.tlut', 'text_model.encoder.layers.6.mlp.fc1.tp_rank', 'text_model.encoder.layers.6.mlp.fc1.trellis', 'text_model.encoder.layers.6.mlp.fc2.SU', 'text_model.encoder.layers.6.mlp.fc2.SV', 'text_model.encoder.layers.6.mlp.fc2.rcp', 'text_model.encoder.layers.6.mlp.fc2.tlut', 'text_model.encoder.layers.6.mlp.fc2.tp_rank', 'text_model.encoder.layers.6.mlp.fc2.trellis', 'text_model.encoder.layers.6.self_attn.k_proj.SU', 'text_model.encoder.layers.6.self_attn.k_proj.SV', 'text_model.encoder.layers.6.self_attn.k_proj.rcp', 'text_model.encoder.layers.6.self_attn.k_proj.tlut', 'text_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.k_proj.trellis', 'text_model.encoder.layers.6.self_attn.out_proj.SU', 'text_model.encoder.layers.6.self_attn.out_proj.SV', 'text_model.encoder.layers.6.self_attn.out_proj.rcp', 'text_model.encoder.layers.6.self_attn.out_proj.tlut', 'text_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.out_proj.trellis', 'text_model.encoder.layers.6.self_attn.q_proj.SU', 'text_model.encoder.layers.6.self_attn.q_proj.SV', 'text_model.encoder.layers.6.self_attn.q_proj.rcp', 'text_model.encoder.layers.6.self_attn.q_proj.tlut', 'text_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.q_proj.trellis', 'text_model.encoder.layers.6.self_attn.v_proj.SU', 'text_model.encoder.layers.6.self_attn.v_proj.SV', 'text_model.encoder.layers.6.self_attn.v_proj.rcp', 'text_model.encoder.layers.6.self_attn.v_proj.tlut', 'text_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.v_proj.trellis', 'text_model.encoder.layers.7.mlp.fc1.SU', 'text_model.encoder.layers.7.mlp.fc1.SV', 'text_model.encoder.layers.7.mlp.fc1.rcp', 'text_model.encoder.layers.7.mlp.fc1.tlut', 'text_model.encoder.layers.7.mlp.fc1.tp_rank', 'text_model.encoder.layers.7.mlp.fc1.trellis', 'text_model.encoder.layers.7.mlp.fc2.SU', 'text_model.encoder.layers.7.mlp.fc2.SV', 'text_model.encoder.layers.7.mlp.fc2.rcp', 'text_model.encoder.layers.7.mlp.fc2.tlut', 'text_model.encoder.layers.7.mlp.fc2.tp_rank', 'text_model.encoder.layers.7.mlp.fc2.trellis', 'text_model.encoder.layers.7.self_attn.k_proj.SU', 'text_model.encoder.layers.7.self_attn.k_proj.SV', 'text_model.encoder.layers.7.self_attn.k_proj.rcp', 'text_model.encoder.layers.7.self_attn.k_proj.tlut', 'text_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.k_proj.trellis', 'text_model.encoder.layers.7.self_attn.out_proj.SU', 'text_model.encoder.layers.7.self_attn.out_proj.SV', 'text_model.encoder.layers.7.self_attn.out_proj.rcp', 'text_model.encoder.layers.7.self_attn.out_proj.tlut', 'text_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.out_proj.trellis', 'text_model.encoder.layers.7.self_attn.q_proj.SU', 'text_model.encoder.layers.7.self_attn.q_proj.SV', 'text_model.encoder.layers.7.self_attn.q_proj.rcp', 'text_model.encoder.layers.7.self_attn.q_proj.tlut', 'text_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.q_proj.trellis', 'text_model.encoder.layers.7.self_attn.v_proj.SU', 'text_model.encoder.layers.7.self_attn.v_proj.SV', 'text_model.encoder.layers.7.self_attn.v_proj.rcp', 'text_model.encoder.layers.7.self_attn.v_proj.tlut', 'text_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.v_proj.trellis', 'text_model.encoder.layers.8.mlp.fc1.SU', 'text_model.encoder.layers.8.mlp.fc1.SV', 'text_model.encoder.layers.8.mlp.fc1.rcp', 'text_model.encoder.layers.8.mlp.fc1.tlut', 'text_model.encoder.layers.8.mlp.fc1.tp_rank', 'text_model.encoder.layers.8.mlp.fc1.trellis', 'text_model.encoder.layers.8.mlp.fc2.SU', 'text_model.encoder.layers.8.mlp.fc2.SV', 'text_model.encoder.layers.8.mlp.fc2.rcp', 'text_model.encoder.layers.8.mlp.fc2.tlut', 'text_model.encoder.layers.8.mlp.fc2.tp_rank', 'text_model.encoder.layers.8.mlp.fc2.trellis', 'text_model.encoder.layers.8.self_attn.k_proj.SU', 'text_model.encoder.layers.8.self_attn.k_proj.SV', 'text_model.encoder.layers.8.self_attn.k_proj.rcp', 'text_model.encoder.layers.8.self_attn.k_proj.tlut', 'text_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.k_proj.trellis', 'text_model.encoder.layers.8.self_attn.out_proj.SU', 'text_model.encoder.layers.8.self_attn.out_proj.SV', 'text_model.encoder.layers.8.self_attn.out_proj.rcp', 'text_model.encoder.layers.8.self_attn.out_proj.tlut', 'text_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.out_proj.trellis', 'text_model.encoder.layers.8.self_attn.q_proj.SU', 'text_model.encoder.layers.8.self_attn.q_proj.SV', 'text_model.encoder.layers.8.self_attn.q_proj.rcp', 'text_model.encoder.layers.8.self_attn.q_proj.tlut', 'text_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.q_proj.trellis', 'text_model.encoder.layers.8.self_attn.v_proj.SU', 'text_model.encoder.layers.8.self_attn.v_proj.SV', 'text_model.encoder.layers.8.self_attn.v_proj.rcp', 'text_model.encoder.layers.8.self_attn.v_proj.tlut', 'text_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.v_proj.trellis', 'text_model.encoder.layers.9.mlp.fc1.SU', 'text_model.encoder.layers.9.mlp.fc1.SV', 'text_model.encoder.layers.9.mlp.fc1.rcp', 'text_model.encoder.layers.9.mlp.fc1.tlut', 'text_model.encoder.layers.9.mlp.fc1.tp_rank', 'text_model.encoder.layers.9.mlp.fc1.trellis', 'text_model.encoder.layers.9.mlp.fc2.SU', 'text_model.encoder.layers.9.mlp.fc2.SV', 'text_model.encoder.layers.9.mlp.fc2.rcp', 'text_model.encoder.layers.9.mlp.fc2.tlut', 'text_model.encoder.layers.9.mlp.fc2.tp_rank', 'text_model.encoder.layers.9.mlp.fc2.trellis', 'text_model.encoder.layers.9.self_attn.k_proj.SU', 'text_model.encoder.layers.9.self_attn.k_proj.SV', 'text_model.encoder.layers.9.self_attn.k_proj.rcp', 'text_model.encoder.layers.9.self_attn.k_proj.tlut', 'text_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.k_proj.trellis', 'text_model.encoder.layers.9.self_attn.out_proj.SU', 'text_model.encoder.layers.9.self_attn.out_proj.SV', 'text_model.encoder.layers.9.self_attn.out_proj.rcp', 'text_model.encoder.layers.9.self_attn.out_proj.tlut', 'text_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.out_proj.trellis', 'text_model.encoder.layers.9.self_attn.q_proj.SU', 'text_model.encoder.layers.9.self_attn.q_proj.SV', 'text_model.encoder.layers.9.self_attn.q_proj.rcp', 'text_model.encoder.layers.9.self_attn.q_proj.tlut', 'text_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.q_proj.trellis', 'text_model.encoder.layers.9.self_attn.v_proj.SU', 'text_model.encoder.layers.9.self_attn.v_proj.SV', 'text_model.encoder.layers.9.self_attn.v_proj.rcp', 'text_model.encoder.layers.9.self_attn.v_proj.tlut', 'text_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.v_proj.trellis', 'vision_model.encoder.layers.0.mlp.fc1.SU', 'vision_model.encoder.layers.0.mlp.fc1.SV', 'vision_model.encoder.layers.0.mlp.fc1.rcp', 'vision_model.encoder.layers.0.mlp.fc1.tlut', 'vision_model.encoder.layers.0.mlp.fc1.tp_rank', 'vision_model.encoder.layers.0.mlp.fc1.trellis', 'vision_model.encoder.layers.0.mlp.fc2.SU', 'vision_model.encoder.layers.0.mlp.fc2.SV', 'vision_model.encoder.layers.0.mlp.fc2.rcp', 'vision_model.encoder.layers.0.mlp.fc2.tlut', 'vision_model.encoder.layers.0.mlp.fc2.tp_rank', 'vision_model.encoder.layers.0.mlp.fc2.trellis', 'vision_model.encoder.layers.0.self_attn.k_proj.SU', 'vision_model.encoder.layers.0.self_attn.k_proj.SV', 'vision_model.encoder.layers.0.self_attn.k_proj.rcp', 'vision_model.encoder.layers.0.self_attn.k_proj.tlut', 'vision_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.k_proj.trellis', 'vision_model.encoder.layers.0.self_attn.out_proj.SU', 'vision_model.encoder.layers.0.self_attn.out_proj.SV', 'vision_model.encoder.layers.0.self_attn.out_proj.rcp', 'vision_model.encoder.layers.0.self_attn.out_proj.tlut', 'vision_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.out_proj.trellis', 'vision_model.encoder.layers.0.self_attn.q_proj.SU', 'vision_model.encoder.layers.0.self_attn.q_proj.SV', 'vision_model.encoder.layers.0.self_attn.q_proj.rcp', 'vision_model.encoder.layers.0.self_attn.q_proj.tlut', 'vision_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.q_proj.trellis', 'vision_model.encoder.layers.0.self_attn.v_proj.SU', 'vision_model.encoder.layers.0.self_attn.v_proj.SV', 'vision_model.encoder.layers.0.self_attn.v_proj.rcp', 'vision_model.encoder.layers.0.self_attn.v_proj.tlut', 'vision_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.v_proj.trellis', 'vision_model.encoder.layers.1.mlp.fc1.SU', 'vision_model.encoder.layers.1.mlp.fc1.SV', 'vision_model.encoder.layers.1.mlp.fc1.rcp', 'vision_model.encoder.layers.1.mlp.fc1.tlut', 'vision_model.encoder.layers.1.mlp.fc1.tp_rank', 'vision_model.encoder.layers.1.mlp.fc1.trellis', 'vision_model.encoder.layers.1.mlp.fc2.SU', 'vision_model.encoder.layers.1.mlp.fc2.SV', 'vision_model.encoder.layers.1.mlp.fc2.rcp', 'vision_model.encoder.layers.1.mlp.fc2.tlut', 'vision_model.encoder.layers.1.mlp.fc2.tp_rank', 'vision_model.encoder.layers.1.mlp.fc2.trellis', 'vision_model.encoder.layers.1.self_attn.k_proj.SU', 'vision_model.encoder.layers.1.self_attn.k_proj.SV', 'vision_model.encoder.layers.1.self_attn.k_proj.rcp', 'vision_model.encoder.layers.1.self_attn.k_proj.tlut', 'vision_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.k_proj.trellis', 'vision_model.encoder.layers.1.self_attn.out_proj.SU', 'vision_model.encoder.layers.1.self_attn.out_proj.SV', 'vision_model.encoder.layers.1.self_attn.out_proj.rcp', 'vision_model.encoder.layers.1.self_attn.out_proj.tlut', 'vision_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.out_proj.trellis', 'vision_model.encoder.layers.1.self_attn.q_proj.SU', 'vision_model.encoder.layers.1.self_attn.q_proj.SV', 'vision_model.encoder.layers.1.self_attn.q_proj.rcp', 'vision_model.encoder.layers.1.self_attn.q_proj.tlut', 'vision_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.q_proj.trellis', 'vision_model.encoder.layers.1.self_attn.v_proj.SU', 'vision_model.encoder.layers.1.self_attn.v_proj.SV', 'vision_model.encoder.layers.1.self_attn.v_proj.rcp', 'vision_model.encoder.layers.1.self_attn.v_proj.tlut', 'vision_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.v_proj.trellis', 'vision_model.encoder.layers.10.mlp.fc1.SU', 'vision_model.encoder.layers.10.mlp.fc1.SV', 'vision_model.encoder.layers.10.mlp.fc1.rcp', 'vision_model.encoder.layers.10.mlp.fc1.tlut', 'vision_model.encoder.layers.10.mlp.fc1.tp_rank', 'vision_model.encoder.layers.10.mlp.fc1.trellis', 'vision_model.encoder.layers.10.mlp.fc2.SU', 'vision_model.encoder.layers.10.mlp.fc2.SV', 'vision_model.encoder.layers.10.mlp.fc2.rcp', 'vision_model.encoder.layers.10.mlp.fc2.tlut', 'vision_model.encoder.layers.10.mlp.fc2.tp_rank', 'vision_model.encoder.layers.10.mlp.fc2.trellis', 'vision_model.encoder.layers.10.self_attn.k_proj.SU', 'vision_model.encoder.layers.10.self_attn.k_proj.SV', 'vision_model.encoder.layers.10.self_attn.k_proj.rcp', 'vision_model.encoder.layers.10.self_attn.k_proj.tlut', 'vision_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.k_proj.trellis', 'vision_model.encoder.layers.10.self_attn.out_proj.SU', 'vision_model.encoder.layers.10.self_attn.out_proj.SV', 'vision_model.encoder.layers.10.self_attn.out_proj.rcp', 'vision_model.encoder.layers.10.self_attn.out_proj.tlut', 'vision_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.out_proj.trellis', 'vision_model.encoder.layers.10.self_attn.q_proj.SU', 'vision_model.encoder.layers.10.self_attn.q_proj.SV', 'vision_model.encoder.layers.10.self_attn.q_proj.rcp', 'vision_model.encoder.layers.10.self_attn.q_proj.tlut', 'vision_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.q_proj.trellis', 'vision_model.encoder.layers.10.self_attn.v_proj.SU', 'vision_model.encoder.layers.10.self_attn.v_proj.SV', 'vision_model.encoder.layers.10.self_attn.v_proj.rcp', 'vision_model.encoder.layers.10.self_attn.v_proj.tlut', 'vision_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.v_proj.trellis', 'vision_model.encoder.layers.11.mlp.fc1.SU', 'vision_model.encoder.layers.11.mlp.fc1.SV', 'vision_model.encoder.layers.11.mlp.fc1.rcp', 'vision_model.encoder.layers.11.mlp.fc1.tlut', 'vision_model.encoder.layers.11.mlp.fc1.tp_rank', 'vision_model.encoder.layers.11.mlp.fc1.trellis', 'vision_model.encoder.layers.11.mlp.fc2.SU', 'vision_model.encoder.layers.11.mlp.fc2.SV', 'vision_model.encoder.layers.11.mlp.fc2.rcp', 'vision_model.encoder.layers.11.mlp.fc2.tlut', 'vision_model.encoder.layers.11.mlp.fc2.tp_rank', 'vision_model.encoder.layers.11.mlp.fc2.trellis', 'vision_model.encoder.layers.11.self_attn.k_proj.SU', 'vision_model.encoder.layers.11.self_attn.k_proj.SV', 'vision_model.encoder.layers.11.self_attn.k_proj.rcp', 'vision_model.encoder.layers.11.self_attn.k_proj.tlut', 'vision_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.k_proj.trellis', 'vision_model.encoder.layers.11.self_attn.out_proj.SU', 'vision_model.encoder.layers.11.self_attn.out_proj.SV', 'vision_model.encoder.layers.11.self_attn.out_proj.rcp', 'vision_model.encoder.layers.11.self_attn.out_proj.tlut', 'vision_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.out_proj.trellis', 'vision_model.encoder.layers.11.self_attn.q_proj.SU', 'vision_model.encoder.layers.11.self_attn.q_proj.SV', 'vision_model.encoder.layers.11.self_attn.q_proj.rcp', 'vision_model.encoder.layers.11.self_attn.q_proj.tlut', 'vision_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.q_proj.trellis', 'vision_model.encoder.layers.11.self_attn.v_proj.SU', 'vision_model.encoder.layers.11.self_attn.v_proj.SV', 'vision_model.encoder.layers.11.self_attn.v_proj.rcp', 'vision_model.encoder.layers.11.self_attn.v_proj.tlut', 'vision_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.v_proj.trellis', 'vision_model.encoder.layers.12.mlp.fc1.SU', 'vision_model.encoder.layers.12.mlp.fc1.SV', 'vision_model.encoder.layers.12.mlp.fc1.rcp', 'vision_model.encoder.layers.12.mlp.fc1.tlut', 'vision_model.encoder.layers.12.mlp.fc1.tp_rank', 'vision_model.encoder.layers.12.mlp.fc1.trellis', 'vision_model.encoder.layers.12.mlp.fc2.SU', 'vision_model.encoder.layers.12.mlp.fc2.SV', 'vision_model.encoder.layers.12.mlp.fc2.rcp', 'vision_model.encoder.layers.12.mlp.fc2.tlut', 'vision_model.encoder.layers.12.mlp.fc2.tp_rank', 'vision_model.encoder.layers.12.mlp.fc2.trellis', 'vision_model.encoder.layers.12.self_attn.k_proj.SU', 'vision_model.encoder.layers.12.self_attn.k_proj.SV', 'vision_model.encoder.layers.12.self_attn.k_proj.rcp', 'vision_model.encoder.layers.12.self_attn.k_proj.tlut', 'vision_model.encoder.layers.12.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.k_proj.trellis', 'vision_model.encoder.layers.12.self_attn.out_proj.SU', 'vision_model.encoder.layers.12.self_attn.out_proj.SV', 'vision_model.encoder.layers.12.self_attn.out_proj.rcp', 'vision_model.encoder.layers.12.self_attn.out_proj.tlut', 'vision_model.encoder.layers.12.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.out_proj.trellis', 'vision_model.encoder.layers.12.self_attn.q_proj.SU', 'vision_model.encoder.layers.12.self_attn.q_proj.SV', 'vision_model.encoder.layers.12.self_attn.q_proj.rcp', 'vision_model.encoder.layers.12.self_attn.q_proj.tlut', 'vision_model.encoder.layers.12.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.q_proj.trellis', 'vision_model.encoder.layers.12.self_attn.v_proj.SU', 'vision_model.encoder.layers.12.self_attn.v_proj.SV', 'vision_model.encoder.layers.12.self_attn.v_proj.rcp', 'vision_model.encoder.layers.12.self_attn.v_proj.tlut', 'vision_model.encoder.layers.12.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.v_proj.trellis', 'vision_model.encoder.layers.13.mlp.fc1.SU', 'vision_model.encoder.layers.13.mlp.fc1.SV', 'vision_model.encoder.layers.13.mlp.fc1.rcp', 'vision_model.encoder.layers.13.mlp.fc1.tlut', 'vision_model.encoder.layers.13.mlp.fc1.tp_rank', 'vision_model.encoder.layers.13.mlp.fc1.trellis', 'vision_model.encoder.layers.13.mlp.fc2.SU', 'vision_model.encoder.layers.13.mlp.fc2.SV', 'vision_model.encoder.layers.13.mlp.fc2.rcp', 'vision_model.encoder.layers.13.mlp.fc2.tlut', 'vision_model.encoder.layers.13.mlp.fc2.tp_rank', 'vision_model.encoder.layers.13.mlp.fc2.trellis', 'vision_model.encoder.layers.13.self_attn.k_proj.SU', 'vision_model.encoder.layers.13.self_attn.k_proj.SV', 'vision_model.encoder.layers.13.self_attn.k_proj.rcp', 'vision_model.encoder.layers.13.self_attn.k_proj.tlut', 'vision_model.encoder.layers.13.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.k_proj.trellis', 'vision_model.encoder.layers.13.self_attn.out_proj.SU', 'vision_model.encoder.layers.13.self_attn.out_proj.SV', 'vision_model.encoder.layers.13.self_attn.out_proj.rcp', 'vision_model.encoder.layers.13.self_attn.out_proj.tlut', 'vision_model.encoder.layers.13.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.out_proj.trellis', 'vision_model.encoder.layers.13.self_attn.q_proj.SU', 'vision_model.encoder.layers.13.self_attn.q_proj.SV', 'vision_model.encoder.layers.13.self_attn.q_proj.rcp', 'vision_model.encoder.layers.13.self_attn.q_proj.tlut', 'vision_model.encoder.layers.13.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.q_proj.trellis', 'vision_model.encoder.layers.13.self_attn.v_proj.SU', 'vision_model.encoder.layers.13.self_attn.v_proj.SV', 'vision_model.encoder.layers.13.self_attn.v_proj.rcp', 'vision_model.encoder.layers.13.self_attn.v_proj.tlut', 'vision_model.encoder.layers.13.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.v_proj.trellis', 'vision_model.encoder.layers.14.mlp.fc1.SU', 'vision_model.encoder.layers.14.mlp.fc1.SV', 'vision_model.encoder.layers.14.mlp.fc1.rcp', 'vision_model.encoder.layers.14.mlp.fc1.tlut', 'vision_model.encoder.layers.14.mlp.fc1.tp_rank', 'vision_model.encoder.layers.14.mlp.fc1.trellis', 'vision_model.encoder.layers.14.mlp.fc2.SU', 'vision_model.encoder.layers.14.mlp.fc2.SV', 'vision_model.encoder.layers.14.mlp.fc2.rcp', 'vision_model.encoder.layers.14.mlp.fc2.tlut', 'vision_model.encoder.layers.14.mlp.fc2.tp_rank', 'vision_model.encoder.layers.14.mlp.fc2.trellis', 'vision_model.encoder.layers.14.self_attn.k_proj.SU', 'vision_model.encoder.layers.14.self_attn.k_proj.SV', 'vision_model.encoder.layers.14.self_attn.k_proj.rcp', 'vision_model.encoder.layers.14.self_attn.k_proj.tlut', 'vision_model.encoder.layers.14.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.k_proj.trellis', 'vision_model.encoder.layers.14.self_attn.out_proj.SU', 'vision_model.encoder.layers.14.self_attn.out_proj.SV', 'vision_model.encoder.layers.14.self_attn.out_proj.rcp', 'vision_model.encoder.layers.14.self_attn.out_proj.tlut', 'vision_model.encoder.layers.14.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.out_proj.trellis', 'vision_model.encoder.layers.14.self_attn.q_proj.SU', 'vision_model.encoder.layers.14.self_attn.q_proj.SV', 'vision_model.encoder.layers.14.self_attn.q_proj.rcp', 'vision_model.encoder.layers.14.self_attn.q_proj.tlut', 'vision_model.encoder.layers.14.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.q_proj.trellis', 'vision_model.encoder.layers.14.self_attn.v_proj.SU', 'vision_model.encoder.layers.14.self_attn.v_proj.SV', 'vision_model.encoder.layers.14.self_attn.v_proj.rcp', 'vision_model.encoder.layers.14.self_attn.v_proj.tlut', 'vision_model.encoder.layers.14.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.v_proj.trellis', 'vision_model.encoder.layers.15.mlp.fc1.SU', 'vision_model.encoder.layers.15.mlp.fc1.SV', 'vision_model.encoder.layers.15.mlp.fc1.rcp', 'vision_model.encoder.layers.15.mlp.fc1.tlut', 'vision_model.encoder.layers.15.mlp.fc1.tp_rank', 'vision_model.encoder.layers.15.mlp.fc1.trellis', 'vision_model.encoder.layers.15.mlp.fc2.SU', 'vision_model.encoder.layers.15.mlp.fc2.SV', 'vision_model.encoder.layers.15.mlp.fc2.rcp', 'vision_model.encoder.layers.15.mlp.fc2.tlut', 'vision_model.encoder.layers.15.mlp.fc2.tp_rank', 'vision_model.encoder.layers.15.mlp.fc2.trellis', 'vision_model.encoder.layers.15.self_attn.k_proj.SU', 'vision_model.encoder.layers.15.self_attn.k_proj.SV', 'vision_model.encoder.layers.15.self_attn.k_proj.rcp', 'vision_model.encoder.layers.15.self_attn.k_proj.tlut', 'vision_model.encoder.layers.15.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.k_proj.trellis', 'vision_model.encoder.layers.15.self_attn.out_proj.SU', 'vision_model.encoder.layers.15.self_attn.out_proj.SV', 'vision_model.encoder.layers.15.self_attn.out_proj.rcp', 'vision_model.encoder.layers.15.self_attn.out_proj.tlut', 'vision_model.encoder.layers.15.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.out_proj.trellis', 'vision_model.encoder.layers.15.self_attn.q_proj.SU', 'vision_model.encoder.layers.15.self_attn.q_proj.SV', 'vision_model.encoder.layers.15.self_attn.q_proj.rcp', 'vision_model.encoder.layers.15.self_attn.q_proj.tlut', 'vision_model.encoder.layers.15.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.q_proj.trellis', 'vision_model.encoder.layers.15.self_attn.v_proj.SU', 'vision_model.encoder.layers.15.self_attn.v_proj.SV', 'vision_model.encoder.layers.15.self_attn.v_proj.rcp', 'vision_model.encoder.layers.15.self_attn.v_proj.tlut', 'vision_model.encoder.layers.15.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.v_proj.trellis', 'vision_model.encoder.layers.16.mlp.fc1.SU', 'vision_model.encoder.layers.16.mlp.fc1.SV', 'vision_model.encoder.layers.16.mlp.fc1.rcp', 'vision_model.encoder.layers.16.mlp.fc1.tlut', 'vision_model.encoder.layers.16.mlp.fc1.tp_rank', 'vision_model.encoder.layers.16.mlp.fc1.trellis', 'vision_model.encoder.layers.16.mlp.fc2.SU', 'vision_model.encoder.layers.16.mlp.fc2.SV', 'vision_model.encoder.layers.16.mlp.fc2.rcp', 'vision_model.encoder.layers.16.mlp.fc2.tlut', 'vision_model.encoder.layers.16.mlp.fc2.tp_rank', 'vision_model.encoder.layers.16.mlp.fc2.trellis', 'vision_model.encoder.layers.16.self_attn.k_proj.SU', 'vision_model.encoder.layers.16.self_attn.k_proj.SV', 'vision_model.encoder.layers.16.self_attn.k_proj.rcp', 'vision_model.encoder.layers.16.self_attn.k_proj.tlut', 'vision_model.encoder.layers.16.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.k_proj.trellis', 'vision_model.encoder.layers.16.self_attn.out_proj.SU', 'vision_model.encoder.layers.16.self_attn.out_proj.SV', 'vision_model.encoder.layers.16.self_attn.out_proj.rcp', 'vision_model.encoder.layers.16.self_attn.out_proj.tlut', 'vision_model.encoder.layers.16.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.out_proj.trellis', 'vision_model.encoder.layers.16.self_attn.q_proj.SU', 'vision_model.encoder.layers.16.self_attn.q_proj.SV', 'vision_model.encoder.layers.16.self_attn.q_proj.rcp', 'vision_model.encoder.layers.16.self_attn.q_proj.tlut', 'vision_model.encoder.layers.16.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.q_proj.trellis', 'vision_model.encoder.layers.16.self_attn.v_proj.SU', 'vision_model.encoder.layers.16.self_attn.v_proj.SV', 'vision_model.encoder.layers.16.self_attn.v_proj.rcp', 'vision_model.encoder.layers.16.self_attn.v_proj.tlut', 'vision_model.encoder.layers.16.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.v_proj.trellis', 'vision_model.encoder.layers.17.mlp.fc1.SU', 'vision_model.encoder.layers.17.mlp.fc1.SV', 'vision_model.encoder.layers.17.mlp.fc1.rcp', 'vision_model.encoder.layers.17.mlp.fc1.tlut', 'vision_model.encoder.layers.17.mlp.fc1.tp_rank', 'vision_model.encoder.layers.17.mlp.fc1.trellis', 'vision_model.encoder.layers.17.mlp.fc2.SU', 'vision_model.encoder.layers.17.mlp.fc2.SV', 'vision_model.encoder.layers.17.mlp.fc2.rcp', 'vision_model.encoder.layers.17.mlp.fc2.tlut', 'vision_model.encoder.layers.17.mlp.fc2.tp_rank', 'vision_model.encoder.layers.17.mlp.fc2.trellis', 'vision_model.encoder.layers.17.self_attn.k_proj.SU', 'vision_model.encoder.layers.17.self_attn.k_proj.SV', 'vision_model.encoder.layers.17.self_attn.k_proj.rcp', 'vision_model.encoder.layers.17.self_attn.k_proj.tlut', 'vision_model.encoder.layers.17.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.k_proj.trellis', 'vision_model.encoder.layers.17.self_attn.out_proj.SU', 'vision_model.encoder.layers.17.self_attn.out_proj.SV', 'vision_model.encoder.layers.17.self_attn.out_proj.rcp', 'vision_model.encoder.layers.17.self_attn.out_proj.tlut', 'vision_model.encoder.layers.17.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.out_proj.trellis', 'vision_model.encoder.layers.17.self_attn.q_proj.SU', 'vision_model.encoder.layers.17.self_attn.q_proj.SV', 'vision_model.encoder.layers.17.self_attn.q_proj.rcp', 'vision_model.encoder.layers.17.self_attn.q_proj.tlut', 'vision_model.encoder.layers.17.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.q_proj.trellis', 'vision_model.encoder.layers.17.self_attn.v_proj.SU', 'vision_model.encoder.layers.17.self_attn.v_proj.SV', 'vision_model.encoder.layers.17.self_attn.v_proj.rcp', 'vision_model.encoder.layers.17.self_attn.v_proj.tlut', 'vision_model.encoder.layers.17.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.v_proj.trellis', 'vision_model.encoder.layers.18.mlp.fc1.SU', 'vision_model.encoder.layers.18.mlp.fc1.SV', 'vision_model.encoder.layers.18.mlp.fc1.rcp', 'vision_model.encoder.layers.18.mlp.fc1.tlut', 'vision_model.encoder.layers.18.mlp.fc1.tp_rank', 'vision_model.encoder.layers.18.mlp.fc1.trellis', 'vision_model.encoder.layers.18.mlp.fc2.SU', 'vision_model.encoder.layers.18.mlp.fc2.SV', 'vision_model.encoder.layers.18.mlp.fc2.rcp', 'vision_model.encoder.layers.18.mlp.fc2.tlut', 'vision_model.encoder.layers.18.mlp.fc2.tp_rank', 'vision_model.encoder.layers.18.mlp.fc2.trellis', 'vision_model.encoder.layers.18.self_attn.k_proj.SU', 'vision_model.encoder.layers.18.self_attn.k_proj.SV', 'vision_model.encoder.layers.18.self_attn.k_proj.rcp', 'vision_model.encoder.layers.18.self_attn.k_proj.tlut', 'vision_model.encoder.layers.18.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.k_proj.trellis', 'vision_model.encoder.layers.18.self_attn.out_proj.SU', 'vision_model.encoder.layers.18.self_attn.out_proj.SV', 'vision_model.encoder.layers.18.self_attn.out_proj.rcp', 'vision_model.encoder.layers.18.self_attn.out_proj.tlut', 'vision_model.encoder.layers.18.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.out_proj.trellis', 'vision_model.encoder.layers.18.self_attn.q_proj.SU', 'vision_model.encoder.layers.18.self_attn.q_proj.SV', 'vision_model.encoder.layers.18.self_attn.q_proj.rcp', 'vision_model.encoder.layers.18.self_attn.q_proj.tlut', 'vision_model.encoder.layers.18.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.q_proj.trellis', 'vision_model.encoder.layers.18.self_attn.v_proj.SU', 'vision_model.encoder.layers.18.self_attn.v_proj.SV', 'vision_model.encoder.layers.18.self_attn.v_proj.rcp', 'vision_model.encoder.layers.18.self_attn.v_proj.tlut', 'vision_model.encoder.layers.18.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.v_proj.trellis', 'vision_model.encoder.layers.19.mlp.fc1.SU', 'vision_model.encoder.layers.19.mlp.fc1.SV', 'vision_model.encoder.layers.19.mlp.fc1.rcp', 'vision_model.encoder.layers.19.mlp.fc1.tlut', 'vision_model.encoder.layers.19.mlp.fc1.tp_rank', 'vision_model.encoder.layers.19.mlp.fc1.trellis', 'vision_model.encoder.layers.19.mlp.fc2.SU', 'vision_model.encoder.layers.19.mlp.fc2.SV', 'vision_model.encoder.layers.19.mlp.fc2.rcp', 'vision_model.encoder.layers.19.mlp.fc2.tlut', 'vision_model.encoder.layers.19.mlp.fc2.tp_rank', 'vision_model.encoder.layers.19.mlp.fc2.trellis', 'vision_model.encoder.layers.19.self_attn.k_proj.SU', 'vision_model.encoder.layers.19.self_attn.k_proj.SV', 'vision_model.encoder.layers.19.self_attn.k_proj.rcp', 'vision_model.encoder.layers.19.self_attn.k_proj.tlut', 'vision_model.encoder.layers.19.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.k_proj.trellis', 'vision_model.encoder.layers.19.self_attn.out_proj.SU', 'vision_model.encoder.layers.19.self_attn.out_proj.SV', 'vision_model.encoder.layers.19.self_attn.out_proj.rcp', 'vision_model.encoder.layers.19.self_attn.out_proj.tlut', 'vision_model.encoder.layers.19.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.out_proj.trellis', 'vision_model.encoder.layers.19.self_attn.q_proj.SU', 'vision_model.encoder.layers.19.self_attn.q_proj.SV', 'vision_model.encoder.layers.19.self_attn.q_proj.rcp', 'vision_model.encoder.layers.19.self_attn.q_proj.tlut', 'vision_model.encoder.layers.19.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.q_proj.trellis', 'vision_model.encoder.layers.19.self_attn.v_proj.SU', 'vision_model.encoder.layers.19.self_attn.v_proj.SV', 'vision_model.encoder.layers.19.self_attn.v_proj.rcp', 'vision_model.encoder.layers.19.self_attn.v_proj.tlut', 'vision_model.encoder.layers.19.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.v_proj.trellis', 'vision_model.encoder.layers.2.mlp.fc1.SU', 'vision_model.encoder.layers.2.mlp.fc1.SV', 'vision_model.encoder.layers.2.mlp.fc1.rcp', 'vision_model.encoder.layers.2.mlp.fc1.tlut', 'vision_model.encoder.layers.2.mlp.fc1.tp_rank', 'vision_model.encoder.layers.2.mlp.fc1.trellis', 'vision_model.encoder.layers.2.mlp.fc2.SU', 'vision_model.encoder.layers.2.mlp.fc2.SV', 'vision_model.encoder.layers.2.mlp.fc2.rcp', 'vision_model.encoder.layers.2.mlp.fc2.tlut', 'vision_model.encoder.layers.2.mlp.fc2.tp_rank', 'vision_model.encoder.layers.2.mlp.fc2.trellis', 'vision_model.encoder.layers.2.self_attn.k_proj.SU', 'vision_model.encoder.layers.2.self_attn.k_proj.SV', 'vision_model.encoder.layers.2.self_attn.k_proj.rcp', 'vision_model.encoder.layers.2.self_attn.k_proj.tlut', 'vision_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.k_proj.trellis', 'vision_model.encoder.layers.2.self_attn.out_proj.SU', 'vision_model.encoder.layers.2.self_attn.out_proj.SV', 'vision_model.encoder.layers.2.self_attn.out_proj.rcp', 'vision_model.encoder.layers.2.self_attn.out_proj.tlut', 'vision_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.out_proj.trellis', 'vision_model.encoder.layers.2.self_attn.q_proj.SU', 'vision_model.encoder.layers.2.self_attn.q_proj.SV', 'vision_model.encoder.layers.2.self_attn.q_proj.rcp', 'vision_model.encoder.layers.2.self_attn.q_proj.tlut', 'vision_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.q_proj.trellis', 'vision_model.encoder.layers.2.self_attn.v_proj.SU', 'vision_model.encoder.layers.2.self_attn.v_proj.SV', 'vision_model.encoder.layers.2.self_attn.v_proj.rcp', 'vision_model.encoder.layers.2.self_attn.v_proj.tlut', 'vision_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.v_proj.trellis', 'vision_model.encoder.layers.20.mlp.fc1.SU', 'vision_model.encoder.layers.20.mlp.fc1.SV', 'vision_model.encoder.layers.20.mlp.fc1.rcp', 'vision_model.encoder.layers.20.mlp.fc1.tlut', 'vision_model.encoder.layers.20.mlp.fc1.tp_rank', 'vision_model.encoder.layers.20.mlp.fc1.trellis', 'vision_model.encoder.layers.20.mlp.fc2.SU', 'vision_model.encoder.layers.20.mlp.fc2.SV', 'vision_model.encoder.layers.20.mlp.fc2.rcp', 'vision_model.encoder.layers.20.mlp.fc2.tlut', 'vision_model.encoder.layers.20.mlp.fc2.tp_rank', 'vision_model.encoder.layers.20.mlp.fc2.trellis', 'vision_model.encoder.layers.20.self_attn.k_proj.SU', 'vision_model.encoder.layers.20.self_attn.k_proj.SV', 'vision_model.encoder.layers.20.self_attn.k_proj.rcp', 'vision_model.encoder.layers.20.self_attn.k_proj.tlut', 'vision_model.encoder.layers.20.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.k_proj.trellis', 'vision_model.encoder.layers.20.self_attn.out_proj.SU', 'vision_model.encoder.layers.20.self_attn.out_proj.SV', 'vision_model.encoder.layers.20.self_attn.out_proj.rcp', 'vision_model.encoder.layers.20.self_attn.out_proj.tlut', 'vision_model.encoder.layers.20.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.out_proj.trellis', 'vision_model.encoder.layers.20.self_attn.q_proj.SU', 'vision_model.encoder.layers.20.self_attn.q_proj.SV', 'vision_model.encoder.layers.20.self_attn.q_proj.rcp', 'vision_model.encoder.layers.20.self_attn.q_proj.tlut', 'vision_model.encoder.layers.20.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.q_proj.trellis', 'vision_model.encoder.layers.20.self_attn.v_proj.SU', 'vision_model.encoder.layers.20.self_attn.v_proj.SV', 'vision_model.encoder.layers.20.self_attn.v_proj.rcp', 'vision_model.encoder.layers.20.self_attn.v_proj.tlut', 'vision_model.encoder.layers.20.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.v_proj.trellis', 'vision_model.encoder.layers.21.mlp.fc1.SU', 'vision_model.encoder.layers.21.mlp.fc1.SV', 'vision_model.encoder.layers.21.mlp.fc1.rcp', 'vision_model.encoder.layers.21.mlp.fc1.tlut', 'vision_model.encoder.layers.21.mlp.fc1.tp_rank', 'vision_model.encoder.layers.21.mlp.fc1.trellis', 'vision_model.encoder.layers.21.mlp.fc2.SU', 'vision_model.encoder.layers.21.mlp.fc2.SV', 'vision_model.encoder.layers.21.mlp.fc2.rcp', 'vision_model.encoder.layers.21.mlp.fc2.tlut', 'vision_model.encoder.layers.21.mlp.fc2.tp_rank', 'vision_model.encoder.layers.21.mlp.fc2.trellis', 'vision_model.encoder.layers.21.self_attn.k_proj.SU', 'vision_model.encoder.layers.21.self_attn.k_proj.SV', 'vision_model.encoder.layers.21.self_attn.k_proj.rcp', 'vision_model.encoder.layers.21.self_attn.k_proj.tlut', 'vision_model.encoder.layers.21.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.k_proj.trellis', 'vision_model.encoder.layers.21.self_attn.out_proj.SU', 'vision_model.encoder.layers.21.self_attn.out_proj.SV', 'vision_model.encoder.layers.21.self_attn.out_proj.rcp', 'vision_model.encoder.layers.21.self_attn.out_proj.tlut', 'vision_model.encoder.layers.21.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.out_proj.trellis', 'vision_model.encoder.layers.21.self_attn.q_proj.SU', 'vision_model.encoder.layers.21.self_attn.q_proj.SV', 'vision_model.encoder.layers.21.self_attn.q_proj.rcp', 'vision_model.encoder.layers.21.self_attn.q_proj.tlut', 'vision_model.encoder.layers.21.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.q_proj.trellis', 'vision_model.encoder.layers.21.self_attn.v_proj.SU', 'vision_model.encoder.layers.21.self_attn.v_proj.SV', 'vision_model.encoder.layers.21.self_attn.v_proj.rcp', 'vision_model.encoder.layers.21.self_attn.v_proj.tlut', 'vision_model.encoder.layers.21.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.v_proj.trellis', 'vision_model.encoder.layers.22.mlp.fc1.SU', 'vision_model.encoder.layers.22.mlp.fc1.SV', 'vision_model.encoder.layers.22.mlp.fc1.rcp', 'vision_model.encoder.layers.22.mlp.fc1.tlut', 'vision_model.encoder.layers.22.mlp.fc1.tp_rank', 'vision_model.encoder.layers.22.mlp.fc1.trellis', 'vision_model.encoder.layers.22.mlp.fc2.SU', 'vision_model.encoder.layers.22.mlp.fc2.SV', 'vision_model.encoder.layers.22.mlp.fc2.rcp', 'vision_model.encoder.layers.22.mlp.fc2.tlut', 'vision_model.encoder.layers.22.mlp.fc2.tp_rank', 'vision_model.encoder.layers.22.mlp.fc2.trellis', 'vision_model.encoder.layers.22.self_attn.k_proj.SU', 'vision_model.encoder.layers.22.self_attn.k_proj.SV', 'vision_model.encoder.layers.22.self_attn.k_proj.rcp', 'vision_model.encoder.layers.22.self_attn.k_proj.tlut', 'vision_model.encoder.layers.22.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.k_proj.trellis', 'vision_model.encoder.layers.22.self_attn.out_proj.SU', 'vision_model.encoder.layers.22.self_attn.out_proj.SV', 'vision_model.encoder.layers.22.self_attn.out_proj.rcp', 'vision_model.encoder.layers.22.self_attn.out_proj.tlut', 'vision_model.encoder.layers.22.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.out_proj.trellis', 'vision_model.encoder.layers.22.self_attn.q_proj.SU', 'vision_model.encoder.layers.22.self_attn.q_proj.SV', 'vision_model.encoder.layers.22.self_attn.q_proj.rcp', 'vision_model.encoder.layers.22.self_attn.q_proj.tlut', 'vision_model.encoder.layers.22.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.q_proj.trellis', 'vision_model.encoder.layers.22.self_attn.v_proj.SU', 'vision_model.encoder.layers.22.self_attn.v_proj.SV', 'vision_model.encoder.layers.22.self_attn.v_proj.rcp', 'vision_model.encoder.layers.22.self_attn.v_proj.tlut', 'vision_model.encoder.layers.22.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.v_proj.trellis', 'vision_model.encoder.layers.23.mlp.fc1.SU', 'vision_model.encoder.layers.23.mlp.fc1.SV', 'vision_model.encoder.layers.23.mlp.fc1.rcp', 'vision_model.encoder.layers.23.mlp.fc1.tlut', 'vision_model.encoder.layers.23.mlp.fc1.tp_rank', 'vision_model.encoder.layers.23.mlp.fc1.trellis', 'vision_model.encoder.layers.23.mlp.fc2.SU', 'vision_model.encoder.layers.23.mlp.fc2.SV', 'vision_model.encoder.layers.23.mlp.fc2.rcp', 'vision_model.encoder.layers.23.mlp.fc2.tlut', 'vision_model.encoder.layers.23.mlp.fc2.tp_rank', 'vision_model.encoder.layers.23.mlp.fc2.trellis', 'vision_model.encoder.layers.23.self_attn.k_proj.SU', 'vision_model.encoder.layers.23.self_attn.k_proj.SV', 'vision_model.encoder.layers.23.self_attn.k_proj.rcp', 'vision_model.encoder.layers.23.self_attn.k_proj.tlut', 'vision_model.encoder.layers.23.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.k_proj.trellis', 'vision_model.encoder.layers.23.self_attn.out_proj.SU', 'vision_model.encoder.layers.23.self_attn.out_proj.SV', 'vision_model.encoder.layers.23.self_attn.out_proj.rcp', 'vision_model.encoder.layers.23.self_attn.out_proj.tlut', 'vision_model.encoder.layers.23.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.out_proj.trellis', 'vision_model.encoder.layers.23.self_attn.q_proj.SU', 'vision_model.encoder.layers.23.self_attn.q_proj.SV', 'vision_model.encoder.layers.23.self_attn.q_proj.rcp', 'vision_model.encoder.layers.23.self_attn.q_proj.tlut', 'vision_model.encoder.layers.23.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.q_proj.trellis', 'vision_model.encoder.layers.23.self_attn.v_proj.SU', 'vision_model.encoder.layers.23.self_attn.v_proj.SV', 'vision_model.encoder.layers.23.self_attn.v_proj.rcp', 'vision_model.encoder.layers.23.self_attn.v_proj.tlut', 'vision_model.encoder.layers.23.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.v_proj.trellis', 'vision_model.encoder.layers.3.mlp.fc1.SU', 'vision_model.encoder.layers.3.mlp.fc1.SV', 'vision_model.encoder.layers.3.mlp.fc1.rcp', 'vision_model.encoder.layers.3.mlp.fc1.tlut', 'vision_model.encoder.layers.3.mlp.fc1.tp_rank', 'vision_model.encoder.layers.3.mlp.fc1.trellis', 'vision_model.encoder.layers.3.mlp.fc2.SU', 'vision_model.encoder.layers.3.mlp.fc2.SV', 'vision_model.encoder.layers.3.mlp.fc2.rcp', 'vision_model.encoder.layers.3.mlp.fc2.tlut', 'vision_model.encoder.layers.3.mlp.fc2.tp_rank', 'vision_model.encoder.layers.3.mlp.fc2.trellis', 'vision_model.encoder.layers.3.self_attn.k_proj.SU', 'vision_model.encoder.layers.3.self_attn.k_proj.SV', 'vision_model.encoder.layers.3.self_attn.k_proj.rcp', 'vision_model.encoder.layers.3.self_attn.k_proj.tlut', 'vision_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.k_proj.trellis', 'vision_model.encoder.layers.3.self_attn.out_proj.SU', 'vision_model.encoder.layers.3.self_attn.out_proj.SV', 'vision_model.encoder.layers.3.self_attn.out_proj.rcp', 'vision_model.encoder.layers.3.self_attn.out_proj.tlut', 'vision_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.out_proj.trellis', 'vision_model.encoder.layers.3.self_attn.q_proj.SU', 'vision_model.encoder.layers.3.self_attn.q_proj.SV', 'vision_model.encoder.layers.3.self_attn.q_proj.rcp', 'vision_model.encoder.layers.3.self_attn.q_proj.tlut', 'vision_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.q_proj.trellis', 'vision_model.encoder.layers.3.self_attn.v_proj.SU', 'vision_model.encoder.layers.3.self_attn.v_proj.SV', 'vision_model.encoder.layers.3.self_attn.v_proj.rcp', 'vision_model.encoder.layers.3.self_attn.v_proj.tlut', 'vision_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.v_proj.trellis', 'vision_model.encoder.layers.4.mlp.fc1.SU', 'vision_model.encoder.layers.4.mlp.fc1.SV', 'vision_model.encoder.layers.4.mlp.fc1.rcp', 'vision_model.encoder.layers.4.mlp.fc1.tlut', 'vision_model.encoder.layers.4.mlp.fc1.tp_rank', 'vision_model.encoder.layers.4.mlp.fc1.trellis', 'vision_model.encoder.layers.4.mlp.fc2.SU', 'vision_model.encoder.layers.4.mlp.fc2.SV', 'vision_model.encoder.layers.4.mlp.fc2.rcp', 'vision_model.encoder.layers.4.mlp.fc2.tlut', 'vision_model.encoder.layers.4.mlp.fc2.tp_rank', 'vision_model.encoder.layers.4.mlp.fc2.trellis', 'vision_model.encoder.layers.4.self_attn.k_proj.SU', 'vision_model.encoder.layers.4.self_attn.k_proj.SV', 'vision_model.encoder.layers.4.self_attn.k_proj.rcp', 'vision_model.encoder.layers.4.self_attn.k_proj.tlut', 'vision_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.k_proj.trellis', 'vision_model.encoder.layers.4.self_attn.out_proj.SU', 'vision_model.encoder.layers.4.self_attn.out_proj.SV', 'vision_model.encoder.layers.4.self_attn.out_proj.rcp', 'vision_model.encoder.layers.4.self_attn.out_proj.tlut', 'vision_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.out_proj.trellis', 'vision_model.encoder.layers.4.self_attn.q_proj.SU', 'vision_model.encoder.layers.4.self_attn.q_proj.SV', 'vision_model.encoder.layers.4.self_attn.q_proj.rcp', 'vision_model.encoder.layers.4.self_attn.q_proj.tlut', 'vision_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.q_proj.trellis', 'vision_model.encoder.layers.4.self_attn.v_proj.SU', 'vision_model.encoder.layers.4.self_attn.v_proj.SV', 'vision_model.encoder.layers.4.self_attn.v_proj.rcp', 'vision_model.encoder.layers.4.self_attn.v_proj.tlut', 'vision_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.v_proj.trellis', 'vision_model.encoder.layers.5.mlp.fc1.SU', 'vision_model.encoder.layers.5.mlp.fc1.SV', 'vision_model.encoder.layers.5.mlp.fc1.rcp', 'vision_model.encoder.layers.5.mlp.fc1.tlut', 'vision_model.encoder.layers.5.mlp.fc1.tp_rank', 'vision_model.encoder.layers.5.mlp.fc1.trellis', 'vision_model.encoder.layers.5.mlp.fc2.SU', 'vision_model.encoder.layers.5.mlp.fc2.SV', 'vision_model.encoder.layers.5.mlp.fc2.rcp', 'vision_model.encoder.layers.5.mlp.fc2.tlut', 'vision_model.encoder.layers.5.mlp.fc2.tp_rank', 'vision_model.encoder.layers.5.mlp.fc2.trellis', 'vision_model.encoder.layers.5.self_attn.k_proj.SU', 'vision_model.encoder.layers.5.self_attn.k_proj.SV', 'vision_model.encoder.layers.5.self_attn.k_proj.rcp', 'vision_model.encoder.layers.5.self_attn.k_proj.tlut', 'vision_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.k_proj.trellis', 'vision_model.encoder.layers.5.self_attn.out_proj.SU', 'vision_model.encoder.layers.5.self_attn.out_proj.SV', 'vision_model.encoder.layers.5.self_attn.out_proj.rcp', 'vision_model.encoder.layers.5.self_attn.out_proj.tlut', 'vision_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.out_proj.trellis', 'vision_model.encoder.layers.5.self_attn.q_proj.SU', 'vision_model.encoder.layers.5.self_attn.q_proj.SV', 'vision_model.encoder.layers.5.self_attn.q_proj.rcp', 'vision_model.encoder.layers.5.self_attn.q_proj.tlut', 'vision_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.q_proj.trellis', 'vision_model.encoder.layers.5.self_attn.v_proj.SU', 'vision_model.encoder.layers.5.self_attn.v_proj.SV', 'vision_model.encoder.layers.5.self_attn.v_proj.rcp', 'vision_model.encoder.layers.5.self_attn.v_proj.tlut', 'vision_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.v_proj.trellis', 'vision_model.encoder.layers.6.mlp.fc1.SU', 'vision_model.encoder.layers.6.mlp.fc1.SV', 'vision_model.encoder.layers.6.mlp.fc1.rcp', 'vision_model.encoder.layers.6.mlp.fc1.tlut', 'vision_model.encoder.layers.6.mlp.fc1.tp_rank', 'vision_model.encoder.layers.6.mlp.fc1.trellis', 'vision_model.encoder.layers.6.mlp.fc2.SU', 'vision_model.encoder.layers.6.mlp.fc2.SV', 'vision_model.encoder.layers.6.mlp.fc2.rcp', 'vision_model.encoder.layers.6.mlp.fc2.tlut', 'vision_model.encoder.layers.6.mlp.fc2.tp_rank', 'vision_model.encoder.layers.6.mlp.fc2.trellis', 'vision_model.encoder.layers.6.self_attn.k_proj.SU', 'vision_model.encoder.layers.6.self_attn.k_proj.SV', 'vision_model.encoder.layers.6.self_attn.k_proj.rcp', 'vision_model.encoder.layers.6.self_attn.k_proj.tlut', 'vision_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.k_proj.trellis', 'vision_model.encoder.layers.6.self_attn.out_proj.SU', 'vision_model.encoder.layers.6.self_attn.out_proj.SV', 'vision_model.encoder.layers.6.self_attn.out_proj.rcp', 'vision_model.encoder.layers.6.self_attn.out_proj.tlut', 'vision_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.out_proj.trellis', 'vision_model.encoder.layers.6.self_attn.q_proj.SU', 'vision_model.encoder.layers.6.self_attn.q_proj.SV', 'vision_model.encoder.layers.6.self_attn.q_proj.rcp', 'vision_model.encoder.layers.6.self_attn.q_proj.tlut', 'vision_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.q_proj.trellis', 'vision_model.encoder.layers.6.self_attn.v_proj.SU', 'vision_model.encoder.layers.6.self_attn.v_proj.SV', 'vision_model.encoder.layers.6.self_attn.v_proj.rcp', 'vision_model.encoder.layers.6.self_attn.v_proj.tlut', 'vision_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.v_proj.trellis', 'vision_model.encoder.layers.7.mlp.fc1.SU', 'vision_model.encoder.layers.7.mlp.fc1.SV', 'vision_model.encoder.layers.7.mlp.fc1.rcp', 'vision_model.encoder.layers.7.mlp.fc1.tlut', 'vision_model.encoder.layers.7.mlp.fc1.tp_rank', 'vision_model.encoder.layers.7.mlp.fc1.trellis', 'vision_model.encoder.layers.7.mlp.fc2.SU', 'vision_model.encoder.layers.7.mlp.fc2.SV', 'vision_model.encoder.layers.7.mlp.fc2.rcp', 'vision_model.encoder.layers.7.mlp.fc2.tlut', 'vision_model.encoder.layers.7.mlp.fc2.tp_rank', 'vision_model.encoder.layers.7.mlp.fc2.trellis', 'vision_model.encoder.layers.7.self_attn.k_proj.SU', 'vision_model.encoder.layers.7.self_attn.k_proj.SV', 'vision_model.encoder.layers.7.self_attn.k_proj.rcp', 'vision_model.encoder.layers.7.self_attn.k_proj.tlut', 'vision_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.k_proj.trellis', 'vision_model.encoder.layers.7.self_attn.out_proj.SU', 'vision_model.encoder.layers.7.self_attn.out_proj.SV', 'vision_model.encoder.layers.7.self_attn.out_proj.rcp', 'vision_model.encoder.layers.7.self_attn.out_proj.tlut', 'vision_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.out_proj.trellis', 'vision_model.encoder.layers.7.self_attn.q_proj.SU', 'vision_model.encoder.layers.7.self_attn.q_proj.SV', 'vision_model.encoder.layers.7.self_attn.q_proj.rcp', 'vision_model.encoder.layers.7.self_attn.q_proj.tlut', 'vision_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.q_proj.trellis', 'vision_model.encoder.layers.7.self_attn.v_proj.SU', 'vision_model.encoder.layers.7.self_attn.v_proj.SV', 'vision_model.encoder.layers.7.self_attn.v_proj.rcp', 'vision_model.encoder.layers.7.self_attn.v_proj.tlut', 'vision_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.v_proj.trellis', 'vision_model.encoder.layers.8.mlp.fc1.SU', 'vision_model.encoder.layers.8.mlp.fc1.SV', 'vision_model.encoder.layers.8.mlp.fc1.rcp', 'vision_model.encoder.layers.8.mlp.fc1.tlut', 'vision_model.encoder.layers.8.mlp.fc1.tp_rank', 'vision_model.encoder.layers.8.mlp.fc1.trellis', 'vision_model.encoder.layers.8.mlp.fc2.SU', 'vision_model.encoder.layers.8.mlp.fc2.SV', 'vision_model.encoder.layers.8.mlp.fc2.rcp', 'vision_model.encoder.layers.8.mlp.fc2.tlut', 'vision_model.encoder.layers.8.mlp.fc2.tp_rank', 'vision_model.encoder.layers.8.mlp.fc2.trellis', 'vision_model.encoder.layers.8.self_attn.k_proj.SU', 'vision_model.encoder.layers.8.self_attn.k_proj.SV', 'vision_model.encoder.layers.8.self_attn.k_proj.rcp', 'vision_model.encoder.layers.8.self_attn.k_proj.tlut', 'vision_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.k_proj.trellis', 'vision_model.encoder.layers.8.self_attn.out_proj.SU', 'vision_model.encoder.layers.8.self_attn.out_proj.SV', 'vision_model.encoder.layers.8.self_attn.out_proj.rcp', 'vision_model.encoder.layers.8.self_attn.out_proj.tlut', 'vision_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.out_proj.trellis', 'vision_model.encoder.layers.8.self_attn.q_proj.SU', 'vision_model.encoder.layers.8.self_attn.q_proj.SV', 'vision_model.encoder.layers.8.self_attn.q_proj.rcp', 'vision_model.encoder.layers.8.self_attn.q_proj.tlut', 'vision_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.q_proj.trellis', 'vision_model.encoder.layers.8.self_attn.v_proj.SU', 'vision_model.encoder.layers.8.self_attn.v_proj.SV', 'vision_model.encoder.layers.8.self_attn.v_proj.rcp', 'vision_model.encoder.layers.8.self_attn.v_proj.tlut', 'vision_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.v_proj.trellis', 'vision_model.encoder.layers.9.mlp.fc1.SU', 'vision_model.encoder.layers.9.mlp.fc1.SV', 'vision_model.encoder.layers.9.mlp.fc1.rcp', 'vision_model.encoder.layers.9.mlp.fc1.tlut', 'vision_model.encoder.layers.9.mlp.fc1.tp_rank', 'vision_model.encoder.layers.9.mlp.fc1.trellis', 'vision_model.encoder.layers.9.mlp.fc2.SU', 'vision_model.encoder.layers.9.mlp.fc2.SV', 'vision_model.encoder.layers.9.mlp.fc2.rcp', 'vision_model.encoder.layers.9.mlp.fc2.tlut', 'vision_model.encoder.layers.9.mlp.fc2.tp_rank', 'vision_model.encoder.layers.9.mlp.fc2.trellis', 'vision_model.encoder.layers.9.self_attn.k_proj.SU', 'vision_model.encoder.layers.9.self_attn.k_proj.SV', 'vision_model.encoder.layers.9.self_attn.k_proj.rcp', 'vision_model.encoder.layers.9.self_attn.k_proj.tlut', 'vision_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.k_proj.trellis', 'vision_model.encoder.layers.9.self_attn.out_proj.SU', 'vision_model.encoder.layers.9.self_attn.out_proj.SV', 'vision_model.encoder.layers.9.self_attn.out_proj.rcp', 'vision_model.encoder.layers.9.self_attn.out_proj.tlut', 'vision_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.out_proj.trellis', 'vision_model.encoder.layers.9.self_attn.q_proj.SU', 'vision_model.encoder.layers.9.self_attn.q_proj.SV', 'vision_model.encoder.layers.9.self_attn.q_proj.rcp', 'vision_model.encoder.layers.9.self_attn.q_proj.tlut', 'vision_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.q_proj.trellis', 'vision_model.encoder.layers.9.self_attn.v_proj.SU', 'vision_model.encoder.layers.9.self_attn.v_proj.SV', 'vision_model.encoder.layers.9.self_attn.v_proj.rcp', 'vision_model.encoder.layers.9.self_attn.v_proj.tlut', 'vision_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0416 08:39:56.736762 3359101 hfize_clip.py:65] Loading text layer 0
W0416 08:39:56.737008 3359101 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(ln_path, map_location=cpu)

W0416 08:39:56.738114 3359101 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved = torch.load(f'{path_prefix}/{full_key}.pt', map_location='cpu')

I0416 08:39:56.796022 3359101 hfize_clip.py:65] Loading text layer 1
I0416 08:39:56.856095 3359101 hfize_clip.py:65] Loading text layer 2
I0416 08:39:56.914190 3359101 hfize_clip.py:65] Loading text layer 3
I0416 08:39:56.978254 3359101 hfize_clip.py:65] Loading text layer 4
I0416 08:39:57.038203 3359101 hfize_clip.py:65] Loading text layer 5
I0416 08:39:57.110254 3359101 hfize_clip.py:65] Loading text layer 6
I0416 08:39:57.174867 3359101 hfize_clip.py:65] Loading text layer 7
I0416 08:39:57.191462 3359101 hfize_clip.py:65] Loading text layer 8
I0416 08:39:57.218255 3359101 hfize_clip.py:65] Loading text layer 9
I0416 08:39:57.283620 3359101 hfize_clip.py:65] Loading text layer 10
I0416 08:39:57.350757 3359101 hfize_clip.py:65] Loading text layer 11
I0416 08:39:57.410326 3359101 hfize_clip.py:65] Loading vision layer 0
I0416 08:39:57.488277 3359101 hfize_clip.py:65] Loading vision layer 1
I0416 08:39:57.560772 3359101 hfize_clip.py:65] Loading vision layer 2
I0416 08:39:57.605709 3359101 hfize_clip.py:65] Loading vision layer 3
I0416 08:39:57.673152 3359101 hfize_clip.py:65] Loading vision layer 4
I0416 08:39:57.742591 3359101 hfize_clip.py:65] Loading vision layer 5
I0416 08:39:57.811605 3359101 hfize_clip.py:65] Loading vision layer 6
I0416 08:39:57.874833 3359101 hfize_clip.py:65] Loading vision layer 7
I0416 08:39:57.947410 3359101 hfize_clip.py:65] Loading vision layer 8
I0416 08:39:58.016472 3359101 hfize_clip.py:65] Loading vision layer 9
I0416 08:39:58.084478 3359101 hfize_clip.py:65] Loading vision layer 10
I0416 08:39:58.146188 3359101 hfize_clip.py:65] Loading vision layer 11
I0416 08:39:58.214672 3359101 hfize_clip.py:65] Loading vision layer 12
I0416 08:39:58.276097 3359101 hfize_clip.py:65] Loading vision layer 13
I0416 08:39:58.340429 3359101 hfize_clip.py:65] Loading vision layer 14
I0416 08:39:58.416551 3359101 hfize_clip.py:65] Loading vision layer 15
I0416 08:39:58.490378 3359101 hfize_clip.py:65] Loading vision layer 16
I0416 08:39:58.556087 3359101 hfize_clip.py:65] Loading vision layer 17
I0416 08:39:58.620102 3359101 hfize_clip.py:65] Loading vision layer 18
I0416 08:39:58.690279 3359101 hfize_clip.py:65] Loading vision layer 19
I0416 08:39:58.763627 3359101 hfize_clip.py:65] Loading vision layer 20
I0416 08:39:58.830224 3359101 hfize_clip.py:65] Loading vision layer 21
I0416 08:39:58.896070 3359101 hfize_clip.py:65] Loading vision layer 22
I0416 08:39:58.966199 3359101 hfize_clip.py:65] Loading vision layer 23
I0416 08:39:59.036514 3359101 hfize_clip.py:95] Copying logit_scale
I0416 08:39:59.036645 3359101 hfize_clip.py:90] Copying submodule: visual_projection
I0416 08:39:59.048074 3359101 hfize_clip.py:90] Copying submodule: text_projection
I0416 08:39:59.060420 3359101 hfize_clip.py:90] Copying submodule: text token_embedding
I0416 08:39:59.090693 3359101 hfize_clip.py:90] Copying submodule: text position_embedding
I0416 08:39:59.099894 3359101 hfize_clip.py:90] Copying submodule: text final_layer_norm
I0416 08:39:59.100081 3359101 hfize_clip.py:90] Copying submodule: vision patch_embedding
I0416 08:39:59.107885 3359101 hfize_clip.py:90] Copying submodule: vision position_embedding
I0416 08:39:59.120213 3359101 hfize_clip.py:90] Copying submodule: vision post_layernorm
I0416 08:39:59.120482 3359101 hfize_clip.py:121] Saving model to ../hf_model_comp/qtip/hf/clip-vit-large-patch14_5bit...
  0%|          | 0/196 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  1%|          | 1/196 [00:10<34:46, 10.70s/it]  1%|          | 2/196 [00:13<18:59,  5.87s/it]  2%|▏         | 3/196 [00:15<14:00,  4.36s/it]  2%|▏         | 4/196 [00:18<11:41,  3.65s/it]  3%|▎         | 5/196 [00:20<10:25,  3.27s/it]  3%|▎         | 6/196 [00:23<09:38,  3.04s/it]  4%|▎         | 7/196 [00:25<09:00,  2.86s/it]  4%|▍         | 8/196 [00:28<08:35,  2.74s/it]  5%|▍         | 9/196 [00:30<08:17,  2.66s/it]  5%|▌         | 10/196 [00:33<08:05,  2.61s/it]  6%|▌         | 11/196 [00:35<07:54,  2.56s/it]  6%|▌         | 12/196 [00:38<07:47,  2.54s/it]  7%|▋         | 13/196 [00:40<07:42,  2.52s/it]  7%|▋         | 14/196 [00:43<07:37,  2.51s/it]  8%|▊         | 15/196 [00:45<07:33,  2.51s/it]  8%|▊         | 16/196 [00:48<07:30,  2.51s/it]  9%|▊         | 17/196 [00:50<07:26,  2.49s/it]  9%|▉         | 18/196 [00:53<07:23,  2.49s/it] 10%|▉         | 19/196 [00:55<07:19,  2.49s/it] 10%|█         | 20/196 [00:58<07:17,  2.49s/it] 11%|█         | 21/196 [01:00<07:14,  2.48s/it] 11%|█         | 22/196 [01:03<07:11,  2.48s/it] 12%|█▏        | 23/196 [01:05<07:09,  2.48s/it] 12%|█▏        | 24/196 [01:08<07:07,  2.48s/it] 13%|█▎        | 25/196 [01:10<07:04,  2.48s/it] 13%|█▎        | 26/196 [01:13<07:03,  2.49s/it] 14%|█▍        | 27/196 [01:15<07:00,  2.49s/it] 14%|█▍        | 28/196 [01:18<06:58,  2.49s/it] 15%|█▍        | 29/196 [01:20<06:55,  2.49s/it] 15%|█▌        | 30/196 [01:23<06:54,  2.50s/it] 16%|█▌        | 31/196 [01:25<06:51,  2.49s/it] 16%|█▋        | 32/196 [01:28<06:48,  2.49s/it] 17%|█▋        | 33/196 [01:30<06:45,  2.49s/it] 17%|█▋        | 34/196 [01:33<06:42,  2.49s/it] 18%|█▊        | 35/196 [01:35<06:39,  2.48s/it] 18%|█▊        | 36/196 [01:38<06:37,  2.48s/it] 19%|█▉        | 37/196 [01:40<06:34,  2.48s/it] 19%|█▉        | 38/196 [01:43<06:31,  2.48s/it] 20%|█▉        | 39/196 [01:45<06:28,  2.47s/it] 20%|██        | 40/196 [01:47<06:25,  2.47s/it] 21%|██        | 41/196 [01:50<06:23,  2.47s/it] 21%|██▏       | 42/196 [01:52<06:20,  2.47s/it] 22%|██▏       | 43/196 [01:55<06:18,  2.47s/it] 22%|██▏       | 44/196 [01:57<06:16,  2.47s/it] 23%|██▎       | 45/196 [02:00<06:13,  2.47s/it] 23%|██▎       | 46/196 [02:02<06:10,  2.47s/it] 24%|██▍       | 47/196 [02:05<06:08,  2.47s/it] 24%|██▍       | 48/196 [02:07<06:06,  2.47s/it] 25%|██▌       | 49/196 [02:10<06:03,  2.47s/it] 26%|██▌       | 50/196 [02:12<06:01,  2.48s/it] 26%|██▌       | 51/196 [02:15<05:58,  2.48s/it] 27%|██▋       | 52/196 [02:17<05:56,  2.47s/it] 27%|██▋       | 53/196 [02:20<05:54,  2.48s/it] 28%|██▊       | 54/196 [02:22<05:52,  2.48s/it] 28%|██▊       | 55/196 [02:25<05:50,  2.49s/it] 29%|██▊       | 56/196 [02:27<05:48,  2.49s/it] 29%|██▉       | 57/196 [02:30<05:47,  2.50s/it] 30%|██▉       | 58/196 [02:32<05:44,  2.49s/it] 30%|███       | 59/196 [02:35<05:41,  2.49s/it] 31%|███       | 60/196 [02:37<05:39,  2.49s/it] 31%|███       | 61/196 [02:40<05:36,  2.49s/it] 32%|███▏      | 62/196 [02:42<05:33,  2.49s/it] 32%|███▏      | 63/196 [02:45<05:31,  2.49s/it] 33%|███▎      | 64/196 [02:47<05:29,  2.49s/it] 33%|███▎      | 65/196 [02:50<05:25,  2.49s/it] 34%|███▎      | 66/196 [02:52<05:23,  2.49s/it] 34%|███▍      | 67/196 [02:54<05:20,  2.48s/it] 35%|███▍      | 68/196 [02:57<05:17,  2.48s/it] 35%|███▌      | 69/196 [02:59<05:15,  2.48s/it] 36%|███▌      | 70/196 [03:02<05:12,  2.48s/it] 36%|███▌      | 71/196 [03:04<05:10,  2.48s/it] 37%|███▋      | 72/196 [03:07<05:19,  2.58s/it] 37%|███▋      | 73/196 [03:10<05:13,  2.55s/it] 38%|███▊      | 74/196 [03:12<05:08,  2.53s/it] 38%|███▊      | 75/196 [03:15<05:03,  2.51s/it] 39%|███▉      | 76/196 [03:17<05:00,  2.50s/it] 39%|███▉      | 77/196 [03:20<04:57,  2.50s/it] 40%|███▉      | 78/196 [03:22<04:54,  2.50s/it] 40%|████      | 79/196 [03:25<04:51,  2.49s/it] 41%|████      | 80/196 [03:27<04:49,  2.49s/it] 41%|████▏     | 81/196 [03:30<04:46,  2.49s/it] 42%|████▏     | 82/196 [03:32<04:43,  2.49s/it] 42%|████▏     | 83/196 [03:35<04:40,  2.49s/it] 43%|████▎     | 84/196 [03:37<04:38,  2.49s/it] 43%|████▎     | 85/196 [03:40<04:35,  2.48s/it] 44%|████▍     | 86/196 [03:42<04:32,  2.48s/it] 44%|████▍     | 87/196 [03:44<04:29,  2.47s/it] 45%|████▍     | 88/196 [03:47<04:27,  2.48s/it] 45%|████▌     | 89/196 [03:49<04:24,  2.47s/it] 46%|████▌     | 90/196 [03:52<04:21,  2.47s/it] 46%|████▋     | 91/196 [03:54<04:19,  2.47s/it] 47%|████▋     | 92/196 [03:57<04:18,  2.48s/it] 47%|████▋     | 93/196 [03:59<04:15,  2.48s/it] 48%|████▊     | 94/196 [04:02<04:12,  2.48s/it] 48%|████▊     | 95/196 [04:04<04:10,  2.48s/it] 49%|████▉     | 96/196 [04:07<04:08,  2.48s/it] 49%|████▉     | 97/196 [04:09<04:05,  2.48s/it] 50%|█████     | 98/196 [04:12<04:03,  2.49s/it] 51%|█████     | 99/196 [04:14<04:01,  2.49s/it] 51%|█████     | 100/196 [04:17<03:58,  2.49s/it] 52%|█████▏    | 101/196 [04:19<03:55,  2.48s/it] 52%|█████▏    | 102/196 [04:22<03:53,  2.48s/it] 53%|█████▎    | 103/196 [04:24<03:51,  2.48s/it] 53%|█████▎    | 104/196 [04:27<03:48,  2.49s/it] 54%|█████▎    | 105/196 [04:29<03:46,  2.49s/it] 54%|█████▍    | 106/196 [04:32<03:43,  2.49s/it] 55%|█████▍    | 107/196 [04:34<03:41,  2.49s/it] 55%|█████▌    | 108/196 [04:37<03:38,  2.49s/it] 56%|█████▌    | 109/196 [04:39<03:36,  2.49s/it] 56%|█████▌    | 110/196 [04:42<03:34,  2.49s/it] 57%|█████▋    | 111/196 [04:44<03:31,  2.49s/it] 57%|█████▋    | 112/196 [04:47<03:29,  2.49s/it] 58%|█████▊    | 113/196 [04:49<03:26,  2.48s/it] 58%|█████▊    | 114/196 [04:52<03:23,  2.48s/it] 59%|█████▊    | 115/196 [04:54<03:20,  2.48s/it] 59%|█████▉    | 116/196 [04:56<03:18,  2.48s/it] 60%|█████▉    | 117/196 [04:59<03:15,  2.48s/it] 60%|██████    | 118/196 [05:01<03:13,  2.48s/it] 61%|██████    | 119/196 [05:04<03:11,  2.48s/it] 61%|██████    | 120/196 [05:06<03:08,  2.48s/it] 62%|██████▏   | 121/196 [05:09<03:05,  2.48s/it] 62%|██████▏   | 122/196 [05:11<03:03,  2.48s/it] 63%|██████▎   | 123/196 [05:14<03:00,  2.48s/it] 63%|██████▎   | 124/196 [05:16<02:58,  2.47s/it] 64%|██████▍   | 125/196 [05:19<02:55,  2.47s/it] 64%|██████▍   | 126/196 [05:21<02:53,  2.48s/it] 65%|██████▍   | 127/196 [05:24<02:50,  2.47s/it] 65%|██████▌   | 128/196 [05:26<02:47,  2.47s/it] 66%|██████▌   | 129/196 [05:29<02:45,  2.47s/it] 66%|██████▋   | 130/196 [05:31<02:42,  2.47s/it] 67%|██████▋   | 131/196 [05:34<02:40,  2.47s/it] 67%|██████▋   | 132/196 [05:36<02:37,  2.46s/it] 68%|██████▊   | 133/196 [05:38<02:35,  2.47s/it] 68%|██████▊   | 134/196 [05:41<02:32,  2.47s/it] 69%|██████▉   | 135/196 [05:43<02:30,  2.47s/it] 69%|██████▉   | 136/196 [05:46<02:28,  2.47s/it] 70%|██████▉   | 137/196 [05:48<02:25,  2.47s/it] 70%|███████   | 138/196 [05:51<02:23,  2.47s/it] 71%|███████   | 139/196 [05:53<02:21,  2.47s/it] 71%|███████▏  | 140/196 [05:56<02:18,  2.47s/it] 72%|███████▏  | 141/196 [05:58<02:16,  2.48s/it] 72%|███████▏  | 142/196 [06:01<02:13,  2.48s/it] 73%|███████▎  | 143/196 [06:03<02:11,  2.48s/it] 73%|███████▎  | 144/196 [06:06<02:08,  2.47s/it] 74%|███████▍  | 145/196 [06:08<02:06,  2.48s/it] 74%|███████▍  | 146/196 [06:11<02:03,  2.47s/it] 75%|███████▌  | 147/196 [06:13<02:01,  2.47s/it] 76%|███████▌  | 148/196 [06:16<01:58,  2.47s/it] 76%|███████▌  | 149/196 [06:18<01:56,  2.47s/it] 77%|███████▋  | 150/196 [06:21<01:53,  2.47s/it] 77%|███████▋  | 151/196 [06:23<01:51,  2.47s/it] 78%|███████▊  | 152/196 [06:25<01:48,  2.47s/it] 78%|███████▊  | 153/196 [06:28<01:46,  2.47s/it] 79%|███████▊  | 154/196 [06:30<01:43,  2.47s/it] 79%|███████▉  | 155/196 [06:33<01:41,  2.47s/it] 80%|███████▉  | 156/196 [06:35<01:38,  2.47s/it] 80%|████████  | 157/196 [06:38<01:36,  2.47s/it] 81%|████████  | 158/196 [06:40<01:33,  2.47s/it] 81%|████████  | 159/196 [06:43<01:31,  2.47s/it] 82%|████████▏ | 160/196 [06:45<01:29,  2.48s/it] 82%|████████▏ | 161/196 [06:48<01:26,  2.48s/it] 83%|████████▎ | 162/196 [06:50<01:26,  2.54s/it] 83%|████████▎ | 163/196 [06:53<01:23,  2.52s/it] 84%|████████▎ | 164/196 [06:55<01:20,  2.51s/it] 84%|████████▍ | 165/196 [06:58<01:17,  2.50s/it] 85%|████████▍ | 166/196 [07:00<01:14,  2.50s/it] 85%|████████▌ | 167/196 [07:03<01:12,  2.49s/it] 86%|████████▌ | 168/196 [07:05<01:09,  2.49s/it] 86%|████████▌ | 169/196 [07:08<01:07,  2.49s/it] 87%|████████▋ | 170/196 [07:10<01:04,  2.49s/it] 87%|████████▋ | 171/196 [07:13<01:02,  2.49s/it] 88%|████████▊ | 172/196 [07:15<00:59,  2.48s/it] 88%|████████▊ | 173/196 [07:18<00:57,  2.48s/it] 89%|████████▉ | 174/196 [07:20<00:54,  2.49s/it] 89%|████████▉ | 175/196 [07:23<00:52,  2.49s/it] 90%|████████▉ | 176/196 [07:25<00:49,  2.49s/it] 90%|█████████ | 177/196 [07:28<00:47,  2.48s/it] 91%|█████████ | 178/196 [07:30<00:44,  2.48s/it] 91%|█████████▏| 179/196 [07:33<00:42,  2.48s/it] 92%|█████████▏| 180/196 [07:35<00:39,  2.48s/it] 92%|█████████▏| 181/196 [07:38<00:37,  2.47s/it] 93%|█████████▎| 182/196 [07:40<00:34,  2.47s/it] 93%|█████████▎| 183/196 [07:43<00:32,  2.47s/it] 94%|█████████▍| 184/196 [07:45<00:29,  2.47s/it] 94%|█████████▍| 185/196 [07:47<00:27,  2.47s/it] 95%|█████████▍| 186/196 [07:50<00:24,  2.47s/it] 95%|█████████▌| 187/196 [07:52<00:22,  2.47s/it] 96%|█████████▌| 188/196 [07:55<00:19,  2.47s/it] 96%|█████████▋| 189/196 [07:57<00:17,  2.48s/it] 97%|█████████▋| 190/196 [08:00<00:14,  2.47s/it] 97%|█████████▋| 191/196 [08:02<00:12,  2.47s/it] 98%|█████████▊| 192/196 [08:05<00:09,  2.47s/it] 98%|█████████▊| 193/196 [08:07<00:07,  2.47s/it] 99%|█████████▉| 194/196 [08:10<00:04,  2.47s/it] 99%|█████████▉| 195/196 [08:12<00:02,  2.48s/it]100%|██████████| 196/196 [08:14<00:00,  2.32s/it]100%|██████████| 196/196 [08:14<00:00,  2.52s/it]
Top-1 Accuracy: 0.10%
Top-5 Accuracy: 0.52%
I0429 17:17:33.991986 1121123 utils.py:146] Note: detected 128 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
I0429 17:17:33.992109 1121123 utils.py:149] Note: NumExpr detected 128 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0429 17:17:33.992150 1121123 utils.py:162] NumExpr defaulting to 16 threads.
I0429 17:17:34.116879 1121123 config.py:58] PyTorch version 2.4.0 available.
W0429 17:17:35.671206 1121123 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0429 17:17:35.672031 1121123 hfize_clip.py:43] CLIPConfig {
  "_name_or_path": "../Wparam_dataset/hf_model/openai--clip-vit-large-patch14",
  "architectures": [
    "CLIPModel"
  ],
  "initializer_factor": 1.0,
  "logit_scale_init_value": 2.6592,
  "model_type": "clip",
  "projection_dim": 768,
  "quip_params": {
    "K": 5,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "text_config": {
    "dropout": 0.0,
    "hidden_size": 768,
    "intermediate_size": 3072,
    "model_type": "clip_text_model",
    "num_attention_heads": 12,
    "projection_dim": 768,
    "torch_dtype": "float32"
  },
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "vision_config": {
    "dropout": 0.0,
    "hidden_size": 1024,
    "intermediate_size": 4096,
    "model_type": "clip_vision_model",
    "num_attention_heads": 16,
    "num_hidden_layers": 24,
    "patch_size": 14,
    "projection_dim": 768,
    "torch_dtype": "float32"
  }
}

Some weights of the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 were not used when initializing CLIPModel: ['text_model.encoder.layers.0.mlp.fc1.weight', 'text_model.encoder.layers.0.mlp.fc2.weight', 'text_model.encoder.layers.0.self_attn.k_proj.weight', 'text_model.encoder.layers.0.self_attn.out_proj.weight', 'text_model.encoder.layers.0.self_attn.q_proj.weight', 'text_model.encoder.layers.0.self_attn.v_proj.weight', 'text_model.encoder.layers.1.mlp.fc1.weight', 'text_model.encoder.layers.1.mlp.fc2.weight', 'text_model.encoder.layers.1.self_attn.k_proj.weight', 'text_model.encoder.layers.1.self_attn.out_proj.weight', 'text_model.encoder.layers.1.self_attn.q_proj.weight', 'text_model.encoder.layers.1.self_attn.v_proj.weight', 'text_model.encoder.layers.10.mlp.fc1.weight', 'text_model.encoder.layers.10.mlp.fc2.weight', 'text_model.encoder.layers.10.self_attn.k_proj.weight', 'text_model.encoder.layers.10.self_attn.out_proj.weight', 'text_model.encoder.layers.10.self_attn.q_proj.weight', 'text_model.encoder.layers.10.self_attn.v_proj.weight', 'text_model.encoder.layers.11.mlp.fc1.weight', 'text_model.encoder.layers.11.mlp.fc2.weight', 'text_model.encoder.layers.11.self_attn.k_proj.weight', 'text_model.encoder.layers.11.self_attn.out_proj.weight', 'text_model.encoder.layers.11.self_attn.q_proj.weight', 'text_model.encoder.layers.11.self_attn.v_proj.weight', 'text_model.encoder.layers.2.mlp.fc1.weight', 'text_model.encoder.layers.2.mlp.fc2.weight', 'text_model.encoder.layers.2.self_attn.k_proj.weight', 'text_model.encoder.layers.2.self_attn.out_proj.weight', 'text_model.encoder.layers.2.self_attn.q_proj.weight', 'text_model.encoder.layers.2.self_attn.v_proj.weight', 'text_model.encoder.layers.3.mlp.fc1.weight', 'text_model.encoder.layers.3.mlp.fc2.weight', 'text_model.encoder.layers.3.self_attn.k_proj.weight', 'text_model.encoder.layers.3.self_attn.out_proj.weight', 'text_model.encoder.layers.3.self_attn.q_proj.weight', 'text_model.encoder.layers.3.self_attn.v_proj.weight', 'text_model.encoder.layers.4.mlp.fc1.weight', 'text_model.encoder.layers.4.mlp.fc2.weight', 'text_model.encoder.layers.4.self_attn.k_proj.weight', 'text_model.encoder.layers.4.self_attn.out_proj.weight', 'text_model.encoder.layers.4.self_attn.q_proj.weight', 'text_model.encoder.layers.4.self_attn.v_proj.weight', 'text_model.encoder.layers.5.mlp.fc1.weight', 'text_model.encoder.layers.5.mlp.fc2.weight', 'text_model.encoder.layers.5.self_attn.k_proj.weight', 'text_model.encoder.layers.5.self_attn.out_proj.weight', 'text_model.encoder.layers.5.self_attn.q_proj.weight', 'text_model.encoder.layers.5.self_attn.v_proj.weight', 'text_model.encoder.layers.6.mlp.fc1.weight', 'text_model.encoder.layers.6.mlp.fc2.weight', 'text_model.encoder.layers.6.self_attn.k_proj.weight', 'text_model.encoder.layers.6.self_attn.out_proj.weight', 'text_model.encoder.layers.6.self_attn.q_proj.weight', 'text_model.encoder.layers.6.self_attn.v_proj.weight', 'text_model.encoder.layers.7.mlp.fc1.weight', 'text_model.encoder.layers.7.mlp.fc2.weight', 'text_model.encoder.layers.7.self_attn.k_proj.weight', 'text_model.encoder.layers.7.self_attn.out_proj.weight', 'text_model.encoder.layers.7.self_attn.q_proj.weight', 'text_model.encoder.layers.7.self_attn.v_proj.weight', 'text_model.encoder.layers.8.mlp.fc1.weight', 'text_model.encoder.layers.8.mlp.fc2.weight', 'text_model.encoder.layers.8.self_attn.k_proj.weight', 'text_model.encoder.layers.8.self_attn.out_proj.weight', 'text_model.encoder.layers.8.self_attn.q_proj.weight', 'text_model.encoder.layers.8.self_attn.v_proj.weight', 'text_model.encoder.layers.9.mlp.fc1.weight', 'text_model.encoder.layers.9.mlp.fc2.weight', 'text_model.encoder.layers.9.self_attn.k_proj.weight', 'text_model.encoder.layers.9.self_attn.out_proj.weight', 'text_model.encoder.layers.9.self_attn.q_proj.weight', 'text_model.encoder.layers.9.self_attn.v_proj.weight', 'vision_model.encoder.layers.0.mlp.fc1.weight', 'vision_model.encoder.layers.0.mlp.fc2.weight', 'vision_model.encoder.layers.0.self_attn.k_proj.weight', 'vision_model.encoder.layers.0.self_attn.out_proj.weight', 'vision_model.encoder.layers.0.self_attn.q_proj.weight', 'vision_model.encoder.layers.0.self_attn.v_proj.weight', 'vision_model.encoder.layers.1.mlp.fc1.weight', 'vision_model.encoder.layers.1.mlp.fc2.weight', 'vision_model.encoder.layers.1.self_attn.k_proj.weight', 'vision_model.encoder.layers.1.self_attn.out_proj.weight', 'vision_model.encoder.layers.1.self_attn.q_proj.weight', 'vision_model.encoder.layers.1.self_attn.v_proj.weight', 'vision_model.encoder.layers.10.mlp.fc1.weight', 'vision_model.encoder.layers.10.mlp.fc2.weight', 'vision_model.encoder.layers.10.self_attn.k_proj.weight', 'vision_model.encoder.layers.10.self_attn.out_proj.weight', 'vision_model.encoder.layers.10.self_attn.q_proj.weight', 'vision_model.encoder.layers.10.self_attn.v_proj.weight', 'vision_model.encoder.layers.11.mlp.fc1.weight', 'vision_model.encoder.layers.11.mlp.fc2.weight', 'vision_model.encoder.layers.11.self_attn.k_proj.weight', 'vision_model.encoder.layers.11.self_attn.out_proj.weight', 'vision_model.encoder.layers.11.self_attn.q_proj.weight', 'vision_model.encoder.layers.11.self_attn.v_proj.weight', 'vision_model.encoder.layers.12.mlp.fc1.weight', 'vision_model.encoder.layers.12.mlp.fc2.weight', 'vision_model.encoder.layers.12.self_attn.k_proj.weight', 'vision_model.encoder.layers.12.self_attn.out_proj.weight', 'vision_model.encoder.layers.12.self_attn.q_proj.weight', 'vision_model.encoder.layers.12.self_attn.v_proj.weight', 'vision_model.encoder.layers.13.mlp.fc1.weight', 'vision_model.encoder.layers.13.mlp.fc2.weight', 'vision_model.encoder.layers.13.self_attn.k_proj.weight', 'vision_model.encoder.layers.13.self_attn.out_proj.weight', 'vision_model.encoder.layers.13.self_attn.q_proj.weight', 'vision_model.encoder.layers.13.self_attn.v_proj.weight', 'vision_model.encoder.layers.14.mlp.fc1.weight', 'vision_model.encoder.layers.14.mlp.fc2.weight', 'vision_model.encoder.layers.14.self_attn.k_proj.weight', 'vision_model.encoder.layers.14.self_attn.out_proj.weight', 'vision_model.encoder.layers.14.self_attn.q_proj.weight', 'vision_model.encoder.layers.14.self_attn.v_proj.weight', 'vision_model.encoder.layers.15.mlp.fc1.weight', 'vision_model.encoder.layers.15.mlp.fc2.weight', 'vision_model.encoder.layers.15.self_attn.k_proj.weight', 'vision_model.encoder.layers.15.self_attn.out_proj.weight', 'vision_model.encoder.layers.15.self_attn.q_proj.weight', 'vision_model.encoder.layers.15.self_attn.v_proj.weight', 'vision_model.encoder.layers.16.mlp.fc1.weight', 'vision_model.encoder.layers.16.mlp.fc2.weight', 'vision_model.encoder.layers.16.self_attn.k_proj.weight', 'vision_model.encoder.layers.16.self_attn.out_proj.weight', 'vision_model.encoder.layers.16.self_attn.q_proj.weight', 'vision_model.encoder.layers.16.self_attn.v_proj.weight', 'vision_model.encoder.layers.17.mlp.fc1.weight', 'vision_model.encoder.layers.17.mlp.fc2.weight', 'vision_model.encoder.layers.17.self_attn.k_proj.weight', 'vision_model.encoder.layers.17.self_attn.out_proj.weight', 'vision_model.encoder.layers.17.self_attn.q_proj.weight', 'vision_model.encoder.layers.17.self_attn.v_proj.weight', 'vision_model.encoder.layers.18.mlp.fc1.weight', 'vision_model.encoder.layers.18.mlp.fc2.weight', 'vision_model.encoder.layers.18.self_attn.k_proj.weight', 'vision_model.encoder.layers.18.self_attn.out_proj.weight', 'vision_model.encoder.layers.18.self_attn.q_proj.weight', 'vision_model.encoder.layers.18.self_attn.v_proj.weight', 'vision_model.encoder.layers.19.mlp.fc1.weight', 'vision_model.encoder.layers.19.mlp.fc2.weight', 'vision_model.encoder.layers.19.self_attn.k_proj.weight', 'vision_model.encoder.layers.19.self_attn.out_proj.weight', 'vision_model.encoder.layers.19.self_attn.q_proj.weight', 'vision_model.encoder.layers.19.self_attn.v_proj.weight', 'vision_model.encoder.layers.2.mlp.fc1.weight', 'vision_model.encoder.layers.2.mlp.fc2.weight', 'vision_model.encoder.layers.2.self_attn.k_proj.weight', 'vision_model.encoder.layers.2.self_attn.out_proj.weight', 'vision_model.encoder.layers.2.self_attn.q_proj.weight', 'vision_model.encoder.layers.2.self_attn.v_proj.weight', 'vision_model.encoder.layers.20.mlp.fc1.weight', 'vision_model.encoder.layers.20.mlp.fc2.weight', 'vision_model.encoder.layers.20.self_attn.k_proj.weight', 'vision_model.encoder.layers.20.self_attn.out_proj.weight', 'vision_model.encoder.layers.20.self_attn.q_proj.weight', 'vision_model.encoder.layers.20.self_attn.v_proj.weight', 'vision_model.encoder.layers.21.mlp.fc1.weight', 'vision_model.encoder.layers.21.mlp.fc2.weight', 'vision_model.encoder.layers.21.self_attn.k_proj.weight', 'vision_model.encoder.layers.21.self_attn.out_proj.weight', 'vision_model.encoder.layers.21.self_attn.q_proj.weight', 'vision_model.encoder.layers.21.self_attn.v_proj.weight', 'vision_model.encoder.layers.22.mlp.fc1.weight', 'vision_model.encoder.layers.22.mlp.fc2.weight', 'vision_model.encoder.layers.22.self_attn.k_proj.weight', 'vision_model.encoder.layers.22.self_attn.out_proj.weight', 'vision_model.encoder.layers.22.self_attn.q_proj.weight', 'vision_model.encoder.layers.22.self_attn.v_proj.weight', 'vision_model.encoder.layers.23.mlp.fc1.weight', 'vision_model.encoder.layers.23.mlp.fc2.weight', 'vision_model.encoder.layers.23.self_attn.k_proj.weight', 'vision_model.encoder.layers.23.self_attn.out_proj.weight', 'vision_model.encoder.layers.23.self_attn.q_proj.weight', 'vision_model.encoder.layers.23.self_attn.v_proj.weight', 'vision_model.encoder.layers.3.mlp.fc1.weight', 'vision_model.encoder.layers.3.mlp.fc2.weight', 'vision_model.encoder.layers.3.self_attn.k_proj.weight', 'vision_model.encoder.layers.3.self_attn.out_proj.weight', 'vision_model.encoder.layers.3.self_attn.q_proj.weight', 'vision_model.encoder.layers.3.self_attn.v_proj.weight', 'vision_model.encoder.layers.4.mlp.fc1.weight', 'vision_model.encoder.layers.4.mlp.fc2.weight', 'vision_model.encoder.layers.4.self_attn.k_proj.weight', 'vision_model.encoder.layers.4.self_attn.out_proj.weight', 'vision_model.encoder.layers.4.self_attn.q_proj.weight', 'vision_model.encoder.layers.4.self_attn.v_proj.weight', 'vision_model.encoder.layers.5.mlp.fc1.weight', 'vision_model.encoder.layers.5.mlp.fc2.weight', 'vision_model.encoder.layers.5.self_attn.k_proj.weight', 'vision_model.encoder.layers.5.self_attn.out_proj.weight', 'vision_model.encoder.layers.5.self_attn.q_proj.weight', 'vision_model.encoder.layers.5.self_attn.v_proj.weight', 'vision_model.encoder.layers.6.mlp.fc1.weight', 'vision_model.encoder.layers.6.mlp.fc2.weight', 'vision_model.encoder.layers.6.self_attn.k_proj.weight', 'vision_model.encoder.layers.6.self_attn.out_proj.weight', 'vision_model.encoder.layers.6.self_attn.q_proj.weight', 'vision_model.encoder.layers.6.self_attn.v_proj.weight', 'vision_model.encoder.layers.7.mlp.fc1.weight', 'vision_model.encoder.layers.7.mlp.fc2.weight', 'vision_model.encoder.layers.7.self_attn.k_proj.weight', 'vision_model.encoder.layers.7.self_attn.out_proj.weight', 'vision_model.encoder.layers.7.self_attn.q_proj.weight', 'vision_model.encoder.layers.7.self_attn.v_proj.weight', 'vision_model.encoder.layers.8.mlp.fc1.weight', 'vision_model.encoder.layers.8.mlp.fc2.weight', 'vision_model.encoder.layers.8.self_attn.k_proj.weight', 'vision_model.encoder.layers.8.self_attn.out_proj.weight', 'vision_model.encoder.layers.8.self_attn.q_proj.weight', 'vision_model.encoder.layers.8.self_attn.v_proj.weight', 'vision_model.encoder.layers.9.mlp.fc1.weight', 'vision_model.encoder.layers.9.mlp.fc2.weight', 'vision_model.encoder.layers.9.self_attn.k_proj.weight', 'vision_model.encoder.layers.9.self_attn.out_proj.weight', 'vision_model.encoder.layers.9.self_attn.q_proj.weight', 'vision_model.encoder.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing CLIPModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing CLIPModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of CLIPModel were not initialized from the model checkpoint at ../Wparam_dataset/hf_model/openai--clip-vit-large-patch14 and are newly initialized: ['text_model.encoder.layers.0.mlp.fc1.SU', 'text_model.encoder.layers.0.mlp.fc1.SV', 'text_model.encoder.layers.0.mlp.fc1.rcp', 'text_model.encoder.layers.0.mlp.fc1.tlut', 'text_model.encoder.layers.0.mlp.fc1.tp_rank', 'text_model.encoder.layers.0.mlp.fc1.trellis', 'text_model.encoder.layers.0.mlp.fc2.SU', 'text_model.encoder.layers.0.mlp.fc2.SV', 'text_model.encoder.layers.0.mlp.fc2.rcp', 'text_model.encoder.layers.0.mlp.fc2.tlut', 'text_model.encoder.layers.0.mlp.fc2.tp_rank', 'text_model.encoder.layers.0.mlp.fc2.trellis', 'text_model.encoder.layers.0.self_attn.k_proj.SU', 'text_model.encoder.layers.0.self_attn.k_proj.SV', 'text_model.encoder.layers.0.self_attn.k_proj.rcp', 'text_model.encoder.layers.0.self_attn.k_proj.tlut', 'text_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.k_proj.trellis', 'text_model.encoder.layers.0.self_attn.out_proj.SU', 'text_model.encoder.layers.0.self_attn.out_proj.SV', 'text_model.encoder.layers.0.self_attn.out_proj.rcp', 'text_model.encoder.layers.0.self_attn.out_proj.tlut', 'text_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.out_proj.trellis', 'text_model.encoder.layers.0.self_attn.q_proj.SU', 'text_model.encoder.layers.0.self_attn.q_proj.SV', 'text_model.encoder.layers.0.self_attn.q_proj.rcp', 'text_model.encoder.layers.0.self_attn.q_proj.tlut', 'text_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.q_proj.trellis', 'text_model.encoder.layers.0.self_attn.v_proj.SU', 'text_model.encoder.layers.0.self_attn.v_proj.SV', 'text_model.encoder.layers.0.self_attn.v_proj.rcp', 'text_model.encoder.layers.0.self_attn.v_proj.tlut', 'text_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.0.self_attn.v_proj.trellis', 'text_model.encoder.layers.1.mlp.fc1.SU', 'text_model.encoder.layers.1.mlp.fc1.SV', 'text_model.encoder.layers.1.mlp.fc1.rcp', 'text_model.encoder.layers.1.mlp.fc1.tlut', 'text_model.encoder.layers.1.mlp.fc1.tp_rank', 'text_model.encoder.layers.1.mlp.fc1.trellis', 'text_model.encoder.layers.1.mlp.fc2.SU', 'text_model.encoder.layers.1.mlp.fc2.SV', 'text_model.encoder.layers.1.mlp.fc2.rcp', 'text_model.encoder.layers.1.mlp.fc2.tlut', 'text_model.encoder.layers.1.mlp.fc2.tp_rank', 'text_model.encoder.layers.1.mlp.fc2.trellis', 'text_model.encoder.layers.1.self_attn.k_proj.SU', 'text_model.encoder.layers.1.self_attn.k_proj.SV', 'text_model.encoder.layers.1.self_attn.k_proj.rcp', 'text_model.encoder.layers.1.self_attn.k_proj.tlut', 'text_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.k_proj.trellis', 'text_model.encoder.layers.1.self_attn.out_proj.SU', 'text_model.encoder.layers.1.self_attn.out_proj.SV', 'text_model.encoder.layers.1.self_attn.out_proj.rcp', 'text_model.encoder.layers.1.self_attn.out_proj.tlut', 'text_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.out_proj.trellis', 'text_model.encoder.layers.1.self_attn.q_proj.SU', 'text_model.encoder.layers.1.self_attn.q_proj.SV', 'text_model.encoder.layers.1.self_attn.q_proj.rcp', 'text_model.encoder.layers.1.self_attn.q_proj.tlut', 'text_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.q_proj.trellis', 'text_model.encoder.layers.1.self_attn.v_proj.SU', 'text_model.encoder.layers.1.self_attn.v_proj.SV', 'text_model.encoder.layers.1.self_attn.v_proj.rcp', 'text_model.encoder.layers.1.self_attn.v_proj.tlut', 'text_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.1.self_attn.v_proj.trellis', 'text_model.encoder.layers.10.mlp.fc1.SU', 'text_model.encoder.layers.10.mlp.fc1.SV', 'text_model.encoder.layers.10.mlp.fc1.rcp', 'text_model.encoder.layers.10.mlp.fc1.tlut', 'text_model.encoder.layers.10.mlp.fc1.tp_rank', 'text_model.encoder.layers.10.mlp.fc1.trellis', 'text_model.encoder.layers.10.mlp.fc2.SU', 'text_model.encoder.layers.10.mlp.fc2.SV', 'text_model.encoder.layers.10.mlp.fc2.rcp', 'text_model.encoder.layers.10.mlp.fc2.tlut', 'text_model.encoder.layers.10.mlp.fc2.tp_rank', 'text_model.encoder.layers.10.mlp.fc2.trellis', 'text_model.encoder.layers.10.self_attn.k_proj.SU', 'text_model.encoder.layers.10.self_attn.k_proj.SV', 'text_model.encoder.layers.10.self_attn.k_proj.rcp', 'text_model.encoder.layers.10.self_attn.k_proj.tlut', 'text_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.k_proj.trellis', 'text_model.encoder.layers.10.self_attn.out_proj.SU', 'text_model.encoder.layers.10.self_attn.out_proj.SV', 'text_model.encoder.layers.10.self_attn.out_proj.rcp', 'text_model.encoder.layers.10.self_attn.out_proj.tlut', 'text_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.out_proj.trellis', 'text_model.encoder.layers.10.self_attn.q_proj.SU', 'text_model.encoder.layers.10.self_attn.q_proj.SV', 'text_model.encoder.layers.10.self_attn.q_proj.rcp', 'text_model.encoder.layers.10.self_attn.q_proj.tlut', 'text_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.q_proj.trellis', 'text_model.encoder.layers.10.self_attn.v_proj.SU', 'text_model.encoder.layers.10.self_attn.v_proj.SV', 'text_model.encoder.layers.10.self_attn.v_proj.rcp', 'text_model.encoder.layers.10.self_attn.v_proj.tlut', 'text_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.10.self_attn.v_proj.trellis', 'text_model.encoder.layers.11.mlp.fc1.SU', 'text_model.encoder.layers.11.mlp.fc1.SV', 'text_model.encoder.layers.11.mlp.fc1.rcp', 'text_model.encoder.layers.11.mlp.fc1.tlut', 'text_model.encoder.layers.11.mlp.fc1.tp_rank', 'text_model.encoder.layers.11.mlp.fc1.trellis', 'text_model.encoder.layers.11.mlp.fc2.SU', 'text_model.encoder.layers.11.mlp.fc2.SV', 'text_model.encoder.layers.11.mlp.fc2.rcp', 'text_model.encoder.layers.11.mlp.fc2.tlut', 'text_model.encoder.layers.11.mlp.fc2.tp_rank', 'text_model.encoder.layers.11.mlp.fc2.trellis', 'text_model.encoder.layers.11.self_attn.k_proj.SU', 'text_model.encoder.layers.11.self_attn.k_proj.SV', 'text_model.encoder.layers.11.self_attn.k_proj.rcp', 'text_model.encoder.layers.11.self_attn.k_proj.tlut', 'text_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.k_proj.trellis', 'text_model.encoder.layers.11.self_attn.out_proj.SU', 'text_model.encoder.layers.11.self_attn.out_proj.SV', 'text_model.encoder.layers.11.self_attn.out_proj.rcp', 'text_model.encoder.layers.11.self_attn.out_proj.tlut', 'text_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.out_proj.trellis', 'text_model.encoder.layers.11.self_attn.q_proj.SU', 'text_model.encoder.layers.11.self_attn.q_proj.SV', 'text_model.encoder.layers.11.self_attn.q_proj.rcp', 'text_model.encoder.layers.11.self_attn.q_proj.tlut', 'text_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.q_proj.trellis', 'text_model.encoder.layers.11.self_attn.v_proj.SU', 'text_model.encoder.layers.11.self_attn.v_proj.SV', 'text_model.encoder.layers.11.self_attn.v_proj.rcp', 'text_model.encoder.layers.11.self_attn.v_proj.tlut', 'text_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.11.self_attn.v_proj.trellis', 'text_model.encoder.layers.2.mlp.fc1.SU', 'text_model.encoder.layers.2.mlp.fc1.SV', 'text_model.encoder.layers.2.mlp.fc1.rcp', 'text_model.encoder.layers.2.mlp.fc1.tlut', 'text_model.encoder.layers.2.mlp.fc1.tp_rank', 'text_model.encoder.layers.2.mlp.fc1.trellis', 'text_model.encoder.layers.2.mlp.fc2.SU', 'text_model.encoder.layers.2.mlp.fc2.SV', 'text_model.encoder.layers.2.mlp.fc2.rcp', 'text_model.encoder.layers.2.mlp.fc2.tlut', 'text_model.encoder.layers.2.mlp.fc2.tp_rank', 'text_model.encoder.layers.2.mlp.fc2.trellis', 'text_model.encoder.layers.2.self_attn.k_proj.SU', 'text_model.encoder.layers.2.self_attn.k_proj.SV', 'text_model.encoder.layers.2.self_attn.k_proj.rcp', 'text_model.encoder.layers.2.self_attn.k_proj.tlut', 'text_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.k_proj.trellis', 'text_model.encoder.layers.2.self_attn.out_proj.SU', 'text_model.encoder.layers.2.self_attn.out_proj.SV', 'text_model.encoder.layers.2.self_attn.out_proj.rcp', 'text_model.encoder.layers.2.self_attn.out_proj.tlut', 'text_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.out_proj.trellis', 'text_model.encoder.layers.2.self_attn.q_proj.SU', 'text_model.encoder.layers.2.self_attn.q_proj.SV', 'text_model.encoder.layers.2.self_attn.q_proj.rcp', 'text_model.encoder.layers.2.self_attn.q_proj.tlut', 'text_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.q_proj.trellis', 'text_model.encoder.layers.2.self_attn.v_proj.SU', 'text_model.encoder.layers.2.self_attn.v_proj.SV', 'text_model.encoder.layers.2.self_attn.v_proj.rcp', 'text_model.encoder.layers.2.self_attn.v_proj.tlut', 'text_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.2.self_attn.v_proj.trellis', 'text_model.encoder.layers.3.mlp.fc1.SU', 'text_model.encoder.layers.3.mlp.fc1.SV', 'text_model.encoder.layers.3.mlp.fc1.rcp', 'text_model.encoder.layers.3.mlp.fc1.tlut', 'text_model.encoder.layers.3.mlp.fc1.tp_rank', 'text_model.encoder.layers.3.mlp.fc1.trellis', 'text_model.encoder.layers.3.mlp.fc2.SU', 'text_model.encoder.layers.3.mlp.fc2.SV', 'text_model.encoder.layers.3.mlp.fc2.rcp', 'text_model.encoder.layers.3.mlp.fc2.tlut', 'text_model.encoder.layers.3.mlp.fc2.tp_rank', 'text_model.encoder.layers.3.mlp.fc2.trellis', 'text_model.encoder.layers.3.self_attn.k_proj.SU', 'text_model.encoder.layers.3.self_attn.k_proj.SV', 'text_model.encoder.layers.3.self_attn.k_proj.rcp', 'text_model.encoder.layers.3.self_attn.k_proj.tlut', 'text_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.k_proj.trellis', 'text_model.encoder.layers.3.self_attn.out_proj.SU', 'text_model.encoder.layers.3.self_attn.out_proj.SV', 'text_model.encoder.layers.3.self_attn.out_proj.rcp', 'text_model.encoder.layers.3.self_attn.out_proj.tlut', 'text_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.out_proj.trellis', 'text_model.encoder.layers.3.self_attn.q_proj.SU', 'text_model.encoder.layers.3.self_attn.q_proj.SV', 'text_model.encoder.layers.3.self_attn.q_proj.rcp', 'text_model.encoder.layers.3.self_attn.q_proj.tlut', 'text_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.q_proj.trellis', 'text_model.encoder.layers.3.self_attn.v_proj.SU', 'text_model.encoder.layers.3.self_attn.v_proj.SV', 'text_model.encoder.layers.3.self_attn.v_proj.rcp', 'text_model.encoder.layers.3.self_attn.v_proj.tlut', 'text_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.3.self_attn.v_proj.trellis', 'text_model.encoder.layers.4.mlp.fc1.SU', 'text_model.encoder.layers.4.mlp.fc1.SV', 'text_model.encoder.layers.4.mlp.fc1.rcp', 'text_model.encoder.layers.4.mlp.fc1.tlut', 'text_model.encoder.layers.4.mlp.fc1.tp_rank', 'text_model.encoder.layers.4.mlp.fc1.trellis', 'text_model.encoder.layers.4.mlp.fc2.SU', 'text_model.encoder.layers.4.mlp.fc2.SV', 'text_model.encoder.layers.4.mlp.fc2.rcp', 'text_model.encoder.layers.4.mlp.fc2.tlut', 'text_model.encoder.layers.4.mlp.fc2.tp_rank', 'text_model.encoder.layers.4.mlp.fc2.trellis', 'text_model.encoder.layers.4.self_attn.k_proj.SU', 'text_model.encoder.layers.4.self_attn.k_proj.SV', 'text_model.encoder.layers.4.self_attn.k_proj.rcp', 'text_model.encoder.layers.4.self_attn.k_proj.tlut', 'text_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.k_proj.trellis', 'text_model.encoder.layers.4.self_attn.out_proj.SU', 'text_model.encoder.layers.4.self_attn.out_proj.SV', 'text_model.encoder.layers.4.self_attn.out_proj.rcp', 'text_model.encoder.layers.4.self_attn.out_proj.tlut', 'text_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.out_proj.trellis', 'text_model.encoder.layers.4.self_attn.q_proj.SU', 'text_model.encoder.layers.4.self_attn.q_proj.SV', 'text_model.encoder.layers.4.self_attn.q_proj.rcp', 'text_model.encoder.layers.4.self_attn.q_proj.tlut', 'text_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.q_proj.trellis', 'text_model.encoder.layers.4.self_attn.v_proj.SU', 'text_model.encoder.layers.4.self_attn.v_proj.SV', 'text_model.encoder.layers.4.self_attn.v_proj.rcp', 'text_model.encoder.layers.4.self_attn.v_proj.tlut', 'text_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.4.self_attn.v_proj.trellis', 'text_model.encoder.layers.5.mlp.fc1.SU', 'text_model.encoder.layers.5.mlp.fc1.SV', 'text_model.encoder.layers.5.mlp.fc1.rcp', 'text_model.encoder.layers.5.mlp.fc1.tlut', 'text_model.encoder.layers.5.mlp.fc1.tp_rank', 'text_model.encoder.layers.5.mlp.fc1.trellis', 'text_model.encoder.layers.5.mlp.fc2.SU', 'text_model.encoder.layers.5.mlp.fc2.SV', 'text_model.encoder.layers.5.mlp.fc2.rcp', 'text_model.encoder.layers.5.mlp.fc2.tlut', 'text_model.encoder.layers.5.mlp.fc2.tp_rank', 'text_model.encoder.layers.5.mlp.fc2.trellis', 'text_model.encoder.layers.5.self_attn.k_proj.SU', 'text_model.encoder.layers.5.self_attn.k_proj.SV', 'text_model.encoder.layers.5.self_attn.k_proj.rcp', 'text_model.encoder.layers.5.self_attn.k_proj.tlut', 'text_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.k_proj.trellis', 'text_model.encoder.layers.5.self_attn.out_proj.SU', 'text_model.encoder.layers.5.self_attn.out_proj.SV', 'text_model.encoder.layers.5.self_attn.out_proj.rcp', 'text_model.encoder.layers.5.self_attn.out_proj.tlut', 'text_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.out_proj.trellis', 'text_model.encoder.layers.5.self_attn.q_proj.SU', 'text_model.encoder.layers.5.self_attn.q_proj.SV', 'text_model.encoder.layers.5.self_attn.q_proj.rcp', 'text_model.encoder.layers.5.self_attn.q_proj.tlut', 'text_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.q_proj.trellis', 'text_model.encoder.layers.5.self_attn.v_proj.SU', 'text_model.encoder.layers.5.self_attn.v_proj.SV', 'text_model.encoder.layers.5.self_attn.v_proj.rcp', 'text_model.encoder.layers.5.self_attn.v_proj.tlut', 'text_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.5.self_attn.v_proj.trellis', 'text_model.encoder.layers.6.mlp.fc1.SU', 'text_model.encoder.layers.6.mlp.fc1.SV', 'text_model.encoder.layers.6.mlp.fc1.rcp', 'text_model.encoder.layers.6.mlp.fc1.tlut', 'text_model.encoder.layers.6.mlp.fc1.tp_rank', 'text_model.encoder.layers.6.mlp.fc1.trellis', 'text_model.encoder.layers.6.mlp.fc2.SU', 'text_model.encoder.layers.6.mlp.fc2.SV', 'text_model.encoder.layers.6.mlp.fc2.rcp', 'text_model.encoder.layers.6.mlp.fc2.tlut', 'text_model.encoder.layers.6.mlp.fc2.tp_rank', 'text_model.encoder.layers.6.mlp.fc2.trellis', 'text_model.encoder.layers.6.self_attn.k_proj.SU', 'text_model.encoder.layers.6.self_attn.k_proj.SV', 'text_model.encoder.layers.6.self_attn.k_proj.rcp', 'text_model.encoder.layers.6.self_attn.k_proj.tlut', 'text_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.k_proj.trellis', 'text_model.encoder.layers.6.self_attn.out_proj.SU', 'text_model.encoder.layers.6.self_attn.out_proj.SV', 'text_model.encoder.layers.6.self_attn.out_proj.rcp', 'text_model.encoder.layers.6.self_attn.out_proj.tlut', 'text_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.out_proj.trellis', 'text_model.encoder.layers.6.self_attn.q_proj.SU', 'text_model.encoder.layers.6.self_attn.q_proj.SV', 'text_model.encoder.layers.6.self_attn.q_proj.rcp', 'text_model.encoder.layers.6.self_attn.q_proj.tlut', 'text_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.q_proj.trellis', 'text_model.encoder.layers.6.self_attn.v_proj.SU', 'text_model.encoder.layers.6.self_attn.v_proj.SV', 'text_model.encoder.layers.6.self_attn.v_proj.rcp', 'text_model.encoder.layers.6.self_attn.v_proj.tlut', 'text_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.6.self_attn.v_proj.trellis', 'text_model.encoder.layers.7.mlp.fc1.SU', 'text_model.encoder.layers.7.mlp.fc1.SV', 'text_model.encoder.layers.7.mlp.fc1.rcp', 'text_model.encoder.layers.7.mlp.fc1.tlut', 'text_model.encoder.layers.7.mlp.fc1.tp_rank', 'text_model.encoder.layers.7.mlp.fc1.trellis', 'text_model.encoder.layers.7.mlp.fc2.SU', 'text_model.encoder.layers.7.mlp.fc2.SV', 'text_model.encoder.layers.7.mlp.fc2.rcp', 'text_model.encoder.layers.7.mlp.fc2.tlut', 'text_model.encoder.layers.7.mlp.fc2.tp_rank', 'text_model.encoder.layers.7.mlp.fc2.trellis', 'text_model.encoder.layers.7.self_attn.k_proj.SU', 'text_model.encoder.layers.7.self_attn.k_proj.SV', 'text_model.encoder.layers.7.self_attn.k_proj.rcp', 'text_model.encoder.layers.7.self_attn.k_proj.tlut', 'text_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.k_proj.trellis', 'text_model.encoder.layers.7.self_attn.out_proj.SU', 'text_model.encoder.layers.7.self_attn.out_proj.SV', 'text_model.encoder.layers.7.self_attn.out_proj.rcp', 'text_model.encoder.layers.7.self_attn.out_proj.tlut', 'text_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.out_proj.trellis', 'text_model.encoder.layers.7.self_attn.q_proj.SU', 'text_model.encoder.layers.7.self_attn.q_proj.SV', 'text_model.encoder.layers.7.self_attn.q_proj.rcp', 'text_model.encoder.layers.7.self_attn.q_proj.tlut', 'text_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.q_proj.trellis', 'text_model.encoder.layers.7.self_attn.v_proj.SU', 'text_model.encoder.layers.7.self_attn.v_proj.SV', 'text_model.encoder.layers.7.self_attn.v_proj.rcp', 'text_model.encoder.layers.7.self_attn.v_proj.tlut', 'text_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.7.self_attn.v_proj.trellis', 'text_model.encoder.layers.8.mlp.fc1.SU', 'text_model.encoder.layers.8.mlp.fc1.SV', 'text_model.encoder.layers.8.mlp.fc1.rcp', 'text_model.encoder.layers.8.mlp.fc1.tlut', 'text_model.encoder.layers.8.mlp.fc1.tp_rank', 'text_model.encoder.layers.8.mlp.fc1.trellis', 'text_model.encoder.layers.8.mlp.fc2.SU', 'text_model.encoder.layers.8.mlp.fc2.SV', 'text_model.encoder.layers.8.mlp.fc2.rcp', 'text_model.encoder.layers.8.mlp.fc2.tlut', 'text_model.encoder.layers.8.mlp.fc2.tp_rank', 'text_model.encoder.layers.8.mlp.fc2.trellis', 'text_model.encoder.layers.8.self_attn.k_proj.SU', 'text_model.encoder.layers.8.self_attn.k_proj.SV', 'text_model.encoder.layers.8.self_attn.k_proj.rcp', 'text_model.encoder.layers.8.self_attn.k_proj.tlut', 'text_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.k_proj.trellis', 'text_model.encoder.layers.8.self_attn.out_proj.SU', 'text_model.encoder.layers.8.self_attn.out_proj.SV', 'text_model.encoder.layers.8.self_attn.out_proj.rcp', 'text_model.encoder.layers.8.self_attn.out_proj.tlut', 'text_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.out_proj.trellis', 'text_model.encoder.layers.8.self_attn.q_proj.SU', 'text_model.encoder.layers.8.self_attn.q_proj.SV', 'text_model.encoder.layers.8.self_attn.q_proj.rcp', 'text_model.encoder.layers.8.self_attn.q_proj.tlut', 'text_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.q_proj.trellis', 'text_model.encoder.layers.8.self_attn.v_proj.SU', 'text_model.encoder.layers.8.self_attn.v_proj.SV', 'text_model.encoder.layers.8.self_attn.v_proj.rcp', 'text_model.encoder.layers.8.self_attn.v_proj.tlut', 'text_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.8.self_attn.v_proj.trellis', 'text_model.encoder.layers.9.mlp.fc1.SU', 'text_model.encoder.layers.9.mlp.fc1.SV', 'text_model.encoder.layers.9.mlp.fc1.rcp', 'text_model.encoder.layers.9.mlp.fc1.tlut', 'text_model.encoder.layers.9.mlp.fc1.tp_rank', 'text_model.encoder.layers.9.mlp.fc1.trellis', 'text_model.encoder.layers.9.mlp.fc2.SU', 'text_model.encoder.layers.9.mlp.fc2.SV', 'text_model.encoder.layers.9.mlp.fc2.rcp', 'text_model.encoder.layers.9.mlp.fc2.tlut', 'text_model.encoder.layers.9.mlp.fc2.tp_rank', 'text_model.encoder.layers.9.mlp.fc2.trellis', 'text_model.encoder.layers.9.self_attn.k_proj.SU', 'text_model.encoder.layers.9.self_attn.k_proj.SV', 'text_model.encoder.layers.9.self_attn.k_proj.rcp', 'text_model.encoder.layers.9.self_attn.k_proj.tlut', 'text_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.k_proj.trellis', 'text_model.encoder.layers.9.self_attn.out_proj.SU', 'text_model.encoder.layers.9.self_attn.out_proj.SV', 'text_model.encoder.layers.9.self_attn.out_proj.rcp', 'text_model.encoder.layers.9.self_attn.out_proj.tlut', 'text_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.out_proj.trellis', 'text_model.encoder.layers.9.self_attn.q_proj.SU', 'text_model.encoder.layers.9.self_attn.q_proj.SV', 'text_model.encoder.layers.9.self_attn.q_proj.rcp', 'text_model.encoder.layers.9.self_attn.q_proj.tlut', 'text_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.q_proj.trellis', 'text_model.encoder.layers.9.self_attn.v_proj.SU', 'text_model.encoder.layers.9.self_attn.v_proj.SV', 'text_model.encoder.layers.9.self_attn.v_proj.rcp', 'text_model.encoder.layers.9.self_attn.v_proj.tlut', 'text_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'text_model.encoder.layers.9.self_attn.v_proj.trellis', 'vision_model.encoder.layers.0.mlp.fc1.SU', 'vision_model.encoder.layers.0.mlp.fc1.SV', 'vision_model.encoder.layers.0.mlp.fc1.rcp', 'vision_model.encoder.layers.0.mlp.fc1.tlut', 'vision_model.encoder.layers.0.mlp.fc1.tp_rank', 'vision_model.encoder.layers.0.mlp.fc1.trellis', 'vision_model.encoder.layers.0.mlp.fc2.SU', 'vision_model.encoder.layers.0.mlp.fc2.SV', 'vision_model.encoder.layers.0.mlp.fc2.rcp', 'vision_model.encoder.layers.0.mlp.fc2.tlut', 'vision_model.encoder.layers.0.mlp.fc2.tp_rank', 'vision_model.encoder.layers.0.mlp.fc2.trellis', 'vision_model.encoder.layers.0.self_attn.k_proj.SU', 'vision_model.encoder.layers.0.self_attn.k_proj.SV', 'vision_model.encoder.layers.0.self_attn.k_proj.rcp', 'vision_model.encoder.layers.0.self_attn.k_proj.tlut', 'vision_model.encoder.layers.0.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.k_proj.trellis', 'vision_model.encoder.layers.0.self_attn.out_proj.SU', 'vision_model.encoder.layers.0.self_attn.out_proj.SV', 'vision_model.encoder.layers.0.self_attn.out_proj.rcp', 'vision_model.encoder.layers.0.self_attn.out_proj.tlut', 'vision_model.encoder.layers.0.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.out_proj.trellis', 'vision_model.encoder.layers.0.self_attn.q_proj.SU', 'vision_model.encoder.layers.0.self_attn.q_proj.SV', 'vision_model.encoder.layers.0.self_attn.q_proj.rcp', 'vision_model.encoder.layers.0.self_attn.q_proj.tlut', 'vision_model.encoder.layers.0.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.q_proj.trellis', 'vision_model.encoder.layers.0.self_attn.v_proj.SU', 'vision_model.encoder.layers.0.self_attn.v_proj.SV', 'vision_model.encoder.layers.0.self_attn.v_proj.rcp', 'vision_model.encoder.layers.0.self_attn.v_proj.tlut', 'vision_model.encoder.layers.0.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.0.self_attn.v_proj.trellis', 'vision_model.encoder.layers.1.mlp.fc1.SU', 'vision_model.encoder.layers.1.mlp.fc1.SV', 'vision_model.encoder.layers.1.mlp.fc1.rcp', 'vision_model.encoder.layers.1.mlp.fc1.tlut', 'vision_model.encoder.layers.1.mlp.fc1.tp_rank', 'vision_model.encoder.layers.1.mlp.fc1.trellis', 'vision_model.encoder.layers.1.mlp.fc2.SU', 'vision_model.encoder.layers.1.mlp.fc2.SV', 'vision_model.encoder.layers.1.mlp.fc2.rcp', 'vision_model.encoder.layers.1.mlp.fc2.tlut', 'vision_model.encoder.layers.1.mlp.fc2.tp_rank', 'vision_model.encoder.layers.1.mlp.fc2.trellis', 'vision_model.encoder.layers.1.self_attn.k_proj.SU', 'vision_model.encoder.layers.1.self_attn.k_proj.SV', 'vision_model.encoder.layers.1.self_attn.k_proj.rcp', 'vision_model.encoder.layers.1.self_attn.k_proj.tlut', 'vision_model.encoder.layers.1.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.k_proj.trellis', 'vision_model.encoder.layers.1.self_attn.out_proj.SU', 'vision_model.encoder.layers.1.self_attn.out_proj.SV', 'vision_model.encoder.layers.1.self_attn.out_proj.rcp', 'vision_model.encoder.layers.1.self_attn.out_proj.tlut', 'vision_model.encoder.layers.1.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.out_proj.trellis', 'vision_model.encoder.layers.1.self_attn.q_proj.SU', 'vision_model.encoder.layers.1.self_attn.q_proj.SV', 'vision_model.encoder.layers.1.self_attn.q_proj.rcp', 'vision_model.encoder.layers.1.self_attn.q_proj.tlut', 'vision_model.encoder.layers.1.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.q_proj.trellis', 'vision_model.encoder.layers.1.self_attn.v_proj.SU', 'vision_model.encoder.layers.1.self_attn.v_proj.SV', 'vision_model.encoder.layers.1.self_attn.v_proj.rcp', 'vision_model.encoder.layers.1.self_attn.v_proj.tlut', 'vision_model.encoder.layers.1.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.1.self_attn.v_proj.trellis', 'vision_model.encoder.layers.10.mlp.fc1.SU', 'vision_model.encoder.layers.10.mlp.fc1.SV', 'vision_model.encoder.layers.10.mlp.fc1.rcp', 'vision_model.encoder.layers.10.mlp.fc1.tlut', 'vision_model.encoder.layers.10.mlp.fc1.tp_rank', 'vision_model.encoder.layers.10.mlp.fc1.trellis', 'vision_model.encoder.layers.10.mlp.fc2.SU', 'vision_model.encoder.layers.10.mlp.fc2.SV', 'vision_model.encoder.layers.10.mlp.fc2.rcp', 'vision_model.encoder.layers.10.mlp.fc2.tlut', 'vision_model.encoder.layers.10.mlp.fc2.tp_rank', 'vision_model.encoder.layers.10.mlp.fc2.trellis', 'vision_model.encoder.layers.10.self_attn.k_proj.SU', 'vision_model.encoder.layers.10.self_attn.k_proj.SV', 'vision_model.encoder.layers.10.self_attn.k_proj.rcp', 'vision_model.encoder.layers.10.self_attn.k_proj.tlut', 'vision_model.encoder.layers.10.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.k_proj.trellis', 'vision_model.encoder.layers.10.self_attn.out_proj.SU', 'vision_model.encoder.layers.10.self_attn.out_proj.SV', 'vision_model.encoder.layers.10.self_attn.out_proj.rcp', 'vision_model.encoder.layers.10.self_attn.out_proj.tlut', 'vision_model.encoder.layers.10.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.out_proj.trellis', 'vision_model.encoder.layers.10.self_attn.q_proj.SU', 'vision_model.encoder.layers.10.self_attn.q_proj.SV', 'vision_model.encoder.layers.10.self_attn.q_proj.rcp', 'vision_model.encoder.layers.10.self_attn.q_proj.tlut', 'vision_model.encoder.layers.10.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.q_proj.trellis', 'vision_model.encoder.layers.10.self_attn.v_proj.SU', 'vision_model.encoder.layers.10.self_attn.v_proj.SV', 'vision_model.encoder.layers.10.self_attn.v_proj.rcp', 'vision_model.encoder.layers.10.self_attn.v_proj.tlut', 'vision_model.encoder.layers.10.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.10.self_attn.v_proj.trellis', 'vision_model.encoder.layers.11.mlp.fc1.SU', 'vision_model.encoder.layers.11.mlp.fc1.SV', 'vision_model.encoder.layers.11.mlp.fc1.rcp', 'vision_model.encoder.layers.11.mlp.fc1.tlut', 'vision_model.encoder.layers.11.mlp.fc1.tp_rank', 'vision_model.encoder.layers.11.mlp.fc1.trellis', 'vision_model.encoder.layers.11.mlp.fc2.SU', 'vision_model.encoder.layers.11.mlp.fc2.SV', 'vision_model.encoder.layers.11.mlp.fc2.rcp', 'vision_model.encoder.layers.11.mlp.fc2.tlut', 'vision_model.encoder.layers.11.mlp.fc2.tp_rank', 'vision_model.encoder.layers.11.mlp.fc2.trellis', 'vision_model.encoder.layers.11.self_attn.k_proj.SU', 'vision_model.encoder.layers.11.self_attn.k_proj.SV', 'vision_model.encoder.layers.11.self_attn.k_proj.rcp', 'vision_model.encoder.layers.11.self_attn.k_proj.tlut', 'vision_model.encoder.layers.11.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.k_proj.trellis', 'vision_model.encoder.layers.11.self_attn.out_proj.SU', 'vision_model.encoder.layers.11.self_attn.out_proj.SV', 'vision_model.encoder.layers.11.self_attn.out_proj.rcp', 'vision_model.encoder.layers.11.self_attn.out_proj.tlut', 'vision_model.encoder.layers.11.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.out_proj.trellis', 'vision_model.encoder.layers.11.self_attn.q_proj.SU', 'vision_model.encoder.layers.11.self_attn.q_proj.SV', 'vision_model.encoder.layers.11.self_attn.q_proj.rcp', 'vision_model.encoder.layers.11.self_attn.q_proj.tlut', 'vision_model.encoder.layers.11.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.q_proj.trellis', 'vision_model.encoder.layers.11.self_attn.v_proj.SU', 'vision_model.encoder.layers.11.self_attn.v_proj.SV', 'vision_model.encoder.layers.11.self_attn.v_proj.rcp', 'vision_model.encoder.layers.11.self_attn.v_proj.tlut', 'vision_model.encoder.layers.11.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.11.self_attn.v_proj.trellis', 'vision_model.encoder.layers.12.mlp.fc1.SU', 'vision_model.encoder.layers.12.mlp.fc1.SV', 'vision_model.encoder.layers.12.mlp.fc1.rcp', 'vision_model.encoder.layers.12.mlp.fc1.tlut', 'vision_model.encoder.layers.12.mlp.fc1.tp_rank', 'vision_model.encoder.layers.12.mlp.fc1.trellis', 'vision_model.encoder.layers.12.mlp.fc2.SU', 'vision_model.encoder.layers.12.mlp.fc2.SV', 'vision_model.encoder.layers.12.mlp.fc2.rcp', 'vision_model.encoder.layers.12.mlp.fc2.tlut', 'vision_model.encoder.layers.12.mlp.fc2.tp_rank', 'vision_model.encoder.layers.12.mlp.fc2.trellis', 'vision_model.encoder.layers.12.self_attn.k_proj.SU', 'vision_model.encoder.layers.12.self_attn.k_proj.SV', 'vision_model.encoder.layers.12.self_attn.k_proj.rcp', 'vision_model.encoder.layers.12.self_attn.k_proj.tlut', 'vision_model.encoder.layers.12.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.k_proj.trellis', 'vision_model.encoder.layers.12.self_attn.out_proj.SU', 'vision_model.encoder.layers.12.self_attn.out_proj.SV', 'vision_model.encoder.layers.12.self_attn.out_proj.rcp', 'vision_model.encoder.layers.12.self_attn.out_proj.tlut', 'vision_model.encoder.layers.12.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.out_proj.trellis', 'vision_model.encoder.layers.12.self_attn.q_proj.SU', 'vision_model.encoder.layers.12.self_attn.q_proj.SV', 'vision_model.encoder.layers.12.self_attn.q_proj.rcp', 'vision_model.encoder.layers.12.self_attn.q_proj.tlut', 'vision_model.encoder.layers.12.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.q_proj.trellis', 'vision_model.encoder.layers.12.self_attn.v_proj.SU', 'vision_model.encoder.layers.12.self_attn.v_proj.SV', 'vision_model.encoder.layers.12.self_attn.v_proj.rcp', 'vision_model.encoder.layers.12.self_attn.v_proj.tlut', 'vision_model.encoder.layers.12.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.12.self_attn.v_proj.trellis', 'vision_model.encoder.layers.13.mlp.fc1.SU', 'vision_model.encoder.layers.13.mlp.fc1.SV', 'vision_model.encoder.layers.13.mlp.fc1.rcp', 'vision_model.encoder.layers.13.mlp.fc1.tlut', 'vision_model.encoder.layers.13.mlp.fc1.tp_rank', 'vision_model.encoder.layers.13.mlp.fc1.trellis', 'vision_model.encoder.layers.13.mlp.fc2.SU', 'vision_model.encoder.layers.13.mlp.fc2.SV', 'vision_model.encoder.layers.13.mlp.fc2.rcp', 'vision_model.encoder.layers.13.mlp.fc2.tlut', 'vision_model.encoder.layers.13.mlp.fc2.tp_rank', 'vision_model.encoder.layers.13.mlp.fc2.trellis', 'vision_model.encoder.layers.13.self_attn.k_proj.SU', 'vision_model.encoder.layers.13.self_attn.k_proj.SV', 'vision_model.encoder.layers.13.self_attn.k_proj.rcp', 'vision_model.encoder.layers.13.self_attn.k_proj.tlut', 'vision_model.encoder.layers.13.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.k_proj.trellis', 'vision_model.encoder.layers.13.self_attn.out_proj.SU', 'vision_model.encoder.layers.13.self_attn.out_proj.SV', 'vision_model.encoder.layers.13.self_attn.out_proj.rcp', 'vision_model.encoder.layers.13.self_attn.out_proj.tlut', 'vision_model.encoder.layers.13.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.out_proj.trellis', 'vision_model.encoder.layers.13.self_attn.q_proj.SU', 'vision_model.encoder.layers.13.self_attn.q_proj.SV', 'vision_model.encoder.layers.13.self_attn.q_proj.rcp', 'vision_model.encoder.layers.13.self_attn.q_proj.tlut', 'vision_model.encoder.layers.13.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.q_proj.trellis', 'vision_model.encoder.layers.13.self_attn.v_proj.SU', 'vision_model.encoder.layers.13.self_attn.v_proj.SV', 'vision_model.encoder.layers.13.self_attn.v_proj.rcp', 'vision_model.encoder.layers.13.self_attn.v_proj.tlut', 'vision_model.encoder.layers.13.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.13.self_attn.v_proj.trellis', 'vision_model.encoder.layers.14.mlp.fc1.SU', 'vision_model.encoder.layers.14.mlp.fc1.SV', 'vision_model.encoder.layers.14.mlp.fc1.rcp', 'vision_model.encoder.layers.14.mlp.fc1.tlut', 'vision_model.encoder.layers.14.mlp.fc1.tp_rank', 'vision_model.encoder.layers.14.mlp.fc1.trellis', 'vision_model.encoder.layers.14.mlp.fc2.SU', 'vision_model.encoder.layers.14.mlp.fc2.SV', 'vision_model.encoder.layers.14.mlp.fc2.rcp', 'vision_model.encoder.layers.14.mlp.fc2.tlut', 'vision_model.encoder.layers.14.mlp.fc2.tp_rank', 'vision_model.encoder.layers.14.mlp.fc2.trellis', 'vision_model.encoder.layers.14.self_attn.k_proj.SU', 'vision_model.encoder.layers.14.self_attn.k_proj.SV', 'vision_model.encoder.layers.14.self_attn.k_proj.rcp', 'vision_model.encoder.layers.14.self_attn.k_proj.tlut', 'vision_model.encoder.layers.14.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.k_proj.trellis', 'vision_model.encoder.layers.14.self_attn.out_proj.SU', 'vision_model.encoder.layers.14.self_attn.out_proj.SV', 'vision_model.encoder.layers.14.self_attn.out_proj.rcp', 'vision_model.encoder.layers.14.self_attn.out_proj.tlut', 'vision_model.encoder.layers.14.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.out_proj.trellis', 'vision_model.encoder.layers.14.self_attn.q_proj.SU', 'vision_model.encoder.layers.14.self_attn.q_proj.SV', 'vision_model.encoder.layers.14.self_attn.q_proj.rcp', 'vision_model.encoder.layers.14.self_attn.q_proj.tlut', 'vision_model.encoder.layers.14.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.q_proj.trellis', 'vision_model.encoder.layers.14.self_attn.v_proj.SU', 'vision_model.encoder.layers.14.self_attn.v_proj.SV', 'vision_model.encoder.layers.14.self_attn.v_proj.rcp', 'vision_model.encoder.layers.14.self_attn.v_proj.tlut', 'vision_model.encoder.layers.14.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.14.self_attn.v_proj.trellis', 'vision_model.encoder.layers.15.mlp.fc1.SU', 'vision_model.encoder.layers.15.mlp.fc1.SV', 'vision_model.encoder.layers.15.mlp.fc1.rcp', 'vision_model.encoder.layers.15.mlp.fc1.tlut', 'vision_model.encoder.layers.15.mlp.fc1.tp_rank', 'vision_model.encoder.layers.15.mlp.fc1.trellis', 'vision_model.encoder.layers.15.mlp.fc2.SU', 'vision_model.encoder.layers.15.mlp.fc2.SV', 'vision_model.encoder.layers.15.mlp.fc2.rcp', 'vision_model.encoder.layers.15.mlp.fc2.tlut', 'vision_model.encoder.layers.15.mlp.fc2.tp_rank', 'vision_model.encoder.layers.15.mlp.fc2.trellis', 'vision_model.encoder.layers.15.self_attn.k_proj.SU', 'vision_model.encoder.layers.15.self_attn.k_proj.SV', 'vision_model.encoder.layers.15.self_attn.k_proj.rcp', 'vision_model.encoder.layers.15.self_attn.k_proj.tlut', 'vision_model.encoder.layers.15.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.k_proj.trellis', 'vision_model.encoder.layers.15.self_attn.out_proj.SU', 'vision_model.encoder.layers.15.self_attn.out_proj.SV', 'vision_model.encoder.layers.15.self_attn.out_proj.rcp', 'vision_model.encoder.layers.15.self_attn.out_proj.tlut', 'vision_model.encoder.layers.15.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.out_proj.trellis', 'vision_model.encoder.layers.15.self_attn.q_proj.SU', 'vision_model.encoder.layers.15.self_attn.q_proj.SV', 'vision_model.encoder.layers.15.self_attn.q_proj.rcp', 'vision_model.encoder.layers.15.self_attn.q_proj.tlut', 'vision_model.encoder.layers.15.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.q_proj.trellis', 'vision_model.encoder.layers.15.self_attn.v_proj.SU', 'vision_model.encoder.layers.15.self_attn.v_proj.SV', 'vision_model.encoder.layers.15.self_attn.v_proj.rcp', 'vision_model.encoder.layers.15.self_attn.v_proj.tlut', 'vision_model.encoder.layers.15.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.15.self_attn.v_proj.trellis', 'vision_model.encoder.layers.16.mlp.fc1.SU', 'vision_model.encoder.layers.16.mlp.fc1.SV', 'vision_model.encoder.layers.16.mlp.fc1.rcp', 'vision_model.encoder.layers.16.mlp.fc1.tlut', 'vision_model.encoder.layers.16.mlp.fc1.tp_rank', 'vision_model.encoder.layers.16.mlp.fc1.trellis', 'vision_model.encoder.layers.16.mlp.fc2.SU', 'vision_model.encoder.layers.16.mlp.fc2.SV', 'vision_model.encoder.layers.16.mlp.fc2.rcp', 'vision_model.encoder.layers.16.mlp.fc2.tlut', 'vision_model.encoder.layers.16.mlp.fc2.tp_rank', 'vision_model.encoder.layers.16.mlp.fc2.trellis', 'vision_model.encoder.layers.16.self_attn.k_proj.SU', 'vision_model.encoder.layers.16.self_attn.k_proj.SV', 'vision_model.encoder.layers.16.self_attn.k_proj.rcp', 'vision_model.encoder.layers.16.self_attn.k_proj.tlut', 'vision_model.encoder.layers.16.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.k_proj.trellis', 'vision_model.encoder.layers.16.self_attn.out_proj.SU', 'vision_model.encoder.layers.16.self_attn.out_proj.SV', 'vision_model.encoder.layers.16.self_attn.out_proj.rcp', 'vision_model.encoder.layers.16.self_attn.out_proj.tlut', 'vision_model.encoder.layers.16.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.out_proj.trellis', 'vision_model.encoder.layers.16.self_attn.q_proj.SU', 'vision_model.encoder.layers.16.self_attn.q_proj.SV', 'vision_model.encoder.layers.16.self_attn.q_proj.rcp', 'vision_model.encoder.layers.16.self_attn.q_proj.tlut', 'vision_model.encoder.layers.16.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.q_proj.trellis', 'vision_model.encoder.layers.16.self_attn.v_proj.SU', 'vision_model.encoder.layers.16.self_attn.v_proj.SV', 'vision_model.encoder.layers.16.self_attn.v_proj.rcp', 'vision_model.encoder.layers.16.self_attn.v_proj.tlut', 'vision_model.encoder.layers.16.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.16.self_attn.v_proj.trellis', 'vision_model.encoder.layers.17.mlp.fc1.SU', 'vision_model.encoder.layers.17.mlp.fc1.SV', 'vision_model.encoder.layers.17.mlp.fc1.rcp', 'vision_model.encoder.layers.17.mlp.fc1.tlut', 'vision_model.encoder.layers.17.mlp.fc1.tp_rank', 'vision_model.encoder.layers.17.mlp.fc1.trellis', 'vision_model.encoder.layers.17.mlp.fc2.SU', 'vision_model.encoder.layers.17.mlp.fc2.SV', 'vision_model.encoder.layers.17.mlp.fc2.rcp', 'vision_model.encoder.layers.17.mlp.fc2.tlut', 'vision_model.encoder.layers.17.mlp.fc2.tp_rank', 'vision_model.encoder.layers.17.mlp.fc2.trellis', 'vision_model.encoder.layers.17.self_attn.k_proj.SU', 'vision_model.encoder.layers.17.self_attn.k_proj.SV', 'vision_model.encoder.layers.17.self_attn.k_proj.rcp', 'vision_model.encoder.layers.17.self_attn.k_proj.tlut', 'vision_model.encoder.layers.17.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.k_proj.trellis', 'vision_model.encoder.layers.17.self_attn.out_proj.SU', 'vision_model.encoder.layers.17.self_attn.out_proj.SV', 'vision_model.encoder.layers.17.self_attn.out_proj.rcp', 'vision_model.encoder.layers.17.self_attn.out_proj.tlut', 'vision_model.encoder.layers.17.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.out_proj.trellis', 'vision_model.encoder.layers.17.self_attn.q_proj.SU', 'vision_model.encoder.layers.17.self_attn.q_proj.SV', 'vision_model.encoder.layers.17.self_attn.q_proj.rcp', 'vision_model.encoder.layers.17.self_attn.q_proj.tlut', 'vision_model.encoder.layers.17.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.q_proj.trellis', 'vision_model.encoder.layers.17.self_attn.v_proj.SU', 'vision_model.encoder.layers.17.self_attn.v_proj.SV', 'vision_model.encoder.layers.17.self_attn.v_proj.rcp', 'vision_model.encoder.layers.17.self_attn.v_proj.tlut', 'vision_model.encoder.layers.17.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.17.self_attn.v_proj.trellis', 'vision_model.encoder.layers.18.mlp.fc1.SU', 'vision_model.encoder.layers.18.mlp.fc1.SV', 'vision_model.encoder.layers.18.mlp.fc1.rcp', 'vision_model.encoder.layers.18.mlp.fc1.tlut', 'vision_model.encoder.layers.18.mlp.fc1.tp_rank', 'vision_model.encoder.layers.18.mlp.fc1.trellis', 'vision_model.encoder.layers.18.mlp.fc2.SU', 'vision_model.encoder.layers.18.mlp.fc2.SV', 'vision_model.encoder.layers.18.mlp.fc2.rcp', 'vision_model.encoder.layers.18.mlp.fc2.tlut', 'vision_model.encoder.layers.18.mlp.fc2.tp_rank', 'vision_model.encoder.layers.18.mlp.fc2.trellis', 'vision_model.encoder.layers.18.self_attn.k_proj.SU', 'vision_model.encoder.layers.18.self_attn.k_proj.SV', 'vision_model.encoder.layers.18.self_attn.k_proj.rcp', 'vision_model.encoder.layers.18.self_attn.k_proj.tlut', 'vision_model.encoder.layers.18.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.k_proj.trellis', 'vision_model.encoder.layers.18.self_attn.out_proj.SU', 'vision_model.encoder.layers.18.self_attn.out_proj.SV', 'vision_model.encoder.layers.18.self_attn.out_proj.rcp', 'vision_model.encoder.layers.18.self_attn.out_proj.tlut', 'vision_model.encoder.layers.18.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.out_proj.trellis', 'vision_model.encoder.layers.18.self_attn.q_proj.SU', 'vision_model.encoder.layers.18.self_attn.q_proj.SV', 'vision_model.encoder.layers.18.self_attn.q_proj.rcp', 'vision_model.encoder.layers.18.self_attn.q_proj.tlut', 'vision_model.encoder.layers.18.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.q_proj.trellis', 'vision_model.encoder.layers.18.self_attn.v_proj.SU', 'vision_model.encoder.layers.18.self_attn.v_proj.SV', 'vision_model.encoder.layers.18.self_attn.v_proj.rcp', 'vision_model.encoder.layers.18.self_attn.v_proj.tlut', 'vision_model.encoder.layers.18.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.18.self_attn.v_proj.trellis', 'vision_model.encoder.layers.19.mlp.fc1.SU', 'vision_model.encoder.layers.19.mlp.fc1.SV', 'vision_model.encoder.layers.19.mlp.fc1.rcp', 'vision_model.encoder.layers.19.mlp.fc1.tlut', 'vision_model.encoder.layers.19.mlp.fc1.tp_rank', 'vision_model.encoder.layers.19.mlp.fc1.trellis', 'vision_model.encoder.layers.19.mlp.fc2.SU', 'vision_model.encoder.layers.19.mlp.fc2.SV', 'vision_model.encoder.layers.19.mlp.fc2.rcp', 'vision_model.encoder.layers.19.mlp.fc2.tlut', 'vision_model.encoder.layers.19.mlp.fc2.tp_rank', 'vision_model.encoder.layers.19.mlp.fc2.trellis', 'vision_model.encoder.layers.19.self_attn.k_proj.SU', 'vision_model.encoder.layers.19.self_attn.k_proj.SV', 'vision_model.encoder.layers.19.self_attn.k_proj.rcp', 'vision_model.encoder.layers.19.self_attn.k_proj.tlut', 'vision_model.encoder.layers.19.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.k_proj.trellis', 'vision_model.encoder.layers.19.self_attn.out_proj.SU', 'vision_model.encoder.layers.19.self_attn.out_proj.SV', 'vision_model.encoder.layers.19.self_attn.out_proj.rcp', 'vision_model.encoder.layers.19.self_attn.out_proj.tlut', 'vision_model.encoder.layers.19.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.out_proj.trellis', 'vision_model.encoder.layers.19.self_attn.q_proj.SU', 'vision_model.encoder.layers.19.self_attn.q_proj.SV', 'vision_model.encoder.layers.19.self_attn.q_proj.rcp', 'vision_model.encoder.layers.19.self_attn.q_proj.tlut', 'vision_model.encoder.layers.19.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.q_proj.trellis', 'vision_model.encoder.layers.19.self_attn.v_proj.SU', 'vision_model.encoder.layers.19.self_attn.v_proj.SV', 'vision_model.encoder.layers.19.self_attn.v_proj.rcp', 'vision_model.encoder.layers.19.self_attn.v_proj.tlut', 'vision_model.encoder.layers.19.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.19.self_attn.v_proj.trellis', 'vision_model.encoder.layers.2.mlp.fc1.SU', 'vision_model.encoder.layers.2.mlp.fc1.SV', 'vision_model.encoder.layers.2.mlp.fc1.rcp', 'vision_model.encoder.layers.2.mlp.fc1.tlut', 'vision_model.encoder.layers.2.mlp.fc1.tp_rank', 'vision_model.encoder.layers.2.mlp.fc1.trellis', 'vision_model.encoder.layers.2.mlp.fc2.SU', 'vision_model.encoder.layers.2.mlp.fc2.SV', 'vision_model.encoder.layers.2.mlp.fc2.rcp', 'vision_model.encoder.layers.2.mlp.fc2.tlut', 'vision_model.encoder.layers.2.mlp.fc2.tp_rank', 'vision_model.encoder.layers.2.mlp.fc2.trellis', 'vision_model.encoder.layers.2.self_attn.k_proj.SU', 'vision_model.encoder.layers.2.self_attn.k_proj.SV', 'vision_model.encoder.layers.2.self_attn.k_proj.rcp', 'vision_model.encoder.layers.2.self_attn.k_proj.tlut', 'vision_model.encoder.layers.2.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.k_proj.trellis', 'vision_model.encoder.layers.2.self_attn.out_proj.SU', 'vision_model.encoder.layers.2.self_attn.out_proj.SV', 'vision_model.encoder.layers.2.self_attn.out_proj.rcp', 'vision_model.encoder.layers.2.self_attn.out_proj.tlut', 'vision_model.encoder.layers.2.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.out_proj.trellis', 'vision_model.encoder.layers.2.self_attn.q_proj.SU', 'vision_model.encoder.layers.2.self_attn.q_proj.SV', 'vision_model.encoder.layers.2.self_attn.q_proj.rcp', 'vision_model.encoder.layers.2.self_attn.q_proj.tlut', 'vision_model.encoder.layers.2.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.q_proj.trellis', 'vision_model.encoder.layers.2.self_attn.v_proj.SU', 'vision_model.encoder.layers.2.self_attn.v_proj.SV', 'vision_model.encoder.layers.2.self_attn.v_proj.rcp', 'vision_model.encoder.layers.2.self_attn.v_proj.tlut', 'vision_model.encoder.layers.2.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.2.self_attn.v_proj.trellis', 'vision_model.encoder.layers.20.mlp.fc1.SU', 'vision_model.encoder.layers.20.mlp.fc1.SV', 'vision_model.encoder.layers.20.mlp.fc1.rcp', 'vision_model.encoder.layers.20.mlp.fc1.tlut', 'vision_model.encoder.layers.20.mlp.fc1.tp_rank', 'vision_model.encoder.layers.20.mlp.fc1.trellis', 'vision_model.encoder.layers.20.mlp.fc2.SU', 'vision_model.encoder.layers.20.mlp.fc2.SV', 'vision_model.encoder.layers.20.mlp.fc2.rcp', 'vision_model.encoder.layers.20.mlp.fc2.tlut', 'vision_model.encoder.layers.20.mlp.fc2.tp_rank', 'vision_model.encoder.layers.20.mlp.fc2.trellis', 'vision_model.encoder.layers.20.self_attn.k_proj.SU', 'vision_model.encoder.layers.20.self_attn.k_proj.SV', 'vision_model.encoder.layers.20.self_attn.k_proj.rcp', 'vision_model.encoder.layers.20.self_attn.k_proj.tlut', 'vision_model.encoder.layers.20.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.k_proj.trellis', 'vision_model.encoder.layers.20.self_attn.out_proj.SU', 'vision_model.encoder.layers.20.self_attn.out_proj.SV', 'vision_model.encoder.layers.20.self_attn.out_proj.rcp', 'vision_model.encoder.layers.20.self_attn.out_proj.tlut', 'vision_model.encoder.layers.20.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.out_proj.trellis', 'vision_model.encoder.layers.20.self_attn.q_proj.SU', 'vision_model.encoder.layers.20.self_attn.q_proj.SV', 'vision_model.encoder.layers.20.self_attn.q_proj.rcp', 'vision_model.encoder.layers.20.self_attn.q_proj.tlut', 'vision_model.encoder.layers.20.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.q_proj.trellis', 'vision_model.encoder.layers.20.self_attn.v_proj.SU', 'vision_model.encoder.layers.20.self_attn.v_proj.SV', 'vision_model.encoder.layers.20.self_attn.v_proj.rcp', 'vision_model.encoder.layers.20.self_attn.v_proj.tlut', 'vision_model.encoder.layers.20.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.20.self_attn.v_proj.trellis', 'vision_model.encoder.layers.21.mlp.fc1.SU', 'vision_model.encoder.layers.21.mlp.fc1.SV', 'vision_model.encoder.layers.21.mlp.fc1.rcp', 'vision_model.encoder.layers.21.mlp.fc1.tlut', 'vision_model.encoder.layers.21.mlp.fc1.tp_rank', 'vision_model.encoder.layers.21.mlp.fc1.trellis', 'vision_model.encoder.layers.21.mlp.fc2.SU', 'vision_model.encoder.layers.21.mlp.fc2.SV', 'vision_model.encoder.layers.21.mlp.fc2.rcp', 'vision_model.encoder.layers.21.mlp.fc2.tlut', 'vision_model.encoder.layers.21.mlp.fc2.tp_rank', 'vision_model.encoder.layers.21.mlp.fc2.trellis', 'vision_model.encoder.layers.21.self_attn.k_proj.SU', 'vision_model.encoder.layers.21.self_attn.k_proj.SV', 'vision_model.encoder.layers.21.self_attn.k_proj.rcp', 'vision_model.encoder.layers.21.self_attn.k_proj.tlut', 'vision_model.encoder.layers.21.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.k_proj.trellis', 'vision_model.encoder.layers.21.self_attn.out_proj.SU', 'vision_model.encoder.layers.21.self_attn.out_proj.SV', 'vision_model.encoder.layers.21.self_attn.out_proj.rcp', 'vision_model.encoder.layers.21.self_attn.out_proj.tlut', 'vision_model.encoder.layers.21.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.out_proj.trellis', 'vision_model.encoder.layers.21.self_attn.q_proj.SU', 'vision_model.encoder.layers.21.self_attn.q_proj.SV', 'vision_model.encoder.layers.21.self_attn.q_proj.rcp', 'vision_model.encoder.layers.21.self_attn.q_proj.tlut', 'vision_model.encoder.layers.21.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.q_proj.trellis', 'vision_model.encoder.layers.21.self_attn.v_proj.SU', 'vision_model.encoder.layers.21.self_attn.v_proj.SV', 'vision_model.encoder.layers.21.self_attn.v_proj.rcp', 'vision_model.encoder.layers.21.self_attn.v_proj.tlut', 'vision_model.encoder.layers.21.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.21.self_attn.v_proj.trellis', 'vision_model.encoder.layers.22.mlp.fc1.SU', 'vision_model.encoder.layers.22.mlp.fc1.SV', 'vision_model.encoder.layers.22.mlp.fc1.rcp', 'vision_model.encoder.layers.22.mlp.fc1.tlut', 'vision_model.encoder.layers.22.mlp.fc1.tp_rank', 'vision_model.encoder.layers.22.mlp.fc1.trellis', 'vision_model.encoder.layers.22.mlp.fc2.SU', 'vision_model.encoder.layers.22.mlp.fc2.SV', 'vision_model.encoder.layers.22.mlp.fc2.rcp', 'vision_model.encoder.layers.22.mlp.fc2.tlut', 'vision_model.encoder.layers.22.mlp.fc2.tp_rank', 'vision_model.encoder.layers.22.mlp.fc2.trellis', 'vision_model.encoder.layers.22.self_attn.k_proj.SU', 'vision_model.encoder.layers.22.self_attn.k_proj.SV', 'vision_model.encoder.layers.22.self_attn.k_proj.rcp', 'vision_model.encoder.layers.22.self_attn.k_proj.tlut', 'vision_model.encoder.layers.22.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.k_proj.trellis', 'vision_model.encoder.layers.22.self_attn.out_proj.SU', 'vision_model.encoder.layers.22.self_attn.out_proj.SV', 'vision_model.encoder.layers.22.self_attn.out_proj.rcp', 'vision_model.encoder.layers.22.self_attn.out_proj.tlut', 'vision_model.encoder.layers.22.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.out_proj.trellis', 'vision_model.encoder.layers.22.self_attn.q_proj.SU', 'vision_model.encoder.layers.22.self_attn.q_proj.SV', 'vision_model.encoder.layers.22.self_attn.q_proj.rcp', 'vision_model.encoder.layers.22.self_attn.q_proj.tlut', 'vision_model.encoder.layers.22.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.q_proj.trellis', 'vision_model.encoder.layers.22.self_attn.v_proj.SU', 'vision_model.encoder.layers.22.self_attn.v_proj.SV', 'vision_model.encoder.layers.22.self_attn.v_proj.rcp', 'vision_model.encoder.layers.22.self_attn.v_proj.tlut', 'vision_model.encoder.layers.22.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.22.self_attn.v_proj.trellis', 'vision_model.encoder.layers.23.mlp.fc1.SU', 'vision_model.encoder.layers.23.mlp.fc1.SV', 'vision_model.encoder.layers.23.mlp.fc1.rcp', 'vision_model.encoder.layers.23.mlp.fc1.tlut', 'vision_model.encoder.layers.23.mlp.fc1.tp_rank', 'vision_model.encoder.layers.23.mlp.fc1.trellis', 'vision_model.encoder.layers.23.mlp.fc2.SU', 'vision_model.encoder.layers.23.mlp.fc2.SV', 'vision_model.encoder.layers.23.mlp.fc2.rcp', 'vision_model.encoder.layers.23.mlp.fc2.tlut', 'vision_model.encoder.layers.23.mlp.fc2.tp_rank', 'vision_model.encoder.layers.23.mlp.fc2.trellis', 'vision_model.encoder.layers.23.self_attn.k_proj.SU', 'vision_model.encoder.layers.23.self_attn.k_proj.SV', 'vision_model.encoder.layers.23.self_attn.k_proj.rcp', 'vision_model.encoder.layers.23.self_attn.k_proj.tlut', 'vision_model.encoder.layers.23.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.k_proj.trellis', 'vision_model.encoder.layers.23.self_attn.out_proj.SU', 'vision_model.encoder.layers.23.self_attn.out_proj.SV', 'vision_model.encoder.layers.23.self_attn.out_proj.rcp', 'vision_model.encoder.layers.23.self_attn.out_proj.tlut', 'vision_model.encoder.layers.23.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.out_proj.trellis', 'vision_model.encoder.layers.23.self_attn.q_proj.SU', 'vision_model.encoder.layers.23.self_attn.q_proj.SV', 'vision_model.encoder.layers.23.self_attn.q_proj.rcp', 'vision_model.encoder.layers.23.self_attn.q_proj.tlut', 'vision_model.encoder.layers.23.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.q_proj.trellis', 'vision_model.encoder.layers.23.self_attn.v_proj.SU', 'vision_model.encoder.layers.23.self_attn.v_proj.SV', 'vision_model.encoder.layers.23.self_attn.v_proj.rcp', 'vision_model.encoder.layers.23.self_attn.v_proj.tlut', 'vision_model.encoder.layers.23.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.23.self_attn.v_proj.trellis', 'vision_model.encoder.layers.3.mlp.fc1.SU', 'vision_model.encoder.layers.3.mlp.fc1.SV', 'vision_model.encoder.layers.3.mlp.fc1.rcp', 'vision_model.encoder.layers.3.mlp.fc1.tlut', 'vision_model.encoder.layers.3.mlp.fc1.tp_rank', 'vision_model.encoder.layers.3.mlp.fc1.trellis', 'vision_model.encoder.layers.3.mlp.fc2.SU', 'vision_model.encoder.layers.3.mlp.fc2.SV', 'vision_model.encoder.layers.3.mlp.fc2.rcp', 'vision_model.encoder.layers.3.mlp.fc2.tlut', 'vision_model.encoder.layers.3.mlp.fc2.tp_rank', 'vision_model.encoder.layers.3.mlp.fc2.trellis', 'vision_model.encoder.layers.3.self_attn.k_proj.SU', 'vision_model.encoder.layers.3.self_attn.k_proj.SV', 'vision_model.encoder.layers.3.self_attn.k_proj.rcp', 'vision_model.encoder.layers.3.self_attn.k_proj.tlut', 'vision_model.encoder.layers.3.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.k_proj.trellis', 'vision_model.encoder.layers.3.self_attn.out_proj.SU', 'vision_model.encoder.layers.3.self_attn.out_proj.SV', 'vision_model.encoder.layers.3.self_attn.out_proj.rcp', 'vision_model.encoder.layers.3.self_attn.out_proj.tlut', 'vision_model.encoder.layers.3.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.out_proj.trellis', 'vision_model.encoder.layers.3.self_attn.q_proj.SU', 'vision_model.encoder.layers.3.self_attn.q_proj.SV', 'vision_model.encoder.layers.3.self_attn.q_proj.rcp', 'vision_model.encoder.layers.3.self_attn.q_proj.tlut', 'vision_model.encoder.layers.3.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.q_proj.trellis', 'vision_model.encoder.layers.3.self_attn.v_proj.SU', 'vision_model.encoder.layers.3.self_attn.v_proj.SV', 'vision_model.encoder.layers.3.self_attn.v_proj.rcp', 'vision_model.encoder.layers.3.self_attn.v_proj.tlut', 'vision_model.encoder.layers.3.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.3.self_attn.v_proj.trellis', 'vision_model.encoder.layers.4.mlp.fc1.SU', 'vision_model.encoder.layers.4.mlp.fc1.SV', 'vision_model.encoder.layers.4.mlp.fc1.rcp', 'vision_model.encoder.layers.4.mlp.fc1.tlut', 'vision_model.encoder.layers.4.mlp.fc1.tp_rank', 'vision_model.encoder.layers.4.mlp.fc1.trellis', 'vision_model.encoder.layers.4.mlp.fc2.SU', 'vision_model.encoder.layers.4.mlp.fc2.SV', 'vision_model.encoder.layers.4.mlp.fc2.rcp', 'vision_model.encoder.layers.4.mlp.fc2.tlut', 'vision_model.encoder.layers.4.mlp.fc2.tp_rank', 'vision_model.encoder.layers.4.mlp.fc2.trellis', 'vision_model.encoder.layers.4.self_attn.k_proj.SU', 'vision_model.encoder.layers.4.self_attn.k_proj.SV', 'vision_model.encoder.layers.4.self_attn.k_proj.rcp', 'vision_model.encoder.layers.4.self_attn.k_proj.tlut', 'vision_model.encoder.layers.4.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.k_proj.trellis', 'vision_model.encoder.layers.4.self_attn.out_proj.SU', 'vision_model.encoder.layers.4.self_attn.out_proj.SV', 'vision_model.encoder.layers.4.self_attn.out_proj.rcp', 'vision_model.encoder.layers.4.self_attn.out_proj.tlut', 'vision_model.encoder.layers.4.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.out_proj.trellis', 'vision_model.encoder.layers.4.self_attn.q_proj.SU', 'vision_model.encoder.layers.4.self_attn.q_proj.SV', 'vision_model.encoder.layers.4.self_attn.q_proj.rcp', 'vision_model.encoder.layers.4.self_attn.q_proj.tlut', 'vision_model.encoder.layers.4.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.q_proj.trellis', 'vision_model.encoder.layers.4.self_attn.v_proj.SU', 'vision_model.encoder.layers.4.self_attn.v_proj.SV', 'vision_model.encoder.layers.4.self_attn.v_proj.rcp', 'vision_model.encoder.layers.4.self_attn.v_proj.tlut', 'vision_model.encoder.layers.4.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.4.self_attn.v_proj.trellis', 'vision_model.encoder.layers.5.mlp.fc1.SU', 'vision_model.encoder.layers.5.mlp.fc1.SV', 'vision_model.encoder.layers.5.mlp.fc1.rcp', 'vision_model.encoder.layers.5.mlp.fc1.tlut', 'vision_model.encoder.layers.5.mlp.fc1.tp_rank', 'vision_model.encoder.layers.5.mlp.fc1.trellis', 'vision_model.encoder.layers.5.mlp.fc2.SU', 'vision_model.encoder.layers.5.mlp.fc2.SV', 'vision_model.encoder.layers.5.mlp.fc2.rcp', 'vision_model.encoder.layers.5.mlp.fc2.tlut', 'vision_model.encoder.layers.5.mlp.fc2.tp_rank', 'vision_model.encoder.layers.5.mlp.fc2.trellis', 'vision_model.encoder.layers.5.self_attn.k_proj.SU', 'vision_model.encoder.layers.5.self_attn.k_proj.SV', 'vision_model.encoder.layers.5.self_attn.k_proj.rcp', 'vision_model.encoder.layers.5.self_attn.k_proj.tlut', 'vision_model.encoder.layers.5.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.k_proj.trellis', 'vision_model.encoder.layers.5.self_attn.out_proj.SU', 'vision_model.encoder.layers.5.self_attn.out_proj.SV', 'vision_model.encoder.layers.5.self_attn.out_proj.rcp', 'vision_model.encoder.layers.5.self_attn.out_proj.tlut', 'vision_model.encoder.layers.5.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.out_proj.trellis', 'vision_model.encoder.layers.5.self_attn.q_proj.SU', 'vision_model.encoder.layers.5.self_attn.q_proj.SV', 'vision_model.encoder.layers.5.self_attn.q_proj.rcp', 'vision_model.encoder.layers.5.self_attn.q_proj.tlut', 'vision_model.encoder.layers.5.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.q_proj.trellis', 'vision_model.encoder.layers.5.self_attn.v_proj.SU', 'vision_model.encoder.layers.5.self_attn.v_proj.SV', 'vision_model.encoder.layers.5.self_attn.v_proj.rcp', 'vision_model.encoder.layers.5.self_attn.v_proj.tlut', 'vision_model.encoder.layers.5.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.5.self_attn.v_proj.trellis', 'vision_model.encoder.layers.6.mlp.fc1.SU', 'vision_model.encoder.layers.6.mlp.fc1.SV', 'vision_model.encoder.layers.6.mlp.fc1.rcp', 'vision_model.encoder.layers.6.mlp.fc1.tlut', 'vision_model.encoder.layers.6.mlp.fc1.tp_rank', 'vision_model.encoder.layers.6.mlp.fc1.trellis', 'vision_model.encoder.layers.6.mlp.fc2.SU', 'vision_model.encoder.layers.6.mlp.fc2.SV', 'vision_model.encoder.layers.6.mlp.fc2.rcp', 'vision_model.encoder.layers.6.mlp.fc2.tlut', 'vision_model.encoder.layers.6.mlp.fc2.tp_rank', 'vision_model.encoder.layers.6.mlp.fc2.trellis', 'vision_model.encoder.layers.6.self_attn.k_proj.SU', 'vision_model.encoder.layers.6.self_attn.k_proj.SV', 'vision_model.encoder.layers.6.self_attn.k_proj.rcp', 'vision_model.encoder.layers.6.self_attn.k_proj.tlut', 'vision_model.encoder.layers.6.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.k_proj.trellis', 'vision_model.encoder.layers.6.self_attn.out_proj.SU', 'vision_model.encoder.layers.6.self_attn.out_proj.SV', 'vision_model.encoder.layers.6.self_attn.out_proj.rcp', 'vision_model.encoder.layers.6.self_attn.out_proj.tlut', 'vision_model.encoder.layers.6.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.out_proj.trellis', 'vision_model.encoder.layers.6.self_attn.q_proj.SU', 'vision_model.encoder.layers.6.self_attn.q_proj.SV', 'vision_model.encoder.layers.6.self_attn.q_proj.rcp', 'vision_model.encoder.layers.6.self_attn.q_proj.tlut', 'vision_model.encoder.layers.6.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.q_proj.trellis', 'vision_model.encoder.layers.6.self_attn.v_proj.SU', 'vision_model.encoder.layers.6.self_attn.v_proj.SV', 'vision_model.encoder.layers.6.self_attn.v_proj.rcp', 'vision_model.encoder.layers.6.self_attn.v_proj.tlut', 'vision_model.encoder.layers.6.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.6.self_attn.v_proj.trellis', 'vision_model.encoder.layers.7.mlp.fc1.SU', 'vision_model.encoder.layers.7.mlp.fc1.SV', 'vision_model.encoder.layers.7.mlp.fc1.rcp', 'vision_model.encoder.layers.7.mlp.fc1.tlut', 'vision_model.encoder.layers.7.mlp.fc1.tp_rank', 'vision_model.encoder.layers.7.mlp.fc1.trellis', 'vision_model.encoder.layers.7.mlp.fc2.SU', 'vision_model.encoder.layers.7.mlp.fc2.SV', 'vision_model.encoder.layers.7.mlp.fc2.rcp', 'vision_model.encoder.layers.7.mlp.fc2.tlut', 'vision_model.encoder.layers.7.mlp.fc2.tp_rank', 'vision_model.encoder.layers.7.mlp.fc2.trellis', 'vision_model.encoder.layers.7.self_attn.k_proj.SU', 'vision_model.encoder.layers.7.self_attn.k_proj.SV', 'vision_model.encoder.layers.7.self_attn.k_proj.rcp', 'vision_model.encoder.layers.7.self_attn.k_proj.tlut', 'vision_model.encoder.layers.7.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.k_proj.trellis', 'vision_model.encoder.layers.7.self_attn.out_proj.SU', 'vision_model.encoder.layers.7.self_attn.out_proj.SV', 'vision_model.encoder.layers.7.self_attn.out_proj.rcp', 'vision_model.encoder.layers.7.self_attn.out_proj.tlut', 'vision_model.encoder.layers.7.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.out_proj.trellis', 'vision_model.encoder.layers.7.self_attn.q_proj.SU', 'vision_model.encoder.layers.7.self_attn.q_proj.SV', 'vision_model.encoder.layers.7.self_attn.q_proj.rcp', 'vision_model.encoder.layers.7.self_attn.q_proj.tlut', 'vision_model.encoder.layers.7.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.q_proj.trellis', 'vision_model.encoder.layers.7.self_attn.v_proj.SU', 'vision_model.encoder.layers.7.self_attn.v_proj.SV', 'vision_model.encoder.layers.7.self_attn.v_proj.rcp', 'vision_model.encoder.layers.7.self_attn.v_proj.tlut', 'vision_model.encoder.layers.7.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.7.self_attn.v_proj.trellis', 'vision_model.encoder.layers.8.mlp.fc1.SU', 'vision_model.encoder.layers.8.mlp.fc1.SV', 'vision_model.encoder.layers.8.mlp.fc1.rcp', 'vision_model.encoder.layers.8.mlp.fc1.tlut', 'vision_model.encoder.layers.8.mlp.fc1.tp_rank', 'vision_model.encoder.layers.8.mlp.fc1.trellis', 'vision_model.encoder.layers.8.mlp.fc2.SU', 'vision_model.encoder.layers.8.mlp.fc2.SV', 'vision_model.encoder.layers.8.mlp.fc2.rcp', 'vision_model.encoder.layers.8.mlp.fc2.tlut', 'vision_model.encoder.layers.8.mlp.fc2.tp_rank', 'vision_model.encoder.layers.8.mlp.fc2.trellis', 'vision_model.encoder.layers.8.self_attn.k_proj.SU', 'vision_model.encoder.layers.8.self_attn.k_proj.SV', 'vision_model.encoder.layers.8.self_attn.k_proj.rcp', 'vision_model.encoder.layers.8.self_attn.k_proj.tlut', 'vision_model.encoder.layers.8.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.k_proj.trellis', 'vision_model.encoder.layers.8.self_attn.out_proj.SU', 'vision_model.encoder.layers.8.self_attn.out_proj.SV', 'vision_model.encoder.layers.8.self_attn.out_proj.rcp', 'vision_model.encoder.layers.8.self_attn.out_proj.tlut', 'vision_model.encoder.layers.8.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.out_proj.trellis', 'vision_model.encoder.layers.8.self_attn.q_proj.SU', 'vision_model.encoder.layers.8.self_attn.q_proj.SV', 'vision_model.encoder.layers.8.self_attn.q_proj.rcp', 'vision_model.encoder.layers.8.self_attn.q_proj.tlut', 'vision_model.encoder.layers.8.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.q_proj.trellis', 'vision_model.encoder.layers.8.self_attn.v_proj.SU', 'vision_model.encoder.layers.8.self_attn.v_proj.SV', 'vision_model.encoder.layers.8.self_attn.v_proj.rcp', 'vision_model.encoder.layers.8.self_attn.v_proj.tlut', 'vision_model.encoder.layers.8.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.8.self_attn.v_proj.trellis', 'vision_model.encoder.layers.9.mlp.fc1.SU', 'vision_model.encoder.layers.9.mlp.fc1.SV', 'vision_model.encoder.layers.9.mlp.fc1.rcp', 'vision_model.encoder.layers.9.mlp.fc1.tlut', 'vision_model.encoder.layers.9.mlp.fc1.tp_rank', 'vision_model.encoder.layers.9.mlp.fc1.trellis', 'vision_model.encoder.layers.9.mlp.fc2.SU', 'vision_model.encoder.layers.9.mlp.fc2.SV', 'vision_model.encoder.layers.9.mlp.fc2.rcp', 'vision_model.encoder.layers.9.mlp.fc2.tlut', 'vision_model.encoder.layers.9.mlp.fc2.tp_rank', 'vision_model.encoder.layers.9.mlp.fc2.trellis', 'vision_model.encoder.layers.9.self_attn.k_proj.SU', 'vision_model.encoder.layers.9.self_attn.k_proj.SV', 'vision_model.encoder.layers.9.self_attn.k_proj.rcp', 'vision_model.encoder.layers.9.self_attn.k_proj.tlut', 'vision_model.encoder.layers.9.self_attn.k_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.k_proj.trellis', 'vision_model.encoder.layers.9.self_attn.out_proj.SU', 'vision_model.encoder.layers.9.self_attn.out_proj.SV', 'vision_model.encoder.layers.9.self_attn.out_proj.rcp', 'vision_model.encoder.layers.9.self_attn.out_proj.tlut', 'vision_model.encoder.layers.9.self_attn.out_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.out_proj.trellis', 'vision_model.encoder.layers.9.self_attn.q_proj.SU', 'vision_model.encoder.layers.9.self_attn.q_proj.SV', 'vision_model.encoder.layers.9.self_attn.q_proj.rcp', 'vision_model.encoder.layers.9.self_attn.q_proj.tlut', 'vision_model.encoder.layers.9.self_attn.q_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.q_proj.trellis', 'vision_model.encoder.layers.9.self_attn.v_proj.SU', 'vision_model.encoder.layers.9.self_attn.v_proj.SV', 'vision_model.encoder.layers.9.self_attn.v_proj.rcp', 'vision_model.encoder.layers.9.self_attn.v_proj.tlut', 'vision_model.encoder.layers.9.self_attn.v_proj.tp_rank', 'vision_model.encoder.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
I0429 17:17:49.060908 1121123 hfize_clip.py:65] Loading text layer 0
W0429 17:17:49.061161 1121123 warnings.py:110] /workspace/Weight_compression/qtip/quantize_llama/hfize_clip.py:32: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved = torch.load(f'{path_prefix}/{full_key}.pt', map_location='cpu')

I0429 17:17:49.196003 1121123 hfize_clip.py:65] Loading text layer 1
I0429 17:17:49.453390 1121123 hfize_clip.py:65] Loading text layer 2
I0429 17:17:49.777383 1121123 hfize_clip.py:65] Loading text layer 3
I0429 17:17:50.138224 1121123 hfize_clip.py:65] Loading text layer 4
I0429 17:17:50.481287 1121123 hfize_clip.py:65] Loading text layer 5
I0429 17:17:50.816218 1121123 hfize_clip.py:65] Loading text layer 6
I0429 17:17:51.183233 1121123 hfize_clip.py:65] Loading text layer 7
I0429 17:17:51.569035 1121123 hfize_clip.py:65] Loading text layer 8
I0429 17:17:51.904113 1121123 hfize_clip.py:65] Loading text layer 9
I0429 17:17:52.232031 1121123 hfize_clip.py:65] Loading text layer 10
I0429 17:17:52.623583 1121123 hfize_clip.py:65] Loading text layer 11
I0429 17:17:52.970467 1121123 hfize_clip.py:65] Loading vision layer 0
I0429 17:17:53.328375 1121123 hfize_clip.py:65] Loading vision layer 1
I0429 17:17:53.690169 1121123 hfize_clip.py:65] Loading vision layer 2
I0429 17:17:54.068168 1121123 hfize_clip.py:65] Loading vision layer 3
I0429 17:17:54.408488 1121123 hfize_clip.py:65] Loading vision layer 4
I0429 17:17:54.749475 1121123 hfize_clip.py:65] Loading vision layer 5
I0429 17:17:55.151172 1121123 hfize_clip.py:65] Loading vision layer 6
I0429 17:17:55.520545 1121123 hfize_clip.py:65] Loading vision layer 7
I0429 17:17:55.887222 1121123 hfize_clip.py:65] Loading vision layer 8
I0429 17:17:56.233038 1121123 hfize_clip.py:65] Loading vision layer 9
I0429 17:17:56.597594 1121123 hfize_clip.py:65] Loading vision layer 10
I0429 17:17:56.970302 1121123 hfize_clip.py:65] Loading vision layer 11
I0429 17:17:57.279461 1121123 hfize_clip.py:65] Loading vision layer 12
I0429 17:17:57.693250 1121123 hfize_clip.py:65] Loading vision layer 13
I0429 17:17:58.029745 1121123 hfize_clip.py:65] Loading vision layer 14
I0429 17:17:58.408105 1121123 hfize_clip.py:65] Loading vision layer 15
I0429 17:17:58.767640 1121123 hfize_clip.py:65] Loading vision layer 16
I0429 17:17:59.113066 1121123 hfize_clip.py:65] Loading vision layer 17
I0429 17:17:59.467048 1121123 hfize_clip.py:65] Loading vision layer 18
I0429 17:17:59.813961 1121123 hfize_clip.py:65] Loading vision layer 19
I0429 17:18:00.164655 1121123 hfize_clip.py:65] Loading vision layer 20
I0429 17:18:00.540751 1121123 hfize_clip.py:65] Loading vision layer 21
I0429 17:18:00.888792 1121123 hfize_clip.py:65] Loading vision layer 22
I0429 17:18:01.229357 1121123 hfize_clip.py:65] Loading vision layer 23
I0429 17:18:01.596079 1121123 hfize_clip.py:95] Copying logit_scale
I0429 17:18:01.596611 1121123 hfize_clip.py:90] Copying submodule: visual_projection
I0429 17:18:01.651943 1121123 hfize_clip.py:90] Copying submodule: text_projection
I0429 17:18:01.723888 1121123 hfize_clip.py:90] Copying submodule: text token_embedding
I0429 17:18:01.872419 1121123 hfize_clip.py:90] Copying submodule: text position_embedding
I0429 17:18:01.940422 1121123 hfize_clip.py:90] Copying submodule: text final_layer_norm
I0429 17:18:01.941028 1121123 hfize_clip.py:90] Copying submodule: vision patch_embedding
I0429 17:18:01.983181 1121123 hfize_clip.py:90] Copying submodule: vision position_embedding
I0429 17:18:02.057706 1121123 hfize_clip.py:90] Copying submodule: vision post_layernorm
I0429 17:18:02.058135 1121123 hfize_clip.py:121] Saving model to ../hf_model_comp/qtip/hf/clip-vit-large-patch14_5bit...
  0%|          | 0/196 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  1%|          | 1/196 [00:11<37:35, 11.57s/it]  1%|          | 2/196 [00:14<20:30,  6.34s/it]  2%|▏         | 3/196 [00:16<14:59,  4.66s/it]  2%|▏         | 4/196 [00:19<12:23,  3.87s/it]  3%|▎         | 5/196 [00:22<10:54,  3.42s/it]  3%|▎         | 6/196 [00:24<09:54,  3.13s/it]  4%|▎         | 7/196 [00:27<09:17,  2.95s/it]  4%|▍         | 8/196 [00:29<08:51,  2.83s/it]  5%|▍         | 9/196 [00:32<08:33,  2.74s/it]  5%|▌         | 10/196 [00:35<08:19,  2.68s/it]  6%|▌         | 11/196 [00:37<08:08,  2.64s/it]  6%|▌         | 12/196 [00:40<08:02,  2.62s/it]  7%|▋         | 13/196 [00:42<07:55,  2.60s/it]  7%|▋         | 14/196 [00:45<07:49,  2.58s/it]  8%|▊         | 15/196 [00:47<07:45,  2.57s/it]  8%|▊         | 16/196 [00:50<07:41,  2.56s/it]  9%|▊         | 17/196 [00:52<07:37,  2.56s/it]  9%|▉         | 18/196 [00:55<07:36,  2.57s/it] 10%|▉         | 19/196 [00:58<07:36,  2.58s/it] 10%|█         | 20/196 [01:00<07:33,  2.58s/it] 11%|█         | 21/196 [01:03<07:30,  2.58s/it] 11%|█         | 22/196 [01:05<07:26,  2.57s/it] 12%|█▏        | 23/196 [01:08<07:23,  2.56s/it] 12%|█▏        | 24/196 [01:10<07:21,  2.57s/it] 13%|█▎        | 25/196 [01:13<07:18,  2.56s/it] 13%|█▎        | 26/196 [01:15<07:15,  2.56s/it] 14%|█▍        | 27/196 [01:18<07:12,  2.56s/it] 14%|█▍        | 28/196 [01:21<07:09,  2.56s/it] 15%|█▍        | 29/196 [01:23<07:07,  2.56s/it] 15%|█▌        | 30/196 [01:26<07:04,  2.56s/it] 16%|█▌        | 31/196 [01:28<07:02,  2.56s/it] 16%|█▋        | 32/196 [01:31<06:59,  2.56s/it] 17%|█▋        | 33/196 [01:33<06:56,  2.55s/it] 17%|█▋        | 34/196 [01:36<06:54,  2.56s/it] 18%|█▊        | 35/196 [01:38<06:51,  2.56s/it] 18%|█▊        | 36/196 [01:41<06:50,  2.57s/it] 19%|█▉        | 37/196 [01:44<06:48,  2.57s/it] 19%|█▉        | 38/196 [01:46<06:48,  2.59s/it] 20%|█▉        | 39/196 [01:49<06:45,  2.58s/it] 20%|██        | 40/196 [01:51<06:42,  2.58s/it] 21%|██        | 41/196 [01:54<06:38,  2.57s/it] 21%|██▏       | 42/196 [01:57<06:35,  2.57s/it] 22%|██▏       | 43/196 [01:59<06:33,  2.57s/it] 22%|██▏       | 44/196 [02:02<06:30,  2.57s/it] 23%|██▎       | 45/196 [02:04<06:27,  2.57s/it] 23%|██▎       | 46/196 [02:07<06:24,  2.56s/it] 24%|██▍       | 47/196 [02:09<06:22,  2.56s/it] 24%|██▍       | 48/196 [02:12<06:18,  2.56s/it] 25%|██▌       | 49/196 [02:14<06:16,  2.56s/it] 26%|██▌       | 50/196 [02:17<06:13,  2.56s/it] 26%|██▌       | 51/196 [02:20<06:11,  2.56s/it] 27%|██▋       | 52/196 [02:22<06:08,  2.56s/it] 27%|██▋       | 53/196 [02:25<06:06,  2.56s/it] 28%|██▊       | 54/196 [02:27<06:04,  2.57s/it] 28%|██▊       | 55/196 [02:30<06:02,  2.57s/it] 29%|██▊       | 56/196 [02:32<05:59,  2.57s/it] 29%|██▉       | 57/196 [02:35<05:57,  2.57s/it] 30%|██▉       | 58/196 [02:38<05:56,  2.58s/it] 30%|███       | 59/196 [02:40<05:53,  2.58s/it] 31%|███       | 60/196 [02:43<05:50,  2.58s/it] 31%|███       | 61/196 [02:45<05:47,  2.57s/it] 32%|███▏      | 62/196 [02:48<05:44,  2.57s/it] 32%|███▏      | 63/196 [02:50<05:42,  2.57s/it] 33%|███▎      | 64/196 [02:53<05:39,  2.57s/it] 33%|███▎      | 65/196 [02:56<05:36,  2.57s/it] 34%|███▎      | 66/196 [02:58<05:33,  2.57s/it] 34%|███▍      | 67/196 [03:01<05:31,  2.57s/it] 35%|███▍      | 68/196 [03:03<05:27,  2.56s/it] 35%|███▌      | 69/196 [03:06<05:24,  2.55s/it] 36%|███▌      | 70/196 [03:08<05:21,  2.55s/it] 36%|███▌      | 71/196 [03:11<05:20,  2.56s/it] 37%|███▋      | 72/196 [03:14<05:18,  2.57s/it] 37%|███▋      | 73/196 [03:16<05:16,  2.57s/it] 38%|███▊      | 74/196 [03:19<05:13,  2.57s/it] 38%|███▊      | 75/196 [03:21<05:11,  2.58s/it] 39%|███▉      | 76/196 [03:24<05:08,  2.57s/it] 39%|███▉      | 77/196 [03:26<05:08,  2.59s/it] 40%|███▉      | 78/196 [03:29<05:05,  2.59s/it] 40%|████      | 79/196 [03:32<05:01,  2.58s/it] 41%|████      | 80/196 [03:34<04:59,  2.58s/it] 41%|████▏     | 81/196 [03:37<04:56,  2.57s/it] 42%|████▏     | 82/196 [03:39<04:53,  2.57s/it] 42%|████▏     | 83/196 [03:42<04:50,  2.57s/it] 43%|████▎     | 84/196 [03:44<04:48,  2.58s/it] 43%|████▎     | 85/196 [03:47<04:45,  2.58s/it] 44%|████▍     | 86/196 [03:50<04:43,  2.57s/it] 44%|████▍     | 87/196 [03:52<04:40,  2.58s/it] 45%|████▍     | 88/196 [03:55<04:38,  2.58s/it] 45%|████▌     | 89/196 [03:57<04:35,  2.58s/it] 46%|████▌     | 90/196 [04:00<04:33,  2.58s/it] 46%|████▋     | 91/196 [04:03<04:30,  2.58s/it] 47%|████▋     | 92/196 [04:05<04:28,  2.58s/it] 47%|████▋     | 93/196 [04:08<04:25,  2.58s/it] 48%|████▊     | 94/196 [04:10<04:22,  2.58s/it] 48%|████▊     | 95/196 [04:13<04:20,  2.58s/it] 49%|████▉     | 96/196 [04:15<04:18,  2.58s/it] 49%|████▉     | 97/196 [04:18<04:16,  2.59s/it] 50%|█████     | 98/196 [04:21<04:12,  2.58s/it] 51%|█████     | 99/196 [04:23<04:09,  2.57s/it] 51%|█████     | 100/196 [04:26<04:05,  2.56s/it] 52%|█████▏    | 101/196 [04:28<04:03,  2.56s/it] 52%|█████▏    | 102/196 [04:31<04:00,  2.56s/it] 53%|█████▎    | 103/196 [04:33<03:57,  2.56s/it] 53%|█████▎    | 104/196 [04:36<03:55,  2.56s/it] 54%|█████▎    | 105/196 [04:38<03:52,  2.56s/it] 54%|█████▍    | 106/196 [04:41<03:49,  2.55s/it] 55%|█████▍    | 107/196 [04:44<03:47,  2.55s/it] 55%|█████▌    | 108/196 [04:46<03:44,  2.56s/it] 56%|█████▌    | 109/196 [04:49<03:42,  2.56s/it] 56%|█████▌    | 110/196 [04:51<03:39,  2.56s/it] 57%|█████▋    | 111/196 [04:54<03:37,  2.56s/it] 57%|█████▋    | 112/196 [04:56<03:34,  2.56s/it] 58%|█████▊    | 113/196 [04:59<03:32,  2.56s/it] 58%|█████▊    | 114/196 [05:02<03:30,  2.57s/it] 59%|█████▊    | 115/196 [05:04<03:28,  2.57s/it] 59%|█████▉    | 116/196 [05:07<03:26,  2.59s/it] 60%|█████▉    | 117/196 [05:09<03:24,  2.59s/it] 60%|██████    | 118/196 [05:12<03:21,  2.59s/it] 61%|██████    | 119/196 [05:14<03:19,  2.59s/it] 61%|██████    | 120/196 [05:17<03:16,  2.59s/it] 62%|██████▏   | 121/196 [05:20<03:14,  2.59s/it] 62%|██████▏   | 122/196 [05:22<03:11,  2.59s/it] 63%|██████▎   | 123/196 [05:25<03:09,  2.59s/it] 63%|██████▎   | 124/196 [05:27<03:06,  2.59s/it] 64%|██████▍   | 125/196 [05:30<03:03,  2.59s/it] 64%|██████▍   | 126/196 [05:33<03:00,  2.58s/it] 65%|██████▍   | 127/196 [05:35<02:57,  2.58s/it] 65%|██████▌   | 128/196 [05:38<02:54,  2.57s/it] 66%|██████▌   | 129/196 [05:40<02:52,  2.58s/it] 66%|██████▋   | 130/196 [05:43<02:50,  2.58s/it] 67%|██████▋   | 131/196 [05:45<02:47,  2.58s/it] 67%|██████▋   | 132/196 [05:48<02:45,  2.58s/it] 68%|██████▊   | 133/196 [05:51<02:43,  2.59s/it] 68%|██████▊   | 134/196 [05:53<02:40,  2.58s/it] 69%|██████▉   | 135/196 [05:56<02:38,  2.59s/it] 69%|██████▉   | 136/196 [05:58<02:35,  2.60s/it] 70%|██████▉   | 137/196 [06:01<02:33,  2.60s/it] 70%|███████   | 138/196 [06:04<02:30,  2.60s/it] 71%|███████   | 139/196 [06:06<02:27,  2.59s/it] 71%|███████▏  | 140/196 [06:09<02:24,  2.59s/it] 72%|███████▏  | 141/196 [06:11<02:22,  2.58s/it] 72%|███████▏  | 142/196 [06:14<02:19,  2.58s/it] 73%|███████▎  | 143/196 [06:17<02:16,  2.58s/it] 73%|███████▎  | 144/196 [06:19<02:14,  2.58s/it] 74%|███████▍  | 145/196 [06:22<02:11,  2.58s/it] 74%|███████▍  | 146/196 [06:24<02:08,  2.58s/it] 75%|███████▌  | 147/196 [06:27<02:06,  2.57s/it] 76%|███████▌  | 148/196 [06:29<02:03,  2.58s/it] 76%|███████▌  | 149/196 [06:32<02:01,  2.58s/it] 77%|███████▋  | 150/196 [06:35<01:58,  2.57s/it] 77%|███████▋  | 151/196 [06:37<01:55,  2.57s/it] 78%|███████▊  | 152/196 [06:40<01:53,  2.58s/it] 78%|███████▊  | 153/196 [06:42<01:50,  2.57s/it] 79%|███████▊  | 154/196 [06:45<01:47,  2.56s/it] 79%|███████▉  | 155/196 [06:47<01:44,  2.56s/it] 80%|███████▉  | 156/196 [06:50<01:42,  2.57s/it] 80%|████████  | 157/196 [06:53<01:40,  2.57s/it] 81%|████████  | 158/196 [06:55<01:37,  2.56s/it] 81%|████████  | 159/196 [06:58<01:34,  2.56s/it] 82%|████████▏ | 160/196 [07:00<01:32,  2.56s/it] 82%|████████▏ | 161/196 [07:03<01:29,  2.56s/it] 83%|████████▎ | 162/196 [07:05<01:27,  2.57s/it] 83%|████████▎ | 163/196 [07:08<01:24,  2.57s/it] 84%|████████▎ | 164/196 [07:10<01:22,  2.57s/it] 84%|████████▍ | 165/196 [07:13<01:19,  2.56s/it] 85%|████████▍ | 166/196 [07:16<01:16,  2.56s/it] 85%|████████▌ | 167/196 [07:18<01:14,  2.56s/it] 86%|████████▌ | 168/196 [07:21<01:11,  2.56s/it] 86%|████████▌ | 169/196 [07:23<01:08,  2.55s/it] 87%|████████▋ | 170/196 [07:26<01:06,  2.55s/it] 87%|████████▋ | 171/196 [07:28<01:03,  2.55s/it] 88%|████████▊ | 172/196 [07:31<01:01,  2.55s/it] 88%|████████▊ | 173/196 [07:33<00:58,  2.56s/it] 89%|████████▉ | 174/196 [07:36<00:56,  2.57s/it] 89%|████████▉ | 175/196 [07:39<00:54,  2.59s/it] 90%|████████▉ | 176/196 [07:41<00:51,  2.58s/it] 90%|█████████ | 177/196 [07:44<00:49,  2.58s/it] 91%|█████████ | 178/196 [07:46<00:46,  2.58s/it] 91%|█████████▏| 179/196 [07:49<00:43,  2.58s/it] 92%|█████████▏| 180/196 [07:52<00:41,  2.58s/it] 92%|█████████▏| 181/196 [07:54<00:38,  2.60s/it] 93%|█████████▎| 182/196 [07:57<00:36,  2.59s/it] 93%|█████████▎| 183/196 [07:59<00:33,  2.58s/it] 94%|█████████▍| 184/196 [08:02<00:31,  2.58s/it] 94%|█████████▍| 185/196 [08:05<00:28,  2.59s/it] 95%|█████████▍| 186/196 [08:07<00:25,  2.58s/it] 95%|█████████▌| 187/196 [08:10<00:23,  2.58s/it] 96%|█████████▌| 188/196 [08:12<00:20,  2.58s/it] 96%|█████████▋| 189/196 [08:15<00:18,  2.58s/it] 97%|█████████▋| 190/196 [08:17<00:15,  2.57s/it] 97%|█████████▋| 191/196 [08:20<00:12,  2.57s/it] 98%|█████████▊| 192/196 [08:23<00:10,  2.57s/it] 98%|█████████▊| 193/196 [08:25<00:07,  2.57s/it] 99%|█████████▉| 194/196 [08:28<00:05,  2.59s/it] 99%|█████████▉| 195/196 [08:30<00:02,  2.59s/it]100%|██████████| 196/196 [08:32<00:00,  2.43s/it]100%|██████████| 196/196 [08:32<00:00,  2.62s/it]
Top-1 Accuracy: 70.19%
Top-5 Accuracy: 91.19%
