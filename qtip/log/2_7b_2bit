I0210 08:33:50.491756 126 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0210 08:33:50.491930 126 utils.py:162] NumExpr defaulting to 16 threads.
I0210 08:33:50.667890 126 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 15, in <module>
    from lib import utils
  File "/home/jgryu/Weight_compression/qtip/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/home/jgryu/Weight_compression/qtip/lib/utils/data_utils.py", line 8, in <module>
    from lib import codebook
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/__init__.py", line 1, in <module>
    import qtip_kernels
ModuleNotFoundError: No module named 'qtip_kernels'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/__init__.py", line 1, in <module>
    import qtip_kernels
ModuleNotFoundError: No module named 'qtip_kernels'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 23, in <module>
    from lib import codebook, utils
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/__init__.py", line 1, in <module>
    import qtip_kernels
ModuleNotFoundError: No module named 'qtip_kernels'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 12, in <module>
    from lib.linear import QuantizedLinear
  File "/home/jgryu/Weight_compression/qtip/lib/linear/__init__.py", line 1, in <module>
    from .quantized_linear import QuantizedLinear
  File "/home/jgryu/Weight_compression/qtip/lib/linear/quantized_linear.py", line 7, in <module>
    from lib.codebook import bitshift
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/__init__.py", line 1, in <module>
    import qtip_kernels
ModuleNotFoundError: No module named 'qtip_kernels'
W0210 08:33:59.930823 416 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jgryu/Weight_compression/qtip/eval/eval_zeroshot.py", line 13, in <module>
    from lib.linear import QuantizedLinear
  File "/home/jgryu/Weight_compression/qtip/lib/linear/__init__.py", line 1, in <module>
    from .quantized_linear import QuantizedLinear
  File "/home/jgryu/Weight_compression/qtip/lib/linear/quantized_linear.py", line 7, in <module>
    from lib.codebook import bitshift
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/__init__.py", line 1, in <module>
    import qtip_kernels
ModuleNotFoundError: No module named 'qtip_kernels'
I0210 08:34:06.175294 561 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0210 08:34:06.175517 561 utils.py:162] NumExpr defaulting to 16 threads.
I0210 08:34:06.361008 561 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 15, in <module>
    from lib import utils
  File "/home/jgryu/Weight_compression/qtip/lib/utils/__init__.py", line 1, in <module>
    from .data_utils import *
  File "/home/jgryu/Weight_compression/qtip/lib/utils/data_utils.py", line 8, in <module>
    from lib import codebook
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/__init__.py", line 1, in <module>
    import qtip_kernels
ModuleNotFoundError: No module named 'qtip_kernels'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 9, in <module>
    from lib import codebook, utils
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/__init__.py", line 1, in <module>
    import qtip_kernels
ModuleNotFoundError: No module named 'qtip_kernels'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 23, in <module>
    from lib import codebook, utils
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/__init__.py", line 1, in <module>
    import qtip_kernels
ModuleNotFoundError: No module named 'qtip_kernels'
Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 12, in <module>
    from lib.linear import QuantizedLinear
  File "/home/jgryu/Weight_compression/qtip/lib/linear/__init__.py", line 1, in <module>
    from .quantized_linear import QuantizedLinear
  File "/home/jgryu/Weight_compression/qtip/lib/linear/quantized_linear.py", line 7, in <module>
    from lib.codebook import bitshift
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/__init__.py", line 1, in <module>
    import qtip_kernels
ModuleNotFoundError: No module named 'qtip_kernels'
W0210 08:34:15.740206 851 warnings.py:109] /opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(

Traceback (most recent call last):
  File "/opt/conda/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/jgryu/Weight_compression/qtip/eval/eval_zeroshot.py", line 13, in <module>
    from lib.linear import QuantizedLinear
  File "/home/jgryu/Weight_compression/qtip/lib/linear/__init__.py", line 1, in <module>
    from .quantized_linear import QuantizedLinear
  File "/home/jgryu/Weight_compression/qtip/lib/linear/quantized_linear.py", line 7, in <module>
    from lib.codebook import bitshift
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/__init__.py", line 1, in <module>
    import qtip_kernels
ModuleNotFoundError: No module named 'qtip_kernels'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 5, in <module>
    import glog
ModuleNotFoundError: No module named 'glog'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 5, in <module>
    import glog
ModuleNotFoundError: No module named 'glog'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 13, in <module>
    import glog
ModuleNotFoundError: No module named 'glog'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 7, in <module>
    import datasets
ModuleNotFoundError: No module named 'datasets'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_zeroshot.py", line 6, in <module>
    import datasets
ModuleNotFoundError: No module named 'datasets'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 5, in <module>
    import glog
ModuleNotFoundError: No module named 'glog'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 5, in <module>
    import glog
ModuleNotFoundError: No module named 'glog'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 13, in <module>
    import glog
ModuleNotFoundError: No module named 'glog'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 7, in <module>
    import datasets
ModuleNotFoundError: No module named 'datasets'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_zeroshot.py", line 6, in <module>
    import datasets
ModuleNotFoundError: No module named 'datasets'
I0210 14:44:49.132384 1518 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0210 14:44:49.132669 1518 utils.py:162] NumExpr defaulting to 16 threads.
I0210 14:44:49.243934 1518 config.py:58] PyTorch version 2.4.0 available.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:01,  3.12it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:01,  2.79it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:01<00:01,  2.78it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:01<00:00,  2.85it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:01<00:00,  3.10it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  3.28it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:01<00:00,  3.08it/s]
I0210 14:46:11.302293 1518 quantize_finetune_llama.py:134] loaded model
Downloading builder script:   0%|          | 0.00/3.33k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 3.33k/3.33k [00:00<00:00, 10.1MB/s]
Downloading readme:   0%|          | 0.00/3.61k [00:00<?, ?B/s]Downloading readme: 100%|██████████| 3.61k/3.61k [00:00<00:00, 12.8MB/s]
The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample.
You can avoid this prompt in future by passing the argument `trust_remote_code=True`.

Do you wish to run the custom code? [y/N] Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 214, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 136, in main
    devset = utils.sample_rp1t(tokenizer, args.devset_size, args.ctx_size,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/utils/data_utils.py", line 197, in sample_rp1t
    dataset = load_dataset('togethercomputer/RedPajama-Data-1T-Sample',
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 2594, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 2266, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1914, in dataset_module_factory
    raise e1 from None
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1887, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1525, in get_module
    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 133, in resolve_trust_remote_code
    raise ValueError(
ValueError: The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
I0210 14:46:39.637679 1859 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0210 14:46:39.637809 1859 utils.py:162] NumExpr defaulting to 16 threads.
I0210 14:46:39.748165 1859 config.py:58] PyTorch version 2.4.0 available.
W0210 14:46:41.441509 1859 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0210 14:46:41.442036 1859 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.11it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.79it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.88it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.11it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.81it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.11it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.96it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  9.09it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.29it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.29it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.31it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.35it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.42it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.35it/s]
W0210 14:46:45.805129 1859 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 129, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 65, in main
    saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './ckpt/2_7b_2bit/0_q.pt'
I0210 14:46:48.636795 2200 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0210 14:46:48.636934 2200 utils.py:162] NumExpr defaulting to 16 threads.
I0210 14:46:48.820934 2200 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py", line 304, in hf_raise_for_status
    response.raise_for_status()
  File "/opt/conda/lib/python3.11/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
    raise head_call_error
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1673, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 376, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 400, in _request_wrapper
    hf_raise_for_status(response)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py", line 321, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-67aa115a-0f18d426354a723f44c583ad;83834302-4e9c-4e82-ad51-bec5aca90f48)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 128, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 60, in main
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 864, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.
401 Client Error. (Request ID: Root=1-67aa115a-0f18d426354a723f44c583ad;83834302-4e9c-4e82-ad51-bec5aca90f48)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/2_7b_2bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 73, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 28, in main
    model, model_str = model_from_hf_path(args.hf_path, max_mem_ratio=args.max_mem_ratio)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/utils/unsafe_import.py", line 16, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/2_7b_2bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_zeroshot.py", line 9, in <module>
    from lm_eval import evaluator, tasks
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/evaluator.py", line 12, in <module>
    import lm_eval.api.metrics
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/registry.py", line 4, in <module>
    import evaluate as hf_evaluate
  File "/opt/conda/lib/python3.11/site-packages/evaluate/__init__.py", line 29, in <module>
    from .evaluation_suite import EvaluationSuite
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluation_suite/__init__.py", line 10, in <module>
    from ..evaluator import evaluator
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluator/__init__.py", line 17, in <module>
    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS
  File "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py", line 26, in <module>
    from ..image_processing_utils import BaseImageProcessor
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_processing_utils.py", line 21, in <module>
    from .image_transforms import center_crop, normalize, rescale
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_transforms.py", line 22, in <module>
    from .image_utils import (
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_utils.py", line 58, in <module>
    from torchvision.transforms import InterpolationMode
  File "/opt/conda/lib/python3.11/site-packages/torchvision/__init__.py", line 10, in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torchvision/_meta_registrations.py", line 163, in <module>
    @torch.library.register_fake("torchvision::nms")
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 654, in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 154, in _register_fake
    handle = entry.abstract_impl.register(func_to_register, source)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_library/abstract_impl.py", line 31, in register
    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: operator torchvision::nms does not exist
I0210 14:52:03.861159 2897 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0210 14:52:03.861300 2897 utils.py:162] NumExpr defaulting to 16 threads.
I0210 14:52:03.971906 2897 config.py:58] PyTorch version 2.4.0 available.
W0210 14:52:06.042119 2897 warnings.py:110] /home/jgryu/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.45it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.90it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.11it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.08it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.19it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.38it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.19it/s]
I0210 14:52:07.123337 2897 quantize_finetune_llama.py:134] loaded model
The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample.
You can avoid this prompt in future by passing the argument `trust_remote_code=True`.

Do you wish to run the custom code? [y/N] Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 214, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 136, in main
    devset = utils.sample_rp1t(tokenizer, args.devset_size, args.ctx_size,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/utils/data_utils.py", line 197, in sample_rp1t
    dataset = load_dataset('togethercomputer/RedPajama-Data-1T-Sample',
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 2594, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 2266, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1914, in dataset_module_factory
    raise e1 from None
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1887, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1525, in get_module
    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 133, in resolve_trust_remote_code
    raise ValueError(
ValueError: The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
I0210 14:52:30.348947 3238 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0210 14:52:30.349083 3238 utils.py:162] NumExpr defaulting to 16 threads.
I0210 14:52:30.459636 3238 config.py:58] PyTorch version 2.4.0 available.
W0210 14:52:32.167579 3238 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0210 14:52:32.168058 3238 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.27it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.39it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.47it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.74it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.92it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.12it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.85it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.85it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.06it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.08it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.16it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.12it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.22it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.15it/s]
W0210 14:52:36.514951 3238 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 129, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 65, in main
    saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './ckpt/2_7b_2bit/0_q.pt'
I0210 14:52:39.385234 3579 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0210 14:52:39.385374 3579 utils.py:162] NumExpr defaulting to 16 threads.
I0210 14:52:39.559063 3579 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py", line 304, in hf_raise_for_status
    response.raise_for_status()
  File "/opt/conda/lib/python3.11/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
    raise head_call_error
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1673, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 376, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 400, in _request_wrapper
    hf_raise_for_status(response)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py", line 321, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-67aa12b9-54d54e24017e38c75251d127;b8cc46bb-c9a4-4cb5-b6fe-1dbbea8853a2)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 128, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 60, in main
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 864, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.
401 Client Error. (Request ID: Root=1-67aa12b9-54d54e24017e38c75251d127;b8cc46bb-c9a4-4cb5-b6fe-1dbbea8853a2)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/2_7b_2bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 73, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 28, in main
    model, model_str = model_from_hf_path(args.hf_path, max_mem_ratio=args.max_mem_ratio)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/utils/unsafe_import.py", line 16, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/2_7b_2bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_zeroshot.py", line 9, in <module>
    from lm_eval import evaluator, tasks
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/evaluator.py", line 12, in <module>
    import lm_eval.api.metrics
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/registry.py", line 4, in <module>
    import evaluate as hf_evaluate
  File "/opt/conda/lib/python3.11/site-packages/evaluate/__init__.py", line 29, in <module>
    from .evaluation_suite import EvaluationSuite
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluation_suite/__init__.py", line 10, in <module>
    from ..evaluator import evaluator
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluator/__init__.py", line 17, in <module>
    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS
  File "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py", line 26, in <module>
    from ..image_processing_utils import BaseImageProcessor
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_processing_utils.py", line 21, in <module>
    from .image_transforms import center_crop, normalize, rescale
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_transforms.py", line 22, in <module>
    from .image_utils import (
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_utils.py", line 58, in <module>
    from torchvision.transforms import InterpolationMode
  File "/opt/conda/lib/python3.11/site-packages/torchvision/__init__.py", line 10, in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torchvision/_meta_registrations.py", line 163, in <module>
    @torch.library.register_fake("torchvision::nms")
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 654, in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 154, in _register_fake
    handle = entry.abstract_impl.register(func_to_register, source)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_library/abstract_impl.py", line 31, in register
    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: operator torchvision::nms does not exist
I0211 02:16:37.792925 4220 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0211 02:16:37.793056 4220 utils.py:162] NumExpr defaulting to 16 threads.
I0211 02:16:37.904433 4220 config.py:58] PyTorch version 2.4.0 available.
W0211 02:16:39.995504 4220 warnings.py:110] /home/jgryu/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.25it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.77it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.94it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.03it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.94it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.13it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.99it/s]
I0211 02:16:41.101087 4220 quantize_finetune_llama.py:134] loaded model
The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample.
You can avoid this prompt in future by passing the argument `trust_remote_code=True`.

Do you wish to run the custom code? [y/N] Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 214, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 136, in main
    devset = utils.sample_rp1t(tokenizer, args.devset_size, args.ctx_size,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/utils/data_utils.py", line 197, in sample_rp1t
    dataset = load_dataset('togethercomputer/RedPajama-Data-1T-Sample',
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 2594, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 2266, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1914, in dataset_module_factory
    raise e1 from None
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1887, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1525, in get_module
    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 133, in resolve_trust_remote_code
    raise ValueError(
ValueError: The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample.
Please pass the argument `trust_remote_code=True` to allow custom code to be run.
I0211 02:17:03.141610 4562 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0211 02:17:03.141749 4562 utils.py:162] NumExpr defaulting to 16 threads.
I0211 02:17:03.254370 4562 config.py:58] PyTorch version 2.4.0 available.
W0211 02:17:04.968922 4562 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0211 02:17:04.969442 4562 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.45it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.39it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.67it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.87it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.92it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.04it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.81it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.55it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.84it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.91it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.97it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.00it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.99it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.94it/s]
W0211 02:17:09.365611 4562 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 129, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 65, in main
    saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './ckpt/2_7b_2bit/0_q.pt'
I0211 02:17:12.240185 4902 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0211 02:17:12.240313 4902 utils.py:162] NumExpr defaulting to 16 threads.
I0211 02:17:12.415615 4902 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py", line 304, in hf_raise_for_status
    response.raise_for_status()
  File "/opt/conda/lib/python3.11/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
    raise head_call_error
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1673, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 376, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 400, in _request_wrapper
    hf_raise_for_status(response)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py", line 321, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-67aab32f-2d1fb3214b4963594b7ff549;f613c864-9e31-4dc1-a811-161ef07ab5de)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 128, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 60, in main
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 864, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.
401 Client Error. (Request ID: Root=1-67aab32f-2d1fb3214b4963594b7ff549;f613c864-9e31-4dc1-a811-161ef07ab5de)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/2_7b_2bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 73, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 28, in main
    model, model_str = model_from_hf_path(args.hf_path, max_mem_ratio=args.max_mem_ratio)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/utils/unsafe_import.py", line 16, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/2_7b_2bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_zeroshot.py", line 9, in <module>
    from lm_eval import evaluator, tasks
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/evaluator.py", line 12, in <module>
    import lm_eval.api.metrics
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/registry.py", line 4, in <module>
    import evaluate as hf_evaluate
  File "/opt/conda/lib/python3.11/site-packages/evaluate/__init__.py", line 29, in <module>
    from .evaluation_suite import EvaluationSuite
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluation_suite/__init__.py", line 10, in <module>
    from ..evaluator import evaluator
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluator/__init__.py", line 17, in <module>
    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS
  File "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py", line 26, in <module>
    from ..image_processing_utils import BaseImageProcessor
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_processing_utils.py", line 21, in <module>
    from .image_transforms import center_crop, normalize, rescale
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_transforms.py", line 22, in <module>
    from .image_utils import (
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_utils.py", line 58, in <module>
    from torchvision.transforms import InterpolationMode
  File "/opt/conda/lib/python3.11/site-packages/torchvision/__init__.py", line 10, in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torchvision/_meta_registrations.py", line 163, in <module>
    @torch.library.register_fake("torchvision::nms")
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 654, in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 154, in _register_fake
    handle = entry.abstract_impl.register(func_to_register, source)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_library/abstract_impl.py", line 31, in register
    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: operator torchvision::nms does not exist
I0211 02:40:39.896969 5748 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0211 02:40:39.897099 5748 utils.py:162] NumExpr defaulting to 16 threads.
I0211 02:40:40.007377 5748 config.py:58] PyTorch version 2.4.0 available.
W0211 02:40:42.054594 5748 warnings.py:110] /home/jgryu/Weight_compression/qtip/lib/codebook/bitshift.py:161: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  tlut = torch.load(fname)

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.08it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.62it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.84it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.98it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.10it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.23it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.02it/s]
I0211 02:40:43.135791 5748 quantize_finetune_llama.py:134] loaded model
The repository for togethercomputer/RedPajama-Data-1T-Sample contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/togethercomputer/RedPajama-Data-1T-Sample.
You can avoid this prompt in future by passing the argument `trust_remote_code=True`.

Do you wish to run the custom code? [y/N] Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 214, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/quantize_finetune_llama.py", line 136, in main
    devset = utils.sample_rp1t(tokenizer, args.devset_size, args.ctx_size,
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/utils/data_utils.py", line 197, in sample_rp1t
    dataset = load_dataset('togethercomputer/RedPajama-Data-1T-Sample',
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 2594, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 2266, in load_dataset_builder
    dataset_module = dataset_module_factory(
                     ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1887, in dataset_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 1525, in get_module
    trust_remote_code = resolve_trust_remote_code(self.trust_remote_code, self.name)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/datasets/load.py", line 120, in resolve_trust_remote_code
    answer = input(
             ^^^^^^
KeyboardInterrupt
I0211 03:35:33.818738 7893 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0211 03:35:33.819017 7893 utils.py:162] NumExpr defaulting to 16 threads.
I0211 03:35:33.927412 7893 config.py:58] PyTorch version 2.4.0 available.
W0211 03:35:35.629548 7893 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0211 03:35:35.630006 7893 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.52it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.27it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.73it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.95it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.13it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.26it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.95it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.78it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.96it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.10it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.10it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.16it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.21it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.13it/s]
W0211 03:35:39.928137 7893 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0211 03:35:39.929220 7893 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0211 03:35:39.947009 7893 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0211 03:35:39.952600 7893 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0211 03:35:39.968597 7893 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0211 03:35:39.979924 7893 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0211 03:35:39.991608 7893 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0211 03:35:40.004117 7893 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0211 03:35:40.018539 7893 hfize_llama.py:113] loaded layer 0
I0211 03:35:40.118224 7893 hfize_llama.py:113] loaded layer 1
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 129, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 65, in main
    saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './ckpt/2_7b_2bit/2_q.pt'
I0211 03:35:43.048737 8233 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0211 03:35:43.048874 8233 utils.py:162] NumExpr defaulting to 16 threads.
I0211 03:35:43.221883 8233 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py", line 304, in hf_raise_for_status
    response.raise_for_status()
  File "/opt/conda/lib/python3.11/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
    raise head_call_error
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1673, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 376, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 400, in _request_wrapper
    hf_raise_for_status(response)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py", line 321, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-67aac591-0353988e112e3df8570d1dac;643b57be-8d16-4f9c-a755-c95c144f9977)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 128, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 60, in main
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 864, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.
401 Client Error. (Request ID: Root=1-67aac591-0353988e112e3df8570d1dac;643b57be-8d16-4f9c-a755-c95c144f9977)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/2_7b_2bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 73, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 28, in main
    model, model_str = model_from_hf_path(args.hf_path, max_mem_ratio=args.max_mem_ratio)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/utils/unsafe_import.py", line 16, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/2_7b_2bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_zeroshot.py", line 9, in <module>
    from lm_eval import evaluator, tasks
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/evaluator.py", line 12, in <module>
    import lm_eval.api.metrics
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/registry.py", line 4, in <module>
    import evaluate as hf_evaluate
  File "/opt/conda/lib/python3.11/site-packages/evaluate/__init__.py", line 29, in <module>
    from .evaluation_suite import EvaluationSuite
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluation_suite/__init__.py", line 10, in <module>
    from ..evaluator import evaluator
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluator/__init__.py", line 17, in <module>
    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS
  File "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py", line 26, in <module>
    from ..image_processing_utils import BaseImageProcessor
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_processing_utils.py", line 21, in <module>
    from .image_transforms import center_crop, normalize, rescale
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_transforms.py", line 22, in <module>
    from .image_utils import (
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_utils.py", line 58, in <module>
    from torchvision.transforms import InterpolationMode
  File "/opt/conda/lib/python3.11/site-packages/torchvision/__init__.py", line 10, in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torchvision/_meta_registrations.py", line 163, in <module>
    @torch.library.register_fake("torchvision::nms")
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 654, in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 154, in _register_fake
    handle = entry.abstract_impl.register(func_to_register, source)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_library/abstract_impl.py", line 31, in register
    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: operator torchvision::nms does not exist
I0211 08:55:49.897063 10340 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0211 08:55:49.897191 10340 utils.py:162] NumExpr defaulting to 16 threads.
I0211 08:55:50.006852 10340 config.py:58] PyTorch version 2.4.0 available.
W0211 08:55:51.710480 10340 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0211 08:55:51.710945 10340 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.14it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.79it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.07it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.19it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.07it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.10it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.02it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.48it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.97it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.94it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.09it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.14it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.25it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.11it/s]
W0211 08:55:56.050057 10340 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0211 08:55:56.051127 10340 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0211 08:55:56.065209 10340 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0211 08:55:56.076514 10340 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0211 08:55:56.087731 10340 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0211 08:55:56.098322 10340 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0211 08:55:56.110374 10340 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0211 08:55:56.129732 10340 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0211 08:55:56.146152 10340 hfize_llama.py:113] loaded layer 0
I0211 08:55:56.234729 10340 hfize_llama.py:113] loaded layer 1
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 129, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 65, in main
    saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 1065, in load
    with _open_file_like(f, 'rb') as opened_file:
         ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 468, in _open_file_like
    return _open_file(name_or_buffer, mode)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/serialization.py", line 449, in __init__
    super().__init__(open(name, mode))
                     ^^^^^^^^^^^^^^^^
FileNotFoundError: [Errno 2] No such file or directory: './ckpt/2_7b_2bit/2_q.pt'
I0211 08:55:59.092938 10680 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0211 08:55:59.093080 10680 utils.py:162] NumExpr defaulting to 16 threads.
I0211 08:55:59.267424 10680 config.py:58] PyTorch version 2.4.0 available.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py", line 304, in hf_raise_for_status
    response.raise_for_status()
  File "/opt/conda/lib/python3.11/site-packages/requests/models.py", line 1024, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1240, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1347, in _hf_hub_download_to_cache_dir
    _raise_on_head_call_error(head_call_error, force_download, local_files_only)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1854, in _raise_on_head_call_error
    raise head_call_error
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1751, in _get_metadata_or_catch_error
    metadata = get_hf_file_metadata(
               ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 1673, in get_hf_file_metadata
    r = _request_wrapper(
        ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 376, in _request_wrapper
    response = _request_wrapper(
               ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/file_download.py", line 400, in _request_wrapper
    hf_raise_for_status(response)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py", line 321, in hf_raise_for_status
    raise GatedRepoError(message, response) from e
huggingface_hub.utils._errors.GatedRepoError: 401 Client Error. (Request ID: Root=1-67ab10a1-5e716a7a03202f2b629494c7;16060fee-8737-48c1-b73c-6a8e010f353d)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 128, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/finetune_e2e_llama.py", line 60, in main
    tokenizer = AutoTokenizer.from_pretrained(args.base_model)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py", line 864, in from_pretrained
    config = AutoConfig.from_pretrained(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 421, in cached_file
    raise EnvironmentError(
OSError: You are trying to access a gated repo.
Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.
401 Client Error. (Request ID: Root=1-67ab10a1-5e716a7a03202f2b629494c7;16060fee-8737-48c1-b73c-6a8e010f353d)

Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.
Access to model meta-llama/Llama-2-7b-hf is restricted. You must have access to it and be authenticated to access it. Please log in.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 106, in _inner_fn
    validate_repo_id(arg_value)
  File "/opt/conda/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 154, in validate_repo_id
    raise HFValidationError(
huggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': './hf/2_7b_2bit'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 73, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 28, in main
    model, model_str = model_from_hf_path(args.hf_path, max_mem_ratio=args.max_mem_ratio)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/utils/unsafe_import.py", line 16, in model_from_hf_path
    bad_config = transformers.AutoConfig.from_pretrained(path)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/models/auto/configuration_auto.py", line 1006, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 570, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 629, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/transformers/utils/hub.py", line 469, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: './hf/2_7b_2bit'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_zeroshot.py", line 9, in <module>
    from lm_eval import evaluator, tasks
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/__init__.py", line 1, in <module>
    from .evaluator import evaluate, simple_evaluate
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/evaluator.py", line 12, in <module>
    import lm_eval.api.metrics
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/metrics.py", line 12, in <module>
    from lm_eval.api.registry import register_aggregation, register_metric
  File "/opt/conda/lib/python3.11/site-packages/lm_eval/api/registry.py", line 4, in <module>
    import evaluate as hf_evaluate
  File "/opt/conda/lib/python3.11/site-packages/evaluate/__init__.py", line 29, in <module>
    from .evaluation_suite import EvaluationSuite
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluation_suite/__init__.py", line 10, in <module>
    from ..evaluator import evaluator
  File "/opt/conda/lib/python3.11/site-packages/evaluate/evaluator/__init__.py", line 17, in <module>
    from transformers.pipelines import SUPPORTED_TASKS as SUPPORTED_PIPELINE_TASKS
  File "/opt/conda/lib/python3.11/site-packages/transformers/pipelines/__init__.py", line 26, in <module>
    from ..image_processing_utils import BaseImageProcessor
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_processing_utils.py", line 21, in <module>
    from .image_transforms import center_crop, normalize, rescale
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_transforms.py", line 22, in <module>
    from .image_utils import (
  File "/opt/conda/lib/python3.11/site-packages/transformers/image_utils.py", line 58, in <module>
    from torchvision.transforms import InterpolationMode
  File "/opt/conda/lib/python3.11/site-packages/torchvision/__init__.py", line 10, in <module>
    from torchvision import _meta_registrations, datasets, io, models, ops, transforms, utils  # usort:skip
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torchvision/_meta_registrations.py", line 163, in <module>
    @torch.library.register_fake("torchvision::nms")
     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 654, in register
    use_lib._register_fake(op_name, func, _stacklevel=stacklevel + 1)
  File "/opt/conda/lib/python3.11/site-packages/torch/library.py", line 154, in _register_fake
    handle = entry.abstract_impl.register(func_to_register, source)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_library/abstract_impl.py", line 31, in register
    if torch._C._dispatch_has_kernel_for_dispatch_key(self.qualname, "Meta"):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: operator torchvision::nms does not exist
I0212 09:37:14.971129 49763 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0212 09:37:14.971580 49763 utils.py:162] NumExpr defaulting to 16 threads.
I0212 09:37:15.153176 49763 config.py:58] PyTorch version 2.4.0 available.
W0212 09:37:16.834792 49763 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0212 09:37:16.835370 49763 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.70it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.55it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.88it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.09it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.19it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.32it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.06it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.92it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.14it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.21it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.23it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.24it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.31it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.24it/s]
W0212 09:37:20.126763 49763 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0212 09:37:20.127913 49763 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0212 09:37:20.138085 49763 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0212 09:37:20.152220 49763 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0212 09:37:20.163233 49763 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0212 09:37:20.167829 49763 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0212 09:37:20.177163 49763 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0212 09:37:20.187323 49763 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0212 09:37:20.197037 49763 hfize_llama.py:113] loaded layer 0
I0212 09:37:20.242800 49763 hfize_llama.py:113] loaded layer 1
I0212 09:37:20.327431 49763 hfize_llama.py:113] loaded layer 2
I0212 09:37:20.415104 49763 hfize_llama.py:113] loaded layer 3
I0212 09:37:20.440384 49763 hfize_llama.py:113] loaded layer 4
I0212 09:37:20.464019 49763 hfize_llama.py:113] loaded layer 5
I0212 09:37:20.490941 49763 hfize_llama.py:113] loaded layer 6
I0212 09:37:20.531554 49763 hfize_llama.py:113] loaded layer 7
I0212 09:37:20.573819 49763 hfize_llama.py:113] loaded layer 8
I0212 09:37:20.645604 49763 hfize_llama.py:113] loaded layer 9
I0212 09:37:20.681016 49763 hfize_llama.py:113] loaded layer 10
I0212 09:37:20.711127 49763 hfize_llama.py:113] loaded layer 11
I0212 09:37:20.732973 49763 hfize_llama.py:113] loaded layer 12
I0212 09:37:20.816737 49763 hfize_llama.py:113] loaded layer 13
I0212 09:37:20.860561 49763 hfize_llama.py:113] loaded layer 14
I0212 09:37:20.893051 49763 hfize_llama.py:113] loaded layer 15
I0212 09:37:20.922504 49763 hfize_llama.py:113] loaded layer 16
I0212 09:37:20.976582 49763 hfize_llama.py:113] loaded layer 17
I0212 09:37:20.999261 49763 hfize_llama.py:113] loaded layer 18
I0212 09:37:21.028633 49763 hfize_llama.py:113] loaded layer 19
I0212 09:37:21.068720 49763 hfize_llama.py:113] loaded layer 20
I0212 09:37:21.118472 49763 hfize_llama.py:113] loaded layer 21
I0212 09:37:21.192795 49763 hfize_llama.py:113] loaded layer 22
I0212 09:37:21.235684 49763 hfize_llama.py:113] loaded layer 23
I0212 09:37:21.274689 49763 hfize_llama.py:113] loaded layer 24
I0212 09:37:21.309320 49763 hfize_llama.py:113] loaded layer 25
I0212 09:37:21.337281 49763 hfize_llama.py:113] loaded layer 26
I0212 09:37:21.366234 49763 hfize_llama.py:113] loaded layer 27
I0212 09:37:21.450395 49763 hfize_llama.py:113] loaded layer 28
I0212 09:37:21.508801 49763 hfize_llama.py:113] loaded layer 29
I0212 09:37:21.556659 49763 hfize_llama.py:113] loaded layer 30
I0212 09:37:21.608682 49763 hfize_llama.py:113] loaded layer 31
I0212 09:37:21.608753 49763 hfize_llama.py:115] saving model...
I0212 09:37:28.332409 49763 hfize_llama.py:122] successfully loaded hfized model
I0212 11:35:33.969119 65181 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0212 11:35:33.969356 65181 utils.py:162] NumExpr defaulting to 16 threads.
I0212 11:35:34.151250 65181 config.py:58] PyTorch version 2.4.0 available.
W0212 11:35:35.828142 65181 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0212 11:35:35.828680 65181 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.89it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.64it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.01it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.21it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.23it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.32it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.11it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.97it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.20it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.21it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.19it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.26it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.34it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.26it/s]
W0212 11:35:38.969179 65181 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0212 11:35:38.970286 65181 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0212 11:35:38.976699 65181 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0212 11:35:38.980988 65181 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0212 11:35:38.986440 65181 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0212 11:35:38.991308 65181 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0212 11:35:39.000125 65181 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0212 11:35:39.009859 65181 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0212 11:35:39.021855 65181 hfize_llama.py:113] loaded layer 0
I0212 11:35:39.065878 65181 hfize_llama.py:113] loaded layer 1
I0212 11:35:39.087876 65181 hfize_llama.py:113] loaded layer 2
I0212 11:35:39.113828 65181 hfize_llama.py:113] loaded layer 3
I0212 11:35:39.147938 65181 hfize_llama.py:113] loaded layer 4
I0212 11:35:39.201545 65181 hfize_llama.py:113] loaded layer 5
I0212 11:35:39.258985 65181 hfize_llama.py:113] loaded layer 6
I0212 11:35:39.308200 65181 hfize_llama.py:113] loaded layer 7
I0212 11:35:39.354382 65181 hfize_llama.py:113] loaded layer 8
I0212 11:35:39.398455 65181 hfize_llama.py:113] loaded layer 9
I0212 11:35:39.446664 65181 hfize_llama.py:113] loaded layer 10
I0212 11:35:39.474698 65181 hfize_llama.py:113] loaded layer 11
I0212 11:35:39.527009 65181 hfize_llama.py:113] loaded layer 12
I0212 11:35:39.599432 65181 hfize_llama.py:113] loaded layer 13
I0212 11:35:39.660133 65181 hfize_llama.py:113] loaded layer 14
I0212 11:35:39.683258 65181 hfize_llama.py:113] loaded layer 15
I0212 11:35:39.727193 65181 hfize_llama.py:113] loaded layer 16
I0212 11:35:39.778498 65181 hfize_llama.py:113] loaded layer 17
I0212 11:35:39.807057 65181 hfize_llama.py:113] loaded layer 18
I0212 11:35:39.853399 65181 hfize_llama.py:113] loaded layer 19
I0212 11:35:39.879736 65181 hfize_llama.py:113] loaded layer 20
I0212 11:35:39.905037 65181 hfize_llama.py:113] loaded layer 21
I0212 11:35:39.946104 65181 hfize_llama.py:113] loaded layer 22
I0212 11:35:39.991721 65181 hfize_llama.py:113] loaded layer 23
I0212 11:35:40.015916 65181 hfize_llama.py:113] loaded layer 24
I0212 11:35:40.042823 65181 hfize_llama.py:113] loaded layer 25
I0212 11:35:40.063512 65181 hfize_llama.py:113] loaded layer 26
I0212 11:35:40.088340 65181 hfize_llama.py:113] loaded layer 27
I0212 11:35:40.159667 65181 hfize_llama.py:113] loaded layer 28
I0212 11:35:40.184645 65181 hfize_llama.py:113] loaded layer 29
I0212 11:35:40.223102 65181 hfize_llama.py:113] loaded layer 30
I0212 11:35:40.254463 65181 hfize_llama.py:113] loaded layer 31
I0212 11:35:40.254522 65181 hfize_llama.py:115] saving model...
I0212 11:35:47.734470 65181 hfize_llama.py:122] successfully loaded hfized model
I0212 11:41:06.669260 65881 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0212 11:41:06.669485 65881 utils.py:162] NumExpr defaulting to 16 threads.
I0212 11:41:06.852151 65881 config.py:58] PyTorch version 2.4.0 available.
W0212 11:41:08.535617 65881 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0212 11:41:08.536205 65881 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.40it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.43it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.84it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.04it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.15it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.09it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.90it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.97it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.10it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.99it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.09it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.14it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.24it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.15it/s]
W0212 11:41:11.679936 65881 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0212 11:41:11.681130 65881 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0212 11:41:11.695445 65881 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0212 11:41:11.700297 65881 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0212 11:41:11.705463 65881 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0212 11:41:11.710381 65881 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0212 11:41:11.720587 65881 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0212 11:41:11.731940 65881 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0212 11:41:11.739875 65881 hfize_llama.py:113] loaded layer 0
I0212 11:41:11.763717 65881 hfize_llama.py:113] loaded layer 1
I0212 11:41:11.788953 65881 hfize_llama.py:113] loaded layer 2
I0212 11:41:11.817070 65881 hfize_llama.py:113] loaded layer 3
I0212 11:41:11.851833 65881 hfize_llama.py:113] loaded layer 4
I0212 11:41:11.898528 65881 hfize_llama.py:113] loaded layer 5
I0212 11:41:11.947644 65881 hfize_llama.py:113] loaded layer 6
I0212 11:41:11.994358 65881 hfize_llama.py:113] loaded layer 7
I0212 11:41:12.020697 65881 hfize_llama.py:113] loaded layer 8
I0212 11:41:12.050635 65881 hfize_llama.py:113] loaded layer 9
I0212 11:41:12.080809 65881 hfize_llama.py:113] loaded layer 10
I0212 11:41:12.119620 65881 hfize_llama.py:113] loaded layer 11
I0212 11:41:12.144386 65881 hfize_llama.py:113] loaded layer 12
I0212 11:41:12.178643 65881 hfize_llama.py:113] loaded layer 13
I0212 11:41:12.219862 65881 hfize_llama.py:113] loaded layer 14
I0212 11:41:12.271079 65881 hfize_llama.py:113] loaded layer 15
I0212 11:41:12.318747 65881 hfize_llama.py:113] loaded layer 16
I0212 11:41:12.374032 65881 hfize_llama.py:113] loaded layer 17
I0212 11:41:12.401516 65881 hfize_llama.py:113] loaded layer 18
I0212 11:41:12.454087 65881 hfize_llama.py:113] loaded layer 19
I0212 11:41:12.481193 65881 hfize_llama.py:113] loaded layer 20
I0212 11:41:12.509756 65881 hfize_llama.py:113] loaded layer 21
I0212 11:41:12.544384 65881 hfize_llama.py:113] loaded layer 22
I0212 11:41:12.616679 65881 hfize_llama.py:113] loaded layer 23
I0212 11:41:12.650743 65881 hfize_llama.py:113] loaded layer 24
I0212 11:41:12.684978 65881 hfize_llama.py:113] loaded layer 25
I0212 11:41:12.723849 65881 hfize_llama.py:113] loaded layer 26
I0212 11:41:12.756086 65881 hfize_llama.py:113] loaded layer 27
I0212 11:41:12.791162 65881 hfize_llama.py:113] loaded layer 28
I0212 11:41:12.833300 65881 hfize_llama.py:113] loaded layer 29
I0212 11:41:12.914635 65881 hfize_llama.py:113] loaded layer 30
I0212 11:41:12.944850 65881 hfize_llama.py:113] loaded layer 31
I0212 11:41:12.944910 65881 hfize_llama.py:115] saving model...
I0212 11:41:20.405838 65881 hfize_llama.py:122] successfully loaded hfized model
  0%|          | 0/83 [00:00<?, ?it/s]W0212 11:42:50.447000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.448000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.448000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q0 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.448000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q3 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.448000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.448000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.448000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.472000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.472000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.472000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.472000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.472000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.734000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.734000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.734000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.734000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:50.734000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] q5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.504000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.504000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.505000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z3 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.505000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.505000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z0 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.505000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.505000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.521000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.521000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.521000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.521000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.521000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.707000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.707000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.707000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.707000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:51.708000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] z5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.113000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.113000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.113000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.113000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.113000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.113000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x3 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.113000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.131000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.131000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.131000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.131000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.131000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.916000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.916000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.916000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.916000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:53.916000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/0] x0 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.121000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.121000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q3 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.121000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.121000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.121000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.121000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q0 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.121000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.149000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.149000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.149000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.149000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.149000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.306000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.306000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.306000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.306000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.306000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] q5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.515000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.515000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z3 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.515000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z0 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.515000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.515000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.515000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.516000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.534000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.534000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.534000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.534000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.534000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.593000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.594000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.594000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.594000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:58.594000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] z5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.273000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.560000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.560000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.560000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x3 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.560000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x6 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.560000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.560000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.560000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.579000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.579000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.579000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.579000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.580000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.812000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x4 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.812000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x2 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.812000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x1 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.813000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x5 is not in var_ranges, defaulting to unknown range.
W0212 11:42:59.813000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] x0 is not in var_ranges, defaulting to unknown range.
W0212 11:43:00.135000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/1] ps0 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.662000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.662000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q3 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.662000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.662000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.662000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.662000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q0 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.663000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.700000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.700000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.701000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.701000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.701000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.858000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.858000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q6 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.858000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q4 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.858000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q1 is not in var_ranges, defaulting to unknown range.
W0212 11:43:04.858000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] q5 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.144000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.144000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z3 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.145000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z0 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.145000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.145000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.145000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.145000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.172000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.172000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.172000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.172000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.172000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.238000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.238000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z4 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.238000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z1 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.238000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z6 is not in var_ranges, defaulting to unknown range.
W0212 11:43:05.238000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] z5 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.114000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.126000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.133000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.133000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.549000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.550000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.550000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.550000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.550000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.550000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.550000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.578000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.578000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.578000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.578000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.578000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.847000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.847000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.847000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x3 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.847000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.847000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x6 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.847000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.848000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0212 11:43:06.848000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0212 11:43:07.114000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x4 is not in var_ranges, defaulting to unknown range.
W0212 11:43:07.114000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x2 is not in var_ranges, defaulting to unknown range.
W0212 11:43:07.114000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x1 is not in var_ranges, defaulting to unknown range.
W0212 11:43:07.114000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x5 is not in var_ranges, defaulting to unknown range.
W0212 11:43:07.114000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] x0 is not in var_ranges, defaulting to unknown range.
W0212 11:43:07.501000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps0 is not in var_ranges, defaulting to unknown range.
W0212 11:43:07.506000 140431774877504 torch/fx/experimental/symbolic_shapes.py:4449] [0/2] ps5 is not in var_ranges, defaulting to unknown range.
  0%|          | 0/83 [00:20<?, ?it/s]
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 74, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/eval/eval_ppl.py", line 52, in main
    output = model(input,
             ^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/model/llama.py", line 1441, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/model/llama.py", line 1246, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/model/llama.py", line 985, in forward
    hidden_states = self.mlp(hidden_states)
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/model/llama.py", line 402, in forward
    self.act_fn(self.gate_proj(x)) * self.up_proj(x))
                ^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/linear/quantized_linear.py", line 86, in forward
    return self.no_ckpt_forward(input)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/linear/quantized_linear.py", line 95, in no_ckpt_forward
    self.codebook_class = bitshift.BitshiftLinear(
                          ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/bitshift.py", line 373, in __init__
    self.cb = bitshift_codebook(L, K, V, tlut_bits, decode_mode, tlut=tlut)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/bitshift.py", line 185, in __init__
    self.register_buffer('recons_state', self.recons(self.state))
                                         ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jgryu/Weight_compression/qtip/lib/codebook/bitshift.py", line 201, in recons
    encoded.int().to(self.lut.device)].to(encoded.device)
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
I0212 11:43:59.613408 67348 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0212 11:43:59.613630 67348 utils.py:162] NumExpr defaulting to 16 threads.
I0212 11:43:59.792962 67348 config.py:58] PyTorch version 2.4.0 available.
W0212 11:44:01.470484 67348 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0212 11:44:01.471016 67348 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.04it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.02it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.39it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.73it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.94it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.12it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.73it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.46it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.80it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.96it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.99it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.00it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.06it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.97it/s]
W0212 11:44:04.827234 67348 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0212 11:44:04.828398 67348 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0212 11:44:04.843700 67348 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0212 11:44:04.848628 67348 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0212 11:44:04.860934 67348 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0212 11:44:04.865912 67348 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0212 11:44:04.876495 67348 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0212 11:44:04.888066 67348 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0212 11:44:04.896141 67348 hfize_llama.py:113] loaded layer 0
I0212 11:44:04.920619 67348 hfize_llama.py:113] loaded layer 1
I0212 11:44:04.941276 67348 hfize_llama.py:113] loaded layer 2
I0212 11:44:04.971340 67348 hfize_llama.py:113] loaded layer 3
I0212 11:44:05.005226 67348 hfize_llama.py:113] loaded layer 4
I0212 11:44:05.070895 67348 hfize_llama.py:113] loaded layer 5
I0212 11:44:05.128269 67348 hfize_llama.py:113] loaded layer 6
I0212 11:44:05.186177 67348 hfize_llama.py:113] loaded layer 7
I0212 11:44:05.233782 67348 hfize_llama.py:113] loaded layer 8
I0212 11:44:05.288548 67348 hfize_llama.py:113] loaded layer 9
I0212 11:44:05.337731 67348 hfize_llama.py:113] loaded layer 10
I0212 11:44:05.391127 67348 hfize_llama.py:113] loaded layer 11
I0212 11:44:05.443095 67348 hfize_llama.py:113] loaded layer 12
I0212 11:44:05.523165 67348 hfize_llama.py:113] loaded layer 13
I0212 11:44:05.599904 67348 hfize_llama.py:113] loaded layer 14
I0212 11:44:05.650166 67348 hfize_llama.py:113] loaded layer 15
I0212 11:44:05.712063 67348 hfize_llama.py:113] loaded layer 16
I0212 11:44:05.768923 67348 hfize_llama.py:113] loaded layer 17
I0212 11:44:05.794081 67348 hfize_llama.py:113] loaded layer 18
I0212 11:44:05.837614 67348 hfize_llama.py:113] loaded layer 19
I0212 11:44:05.865231 67348 hfize_llama.py:113] loaded layer 20
I0212 11:44:05.892277 67348 hfize_llama.py:113] loaded layer 21
I0212 11:44:05.929175 67348 hfize_llama.py:113] loaded layer 22
I0212 11:44:05.960199 67348 hfize_llama.py:113] loaded layer 23
I0212 11:44:06.024205 67348 hfize_llama.py:113] loaded layer 24
I0212 11:44:06.068621 67348 hfize_llama.py:113] loaded layer 25
I0212 11:44:06.088948 67348 hfize_llama.py:113] loaded layer 26
I0212 11:44:06.126978 67348 hfize_llama.py:113] loaded layer 27
I0212 11:44:06.201576 67348 hfize_llama.py:113] loaded layer 28
I0212 11:44:06.230285 67348 hfize_llama.py:113] loaded layer 29
I0212 11:44:06.274905 67348 hfize_llama.py:113] loaded layer 30
I0212 11:44:06.301660 67348 hfize_llama.py:113] loaded layer 31
I0212 11:44:06.301723 67348 hfize_llama.py:115] saving model...
I0212 11:44:13.747352 67348 hfize_llama.py:122] successfully loaded hfized model
I0212 11:45:02.029102 68479 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0212 11:45:02.029293 68479 utils.py:162] NumExpr defaulting to 16 threads.
I0212 11:45:02.202441 68479 config.py:58] PyTorch version 2.4.0 available.
W0212 11:45:03.880870 68479 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0212 11:45:03.881396 68479 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.97it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.72it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.98it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.05it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.94it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.16it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.99it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  8.95it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  9.23it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  9.32it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  9.35it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.39it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.43it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.36it/s]
W0212 11:45:07.049045 68479 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0212 11:45:07.050216 68479 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0212 11:45:07.056653 68479 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0212 11:45:07.060771 68479 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0212 11:45:07.066084 68479 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0212 11:45:07.070822 68479 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0212 11:45:07.079845 68479 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0212 11:45:07.091978 68479 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0212 11:45:07.104108 68479 hfize_llama.py:113] loaded layer 0
I0212 11:45:07.145470 68479 hfize_llama.py:113] loaded layer 1
I0212 11:45:07.196286 68479 hfize_llama.py:113] loaded layer 2
I0212 11:45:07.245198 68479 hfize_llama.py:113] loaded layer 3
I0212 11:45:07.283881 68479 hfize_llama.py:113] loaded layer 4
I0212 11:45:07.329944 68479 hfize_llama.py:113] loaded layer 5
I0212 11:45:07.371770 68479 hfize_llama.py:113] loaded layer 6
I0212 11:45:07.400614 68479 hfize_llama.py:113] loaded layer 7
I0212 11:45:07.428164 68479 hfize_llama.py:113] loaded layer 8
I0212 11:45:07.450481 68479 hfize_llama.py:113] loaded layer 9
I0212 11:45:07.488172 68479 hfize_llama.py:113] loaded layer 10
I0212 11:45:07.529588 68479 hfize_llama.py:113] loaded layer 11
I0212 11:45:07.576785 68479 hfize_llama.py:113] loaded layer 12
I0212 11:45:07.648900 68479 hfize_llama.py:113] loaded layer 13
I0212 11:45:07.739617 68479 hfize_llama.py:113] loaded layer 14
I0212 11:45:07.768629 68479 hfize_llama.py:113] loaded layer 15
I0212 11:45:07.789839 68479 hfize_llama.py:113] loaded layer 16
I0212 11:45:07.822490 68479 hfize_llama.py:113] loaded layer 17
I0212 11:45:07.858380 68479 hfize_llama.py:113] loaded layer 18
I0212 11:45:07.932544 68479 hfize_llama.py:113] loaded layer 19
I0212 11:45:07.961878 68479 hfize_llama.py:113] loaded layer 20
I0212 11:45:07.992300 68479 hfize_llama.py:113] loaded layer 21
I0212 11:45:08.022786 68479 hfize_llama.py:113] loaded layer 22
I0212 11:45:08.067081 68479 hfize_llama.py:113] loaded layer 23
I0212 11:45:08.102299 68479 hfize_llama.py:113] loaded layer 24
I0212 11:45:08.137422 68479 hfize_llama.py:113] loaded layer 25
I0212 11:45:08.160919 68479 hfize_llama.py:113] loaded layer 26
I0212 11:45:08.199359 68479 hfize_llama.py:113] loaded layer 27
I0212 11:45:08.271862 68479 hfize_llama.py:113] loaded layer 28
I0212 11:45:08.299242 68479 hfize_llama.py:113] loaded layer 29
I0212 11:45:08.324197 68479 hfize_llama.py:113] loaded layer 30
I0212 11:45:08.348965 68479 hfize_llama.py:113] loaded layer 31
I0212 11:45:08.349025 68479 hfize_llama.py:115] saving model...
I0212 11:45:15.657579 68479 hfize_llama.py:122] successfully loaded hfized model
I0212 11:46:42.134957 69553 utils.py:149] Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
I0212 11:46:42.135180 69553 utils.py:162] NumExpr defaulting to 16 threads.
I0212 11:46:42.315544 69553 config.py:58] PyTorch version 2.4.0 available.
W0212 11:46:43.976812 69553 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_config = torch.load(os.path.join(args.quantized_path, 'config.pt'))

I0212 11:46:43.977400 69553 hfize_llama.py:25] LlamaConfig {
  "_name_or_path": "/home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 1,
  "eos_token_id": 2,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 4096,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pretraining_tp": 1,
  "quip_params": {
    "K": 2,
    "L": 16,
    "V": 2,
    "codebook": "bitshift",
    "codebook_version": 0,
    "decode_mode": "quantlut_sym",
    "skip_list": null,
    "split_for_tp": false,
    "td_x": 16,
    "td_y": 16,
    "tlut_bits": 9
  },
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 10000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "float32",
  "transformers_version": "4.45.2",
  "use_cache": true,
  "vocab_size": 32000
}

Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.58it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.36it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.75it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.93it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  9.05it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.91it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.78it/s]
Some weights of the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.v_proj.weight']
- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jgryu/Weight_compression/Wparam_dataset/hf_model/meta-llama--Llama-2-7b-hf and are newly initialized: ['model.layers.0.mlp.down_proj.SU', 'model.layers.0.mlp.down_proj.SV', 'model.layers.0.mlp.down_proj.rcp', 'model.layers.0.mlp.down_proj.tlut', 'model.layers.0.mlp.down_proj.tp_rank', 'model.layers.0.mlp.down_proj.trellis', 'model.layers.0.mlp.gate_proj.SU', 'model.layers.0.mlp.gate_proj.SV', 'model.layers.0.mlp.gate_proj.rcp', 'model.layers.0.mlp.gate_proj.tlut', 'model.layers.0.mlp.gate_proj.tp_rank', 'model.layers.0.mlp.gate_proj.trellis', 'model.layers.0.mlp.up_proj.SU', 'model.layers.0.mlp.up_proj.SV', 'model.layers.0.mlp.up_proj.rcp', 'model.layers.0.mlp.up_proj.tlut', 'model.layers.0.mlp.up_proj.tp_rank', 'model.layers.0.mlp.up_proj.trellis', 'model.layers.0.self_attn.k_proj.SU', 'model.layers.0.self_attn.k_proj.SV', 'model.layers.0.self_attn.k_proj.rcp', 'model.layers.0.self_attn.k_proj.tlut', 'model.layers.0.self_attn.k_proj.tp_rank', 'model.layers.0.self_attn.k_proj.trellis', 'model.layers.0.self_attn.o_proj.SU', 'model.layers.0.self_attn.o_proj.SV', 'model.layers.0.self_attn.o_proj.rcp', 'model.layers.0.self_attn.o_proj.tlut', 'model.layers.0.self_attn.o_proj.tp_rank', 'model.layers.0.self_attn.o_proj.trellis', 'model.layers.0.self_attn.q_proj.SU', 'model.layers.0.self_attn.q_proj.SV', 'model.layers.0.self_attn.q_proj.rcp', 'model.layers.0.self_attn.q_proj.tlut', 'model.layers.0.self_attn.q_proj.tp_rank', 'model.layers.0.self_attn.q_proj.trellis', 'model.layers.0.self_attn.v_proj.SU', 'model.layers.0.self_attn.v_proj.SV', 'model.layers.0.self_attn.v_proj.rcp', 'model.layers.0.self_attn.v_proj.tlut', 'model.layers.0.self_attn.v_proj.tp_rank', 'model.layers.0.self_attn.v_proj.trellis', 'model.layers.1.mlp.down_proj.SU', 'model.layers.1.mlp.down_proj.SV', 'model.layers.1.mlp.down_proj.rcp', 'model.layers.1.mlp.down_proj.tlut', 'model.layers.1.mlp.down_proj.tp_rank', 'model.layers.1.mlp.down_proj.trellis', 'model.layers.1.mlp.gate_proj.SU', 'model.layers.1.mlp.gate_proj.SV', 'model.layers.1.mlp.gate_proj.rcp', 'model.layers.1.mlp.gate_proj.tlut', 'model.layers.1.mlp.gate_proj.tp_rank', 'model.layers.1.mlp.gate_proj.trellis', 'model.layers.1.mlp.up_proj.SU', 'model.layers.1.mlp.up_proj.SV', 'model.layers.1.mlp.up_proj.rcp', 'model.layers.1.mlp.up_proj.tlut', 'model.layers.1.mlp.up_proj.tp_rank', 'model.layers.1.mlp.up_proj.trellis', 'model.layers.1.self_attn.k_proj.SU', 'model.layers.1.self_attn.k_proj.SV', 'model.layers.1.self_attn.k_proj.rcp', 'model.layers.1.self_attn.k_proj.tlut', 'model.layers.1.self_attn.k_proj.tp_rank', 'model.layers.1.self_attn.k_proj.trellis', 'model.layers.1.self_attn.o_proj.SU', 'model.layers.1.self_attn.o_proj.SV', 'model.layers.1.self_attn.o_proj.rcp', 'model.layers.1.self_attn.o_proj.tlut', 'model.layers.1.self_attn.o_proj.tp_rank', 'model.layers.1.self_attn.o_proj.trellis', 'model.layers.1.self_attn.q_proj.SU', 'model.layers.1.self_attn.q_proj.SV', 'model.layers.1.self_attn.q_proj.rcp', 'model.layers.1.self_attn.q_proj.tlut', 'model.layers.1.self_attn.q_proj.tp_rank', 'model.layers.1.self_attn.q_proj.trellis', 'model.layers.1.self_attn.v_proj.SU', 'model.layers.1.self_attn.v_proj.SV', 'model.layers.1.self_attn.v_proj.rcp', 'model.layers.1.self_attn.v_proj.tlut', 'model.layers.1.self_attn.v_proj.tp_rank', 'model.layers.1.self_attn.v_proj.trellis', 'model.layers.10.mlp.down_proj.SU', 'model.layers.10.mlp.down_proj.SV', 'model.layers.10.mlp.down_proj.rcp', 'model.layers.10.mlp.down_proj.tlut', 'model.layers.10.mlp.down_proj.tp_rank', 'model.layers.10.mlp.down_proj.trellis', 'model.layers.10.mlp.gate_proj.SU', 'model.layers.10.mlp.gate_proj.SV', 'model.layers.10.mlp.gate_proj.rcp', 'model.layers.10.mlp.gate_proj.tlut', 'model.layers.10.mlp.gate_proj.tp_rank', 'model.layers.10.mlp.gate_proj.trellis', 'model.layers.10.mlp.up_proj.SU', 'model.layers.10.mlp.up_proj.SV', 'model.layers.10.mlp.up_proj.rcp', 'model.layers.10.mlp.up_proj.tlut', 'model.layers.10.mlp.up_proj.tp_rank', 'model.layers.10.mlp.up_proj.trellis', 'model.layers.10.self_attn.k_proj.SU', 'model.layers.10.self_attn.k_proj.SV', 'model.layers.10.self_attn.k_proj.rcp', 'model.layers.10.self_attn.k_proj.tlut', 'model.layers.10.self_attn.k_proj.tp_rank', 'model.layers.10.self_attn.k_proj.trellis', 'model.layers.10.self_attn.o_proj.SU', 'model.layers.10.self_attn.o_proj.SV', 'model.layers.10.self_attn.o_proj.rcp', 'model.layers.10.self_attn.o_proj.tlut', 'model.layers.10.self_attn.o_proj.tp_rank', 'model.layers.10.self_attn.o_proj.trellis', 'model.layers.10.self_attn.q_proj.SU', 'model.layers.10.self_attn.q_proj.SV', 'model.layers.10.self_attn.q_proj.rcp', 'model.layers.10.self_attn.q_proj.tlut', 'model.layers.10.self_attn.q_proj.tp_rank', 'model.layers.10.self_attn.q_proj.trellis', 'model.layers.10.self_attn.v_proj.SU', 'model.layers.10.self_attn.v_proj.SV', 'model.layers.10.self_attn.v_proj.rcp', 'model.layers.10.self_attn.v_proj.tlut', 'model.layers.10.self_attn.v_proj.tp_rank', 'model.layers.10.self_attn.v_proj.trellis', 'model.layers.11.mlp.down_proj.SU', 'model.layers.11.mlp.down_proj.SV', 'model.layers.11.mlp.down_proj.rcp', 'model.layers.11.mlp.down_proj.tlut', 'model.layers.11.mlp.down_proj.tp_rank', 'model.layers.11.mlp.down_proj.trellis', 'model.layers.11.mlp.gate_proj.SU', 'model.layers.11.mlp.gate_proj.SV', 'model.layers.11.mlp.gate_proj.rcp', 'model.layers.11.mlp.gate_proj.tlut', 'model.layers.11.mlp.gate_proj.tp_rank', 'model.layers.11.mlp.gate_proj.trellis', 'model.layers.11.mlp.up_proj.SU', 'model.layers.11.mlp.up_proj.SV', 'model.layers.11.mlp.up_proj.rcp', 'model.layers.11.mlp.up_proj.tlut', 'model.layers.11.mlp.up_proj.tp_rank', 'model.layers.11.mlp.up_proj.trellis', 'model.layers.11.self_attn.k_proj.SU', 'model.layers.11.self_attn.k_proj.SV', 'model.layers.11.self_attn.k_proj.rcp', 'model.layers.11.self_attn.k_proj.tlut', 'model.layers.11.self_attn.k_proj.tp_rank', 'model.layers.11.self_attn.k_proj.trellis', 'model.layers.11.self_attn.o_proj.SU', 'model.layers.11.self_attn.o_proj.SV', 'model.layers.11.self_attn.o_proj.rcp', 'model.layers.11.self_attn.o_proj.tlut', 'model.layers.11.self_attn.o_proj.tp_rank', 'model.layers.11.self_attn.o_proj.trellis', 'model.layers.11.self_attn.q_proj.SU', 'model.layers.11.self_attn.q_proj.SV', 'model.layers.11.self_attn.q_proj.rcp', 'model.layers.11.self_attn.q_proj.tlut', 'model.layers.11.self_attn.q_proj.tp_rank', 'model.layers.11.self_attn.q_proj.trellis', 'model.layers.11.self_attn.v_proj.SU', 'model.layers.11.self_attn.v_proj.SV', 'model.layers.11.self_attn.v_proj.rcp', 'model.layers.11.self_attn.v_proj.tlut', 'model.layers.11.self_attn.v_proj.tp_rank', 'model.layers.11.self_attn.v_proj.trellis', 'model.layers.12.mlp.down_proj.SU', 'model.layers.12.mlp.down_proj.SV', 'model.layers.12.mlp.down_proj.rcp', 'model.layers.12.mlp.down_proj.tlut', 'model.layers.12.mlp.down_proj.tp_rank', 'model.layers.12.mlp.down_proj.trellis', 'model.layers.12.mlp.gate_proj.SU', 'model.layers.12.mlp.gate_proj.SV', 'model.layers.12.mlp.gate_proj.rcp', 'model.layers.12.mlp.gate_proj.tlut', 'model.layers.12.mlp.gate_proj.tp_rank', 'model.layers.12.mlp.gate_proj.trellis', 'model.layers.12.mlp.up_proj.SU', 'model.layers.12.mlp.up_proj.SV', 'model.layers.12.mlp.up_proj.rcp', 'model.layers.12.mlp.up_proj.tlut', 'model.layers.12.mlp.up_proj.tp_rank', 'model.layers.12.mlp.up_proj.trellis', 'model.layers.12.self_attn.k_proj.SU', 'model.layers.12.self_attn.k_proj.SV', 'model.layers.12.self_attn.k_proj.rcp', 'model.layers.12.self_attn.k_proj.tlut', 'model.layers.12.self_attn.k_proj.tp_rank', 'model.layers.12.self_attn.k_proj.trellis', 'model.layers.12.self_attn.o_proj.SU', 'model.layers.12.self_attn.o_proj.SV', 'model.layers.12.self_attn.o_proj.rcp', 'model.layers.12.self_attn.o_proj.tlut', 'model.layers.12.self_attn.o_proj.tp_rank', 'model.layers.12.self_attn.o_proj.trellis', 'model.layers.12.self_attn.q_proj.SU', 'model.layers.12.self_attn.q_proj.SV', 'model.layers.12.self_attn.q_proj.rcp', 'model.layers.12.self_attn.q_proj.tlut', 'model.layers.12.self_attn.q_proj.tp_rank', 'model.layers.12.self_attn.q_proj.trellis', 'model.layers.12.self_attn.v_proj.SU', 'model.layers.12.self_attn.v_proj.SV', 'model.layers.12.self_attn.v_proj.rcp', 'model.layers.12.self_attn.v_proj.tlut', 'model.layers.12.self_attn.v_proj.tp_rank', 'model.layers.12.self_attn.v_proj.trellis', 'model.layers.13.mlp.down_proj.SU', 'model.layers.13.mlp.down_proj.SV', 'model.layers.13.mlp.down_proj.rcp', 'model.layers.13.mlp.down_proj.tlut', 'model.layers.13.mlp.down_proj.tp_rank', 'model.layers.13.mlp.down_proj.trellis', 'model.layers.13.mlp.gate_proj.SU', 'model.layers.13.mlp.gate_proj.SV', 'model.layers.13.mlp.gate_proj.rcp', 'model.layers.13.mlp.gate_proj.tlut', 'model.layers.13.mlp.gate_proj.tp_rank', 'model.layers.13.mlp.gate_proj.trellis', 'model.layers.13.mlp.up_proj.SU', 'model.layers.13.mlp.up_proj.SV', 'model.layers.13.mlp.up_proj.rcp', 'model.layers.13.mlp.up_proj.tlut', 'model.layers.13.mlp.up_proj.tp_rank', 'model.layers.13.mlp.up_proj.trellis', 'model.layers.13.self_attn.k_proj.SU', 'model.layers.13.self_attn.k_proj.SV', 'model.layers.13.self_attn.k_proj.rcp', 'model.layers.13.self_attn.k_proj.tlut', 'model.layers.13.self_attn.k_proj.tp_rank', 'model.layers.13.self_attn.k_proj.trellis', 'model.layers.13.self_attn.o_proj.SU', 'model.layers.13.self_attn.o_proj.SV', 'model.layers.13.self_attn.o_proj.rcp', 'model.layers.13.self_attn.o_proj.tlut', 'model.layers.13.self_attn.o_proj.tp_rank', 'model.layers.13.self_attn.o_proj.trellis', 'model.layers.13.self_attn.q_proj.SU', 'model.layers.13.self_attn.q_proj.SV', 'model.layers.13.self_attn.q_proj.rcp', 'model.layers.13.self_attn.q_proj.tlut', 'model.layers.13.self_attn.q_proj.tp_rank', 'model.layers.13.self_attn.q_proj.trellis', 'model.layers.13.self_attn.v_proj.SU', 'model.layers.13.self_attn.v_proj.SV', 'model.layers.13.self_attn.v_proj.rcp', 'model.layers.13.self_attn.v_proj.tlut', 'model.layers.13.self_attn.v_proj.tp_rank', 'model.layers.13.self_attn.v_proj.trellis', 'model.layers.14.mlp.down_proj.SU', 'model.layers.14.mlp.down_proj.SV', 'model.layers.14.mlp.down_proj.rcp', 'model.layers.14.mlp.down_proj.tlut', 'model.layers.14.mlp.down_proj.tp_rank', 'model.layers.14.mlp.down_proj.trellis', 'model.layers.14.mlp.gate_proj.SU', 'model.layers.14.mlp.gate_proj.SV', 'model.layers.14.mlp.gate_proj.rcp', 'model.layers.14.mlp.gate_proj.tlut', 'model.layers.14.mlp.gate_proj.tp_rank', 'model.layers.14.mlp.gate_proj.trellis', 'model.layers.14.mlp.up_proj.SU', 'model.layers.14.mlp.up_proj.SV', 'model.layers.14.mlp.up_proj.rcp', 'model.layers.14.mlp.up_proj.tlut', 'model.layers.14.mlp.up_proj.tp_rank', 'model.layers.14.mlp.up_proj.trellis', 'model.layers.14.self_attn.k_proj.SU', 'model.layers.14.self_attn.k_proj.SV', 'model.layers.14.self_attn.k_proj.rcp', 'model.layers.14.self_attn.k_proj.tlut', 'model.layers.14.self_attn.k_proj.tp_rank', 'model.layers.14.self_attn.k_proj.trellis', 'model.layers.14.self_attn.o_proj.SU', 'model.layers.14.self_attn.o_proj.SV', 'model.layers.14.self_attn.o_proj.rcp', 'model.layers.14.self_attn.o_proj.tlut', 'model.layers.14.self_attn.o_proj.tp_rank', 'model.layers.14.self_attn.o_proj.trellis', 'model.layers.14.self_attn.q_proj.SU', 'model.layers.14.self_attn.q_proj.SV', 'model.layers.14.self_attn.q_proj.rcp', 'model.layers.14.self_attn.q_proj.tlut', 'model.layers.14.self_attn.q_proj.tp_rank', 'model.layers.14.self_attn.q_proj.trellis', 'model.layers.14.self_attn.v_proj.SU', 'model.layers.14.self_attn.v_proj.SV', 'model.layers.14.self_attn.v_proj.rcp', 'model.layers.14.self_attn.v_proj.tlut', 'model.layers.14.self_attn.v_proj.tp_rank', 'model.layers.14.self_attn.v_proj.trellis', 'model.layers.15.mlp.down_proj.SU', 'model.layers.15.mlp.down_proj.SV', 'model.layers.15.mlp.down_proj.rcp', 'model.layers.15.mlp.down_proj.tlut', 'model.layers.15.mlp.down_proj.tp_rank', 'model.layers.15.mlp.down_proj.trellis', 'model.layers.15.mlp.gate_proj.SU', 'model.layers.15.mlp.gate_proj.SV', 'model.layers.15.mlp.gate_proj.rcp', 'model.layers.15.mlp.gate_proj.tlut', 'model.layers.15.mlp.gate_proj.tp_rank', 'model.layers.15.mlp.gate_proj.trellis', 'model.layers.15.mlp.up_proj.SU', 'model.layers.15.mlp.up_proj.SV', 'model.layers.15.mlp.up_proj.rcp', 'model.layers.15.mlp.up_proj.tlut', 'model.layers.15.mlp.up_proj.tp_rank', 'model.layers.15.mlp.up_proj.trellis', 'model.layers.15.self_attn.k_proj.SU', 'model.layers.15.self_attn.k_proj.SV', 'model.layers.15.self_attn.k_proj.rcp', 'model.layers.15.self_attn.k_proj.tlut', 'model.layers.15.self_attn.k_proj.tp_rank', 'model.layers.15.self_attn.k_proj.trellis', 'model.layers.15.self_attn.o_proj.SU', 'model.layers.15.self_attn.o_proj.SV', 'model.layers.15.self_attn.o_proj.rcp', 'model.layers.15.self_attn.o_proj.tlut', 'model.layers.15.self_attn.o_proj.tp_rank', 'model.layers.15.self_attn.o_proj.trellis', 'model.layers.15.self_attn.q_proj.SU', 'model.layers.15.self_attn.q_proj.SV', 'model.layers.15.self_attn.q_proj.rcp', 'model.layers.15.self_attn.q_proj.tlut', 'model.layers.15.self_attn.q_proj.tp_rank', 'model.layers.15.self_attn.q_proj.trellis', 'model.layers.15.self_attn.v_proj.SU', 'model.layers.15.self_attn.v_proj.SV', 'model.layers.15.self_attn.v_proj.rcp', 'model.layers.15.self_attn.v_proj.tlut', 'model.layers.15.self_attn.v_proj.tp_rank', 'model.layers.15.self_attn.v_proj.trellis', 'model.layers.16.mlp.down_proj.SU', 'model.layers.16.mlp.down_proj.SV', 'model.layers.16.mlp.down_proj.rcp', 'model.layers.16.mlp.down_proj.tlut', 'model.layers.16.mlp.down_proj.tp_rank', 'model.layers.16.mlp.down_proj.trellis', 'model.layers.16.mlp.gate_proj.SU', 'model.layers.16.mlp.gate_proj.SV', 'model.layers.16.mlp.gate_proj.rcp', 'model.layers.16.mlp.gate_proj.tlut', 'model.layers.16.mlp.gate_proj.tp_rank', 'model.layers.16.mlp.gate_proj.trellis', 'model.layers.16.mlp.up_proj.SU', 'model.layers.16.mlp.up_proj.SV', 'model.layers.16.mlp.up_proj.rcp', 'model.layers.16.mlp.up_proj.tlut', 'model.layers.16.mlp.up_proj.tp_rank', 'model.layers.16.mlp.up_proj.trellis', 'model.layers.16.self_attn.k_proj.SU', 'model.layers.16.self_attn.k_proj.SV', 'model.layers.16.self_attn.k_proj.rcp', 'model.layers.16.self_attn.k_proj.tlut', 'model.layers.16.self_attn.k_proj.tp_rank', 'model.layers.16.self_attn.k_proj.trellis', 'model.layers.16.self_attn.o_proj.SU', 'model.layers.16.self_attn.o_proj.SV', 'model.layers.16.self_attn.o_proj.rcp', 'model.layers.16.self_attn.o_proj.tlut', 'model.layers.16.self_attn.o_proj.tp_rank', 'model.layers.16.self_attn.o_proj.trellis', 'model.layers.16.self_attn.q_proj.SU', 'model.layers.16.self_attn.q_proj.SV', 'model.layers.16.self_attn.q_proj.rcp', 'model.layers.16.self_attn.q_proj.tlut', 'model.layers.16.self_attn.q_proj.tp_rank', 'model.layers.16.self_attn.q_proj.trellis', 'model.layers.16.self_attn.v_proj.SU', 'model.layers.16.self_attn.v_proj.SV', 'model.layers.16.self_attn.v_proj.rcp', 'model.layers.16.self_attn.v_proj.tlut', 'model.layers.16.self_attn.v_proj.tp_rank', 'model.layers.16.self_attn.v_proj.trellis', 'model.layers.17.mlp.down_proj.SU', 'model.layers.17.mlp.down_proj.SV', 'model.layers.17.mlp.down_proj.rcp', 'model.layers.17.mlp.down_proj.tlut', 'model.layers.17.mlp.down_proj.tp_rank', 'model.layers.17.mlp.down_proj.trellis', 'model.layers.17.mlp.gate_proj.SU', 'model.layers.17.mlp.gate_proj.SV', 'model.layers.17.mlp.gate_proj.rcp', 'model.layers.17.mlp.gate_proj.tlut', 'model.layers.17.mlp.gate_proj.tp_rank', 'model.layers.17.mlp.gate_proj.trellis', 'model.layers.17.mlp.up_proj.SU', 'model.layers.17.mlp.up_proj.SV', 'model.layers.17.mlp.up_proj.rcp', 'model.layers.17.mlp.up_proj.tlut', 'model.layers.17.mlp.up_proj.tp_rank', 'model.layers.17.mlp.up_proj.trellis', 'model.layers.17.self_attn.k_proj.SU', 'model.layers.17.self_attn.k_proj.SV', 'model.layers.17.self_attn.k_proj.rcp', 'model.layers.17.self_attn.k_proj.tlut', 'model.layers.17.self_attn.k_proj.tp_rank', 'model.layers.17.self_attn.k_proj.trellis', 'model.layers.17.self_attn.o_proj.SU', 'model.layers.17.self_attn.o_proj.SV', 'model.layers.17.self_attn.o_proj.rcp', 'model.layers.17.self_attn.o_proj.tlut', 'model.layers.17.self_attn.o_proj.tp_rank', 'model.layers.17.self_attn.o_proj.trellis', 'model.layers.17.self_attn.q_proj.SU', 'model.layers.17.self_attn.q_proj.SV', 'model.layers.17.self_attn.q_proj.rcp', 'model.layers.17.self_attn.q_proj.tlut', 'model.layers.17.self_attn.q_proj.tp_rank', 'model.layers.17.self_attn.q_proj.trellis', 'model.layers.17.self_attn.v_proj.SU', 'model.layers.17.self_attn.v_proj.SV', 'model.layers.17.self_attn.v_proj.rcp', 'model.layers.17.self_attn.v_proj.tlut', 'model.layers.17.self_attn.v_proj.tp_rank', 'model.layers.17.self_attn.v_proj.trellis', 'model.layers.18.mlp.down_proj.SU', 'model.layers.18.mlp.down_proj.SV', 'model.layers.18.mlp.down_proj.rcp', 'model.layers.18.mlp.down_proj.tlut', 'model.layers.18.mlp.down_proj.tp_rank', 'model.layers.18.mlp.down_proj.trellis', 'model.layers.18.mlp.gate_proj.SU', 'model.layers.18.mlp.gate_proj.SV', 'model.layers.18.mlp.gate_proj.rcp', 'model.layers.18.mlp.gate_proj.tlut', 'model.layers.18.mlp.gate_proj.tp_rank', 'model.layers.18.mlp.gate_proj.trellis', 'model.layers.18.mlp.up_proj.SU', 'model.layers.18.mlp.up_proj.SV', 'model.layers.18.mlp.up_proj.rcp', 'model.layers.18.mlp.up_proj.tlut', 'model.layers.18.mlp.up_proj.tp_rank', 'model.layers.18.mlp.up_proj.trellis', 'model.layers.18.self_attn.k_proj.SU', 'model.layers.18.self_attn.k_proj.SV', 'model.layers.18.self_attn.k_proj.rcp', 'model.layers.18.self_attn.k_proj.tlut', 'model.layers.18.self_attn.k_proj.tp_rank', 'model.layers.18.self_attn.k_proj.trellis', 'model.layers.18.self_attn.o_proj.SU', 'model.layers.18.self_attn.o_proj.SV', 'model.layers.18.self_attn.o_proj.rcp', 'model.layers.18.self_attn.o_proj.tlut', 'model.layers.18.self_attn.o_proj.tp_rank', 'model.layers.18.self_attn.o_proj.trellis', 'model.layers.18.self_attn.q_proj.SU', 'model.layers.18.self_attn.q_proj.SV', 'model.layers.18.self_attn.q_proj.rcp', 'model.layers.18.self_attn.q_proj.tlut', 'model.layers.18.self_attn.q_proj.tp_rank', 'model.layers.18.self_attn.q_proj.trellis', 'model.layers.18.self_attn.v_proj.SU', 'model.layers.18.self_attn.v_proj.SV', 'model.layers.18.self_attn.v_proj.rcp', 'model.layers.18.self_attn.v_proj.tlut', 'model.layers.18.self_attn.v_proj.tp_rank', 'model.layers.18.self_attn.v_proj.trellis', 'model.layers.19.mlp.down_proj.SU', 'model.layers.19.mlp.down_proj.SV', 'model.layers.19.mlp.down_proj.rcp', 'model.layers.19.mlp.down_proj.tlut', 'model.layers.19.mlp.down_proj.tp_rank', 'model.layers.19.mlp.down_proj.trellis', 'model.layers.19.mlp.gate_proj.SU', 'model.layers.19.mlp.gate_proj.SV', 'model.layers.19.mlp.gate_proj.rcp', 'model.layers.19.mlp.gate_proj.tlut', 'model.layers.19.mlp.gate_proj.tp_rank', 'model.layers.19.mlp.gate_proj.trellis', 'model.layers.19.mlp.up_proj.SU', 'model.layers.19.mlp.up_proj.SV', 'model.layers.19.mlp.up_proj.rcp', 'model.layers.19.mlp.up_proj.tlut', 'model.layers.19.mlp.up_proj.tp_rank', 'model.layers.19.mlp.up_proj.trellis', 'model.layers.19.self_attn.k_proj.SU', 'model.layers.19.self_attn.k_proj.SV', 'model.layers.19.self_attn.k_proj.rcp', 'model.layers.19.self_attn.k_proj.tlut', 'model.layers.19.self_attn.k_proj.tp_rank', 'model.layers.19.self_attn.k_proj.trellis', 'model.layers.19.self_attn.o_proj.SU', 'model.layers.19.self_attn.o_proj.SV', 'model.layers.19.self_attn.o_proj.rcp', 'model.layers.19.self_attn.o_proj.tlut', 'model.layers.19.self_attn.o_proj.tp_rank', 'model.layers.19.self_attn.o_proj.trellis', 'model.layers.19.self_attn.q_proj.SU', 'model.layers.19.self_attn.q_proj.SV', 'model.layers.19.self_attn.q_proj.rcp', 'model.layers.19.self_attn.q_proj.tlut', 'model.layers.19.self_attn.q_proj.tp_rank', 'model.layers.19.self_attn.q_proj.trellis', 'model.layers.19.self_attn.v_proj.SU', 'model.layers.19.self_attn.v_proj.SV', 'model.layers.19.self_attn.v_proj.rcp', 'model.layers.19.self_attn.v_proj.tlut', 'model.layers.19.self_attn.v_proj.tp_rank', 'model.layers.19.self_attn.v_proj.trellis', 'model.layers.2.mlp.down_proj.SU', 'model.layers.2.mlp.down_proj.SV', 'model.layers.2.mlp.down_proj.rcp', 'model.layers.2.mlp.down_proj.tlut', 'model.layers.2.mlp.down_proj.tp_rank', 'model.layers.2.mlp.down_proj.trellis', 'model.layers.2.mlp.gate_proj.SU', 'model.layers.2.mlp.gate_proj.SV', 'model.layers.2.mlp.gate_proj.rcp', 'model.layers.2.mlp.gate_proj.tlut', 'model.layers.2.mlp.gate_proj.tp_rank', 'model.layers.2.mlp.gate_proj.trellis', 'model.layers.2.mlp.up_proj.SU', 'model.layers.2.mlp.up_proj.SV', 'model.layers.2.mlp.up_proj.rcp', 'model.layers.2.mlp.up_proj.tlut', 'model.layers.2.mlp.up_proj.tp_rank', 'model.layers.2.mlp.up_proj.trellis', 'model.layers.2.self_attn.k_proj.SU', 'model.layers.2.self_attn.k_proj.SV', 'model.layers.2.self_attn.k_proj.rcp', 'model.layers.2.self_attn.k_proj.tlut', 'model.layers.2.self_attn.k_proj.tp_rank', 'model.layers.2.self_attn.k_proj.trellis', 'model.layers.2.self_attn.o_proj.SU', 'model.layers.2.self_attn.o_proj.SV', 'model.layers.2.self_attn.o_proj.rcp', 'model.layers.2.self_attn.o_proj.tlut', 'model.layers.2.self_attn.o_proj.tp_rank', 'model.layers.2.self_attn.o_proj.trellis', 'model.layers.2.self_attn.q_proj.SU', 'model.layers.2.self_attn.q_proj.SV', 'model.layers.2.self_attn.q_proj.rcp', 'model.layers.2.self_attn.q_proj.tlut', 'model.layers.2.self_attn.q_proj.tp_rank', 'model.layers.2.self_attn.q_proj.trellis', 'model.layers.2.self_attn.v_proj.SU', 'model.layers.2.self_attn.v_proj.SV', 'model.layers.2.self_attn.v_proj.rcp', 'model.layers.2.self_attn.v_proj.tlut', 'model.layers.2.self_attn.v_proj.tp_rank', 'model.layers.2.self_attn.v_proj.trellis', 'model.layers.20.mlp.down_proj.SU', 'model.layers.20.mlp.down_proj.SV', 'model.layers.20.mlp.down_proj.rcp', 'model.layers.20.mlp.down_proj.tlut', 'model.layers.20.mlp.down_proj.tp_rank', 'model.layers.20.mlp.down_proj.trellis', 'model.layers.20.mlp.gate_proj.SU', 'model.layers.20.mlp.gate_proj.SV', 'model.layers.20.mlp.gate_proj.rcp', 'model.layers.20.mlp.gate_proj.tlut', 'model.layers.20.mlp.gate_proj.tp_rank', 'model.layers.20.mlp.gate_proj.trellis', 'model.layers.20.mlp.up_proj.SU', 'model.layers.20.mlp.up_proj.SV', 'model.layers.20.mlp.up_proj.rcp', 'model.layers.20.mlp.up_proj.tlut', 'model.layers.20.mlp.up_proj.tp_rank', 'model.layers.20.mlp.up_proj.trellis', 'model.layers.20.self_attn.k_proj.SU', 'model.layers.20.self_attn.k_proj.SV', 'model.layers.20.self_attn.k_proj.rcp', 'model.layers.20.self_attn.k_proj.tlut', 'model.layers.20.self_attn.k_proj.tp_rank', 'model.layers.20.self_attn.k_proj.trellis', 'model.layers.20.self_attn.o_proj.SU', 'model.layers.20.self_attn.o_proj.SV', 'model.layers.20.self_attn.o_proj.rcp', 'model.layers.20.self_attn.o_proj.tlut', 'model.layers.20.self_attn.o_proj.tp_rank', 'model.layers.20.self_attn.o_proj.trellis', 'model.layers.20.self_attn.q_proj.SU', 'model.layers.20.self_attn.q_proj.SV', 'model.layers.20.self_attn.q_proj.rcp', 'model.layers.20.self_attn.q_proj.tlut', 'model.layers.20.self_attn.q_proj.tp_rank', 'model.layers.20.self_attn.q_proj.trellis', 'model.layers.20.self_attn.v_proj.SU', 'model.layers.20.self_attn.v_proj.SV', 'model.layers.20.self_attn.v_proj.rcp', 'model.layers.20.self_attn.v_proj.tlut', 'model.layers.20.self_attn.v_proj.tp_rank', 'model.layers.20.self_attn.v_proj.trellis', 'model.layers.21.mlp.down_proj.SU', 'model.layers.21.mlp.down_proj.SV', 'model.layers.21.mlp.down_proj.rcp', 'model.layers.21.mlp.down_proj.tlut', 'model.layers.21.mlp.down_proj.tp_rank', 'model.layers.21.mlp.down_proj.trellis', 'model.layers.21.mlp.gate_proj.SU', 'model.layers.21.mlp.gate_proj.SV', 'model.layers.21.mlp.gate_proj.rcp', 'model.layers.21.mlp.gate_proj.tlut', 'model.layers.21.mlp.gate_proj.tp_rank', 'model.layers.21.mlp.gate_proj.trellis', 'model.layers.21.mlp.up_proj.SU', 'model.layers.21.mlp.up_proj.SV', 'model.layers.21.mlp.up_proj.rcp', 'model.layers.21.mlp.up_proj.tlut', 'model.layers.21.mlp.up_proj.tp_rank', 'model.layers.21.mlp.up_proj.trellis', 'model.layers.21.self_attn.k_proj.SU', 'model.layers.21.self_attn.k_proj.SV', 'model.layers.21.self_attn.k_proj.rcp', 'model.layers.21.self_attn.k_proj.tlut', 'model.layers.21.self_attn.k_proj.tp_rank', 'model.layers.21.self_attn.k_proj.trellis', 'model.layers.21.self_attn.o_proj.SU', 'model.layers.21.self_attn.o_proj.SV', 'model.layers.21.self_attn.o_proj.rcp', 'model.layers.21.self_attn.o_proj.tlut', 'model.layers.21.self_attn.o_proj.tp_rank', 'model.layers.21.self_attn.o_proj.trellis', 'model.layers.21.self_attn.q_proj.SU', 'model.layers.21.self_attn.q_proj.SV', 'model.layers.21.self_attn.q_proj.rcp', 'model.layers.21.self_attn.q_proj.tlut', 'model.layers.21.self_attn.q_proj.tp_rank', 'model.layers.21.self_attn.q_proj.trellis', 'model.layers.21.self_attn.v_proj.SU', 'model.layers.21.self_attn.v_proj.SV', 'model.layers.21.self_attn.v_proj.rcp', 'model.layers.21.self_attn.v_proj.tlut', 'model.layers.21.self_attn.v_proj.tp_rank', 'model.layers.21.self_attn.v_proj.trellis', 'model.layers.22.mlp.down_proj.SU', 'model.layers.22.mlp.down_proj.SV', 'model.layers.22.mlp.down_proj.rcp', 'model.layers.22.mlp.down_proj.tlut', 'model.layers.22.mlp.down_proj.tp_rank', 'model.layers.22.mlp.down_proj.trellis', 'model.layers.22.mlp.gate_proj.SU', 'model.layers.22.mlp.gate_proj.SV', 'model.layers.22.mlp.gate_proj.rcp', 'model.layers.22.mlp.gate_proj.tlut', 'model.layers.22.mlp.gate_proj.tp_rank', 'model.layers.22.mlp.gate_proj.trellis', 'model.layers.22.mlp.up_proj.SU', 'model.layers.22.mlp.up_proj.SV', 'model.layers.22.mlp.up_proj.rcp', 'model.layers.22.mlp.up_proj.tlut', 'model.layers.22.mlp.up_proj.tp_rank', 'model.layers.22.mlp.up_proj.trellis', 'model.layers.22.self_attn.k_proj.SU', 'model.layers.22.self_attn.k_proj.SV', 'model.layers.22.self_attn.k_proj.rcp', 'model.layers.22.self_attn.k_proj.tlut', 'model.layers.22.self_attn.k_proj.tp_rank', 'model.layers.22.self_attn.k_proj.trellis', 'model.layers.22.self_attn.o_proj.SU', 'model.layers.22.self_attn.o_proj.SV', 'model.layers.22.self_attn.o_proj.rcp', 'model.layers.22.self_attn.o_proj.tlut', 'model.layers.22.self_attn.o_proj.tp_rank', 'model.layers.22.self_attn.o_proj.trellis', 'model.layers.22.self_attn.q_proj.SU', 'model.layers.22.self_attn.q_proj.SV', 'model.layers.22.self_attn.q_proj.rcp', 'model.layers.22.self_attn.q_proj.tlut', 'model.layers.22.self_attn.q_proj.tp_rank', 'model.layers.22.self_attn.q_proj.trellis', 'model.layers.22.self_attn.v_proj.SU', 'model.layers.22.self_attn.v_proj.SV', 'model.layers.22.self_attn.v_proj.rcp', 'model.layers.22.self_attn.v_proj.tlut', 'model.layers.22.self_attn.v_proj.tp_rank', 'model.layers.22.self_attn.v_proj.trellis', 'model.layers.23.mlp.down_proj.SU', 'model.layers.23.mlp.down_proj.SV', 'model.layers.23.mlp.down_proj.rcp', 'model.layers.23.mlp.down_proj.tlut', 'model.layers.23.mlp.down_proj.tp_rank', 'model.layers.23.mlp.down_proj.trellis', 'model.layers.23.mlp.gate_proj.SU', 'model.layers.23.mlp.gate_proj.SV', 'model.layers.23.mlp.gate_proj.rcp', 'model.layers.23.mlp.gate_proj.tlut', 'model.layers.23.mlp.gate_proj.tp_rank', 'model.layers.23.mlp.gate_proj.trellis', 'model.layers.23.mlp.up_proj.SU', 'model.layers.23.mlp.up_proj.SV', 'model.layers.23.mlp.up_proj.rcp', 'model.layers.23.mlp.up_proj.tlut', 'model.layers.23.mlp.up_proj.tp_rank', 'model.layers.23.mlp.up_proj.trellis', 'model.layers.23.self_attn.k_proj.SU', 'model.layers.23.self_attn.k_proj.SV', 'model.layers.23.self_attn.k_proj.rcp', 'model.layers.23.self_attn.k_proj.tlut', 'model.layers.23.self_attn.k_proj.tp_rank', 'model.layers.23.self_attn.k_proj.trellis', 'model.layers.23.self_attn.o_proj.SU', 'model.layers.23.self_attn.o_proj.SV', 'model.layers.23.self_attn.o_proj.rcp', 'model.layers.23.self_attn.o_proj.tlut', 'model.layers.23.self_attn.o_proj.tp_rank', 'model.layers.23.self_attn.o_proj.trellis', 'model.layers.23.self_attn.q_proj.SU', 'model.layers.23.self_attn.q_proj.SV', 'model.layers.23.self_attn.q_proj.rcp', 'model.layers.23.self_attn.q_proj.tlut', 'model.layers.23.self_attn.q_proj.tp_rank', 'model.layers.23.self_attn.q_proj.trellis', 'model.layers.23.self_attn.v_proj.SU', 'model.layers.23.self_attn.v_proj.SV', 'model.layers.23.self_attn.v_proj.rcp', 'model.layers.23.self_attn.v_proj.tlut', 'model.layers.23.self_attn.v_proj.tp_rank', 'model.layers.23.self_attn.v_proj.trellis', 'model.layers.24.mlp.down_proj.SU', 'model.layers.24.mlp.down_proj.SV', 'model.layers.24.mlp.down_proj.rcp', 'model.layers.24.mlp.down_proj.tlut', 'model.layers.24.mlp.down_proj.tp_rank', 'model.layers.24.mlp.down_proj.trellis', 'model.layers.24.mlp.gate_proj.SU', 'model.layers.24.mlp.gate_proj.SV', 'model.layers.24.mlp.gate_proj.rcp', 'model.layers.24.mlp.gate_proj.tlut', 'model.layers.24.mlp.gate_proj.tp_rank', 'model.layers.24.mlp.gate_proj.trellis', 'model.layers.24.mlp.up_proj.SU', 'model.layers.24.mlp.up_proj.SV', 'model.layers.24.mlp.up_proj.rcp', 'model.layers.24.mlp.up_proj.tlut', 'model.layers.24.mlp.up_proj.tp_rank', 'model.layers.24.mlp.up_proj.trellis', 'model.layers.24.self_attn.k_proj.SU', 'model.layers.24.self_attn.k_proj.SV', 'model.layers.24.self_attn.k_proj.rcp', 'model.layers.24.self_attn.k_proj.tlut', 'model.layers.24.self_attn.k_proj.tp_rank', 'model.layers.24.self_attn.k_proj.trellis', 'model.layers.24.self_attn.o_proj.SU', 'model.layers.24.self_attn.o_proj.SV', 'model.layers.24.self_attn.o_proj.rcp', 'model.layers.24.self_attn.o_proj.tlut', 'model.layers.24.self_attn.o_proj.tp_rank', 'model.layers.24.self_attn.o_proj.trellis', 'model.layers.24.self_attn.q_proj.SU', 'model.layers.24.self_attn.q_proj.SV', 'model.layers.24.self_attn.q_proj.rcp', 'model.layers.24.self_attn.q_proj.tlut', 'model.layers.24.self_attn.q_proj.tp_rank', 'model.layers.24.self_attn.q_proj.trellis', 'model.layers.24.self_attn.v_proj.SU', 'model.layers.24.self_attn.v_proj.SV', 'model.layers.24.self_attn.v_proj.rcp', 'model.layers.24.self_attn.v_proj.tlut', 'model.layers.24.self_attn.v_proj.tp_rank', 'model.layers.24.self_attn.v_proj.trellis', 'model.layers.25.mlp.down_proj.SU', 'model.layers.25.mlp.down_proj.SV', 'model.layers.25.mlp.down_proj.rcp', 'model.layers.25.mlp.down_proj.tlut', 'model.layers.25.mlp.down_proj.tp_rank', 'model.layers.25.mlp.down_proj.trellis', 'model.layers.25.mlp.gate_proj.SU', 'model.layers.25.mlp.gate_proj.SV', 'model.layers.25.mlp.gate_proj.rcp', 'model.layers.25.mlp.gate_proj.tlut', 'model.layers.25.mlp.gate_proj.tp_rank', 'model.layers.25.mlp.gate_proj.trellis', 'model.layers.25.mlp.up_proj.SU', 'model.layers.25.mlp.up_proj.SV', 'model.layers.25.mlp.up_proj.rcp', 'model.layers.25.mlp.up_proj.tlut', 'model.layers.25.mlp.up_proj.tp_rank', 'model.layers.25.mlp.up_proj.trellis', 'model.layers.25.self_attn.k_proj.SU', 'model.layers.25.self_attn.k_proj.SV', 'model.layers.25.self_attn.k_proj.rcp', 'model.layers.25.self_attn.k_proj.tlut', 'model.layers.25.self_attn.k_proj.tp_rank', 'model.layers.25.self_attn.k_proj.trellis', 'model.layers.25.self_attn.o_proj.SU', 'model.layers.25.self_attn.o_proj.SV', 'model.layers.25.self_attn.o_proj.rcp', 'model.layers.25.self_attn.o_proj.tlut', 'model.layers.25.self_attn.o_proj.tp_rank', 'model.layers.25.self_attn.o_proj.trellis', 'model.layers.25.self_attn.q_proj.SU', 'model.layers.25.self_attn.q_proj.SV', 'model.layers.25.self_attn.q_proj.rcp', 'model.layers.25.self_attn.q_proj.tlut', 'model.layers.25.self_attn.q_proj.tp_rank', 'model.layers.25.self_attn.q_proj.trellis', 'model.layers.25.self_attn.v_proj.SU', 'model.layers.25.self_attn.v_proj.SV', 'model.layers.25.self_attn.v_proj.rcp', 'model.layers.25.self_attn.v_proj.tlut', 'model.layers.25.self_attn.v_proj.tp_rank', 'model.layers.25.self_attn.v_proj.trellis', 'model.layers.26.mlp.down_proj.SU', 'model.layers.26.mlp.down_proj.SV', 'model.layers.26.mlp.down_proj.rcp', 'model.layers.26.mlp.down_proj.tlut', 'model.layers.26.mlp.down_proj.tp_rank', 'model.layers.26.mlp.down_proj.trellis', 'model.layers.26.mlp.gate_proj.SU', 'model.layers.26.mlp.gate_proj.SV', 'model.layers.26.mlp.gate_proj.rcp', 'model.layers.26.mlp.gate_proj.tlut', 'model.layers.26.mlp.gate_proj.tp_rank', 'model.layers.26.mlp.gate_proj.trellis', 'model.layers.26.mlp.up_proj.SU', 'model.layers.26.mlp.up_proj.SV', 'model.layers.26.mlp.up_proj.rcp', 'model.layers.26.mlp.up_proj.tlut', 'model.layers.26.mlp.up_proj.tp_rank', 'model.layers.26.mlp.up_proj.trellis', 'model.layers.26.self_attn.k_proj.SU', 'model.layers.26.self_attn.k_proj.SV', 'model.layers.26.self_attn.k_proj.rcp', 'model.layers.26.self_attn.k_proj.tlut', 'model.layers.26.self_attn.k_proj.tp_rank', 'model.layers.26.self_attn.k_proj.trellis', 'model.layers.26.self_attn.o_proj.SU', 'model.layers.26.self_attn.o_proj.SV', 'model.layers.26.self_attn.o_proj.rcp', 'model.layers.26.self_attn.o_proj.tlut', 'model.layers.26.self_attn.o_proj.tp_rank', 'model.layers.26.self_attn.o_proj.trellis', 'model.layers.26.self_attn.q_proj.SU', 'model.layers.26.self_attn.q_proj.SV', 'model.layers.26.self_attn.q_proj.rcp', 'model.layers.26.self_attn.q_proj.tlut', 'model.layers.26.self_attn.q_proj.tp_rank', 'model.layers.26.self_attn.q_proj.trellis', 'model.layers.26.self_attn.v_proj.SU', 'model.layers.26.self_attn.v_proj.SV', 'model.layers.26.self_attn.v_proj.rcp', 'model.layers.26.self_attn.v_proj.tlut', 'model.layers.26.self_attn.v_proj.tp_rank', 'model.layers.26.self_attn.v_proj.trellis', 'model.layers.27.mlp.down_proj.SU', 'model.layers.27.mlp.down_proj.SV', 'model.layers.27.mlp.down_proj.rcp', 'model.layers.27.mlp.down_proj.tlut', 'model.layers.27.mlp.down_proj.tp_rank', 'model.layers.27.mlp.down_proj.trellis', 'model.layers.27.mlp.gate_proj.SU', 'model.layers.27.mlp.gate_proj.SV', 'model.layers.27.mlp.gate_proj.rcp', 'model.layers.27.mlp.gate_proj.tlut', 'model.layers.27.mlp.gate_proj.tp_rank', 'model.layers.27.mlp.gate_proj.trellis', 'model.layers.27.mlp.up_proj.SU', 'model.layers.27.mlp.up_proj.SV', 'model.layers.27.mlp.up_proj.rcp', 'model.layers.27.mlp.up_proj.tlut', 'model.layers.27.mlp.up_proj.tp_rank', 'model.layers.27.mlp.up_proj.trellis', 'model.layers.27.self_attn.k_proj.SU', 'model.layers.27.self_attn.k_proj.SV', 'model.layers.27.self_attn.k_proj.rcp', 'model.layers.27.self_attn.k_proj.tlut', 'model.layers.27.self_attn.k_proj.tp_rank', 'model.layers.27.self_attn.k_proj.trellis', 'model.layers.27.self_attn.o_proj.SU', 'model.layers.27.self_attn.o_proj.SV', 'model.layers.27.self_attn.o_proj.rcp', 'model.layers.27.self_attn.o_proj.tlut', 'model.layers.27.self_attn.o_proj.tp_rank', 'model.layers.27.self_attn.o_proj.trellis', 'model.layers.27.self_attn.q_proj.SU', 'model.layers.27.self_attn.q_proj.SV', 'model.layers.27.self_attn.q_proj.rcp', 'model.layers.27.self_attn.q_proj.tlut', 'model.layers.27.self_attn.q_proj.tp_rank', 'model.layers.27.self_attn.q_proj.trellis', 'model.layers.27.self_attn.v_proj.SU', 'model.layers.27.self_attn.v_proj.SV', 'model.layers.27.self_attn.v_proj.rcp', 'model.layers.27.self_attn.v_proj.tlut', 'model.layers.27.self_attn.v_proj.tp_rank', 'model.layers.27.self_attn.v_proj.trellis', 'model.layers.28.mlp.down_proj.SU', 'model.layers.28.mlp.down_proj.SV', 'model.layers.28.mlp.down_proj.rcp', 'model.layers.28.mlp.down_proj.tlut', 'model.layers.28.mlp.down_proj.tp_rank', 'model.layers.28.mlp.down_proj.trellis', 'model.layers.28.mlp.gate_proj.SU', 'model.layers.28.mlp.gate_proj.SV', 'model.layers.28.mlp.gate_proj.rcp', 'model.layers.28.mlp.gate_proj.tlut', 'model.layers.28.mlp.gate_proj.tp_rank', 'model.layers.28.mlp.gate_proj.trellis', 'model.layers.28.mlp.up_proj.SU', 'model.layers.28.mlp.up_proj.SV', 'model.layers.28.mlp.up_proj.rcp', 'model.layers.28.mlp.up_proj.tlut', 'model.layers.28.mlp.up_proj.tp_rank', 'model.layers.28.mlp.up_proj.trellis', 'model.layers.28.self_attn.k_proj.SU', 'model.layers.28.self_attn.k_proj.SV', 'model.layers.28.self_attn.k_proj.rcp', 'model.layers.28.self_attn.k_proj.tlut', 'model.layers.28.self_attn.k_proj.tp_rank', 'model.layers.28.self_attn.k_proj.trellis', 'model.layers.28.self_attn.o_proj.SU', 'model.layers.28.self_attn.o_proj.SV', 'model.layers.28.self_attn.o_proj.rcp', 'model.layers.28.self_attn.o_proj.tlut', 'model.layers.28.self_attn.o_proj.tp_rank', 'model.layers.28.self_attn.o_proj.trellis', 'model.layers.28.self_attn.q_proj.SU', 'model.layers.28.self_attn.q_proj.SV', 'model.layers.28.self_attn.q_proj.rcp', 'model.layers.28.self_attn.q_proj.tlut', 'model.layers.28.self_attn.q_proj.tp_rank', 'model.layers.28.self_attn.q_proj.trellis', 'model.layers.28.self_attn.v_proj.SU', 'model.layers.28.self_attn.v_proj.SV', 'model.layers.28.self_attn.v_proj.rcp', 'model.layers.28.self_attn.v_proj.tlut', 'model.layers.28.self_attn.v_proj.tp_rank', 'model.layers.28.self_attn.v_proj.trellis', 'model.layers.29.mlp.down_proj.SU', 'model.layers.29.mlp.down_proj.SV', 'model.layers.29.mlp.down_proj.rcp', 'model.layers.29.mlp.down_proj.tlut', 'model.layers.29.mlp.down_proj.tp_rank', 'model.layers.29.mlp.down_proj.trellis', 'model.layers.29.mlp.gate_proj.SU', 'model.layers.29.mlp.gate_proj.SV', 'model.layers.29.mlp.gate_proj.rcp', 'model.layers.29.mlp.gate_proj.tlut', 'model.layers.29.mlp.gate_proj.tp_rank', 'model.layers.29.mlp.gate_proj.trellis', 'model.layers.29.mlp.up_proj.SU', 'model.layers.29.mlp.up_proj.SV', 'model.layers.29.mlp.up_proj.rcp', 'model.layers.29.mlp.up_proj.tlut', 'model.layers.29.mlp.up_proj.tp_rank', 'model.layers.29.mlp.up_proj.trellis', 'model.layers.29.self_attn.k_proj.SU', 'model.layers.29.self_attn.k_proj.SV', 'model.layers.29.self_attn.k_proj.rcp', 'model.layers.29.self_attn.k_proj.tlut', 'model.layers.29.self_attn.k_proj.tp_rank', 'model.layers.29.self_attn.k_proj.trellis', 'model.layers.29.self_attn.o_proj.SU', 'model.layers.29.self_attn.o_proj.SV', 'model.layers.29.self_attn.o_proj.rcp', 'model.layers.29.self_attn.o_proj.tlut', 'model.layers.29.self_attn.o_proj.tp_rank', 'model.layers.29.self_attn.o_proj.trellis', 'model.layers.29.self_attn.q_proj.SU', 'model.layers.29.self_attn.q_proj.SV', 'model.layers.29.self_attn.q_proj.rcp', 'model.layers.29.self_attn.q_proj.tlut', 'model.layers.29.self_attn.q_proj.tp_rank', 'model.layers.29.self_attn.q_proj.trellis', 'model.layers.29.self_attn.v_proj.SU', 'model.layers.29.self_attn.v_proj.SV', 'model.layers.29.self_attn.v_proj.rcp', 'model.layers.29.self_attn.v_proj.tlut', 'model.layers.29.self_attn.v_proj.tp_rank', 'model.layers.29.self_attn.v_proj.trellis', 'model.layers.3.mlp.down_proj.SU', 'model.layers.3.mlp.down_proj.SV', 'model.layers.3.mlp.down_proj.rcp', 'model.layers.3.mlp.down_proj.tlut', 'model.layers.3.mlp.down_proj.tp_rank', 'model.layers.3.mlp.down_proj.trellis', 'model.layers.3.mlp.gate_proj.SU', 'model.layers.3.mlp.gate_proj.SV', 'model.layers.3.mlp.gate_proj.rcp', 'model.layers.3.mlp.gate_proj.tlut', 'model.layers.3.mlp.gate_proj.tp_rank', 'model.layers.3.mlp.gate_proj.trellis', 'model.layers.3.mlp.up_proj.SU', 'model.layers.3.mlp.up_proj.SV', 'model.layers.3.mlp.up_proj.rcp', 'model.layers.3.mlp.up_proj.tlut', 'model.layers.3.mlp.up_proj.tp_rank', 'model.layers.3.mlp.up_proj.trellis', 'model.layers.3.self_attn.k_proj.SU', 'model.layers.3.self_attn.k_proj.SV', 'model.layers.3.self_attn.k_proj.rcp', 'model.layers.3.self_attn.k_proj.tlut', 'model.layers.3.self_attn.k_proj.tp_rank', 'model.layers.3.self_attn.k_proj.trellis', 'model.layers.3.self_attn.o_proj.SU', 'model.layers.3.self_attn.o_proj.SV', 'model.layers.3.self_attn.o_proj.rcp', 'model.layers.3.self_attn.o_proj.tlut', 'model.layers.3.self_attn.o_proj.tp_rank', 'model.layers.3.self_attn.o_proj.trellis', 'model.layers.3.self_attn.q_proj.SU', 'model.layers.3.self_attn.q_proj.SV', 'model.layers.3.self_attn.q_proj.rcp', 'model.layers.3.self_attn.q_proj.tlut', 'model.layers.3.self_attn.q_proj.tp_rank', 'model.layers.3.self_attn.q_proj.trellis', 'model.layers.3.self_attn.v_proj.SU', 'model.layers.3.self_attn.v_proj.SV', 'model.layers.3.self_attn.v_proj.rcp', 'model.layers.3.self_attn.v_proj.tlut', 'model.layers.3.self_attn.v_proj.tp_rank', 'model.layers.3.self_attn.v_proj.trellis', 'model.layers.30.mlp.down_proj.SU', 'model.layers.30.mlp.down_proj.SV', 'model.layers.30.mlp.down_proj.rcp', 'model.layers.30.mlp.down_proj.tlut', 'model.layers.30.mlp.down_proj.tp_rank', 'model.layers.30.mlp.down_proj.trellis', 'model.layers.30.mlp.gate_proj.SU', 'model.layers.30.mlp.gate_proj.SV', 'model.layers.30.mlp.gate_proj.rcp', 'model.layers.30.mlp.gate_proj.tlut', 'model.layers.30.mlp.gate_proj.tp_rank', 'model.layers.30.mlp.gate_proj.trellis', 'model.layers.30.mlp.up_proj.SU', 'model.layers.30.mlp.up_proj.SV', 'model.layers.30.mlp.up_proj.rcp', 'model.layers.30.mlp.up_proj.tlut', 'model.layers.30.mlp.up_proj.tp_rank', 'model.layers.30.mlp.up_proj.trellis', 'model.layers.30.self_attn.k_proj.SU', 'model.layers.30.self_attn.k_proj.SV', 'model.layers.30.self_attn.k_proj.rcp', 'model.layers.30.self_attn.k_proj.tlut', 'model.layers.30.self_attn.k_proj.tp_rank', 'model.layers.30.self_attn.k_proj.trellis', 'model.layers.30.self_attn.o_proj.SU', 'model.layers.30.self_attn.o_proj.SV', 'model.layers.30.self_attn.o_proj.rcp', 'model.layers.30.self_attn.o_proj.tlut', 'model.layers.30.self_attn.o_proj.tp_rank', 'model.layers.30.self_attn.o_proj.trellis', 'model.layers.30.self_attn.q_proj.SU', 'model.layers.30.self_attn.q_proj.SV', 'model.layers.30.self_attn.q_proj.rcp', 'model.layers.30.self_attn.q_proj.tlut', 'model.layers.30.self_attn.q_proj.tp_rank', 'model.layers.30.self_attn.q_proj.trellis', 'model.layers.30.self_attn.v_proj.SU', 'model.layers.30.self_attn.v_proj.SV', 'model.layers.30.self_attn.v_proj.rcp', 'model.layers.30.self_attn.v_proj.tlut', 'model.layers.30.self_attn.v_proj.tp_rank', 'model.layers.30.self_attn.v_proj.trellis', 'model.layers.31.mlp.down_proj.SU', 'model.layers.31.mlp.down_proj.SV', 'model.layers.31.mlp.down_proj.rcp', 'model.layers.31.mlp.down_proj.tlut', 'model.layers.31.mlp.down_proj.tp_rank', 'model.layers.31.mlp.down_proj.trellis', 'model.layers.31.mlp.gate_proj.SU', 'model.layers.31.mlp.gate_proj.SV', 'model.layers.31.mlp.gate_proj.rcp', 'model.layers.31.mlp.gate_proj.tlut', 'model.layers.31.mlp.gate_proj.tp_rank', 'model.layers.31.mlp.gate_proj.trellis', 'model.layers.31.mlp.up_proj.SU', 'model.layers.31.mlp.up_proj.SV', 'model.layers.31.mlp.up_proj.rcp', 'model.layers.31.mlp.up_proj.tlut', 'model.layers.31.mlp.up_proj.tp_rank', 'model.layers.31.mlp.up_proj.trellis', 'model.layers.31.self_attn.k_proj.SU', 'model.layers.31.self_attn.k_proj.SV', 'model.layers.31.self_attn.k_proj.rcp', 'model.layers.31.self_attn.k_proj.tlut', 'model.layers.31.self_attn.k_proj.tp_rank', 'model.layers.31.self_attn.k_proj.trellis', 'model.layers.31.self_attn.o_proj.SU', 'model.layers.31.self_attn.o_proj.SV', 'model.layers.31.self_attn.o_proj.rcp', 'model.layers.31.self_attn.o_proj.tlut', 'model.layers.31.self_attn.o_proj.tp_rank', 'model.layers.31.self_attn.o_proj.trellis', 'model.layers.31.self_attn.q_proj.SU', 'model.layers.31.self_attn.q_proj.SV', 'model.layers.31.self_attn.q_proj.rcp', 'model.layers.31.self_attn.q_proj.tlut', 'model.layers.31.self_attn.q_proj.tp_rank', 'model.layers.31.self_attn.q_proj.trellis', 'model.layers.31.self_attn.v_proj.SU', 'model.layers.31.self_attn.v_proj.SV', 'model.layers.31.self_attn.v_proj.rcp', 'model.layers.31.self_attn.v_proj.tlut', 'model.layers.31.self_attn.v_proj.tp_rank', 'model.layers.31.self_attn.v_proj.trellis', 'model.layers.4.mlp.down_proj.SU', 'model.layers.4.mlp.down_proj.SV', 'model.layers.4.mlp.down_proj.rcp', 'model.layers.4.mlp.down_proj.tlut', 'model.layers.4.mlp.down_proj.tp_rank', 'model.layers.4.mlp.down_proj.trellis', 'model.layers.4.mlp.gate_proj.SU', 'model.layers.4.mlp.gate_proj.SV', 'model.layers.4.mlp.gate_proj.rcp', 'model.layers.4.mlp.gate_proj.tlut', 'model.layers.4.mlp.gate_proj.tp_rank', 'model.layers.4.mlp.gate_proj.trellis', 'model.layers.4.mlp.up_proj.SU', 'model.layers.4.mlp.up_proj.SV', 'model.layers.4.mlp.up_proj.rcp', 'model.layers.4.mlp.up_proj.tlut', 'model.layers.4.mlp.up_proj.tp_rank', 'model.layers.4.mlp.up_proj.trellis', 'model.layers.4.self_attn.k_proj.SU', 'model.layers.4.self_attn.k_proj.SV', 'model.layers.4.self_attn.k_proj.rcp', 'model.layers.4.self_attn.k_proj.tlut', 'model.layers.4.self_attn.k_proj.tp_rank', 'model.layers.4.self_attn.k_proj.trellis', 'model.layers.4.self_attn.o_proj.SU', 'model.layers.4.self_attn.o_proj.SV', 'model.layers.4.self_attn.o_proj.rcp', 'model.layers.4.self_attn.o_proj.tlut', 'model.layers.4.self_attn.o_proj.tp_rank', 'model.layers.4.self_attn.o_proj.trellis', 'model.layers.4.self_attn.q_proj.SU', 'model.layers.4.self_attn.q_proj.SV', 'model.layers.4.self_attn.q_proj.rcp', 'model.layers.4.self_attn.q_proj.tlut', 'model.layers.4.self_attn.q_proj.tp_rank', 'model.layers.4.self_attn.q_proj.trellis', 'model.layers.4.self_attn.v_proj.SU', 'model.layers.4.self_attn.v_proj.SV', 'model.layers.4.self_attn.v_proj.rcp', 'model.layers.4.self_attn.v_proj.tlut', 'model.layers.4.self_attn.v_proj.tp_rank', 'model.layers.4.self_attn.v_proj.trellis', 'model.layers.5.mlp.down_proj.SU', 'model.layers.5.mlp.down_proj.SV', 'model.layers.5.mlp.down_proj.rcp', 'model.layers.5.mlp.down_proj.tlut', 'model.layers.5.mlp.down_proj.tp_rank', 'model.layers.5.mlp.down_proj.trellis', 'model.layers.5.mlp.gate_proj.SU', 'model.layers.5.mlp.gate_proj.SV', 'model.layers.5.mlp.gate_proj.rcp', 'model.layers.5.mlp.gate_proj.tlut', 'model.layers.5.mlp.gate_proj.tp_rank', 'model.layers.5.mlp.gate_proj.trellis', 'model.layers.5.mlp.up_proj.SU', 'model.layers.5.mlp.up_proj.SV', 'model.layers.5.mlp.up_proj.rcp', 'model.layers.5.mlp.up_proj.tlut', 'model.layers.5.mlp.up_proj.tp_rank', 'model.layers.5.mlp.up_proj.trellis', 'model.layers.5.self_attn.k_proj.SU', 'model.layers.5.self_attn.k_proj.SV', 'model.layers.5.self_attn.k_proj.rcp', 'model.layers.5.self_attn.k_proj.tlut', 'model.layers.5.self_attn.k_proj.tp_rank', 'model.layers.5.self_attn.k_proj.trellis', 'model.layers.5.self_attn.o_proj.SU', 'model.layers.5.self_attn.o_proj.SV', 'model.layers.5.self_attn.o_proj.rcp', 'model.layers.5.self_attn.o_proj.tlut', 'model.layers.5.self_attn.o_proj.tp_rank', 'model.layers.5.self_attn.o_proj.trellis', 'model.layers.5.self_attn.q_proj.SU', 'model.layers.5.self_attn.q_proj.SV', 'model.layers.5.self_attn.q_proj.rcp', 'model.layers.5.self_attn.q_proj.tlut', 'model.layers.5.self_attn.q_proj.tp_rank', 'model.layers.5.self_attn.q_proj.trellis', 'model.layers.5.self_attn.v_proj.SU', 'model.layers.5.self_attn.v_proj.SV', 'model.layers.5.self_attn.v_proj.rcp', 'model.layers.5.self_attn.v_proj.tlut', 'model.layers.5.self_attn.v_proj.tp_rank', 'model.layers.5.self_attn.v_proj.trellis', 'model.layers.6.mlp.down_proj.SU', 'model.layers.6.mlp.down_proj.SV', 'model.layers.6.mlp.down_proj.rcp', 'model.layers.6.mlp.down_proj.tlut', 'model.layers.6.mlp.down_proj.tp_rank', 'model.layers.6.mlp.down_proj.trellis', 'model.layers.6.mlp.gate_proj.SU', 'model.layers.6.mlp.gate_proj.SV', 'model.layers.6.mlp.gate_proj.rcp', 'model.layers.6.mlp.gate_proj.tlut', 'model.layers.6.mlp.gate_proj.tp_rank', 'model.layers.6.mlp.gate_proj.trellis', 'model.layers.6.mlp.up_proj.SU', 'model.layers.6.mlp.up_proj.SV', 'model.layers.6.mlp.up_proj.rcp', 'model.layers.6.mlp.up_proj.tlut', 'model.layers.6.mlp.up_proj.tp_rank', 'model.layers.6.mlp.up_proj.trellis', 'model.layers.6.self_attn.k_proj.SU', 'model.layers.6.self_attn.k_proj.SV', 'model.layers.6.self_attn.k_proj.rcp', 'model.layers.6.self_attn.k_proj.tlut', 'model.layers.6.self_attn.k_proj.tp_rank', 'model.layers.6.self_attn.k_proj.trellis', 'model.layers.6.self_attn.o_proj.SU', 'model.layers.6.self_attn.o_proj.SV', 'model.layers.6.self_attn.o_proj.rcp', 'model.layers.6.self_attn.o_proj.tlut', 'model.layers.6.self_attn.o_proj.tp_rank', 'model.layers.6.self_attn.o_proj.trellis', 'model.layers.6.self_attn.q_proj.SU', 'model.layers.6.self_attn.q_proj.SV', 'model.layers.6.self_attn.q_proj.rcp', 'model.layers.6.self_attn.q_proj.tlut', 'model.layers.6.self_attn.q_proj.tp_rank', 'model.layers.6.self_attn.q_proj.trellis', 'model.layers.6.self_attn.v_proj.SU', 'model.layers.6.self_attn.v_proj.SV', 'model.layers.6.self_attn.v_proj.rcp', 'model.layers.6.self_attn.v_proj.tlut', 'model.layers.6.self_attn.v_proj.tp_rank', 'model.layers.6.self_attn.v_proj.trellis', 'model.layers.7.mlp.down_proj.SU', 'model.layers.7.mlp.down_proj.SV', 'model.layers.7.mlp.down_proj.rcp', 'model.layers.7.mlp.down_proj.tlut', 'model.layers.7.mlp.down_proj.tp_rank', 'model.layers.7.mlp.down_proj.trellis', 'model.layers.7.mlp.gate_proj.SU', 'model.layers.7.mlp.gate_proj.SV', 'model.layers.7.mlp.gate_proj.rcp', 'model.layers.7.mlp.gate_proj.tlut', 'model.layers.7.mlp.gate_proj.tp_rank', 'model.layers.7.mlp.gate_proj.trellis', 'model.layers.7.mlp.up_proj.SU', 'model.layers.7.mlp.up_proj.SV', 'model.layers.7.mlp.up_proj.rcp', 'model.layers.7.mlp.up_proj.tlut', 'model.layers.7.mlp.up_proj.tp_rank', 'model.layers.7.mlp.up_proj.trellis', 'model.layers.7.self_attn.k_proj.SU', 'model.layers.7.self_attn.k_proj.SV', 'model.layers.7.self_attn.k_proj.rcp', 'model.layers.7.self_attn.k_proj.tlut', 'model.layers.7.self_attn.k_proj.tp_rank', 'model.layers.7.self_attn.k_proj.trellis', 'model.layers.7.self_attn.o_proj.SU', 'model.layers.7.self_attn.o_proj.SV', 'model.layers.7.self_attn.o_proj.rcp', 'model.layers.7.self_attn.o_proj.tlut', 'model.layers.7.self_attn.o_proj.tp_rank', 'model.layers.7.self_attn.o_proj.trellis', 'model.layers.7.self_attn.q_proj.SU', 'model.layers.7.self_attn.q_proj.SV', 'model.layers.7.self_attn.q_proj.rcp', 'model.layers.7.self_attn.q_proj.tlut', 'model.layers.7.self_attn.q_proj.tp_rank', 'model.layers.7.self_attn.q_proj.trellis', 'model.layers.7.self_attn.v_proj.SU', 'model.layers.7.self_attn.v_proj.SV', 'model.layers.7.self_attn.v_proj.rcp', 'model.layers.7.self_attn.v_proj.tlut', 'model.layers.7.self_attn.v_proj.tp_rank', 'model.layers.7.self_attn.v_proj.trellis', 'model.layers.8.mlp.down_proj.SU', 'model.layers.8.mlp.down_proj.SV', 'model.layers.8.mlp.down_proj.rcp', 'model.layers.8.mlp.down_proj.tlut', 'model.layers.8.mlp.down_proj.tp_rank', 'model.layers.8.mlp.down_proj.trellis', 'model.layers.8.mlp.gate_proj.SU', 'model.layers.8.mlp.gate_proj.SV', 'model.layers.8.mlp.gate_proj.rcp', 'model.layers.8.mlp.gate_proj.tlut', 'model.layers.8.mlp.gate_proj.tp_rank', 'model.layers.8.mlp.gate_proj.trellis', 'model.layers.8.mlp.up_proj.SU', 'model.layers.8.mlp.up_proj.SV', 'model.layers.8.mlp.up_proj.rcp', 'model.layers.8.mlp.up_proj.tlut', 'model.layers.8.mlp.up_proj.tp_rank', 'model.layers.8.mlp.up_proj.trellis', 'model.layers.8.self_attn.k_proj.SU', 'model.layers.8.self_attn.k_proj.SV', 'model.layers.8.self_attn.k_proj.rcp', 'model.layers.8.self_attn.k_proj.tlut', 'model.layers.8.self_attn.k_proj.tp_rank', 'model.layers.8.self_attn.k_proj.trellis', 'model.layers.8.self_attn.o_proj.SU', 'model.layers.8.self_attn.o_proj.SV', 'model.layers.8.self_attn.o_proj.rcp', 'model.layers.8.self_attn.o_proj.tlut', 'model.layers.8.self_attn.o_proj.tp_rank', 'model.layers.8.self_attn.o_proj.trellis', 'model.layers.8.self_attn.q_proj.SU', 'model.layers.8.self_attn.q_proj.SV', 'model.layers.8.self_attn.q_proj.rcp', 'model.layers.8.self_attn.q_proj.tlut', 'model.layers.8.self_attn.q_proj.tp_rank', 'model.layers.8.self_attn.q_proj.trellis', 'model.layers.8.self_attn.v_proj.SU', 'model.layers.8.self_attn.v_proj.SV', 'model.layers.8.self_attn.v_proj.rcp', 'model.layers.8.self_attn.v_proj.tlut', 'model.layers.8.self_attn.v_proj.tp_rank', 'model.layers.8.self_attn.v_proj.trellis', 'model.layers.9.mlp.down_proj.SU', 'model.layers.9.mlp.down_proj.SV', 'model.layers.9.mlp.down_proj.rcp', 'model.layers.9.mlp.down_proj.tlut', 'model.layers.9.mlp.down_proj.tp_rank', 'model.layers.9.mlp.down_proj.trellis', 'model.layers.9.mlp.gate_proj.SU', 'model.layers.9.mlp.gate_proj.SV', 'model.layers.9.mlp.gate_proj.rcp', 'model.layers.9.mlp.gate_proj.tlut', 'model.layers.9.mlp.gate_proj.tp_rank', 'model.layers.9.mlp.gate_proj.trellis', 'model.layers.9.mlp.up_proj.SU', 'model.layers.9.mlp.up_proj.SV', 'model.layers.9.mlp.up_proj.rcp', 'model.layers.9.mlp.up_proj.tlut', 'model.layers.9.mlp.up_proj.tp_rank', 'model.layers.9.mlp.up_proj.trellis', 'model.layers.9.self_attn.k_proj.SU', 'model.layers.9.self_attn.k_proj.SV', 'model.layers.9.self_attn.k_proj.rcp', 'model.layers.9.self_attn.k_proj.tlut', 'model.layers.9.self_attn.k_proj.tp_rank', 'model.layers.9.self_attn.k_proj.trellis', 'model.layers.9.self_attn.o_proj.SU', 'model.layers.9.self_attn.o_proj.SV', 'model.layers.9.self_attn.o_proj.rcp', 'model.layers.9.self_attn.o_proj.tlut', 'model.layers.9.self_attn.o_proj.tp_rank', 'model.layers.9.self_attn.o_proj.trellis', 'model.layers.9.self_attn.q_proj.SU', 'model.layers.9.self_attn.q_proj.SV', 'model.layers.9.self_attn.q_proj.rcp', 'model.layers.9.self_attn.q_proj.tlut', 'model.layers.9.self_attn.q_proj.tp_rank', 'model.layers.9.self_attn.q_proj.trellis', 'model.layers.9.self_attn.v_proj.SU', 'model.layers.9.self_attn.v_proj.SV', 'model.layers.9.self_attn.v_proj.rcp', 'model.layers.9.self_attn.v_proj.tlut', 'model.layers.9.self_attn.v_proj.tp_rank', 'model.layers.9.self_attn.v_proj.trellis']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:00<00:00,  7.89it/s]Loading checkpoint shards:  33%|███▎      | 2/6 [00:00<00:00,  8.55it/s]Loading checkpoint shards:  50%|█████     | 3/6 [00:00<00:00,  8.28it/s]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:00<00:00,  8.60it/s]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:00<00:00,  8.81it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  9.01it/s]Loading checkpoint shards: 100%|██████████| 6/6 [00:00<00:00,  8.74it/s]
W0212 11:46:47.360376 69553 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  ln_data = torch.load(f'{args.quantized_path}/{ii}_layernorm.pt',

W0212 11:46:47.361594 69553 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_q.pt',

W0212 11:46:47.381505 69553 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:72: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_k.pt',

W0212 11:46:47.390353 69553 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_v.pt',

W0212 11:46:47.395253 69553 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:86: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_o.pt',

W0212 11:46:47.399957 69553 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:93: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_up.pt',

W0212 11:46:47.409457 69553 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:100: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_gate.pt',

W0212 11:46:47.419543 69553 warnings.py:110] /home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  saved_layer = torch.load(f'{args.quantized_path}/{ii}_down.pt',

I0212 11:46:47.430206 69553 hfize_llama.py:113] loaded layer 0
I0212 11:46:47.468762 69553 hfize_llama.py:113] loaded layer 1
I0212 11:46:47.507016 69553 hfize_llama.py:113] loaded layer 2
I0212 11:46:47.542795 69553 hfize_llama.py:113] loaded layer 3
I0212 11:46:47.591270 69553 hfize_llama.py:113] loaded layer 4
I0212 11:46:47.637075 69553 hfize_llama.py:113] loaded layer 5
I0212 11:46:47.688303 69553 hfize_llama.py:113] loaded layer 6
I0212 11:46:47.731419 69553 hfize_llama.py:113] loaded layer 7
I0212 11:46:47.771571 69553 hfize_llama.py:113] loaded layer 8
I0212 11:46:47.807318 69553 hfize_llama.py:113] loaded layer 9
I0212 11:46:47.849462 69553 hfize_llama.py:113] loaded layer 10
I0212 11:46:47.888163 69553 hfize_llama.py:113] loaded layer 11
I0212 11:46:47.931435 69553 hfize_llama.py:113] loaded layer 12
I0212 11:46:47.982592 69553 hfize_llama.py:113] loaded layer 13
I0212 11:46:48.018573 69553 hfize_llama.py:113] loaded layer 14
I0212 11:46:48.040823 69553 hfize_llama.py:113] loaded layer 15
I0212 11:46:48.070852 69553 hfize_llama.py:113] loaded layer 16
I0212 11:46:48.136865 69553 hfize_llama.py:113] loaded layer 17
I0212 11:46:48.167937 69553 hfize_llama.py:113] loaded layer 18
I0212 11:46:48.200109 69553 hfize_llama.py:113] loaded layer 19
I0212 11:46:48.220291 69553 hfize_llama.py:113] loaded layer 20
I0212 11:46:48.247621 69553 hfize_llama.py:113] loaded layer 21
I0212 11:46:48.311349 69553 hfize_llama.py:113] loaded layer 22
I0212 11:46:48.367549 69553 hfize_llama.py:113] loaded layer 23
I0212 11:46:48.400926 69553 hfize_llama.py:113] loaded layer 24
I0212 11:46:48.441802 69553 hfize_llama.py:113] loaded layer 25
I0212 11:46:48.466437 69553 hfize_llama.py:113] loaded layer 26
I0212 11:46:48.493991 69553 hfize_llama.py:113] loaded layer 27
I0212 11:46:48.548601 69553 hfize_llama.py:113] loaded layer 28
I0212 11:46:48.600592 69553 hfize_llama.py:113] loaded layer 29
I0212 11:46:48.693100 69553 hfize_llama.py:113] loaded layer 30
I0212 11:46:48.717794 69553 hfize_llama.py:113] loaded layer 31
I0212 11:46:48.717860 69553 hfize_llama.py:115] saving model...
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 129, in <module>
    main(args)
  File "/home/jgryu/Weight_compression/qtip/quantize_llama/hfize_llama.py", line 116, in main
    model.save_pretrained(args.hf_output_path, safe_serialization=True)
  File "/opt/conda/lib/python3.11/site-packages/transformers/modeling_utils.py", line 2830, in save_pretrained
    safe_save_file(shard, os.path.join(save_directory, shard_file), metadata={"format": "pt"})
  File "/opt/conda/lib/python3.11/site-packages/safetensors/torch.py", line 286, in save_file
    serialize_file(_flatten(tensors), filename, metadata=metadata)
KeyboardInterrupt
